Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Natural-language processing
b'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language\xc2\xa0data.'b'Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.'b''b''b'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.'b'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.'b'Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".'b'During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.'b"Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks."b'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.'b'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.'b'In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'b'Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.'b'Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).'b'Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.'b'Systems based on machine-learning algorithms have many advantages over hand-produced rules:'b'The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.'b'Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.'b''Statistical model
b'A statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.'b'The assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.'b'A statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Ad\xc3\xa8r quoting Kenneth Bollen).[1]'b'All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.'b''b''b'Suppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: heighti\xc2\xa0= b0\xc2\xa0+ b1agei\xc2\xa0+ \xce\xb5i, where b0 is the intercept, b1 is a parameter that age is multiplied by in obtaining a prediction of height, \xce\xb5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.'b'An admissible model must be consistent with all the data points. Thus, a straight line (heighti\xc2\xa0= b0\xc2\xa0+ b1agei) cannot be the equation for a model of the data. The line cannot be the equation for a model, unless it exactly fits all the data points\xe2\x80\x94i.e. all the data points lie perfectly on the line. The error term, \xce\xb5i, must be included in the equation, so that the model is consistent with all the data points.'b'To do statistical inference, we would first need to assume some probability distributions for the \xce\xb5i. For instance, we might assume that the \xce\xb5i distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.'b'A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, \xce\xb5 is a stochastic variable; without that variable, the model would be deterministic.'b'Statistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).'b'There are three purposes for a statistical model, according to Konishi\xc2\xa0& Kitagawa.[4]'b'As an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that'b'In this example, the dimension, k, equals 2.'b'As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)'b'Parametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, "These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies".[5]'b'Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model'b'has, nested within it, the linear model'b'\xe2\x80\x94we constrain the parameter b2 to equal 0.'b'In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\xc2\xa01). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.'b'Models can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.'b'Konishi & Kitagawa state: "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models."[6] Relatedly, Sir David Cox has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".[7]'Bioinformatics
b'Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]'b''b''b'Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]'b'Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]'b'Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]'b'To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:'b'The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\xe2\x80\x93protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.'b'Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.'b'Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.'b'Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.'b'Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.'b'Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.'b'Since the Phage \xce\xa6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.'b'Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.'b'Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.'b'In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.'b'The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.'b'Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).'b'Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:'b'Future work endeavours to reconstruct the now more complex tree of life.'b'The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.'b'The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.'b'Many of these studies are based on the homology detection and protein families computation.[22]'b'Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]'b"With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]"b'In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.'b'Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]'b'With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]'b'Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.'b'The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.'b'Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]'b'Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.'b'For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.'b'Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.'b'Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.'b'Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.'b'The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]'b'Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]'b'Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \xe2\x80\x93 a.k.a. Mad Cow Disease \xe2\x80\x93 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]'b"One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably."b'One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]'b'Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.'b'Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\xe2\x80\x93protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.'b'Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.'b'Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\xe2\x80\x93protein interactions only based on these 3D shapes, without performing protein\xe2\x80\x93protein interaction experiments. A variety of methods have been developed to tackle the protein\xe2\x80\x93protein docking problem, though it seems that there is still much work to be done in this field.'b'Other interactions encountered in the field include Protein\xe2\x80\x93ligand (including drug) and protein\xe2\x80\x93peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.'b'The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:'b'The area of research draws from statistics and computational linguistics.'b"Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:"b'Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.'b'Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.'b'Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.'b'The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.'b'Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.'b'Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.'b'Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.'b'Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.'b'The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]'b'An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]'b'SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.'b'Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.'b'A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to'b'Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.'b'In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.'b'It was decided that the BioCompute paradigm would be in the form of digital \xe2\x80\x98lab notebooks\xe2\x80\x99 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]'b'In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \xe2\x80\x9cstandard trial use\xe2\x80\x9d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]'b'Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\xcf\x80 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\xcf\x80 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\xcf\x80 operating system.[51][52]'b"MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications."b'There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).'b''Probabilistic latent semantic analysis
b'Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.'b'Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.'b''b''b'Their parameters are learned using the EM algorithm.'b'PLSA may be used in a discriminative setting, via Fisher kernels.[1]'b'PLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.'b'It is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.[2]'b'This is an example of a latent class model (see references therein), and it is related[5][6] to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.[7]'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'David Blei
b'David M. Blei is a Professor in the Statistics and Computer Science departments at Columbia University. Prior to fall 2014 he was an Associate Professor in the Department of Computer Science at Princeton University. His work is primarily in machine learning.'b''b''b'His research interests include topic models and he was one of the original developers of latent Dirichlet allocation. As of October 25, 2017, his publications have been cited 50,850 times, giving him an h-index of 64.[1]'b'He was named Fellow of ACM "For contributions to the theory and practice of probabilistic topic modeling and Bayesian machine learning" in 2015.[2]'b''Andrew Ng
b"Andrew Yan-Tak Ng (Chinese: \xe5\x90\xb3\xe6\x81\xa9\xe9\x81\x94; born 1976) is a Chinese American computer scientist. He is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor (formerly associate professor) at Stanford University. Ng is also the co-founder and chairman of Coursera, an online education platform.[2]"b''b''b"Ng was born in the UK in 1976. His parents were both from Hong Kong. He spent time in Hong Kong and Singapore[1] and later graduated from Raffles Institution in Singapore in 1992. In 1997, he received his undergraduate degree in computer science from Carnegie Mellon University in Pittsburgh, Pennsylvania. Ng earned his master's degree from Massachusetts Institute of Technology in Cambridge, Massachusetts in 1998 and received his PhD from University of California, Berkeley in 2002. He started working at Stanford University during that year and currently lives in Palo Alto, California. He married Carol E. Reiley in 2014.[3]"b"Andrew was a professor at Stanford University Department of Computer Science and Department of Electrical Engineering. He became Director of the Stanford Artificial Intelligence Lab where he taught students and undertook research related to data mining and machine learning. From 2011 to 2012, he worked at Google, where he founded and led the Google Brain Deep Learning Project. In 2012, he co-founded Coursera to offer free online courses for everyone after over 100,000 students registered for Ng's popular course.[4] Today, several million people have taken the online course. In 2014, he joined[5] Baidu as Chief Scientist, and carried out research related to big data and A.I. In March 2017, he announced his resignation from Baidu.[6]"b'He soon afterwards launched Deeplearning.ai,[7] an online curriculum of classes. Then Ng launchedLanding.ai[8], bringing AI to manufacturing factories, announcing a partnership with FoxConn.[9]'b'In 2018, Ng unveiled the AI Fund,[10] raising $175 million to invest in new startups. He is also the chairman of Woebot and on the board of drive.ai.[11][12]'b'Ng researches primarily in machine learning and deep learning. His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world,[13][14] and the STAIR (STanford Artificial Intelligence Robot) project,[15] which resulted in ROS, a widely used open-source robotics software platform.'b'In 2011, Ng founded the Google Brain project at Google, which developed very large scale artificial neural networks using Google\'s distributed computer infrastructure.[16] Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, that learned to recognize higher-level concepts, such as cats, after watching only YouTube videos, and without ever having been told what a "cat" is.[17][18] The project\'s technology is currently also used in the Android Operating System\'s speech recognition system.[19]'b'He together with David M. Blei and Michael I. Jordan, coauthored the influential paper that introduced Latent Dirichlet allocation.[20]'b'Ng started the Stanford Engineering Everywhere (SEE) program, which in 2008 placed a number of Stanford courses online, for free. Ng taught one of these courses, Machine Learning, which consisted of video lectures by him, along with the student materials used in the Stanford CS229 class.'b'The "applied" version of the Stanford class (CS229a) was hosted on ml-class.org and started in October 2011, with over 100,000 students registered for its first iteration; the course featured quizzes and graded programming assignments and became one of the first successful MOOCs made by Stanford professors.[22] His work subsequently led to the founding of Coursera in 2012.'b"Ng is also the author or co-author of over 100 published papers in machine learning, robotics, and related fields. His work in computer vision and deep learning has been frequently featured in press releases and reviews.[23] In 2008, he was named to the MIT Technology Review TR35 as one of the top 35 innovators in the world under the age of 35.[24][25] Ng was awarded a Sloan Fellowship (2007). For his work in artificial intelligence, he is also a recipient of the Computers and Thought Award (2009). In 2013 at the age of 37, he was named one of Times 100 Most Influential People[26] and Fortune's 40 under 40.[27]"Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Dirichlet distribution
b'The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process.'b''b''b'The Dirichlet distribution of order K\xc2\xa0\xe2\x89\xa5\xc2\xa02 with parameters \xce\xb11, ..., \xce\xb1K >\xc2\xa00 has a probability density function with respect to Lebesgue measure on the Euclidean space RK\xe2\x88\x921 given by'b'The normalizing constant is the multivariate Beta function, which can be expressed in terms of the gamma function:'b'When \xce\xb1=1[2], the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard (K\xc2\xa0\xe2\x88\x92\xc2\xa01)-simplex, i.e. it is uniform over all points in its support. This particular distribution is known as the flat Dirichlet distribution. Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.'b'Let'b'Then[3][4]'b'Note that the matrix so defined is singular.'b'More generally, moments of Dirichlet-distributed random variables can be expressed as[5]'b'The mode of the distribution is[6] the vector (x1, ..., xK) with'b'The marginal distributions are beta distributions:[7]'b"The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations). This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet. Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one. This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties."b'Formally, this can be expressed as follows. Given a model'b'then the following holds:'b'This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector \xce\xb1 as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution.'b'In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models. See the section on applications below for more information.'b'In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out) is a Dirichlet-multinomial distribution. This distribution plays an important role in hierarchical Bayesian models, because when doing inference over such models using methods such as Gibbs sampling or variational Bayes, Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.'b'and'b'If'b'then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum,'b'The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series. It is given by Phillips[11] as'b'For K independently distributed Gamma distributions:'b'we have:[13]:402'b'Although the Xis are not independent from one another, they can be seen to be generated from a set of K independent gamma random variable.[13]:594 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.'b'Because the Dirichlet distribution is an exponential family distribution it has a conjugate prior. The conjugate prior is of the form:[14]'b'Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models. (Note that in many fields, such as in natural language processing, categorical variables are often imprecisely called "multinomial variables". Such a usage is liable to cause confusion, just as if Bernoulli distributions and binomial distributions were commonly conflated.)'b'Inference over hierarchical Bayesian models is often done using Gibbs sampling, and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable. This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution, conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters). One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.'b'and then set'b'The Jacobian now looks like'b'The determinant can be evaluated by noting that it remains unchanged if multiples of a row are added to another row, and adding each of the first K-1 rows to the bottom row to obtain'b'Substituting for x in the joint pdf and including the Jacobian, one obtains:'b'Which is equivalent to'b'Below is example Python code to draw the sample:'b'This formulation is correct regardless of how the Gamma distributions are parameterized (shape/scale vs. shape/rate) because they are equivalent when scale and rate equal 1.0.'b'and let'b'Finally, set'b'This iterative procedure corresponds closely to the "string cutting" intuition described below.'b'Below is example Python code to draw the sample:'b'Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value \xce\xb1 to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.'b'One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. The \xce\xb1/\xce\xb10 values specify the mean lengths of the cut pieces of string resulting from the distribution. The variance around this mean varies inversely with \xce\xb10.'b'Consider an urn containing balls of K different colors. Initially, the urn contains \xce\xb11 balls of color 1, \xce\xb12 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir(\xce\xb11,...,\xce\xb1K).[16]'b'For a formal proof, note that the proportions of the different colored balls form a bounded [0,1]K-valued martingale, hence by the martingale convergence theorem, these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.'b'Note that each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls.'Pachinko allocation
b'In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents. [1] The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive power than latent Dirichlet allocation.[2] While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The model is named for pachinko machines\xe2\x80\x94a game popular in Japan, in which metal balls bounce down around a complex collection of pins until they land in various bins at the bottom.[3]'b''b''b"Pachinko allocation was first described by Wei Li and Andrew McCallum in 2006.[3] The idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007.[4] In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP).[2] The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst."b'\nPAM connects words in V and topics in T with an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the interior levels and the leaves are words.'b'The probability of generating a whole corpus is the product of the probability for every document:'b''File:Topic model scheme.webm
b'https://creativecommons.org/licenses/by-sa/4.0 CC BY-SA 4.0 Creative Commons Attribution-Share Alike 4.0 truetrue'b'Click on a date/time to view the file as it appeared at that time.\n'b'The following other wikis use this file:\n'Pennsylvania Gazette
b"The Pennsylvania Gazette was one of the United States' most prominent newspapers from 1728, before the time period of the American Revolution, until 1800."b''b''b"The newspaper was first published in 1728 by Samuel Keimer and was the second newspaper to be published in Pennsylvania under the name The Universal Instructor in all Arts and Sciences: and Pennsylvania Gazette, alluding to Keimer's intention to print out a page of Ephraim Chambers' Cyclopaedia, or Universal Dictionary of Arts and Sciences in each copy.[1] On October 2, 1729, Benjamin Franklin and Hugh Meredith bought the paper and shortened its name, as well as dropping Keimer's grandiose plan to print out the Cyclopaedia.[1] Franklin not only printed the paper but also often contributed pieces to the paper under aliases. His newspaper soon became the most successful in the colonies."b'On August 6, 1741 Franklin published an editorial about deceased Andrew Hamilton, a lawyer and public figure in Philadelphia who had been a friend. The editorial praised the man highly and showed Franklin had held the man in high esteem.[2]'b'In 1752, Franklin published a third-person account of his pioneering kite experiment in The Pennsylvania Gazette, without mentioning that he himself had performed it.[3]'b'Primarily a publication for classified ads, merchants and individuals listed notices of employment, lost and found goods and items for sale; the newspaper also reprinted foreign news. Most entries involved stories of travel.[4] In the July 31, 1776 edition the front page lists military movements announced by John Hancock alongside the sale of a plantation in Chester County, Pa., a three-dollar reward for a horse that strayed from home, three pounds reward for the return of a fleeing 23-year-old Irish servant named Jane Stepberd, three pounds reward for runaway Negroe man Moses Graves and the sale of an unnamed "hearty Scotch Servant GIRL".'b"This newspaper, among other firsts, would print the first political cartoon in America, Join, or Die, authored by Franklin himself.[5] It ceased publication in 1800, ten years after Franklin's death.[6]"b'It is claimed that the publication later reemerged as the Saturday Evening Post in 1821.[7]'b'There are three known copies of the original issue, which are held by the Historical Society of Pennsylvania, the Library Company of Philadelphia, and the Wisconsin State Historical Society.[1]'b'Today, The Pennsylvania Gazette moniker is used by an unrelated bi-monthly alumni magazine of the University of Pennsylvania, which Franklin founded and served as a trustee.'b'Archives are available online for a fee.[6]'b'President of Pennsylvania (1785\xe2\x80\x931788), Ambassador to France (1779\xe2\x80\x931785)'Proceedings of the National Academy of Sciences of the United States of America
b'Proceedings of the National Academy of Sciences of the United States of America (PNAS) is the official scientific journal of the National Academy of Sciences, published since 1915. With broad coverage, spanning the biological, physical, and social sciences, the journal publishes original research alongside scientific reviews, commentaries, and letters. In 1999\xe2\x80\x932009, the last period for which data are available, PNAS was the second most cited journal across all fields of science.[1] PNAS is published weekly in print, and daily online in PNAS Early Edition.'b''b''b'PNAS was established by the National Academy of Sciences (NAS) in 1914, with its first issue published in 1915. The NAS itself had been founded in 1863 as a private institution, but chartered by the United States Congress, with the goal to "investigate, examine, experiment, and report upon any subject of science or art". By 1914 the Academy had been well established.'b"Prior to the inception of PNAS, the National Academy of Sciences published three volumes of organizational transactions, consisting mostly of minutes of meetings and annual reports. In accordance with the guiding principles established by astronomer George Ellery Hale, the foreign secretary of NAS in 1914, PNAS publishes brief first announcements of Academy members' and foreign associates' more important contributions to research and of work that appears to a member to be of particular importance.[2]"b'The following people have been editors-in-chief of the journal:'b'The first managing editor of the journal was mathematician Edwin Bidwell Wilson.'b'All research papers published in PNAS are peer-reviewed.[2] The standard mode is for papers to be submitted directly to PNAS rather than going through an Academy member. Members may handle the peer review process for up to 4 of their own papers per year\xe2\x80\x94this is an open review process because the member selects and communicates directly with the referees. These submissions and reviews, like all for PNAS, are evaluated for publication by the PNAS Editorial Board. Until July 1, 2010, members were allowed to communicate up to 2 papers from non-members to PNAS every year. The review process for these papers was anonymous in that the identities of the referees were not revealed to the authors. Referees were selected by the NAS member.[2][4][5] PNAS eliminated communicated submissions through NAS members as of July 1, 2010, while continuing to make the final decision on all PNAS papers.[6]'b'In 2003, PNAS issued an editorial stating its policy on publication of sensitive material in the life sciences.[7] PNAS stated that it would "continue to monitor submitted papers for material that may be deemed inappropriate and that could, if published, compromise the public welfare." This statement was in keeping with the efforts of several other journals.[8][9] In 2005 PNAS published an article titled "Analyzing a bioterror attack on the food supply: The case of botulinum toxin in milk"[10] despite objections raised by the U.S. Department of Health and Human Services.[11] The paper was published with a commentary by the president of the Academy at the time, Bruce Alberts, titled "Modeling attacks on the food supply".[12]'b'PNAS is widely read by researchers, particularly those involved in basic sciences, around the world. PNAS Online receives over 21 million hits per month.[13] The journal is notable for its policy of making research articles freely available online to everyone six months after publication (delayed open access), or immediately if authors have chosen the "open access" option (hybrid open access). Immediately free online access (without the six-month delay) is available to more than 100 developing countries[14] and for some categories of papers such as colloquia. Abstracts, tables of contents, and online supporting information are free. Anyone can sign up to receive free tables of contents by email.[15]'b'Because PNAS is self-sustaining and receives no direct funding from the U.S.\xc2\xa0government or the National Academy of Sciences, the journal charges authors publication fees and subscription fees to offset the cost of the editorial and publication process.'b'According to the Journal Citation Reports, the journal has a 2015 impact factor of 9.423.[16] PNAS is the second most cited scientific journal, with nearly 1.4\xc2\xa0million citations from 1999 to 2009 (the Journal of Biological Chemistry is the most cited journal over this period).[17]'b'PNAS has received occasional criticism for releasing papers to science journalists as much as a week before making them available to the general public; this practice is known as a news embargo.[18] According to critics, this allows mainstream news outlets to misrepresent or exaggerate the implications of experimental findings before the scientific community is able to respond.[19][20] Science writer Ed Yong, on the other hand, has argued that the real problem is not embargoes themselves, but the press releases issued by research institutes and universities.[18]'b'In January 2011, PNAS started considering manuscripts for exclusive online publication, "PNAS Plus" papers.[21] These have a larger maximum page limit (10 rather than 6 pages). Accompanying these papers both online and in print was a one- to two-page summary description written by the authors for a broad readership. Since mid-October 2012, PNAS Plus authors no longer need to submit author summaries and are instead asked to submit a 120-word-maximum statement about the significance of their paper. The significance statement will appear both online and in print.[22] Since July 15, 2013, the significance statement is required for all research articles.'b"In 2006 PNAS launched a new section of the journal dedicated to sustainability science, an emerging field of research dealing with the interactions between natural and social systems, and with how those interactions affect the challenge of sustainability: meeting the needs of present and future generations while substantially reducing poverty and conserving the planet's life support systems. See the Sustainability Science portal here."Richmond Times-Dispatch
b'The Richmond Times-Dispatch (RTD or TD for short) is the primary daily newspaper in Richmond, the capital of Virginia, United States. It is also the primary newspaper of record for the state of Virginia.[2][3][4]'b''b''b"The Times-Dispatch has the second-highest circulation of any Virginia newspaper, after Norfolk's The Virginian-Pilot.[5] In addition to the Richmond area (Petersburg, Chester, Hopewell, Colonial Heights and surrounding areas), the Times-Dispatch has substantial readership in Charlottesville, Lynchburg, and Waynesboro. As the primary paper of the state's capital, the Times-Dispatch serves as a newspaper of record for rural regions of the state that lack large local papers."b'Although the Richmond Compiler published in Virginia\'s capitol beginning in 1815, and merged with a later newspaper called The Times, the Times and Compiler failed in 1853, despite an attempt of former banker James A. Cowardin and William H. Davis to revive it several years before. In 1850, Cowardin and Davis established a rival newspaper called the Richmond Dispatch, and by 1852 the Dispatch bragged of having circulation three times as large as any other daily paper in the city, and advertising dominated even its front page. Cowardin began his only term in the Virginia House of Delegates (as a Whig) in 1853, but many thought the city\'s pre-eminent paper the Richmond Examiner.[6] John Hammersley bought half of the newspaper company in 1859, and continued as a joint publisher on the masthead until May 5, 1862, when no name appeared. By April 1861, the newspaper announced its circulation was \xe2\x80\x9cwithin a fraction of 13,000.\xe2\x80\x9d[7] The newspaper had been staunchly pro-slavery since 1852, and called Union soldiers "thieves and cut-throats".[8] Most of its wartime issues are now available online.[9] In 1864, Hammersley brought new presses from England, having run the Union blockade, although he sold half his interest to James W. Lewellen before his dangerous departure (presumably through Wilmington, North Carolina, the last Southern port open to Confederate vessels in 1864).'b'The Richmond Daily Dispatch published its last wartime issue on April 1, 1865; and its office was destroyed the next night during the fire set by Confederate soldiers as they left the city. However, it resumed publication on December 9, 1865, establishing a new office at 12th and Main Streets and accepting Henry K. Ellyson as part-owner as well as editor.[10] By 1866, the Dispatch was one of five papers "carrying prestige from ante bellum days" published in Richmond (of 7 newspapers). Although the newspaper initially opposed the Ku Klux Klan, the Richmond Dispatch accepted Klan advertising in 1868, as it fought Congressional Reconstruction and the Virginia Constitutional Convention of 1868. However, it later accepted the resulting state constitution (after anti-Confederate provisions were stripped) as well as allowing Negroes on juries and in the legislature. Ellyson briefly served as Richmond\'s mayor in 1870, selected by Richmond\'s city council appointed by Governor Gilbert C. Walker. After what some called the "Municipal War" because the prior appointed mayor George Chahoon refused to relinquish his office and mob violence and blockades, the Virginia Supreme Court declared Ellyson the mayor but awaited elections. After skullduggery concerning stolen ballots in the pro-Chahoon Jackson Ward and the election commission declared Ellyson the winner, he refused to serve under the resulting cloud, leading to yet another problematic election won by the Conservative Party candidate. The revived Dispatch later opposed former Confederate General William Mahone and his Readjuster Party.[11] After James Cowardin died in 1882, his son Charles took the helm (with Ellyson\'s assistance, and with Ellyson family members handling business operations), and the paper stopped supporting Negro rights, instead criticizing Del. John Mercer Langston with racial stereotypes.[12]'b"In 1886, Lewis Ginter founded the Richmond Daily Times. A year later, lawyer Joseph Bryan (1845-1908) bought the Daily Times from Ginter, beginning the paper's long association with the Bryan family. Bryan and Ginter had previously helped revitalize the Tanner & Delany Engine Company, transforming it into the Richmond Locomotive Works, which had 800 employees by 1893 and built 200 locomotives per year. In 1890, the Daily Times changed its name to the Richmond Times. In 1896, Bryan acquired the eight-year-old rival Manchester Leader and launched the Evening Leader. In 1899, the evening Richmond News was founded. John L. Williams, owner of the Dispatch, bought the News in 1900."b'By 1903, it was obvious Richmond was not big enough to support four papers. That year, Williams and Bryan agreed to merge Richmond\'s main newspapers. The morning papers merged to become the Richmond Times-Dispatch under Bryan\'s ownership, while the evening papers merged to become The Richmond News Leader under Williams\' ownership. Bryan bought the News Leader in 1908, but died later that year. (Joseph Bryan Park was donated by his widow, Isobel ("Belle") Stewart Bryan, and named for him).'b"His son John Stewart Bryan had given up his own legal career in 1900 to become a reporter working for the Dispatch and helped found the Associated Press and then became vice-president of the publishing company.[13] Upon his father's death, John Stewart Bryan became owner and publisher of the two papers, but in 1914 sold a controlling interest in the Times-Dispatch to three families. He hired Douglas Southall Freeman as editor of the News Leader in 1915, and remained in control until becoming President of the College of William and Mary in 1934 (and publishing a biography of his father the following year). John Stewart Bryan but reacquired the Times-Dispatch in 1940 when the two papers' business interests merged to form Richmond Newspapers, in which Bryan held a 54-percent interest. That conglomeration is now known as Media General. Other publishers in the Bryan family include D. Tennant Bryan and John Stewart Bryan III."b'On June 1, 1992, four days after its sponsored contestant Amanda Goad won the Scripps National Spelling Bee, the News Leader, which had been losing circulation for many years, ceased publication and was folded into the Times-Dispatch.'b"The Richmond Times-Dispatch drew national attention for its coverage of a December 21, 2004, attack by a suicide bomber on an American military base in Mosul, Iraq. The deadliest attack on an American military installation since the war began, the attack injured 69 people and killed 22, including two with the Virginia National Guard's Richmond-based 276th Engineer Battalion. Stories and photographs about the attack by a Times-Dispatch reporter embedded with the 276th were read, heard and seen across the nation."b'In 1990, The RTD borrowed an idea [14] from a local entrepreneur, Barry "Mad Dog" Gottlieb, to encourage a "Tacky Christmas Lights Tour," also known by locals as the "Tacky Light Tour". Every week, the RTD lists the addresses of houses where the most tacky Christmas lights can be found. This tradition has begun to spread to other cities, like Fairfax, Virginia (DC area) [15] as well as San Francisco and Los Angeles.'b"Diane Cantor, the wife of former House Majority Leader Republican Eric Cantor, sits on Media General's Board of Directors.[16] This drew some conflict-of-interest allegations because the RTD serves much of the congressman's 7th district, but no evidence surfaced that she was involved in the paper's content. Her association with the paper was noted at the end of Times-Dispatch stories about Rep. Cantor."b"On May 17, 2012, Media General [17] announced the sale of its newspaper division to BH Media, a subsidiary of Warren Buffett's Berkshire Hathaway company. The sale included all of Media General's newspapers except The Tampa Tribune and its associated publications. Berkshire Hathaway bought 63 newspapers for $142 million and, as part of the deal, offered Media General a $400 million term loan at 10.5 percent interest that will mature in 2020 and a $45 million revolving line of credit. Berkshire Hathaway received a seat on Media General's board of directors and an option to purchase a 19.9% stake in the company.[18] The deal closed on June 25, 2012."b"This also brought to a close Diane Cantor's relationship with the RTD."b'A prominent newspaper in the state, the Times-Dispatch frequently features commentary from important figures from around Virginia, such as officials and presidents from Virginia Commonwealth University, the College of William and Mary, and the University of Virginia. Former Richmond Mayor Douglas Wilder, who had articles published in the paper before he held that position, often outlined policies his administration was implementing. During the 2004 U.S. presidential campaign, its Commentary sections featured some pieces by Retired Admiral Roy Hoffmann, a founding member of the Swift Boat Veterans for Truth and resident of Richmond suburb Chesterfield, against Democratic candidate John Kerry.'b"Editorially, the Times-Dispatch has historically leaned conservative, leading the paper to frequently endorse candidates of the Republican Party. It supported many of former President George W. Bush's policies, including the 2003 invasion of Iraq and a flat income tax. However, the paper is not unilaterally conservative; for example, a 2005 editorial called for the then House Majority Leader Tom DeLay to relinquish his leadership position on ethical grounds. There are also some liberal syndicated columnists who appear frequently, especially Leonard Pitts."b'During the Civil Rights Movement, the Times-Dispatch, like nearly every major newspaper in Virginia, was an ardent supporter of segregation.[19]'b"In the 2016 presidential election, the Times-Dispatch endorsed Libertarian candidate Gary Johnson over major party candidates Donald Trump and Hillary Clinton. Clinton's running mate, Tim Kaine, is a Richmond resident who served as mayor of the city from 1998-2001. From at least 1980 until its Johnson endorsement in 2016, the Times-Dispatch had only endorsed Republican presidential candidates.[20]"b'Like most major papers, the sports section has MLB, NASCAR, MLS, NBA, NCAA, NFL, and NHL scores and results. The Times-Dispatch sports pages naturally focus on Richmond and Virginia professional and college teams. In addition to Richmond Flying Squirrels and Richmond Kickers coverage, readers can see in-depth coverage of the Washington Redskins in the fall and the Washington Nationals in the summer. "Virginians in the Pros" and similar features track all sorts of professional athletes who were born, lived in, or attended college in Virginia. Large automobile racing events like the Sprint Cup (at the Richmond International Raceway) are often given a separate preview guide.'b'Catering to the vast array of Virginia hunters, fishers, hikers, and outdoorsmen, somewhere between half a page to a whole page most days is dedicated to outdoors articles, written by Lee Graves, who succeeded Garvey Winegar in November 2003. The "Scoreboard," which features minor-league standings, Vegas betting, and other sports scores, also gives tide measurements, river levels, and skiing conditions, depending on the season.'b'Virginians have traditionally been highly supportive of high school athletics, and its flagship paper is a testament to that. Particular emphasis is given to American football and basketball; The Times-Dispatch ranks area teams in these sports, in the style of the NCAA polls, and generally updates them weekly. In the fall, Sunday editions have the scores of all high school football games played that weekend from across the state. Prep games are also receive above-average coverage in baseball, cross country, golf, lacrosse, soccer, softball, swimming, tennis, track and field, and volleyball. Stories are frequently done on notable prep athletes, such as those from foreign countries, those with disabilities, those who play a multitude of sports, or those who had little or no prior experience in a sport which they now excel in.'b'The business desk consists of six reporters; they cover technology, retail, energy, insurance, banking, economics, real estate, manufacturing, transportation and consumer issues. Unlike many newspapers, the Times-Dispatch produces a widely read Monday business section, Metro Business. It contains a center cover story on a regional business-related issue and is filled with events for the coming week, advice columnists and gadget reviews. In June 2006, the decision was made to remove the stock tables from the daily sections beginning July 15 and replace the numerous pages with a "Markets Review" section for subscribers who request it. The stock section was eliminated in 2009, as was the Sunday Real Estate section (both were cost-cutting moves). The Sunday Business section, which had been a showcase of general business-interest stories and features, has been rechristened Moneywise and now features primarily consumer-related coverage. Moneywise is also among select Sunday business sections nationwide that print Wall Street Journal Sunday pages.'b'On July 12, 2006, Richmond-based news magazine Style Weekly ran a cover story [21] titled "Truth and Consequences," a piece that took a look at the Times-Dispatch\'s operations as the paper settled into its first year with new management. The report described new editor Glenn Proctor, who took over Nov. 14, 2005, as an "inelegant, blunt and harsh critic \xe2\x80\x94 to the point of saying, repeatedly, that some reporters\' work \'sucks.\'" The piece described a newsroom teetering on the edge, preparing for promised changes \xe2\x80\x94 such as possible layoffs, fewer pages and combined sections \xe2\x80\x94 that eventually were realized. On April 2, 2009, the Times-Dispatch cut 90 jobs, laying off 59 workers, including 28 newsroom jobs. Proctor left the paper in 2011.'b'The front page of the Times-Dispatch\xe2\x80\x99s August 14, 2011 Sunday paper consisted entirely of a Wells Fargo advertisement, commemorating said bank\xe2\x80\x99s acquisition of Wachovia properties in Virginia.[22]'b'Notable columnists published include:'American Civil War
b'Union victory'b'2,200,000:[a]'b'750,000\xe2\x80\x931,000,000:[a][4]'b'110,000+ killed in action/died of wounds\n230,000+ accident/disease deaths[6][7]\n25,000\xe2\x80\x9330,000 died in Confederate prisons[2][6]'b'365,000+ total dead[8] 282,000+ wounded[7]\n181,193 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][9]'b'94,000+ killed in action/died of wounds[6]\n26,000\xe2\x80\x9331,000 died in Union prisons[7]'b'290,000+ total dead\n137,000+ wounded\n436,658 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][10]'b"The American Civil War (known by other names) was a civil war that was fought in the United States from 1861 to 1865. As a result of the long-standing controversy over slavery, war broke out in April 1861, when Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated. The nationalists of the Union proclaimed loyalty to the U.S. Constitution. They faced secessionists of the Confederate States, who advocated for states' rights to expand slavery."b'Among the 34 U.S. states in February 1861, seven Southern slave states individually declared their secession from the U.S. to form the Confederate States of America, or the South. The Confederacy grew to include eleven slave states. The Confederacy was never diplomatically recognized by the United States government, nor was it recognized by any foreign country (although the United Kingdom and France granted it belligerent status). The states that remained loyal to the U.S. (including the border states where slavery was legal) were known as the Union or the North.'b"The Union and Confederacy quickly raised volunteer and conscription armies that fought mostly in the South over four years. The Union finally won the war when General Robert E. Lee surrendered to General Ulysses S. Grant at the Battle of Appomattox Court House, followed by a series of surrenders by Confederate generals throughout the southern states. Four years of intense combat left 620,000 to 750,000 people dead, more than the number of U.S. military deaths in all other wars combined (at least until approximately the Vietnam War).[15] Much of the South's infrastructure was destroyed, especially the transportation systems, railroads, mills, and houses. The Confederacy collapsed, slavery was abolished, and 4 million slaves were freed. The Reconstruction Era (1863\xe2\x80\x931877) overlapped and followed the war, with the process of restoring national unity, strengthening the national government, and granting civil rights to freed slaves throughout the country. The Civil War is the most studied and written about episode in U.S. history.[16]"b''b''b"In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories. The Southern states viewed this as a violation of their constitutional rights and as the first step in a grander Republican plan to eventually abolish slavery. The three pro-Union candidates together received an overwhelming 82% majority of the votes cast nationally: Republican Lincoln's votes centered in the north, Democrat Stephen A. Douglas' votes were distributed nationally and Constitutional Unionist John Bell's votes centered in Tennessee, Kentucky, and Virginia. The Republican Party, dominant in the North, secured a plurality of the popular votes and a majority of the electoral votes nationally, so Lincoln was constitutionally elected president. He was the first Republican Party candidate to win the presidency. However, before his inauguration, seven slave states with cotton-based economies declared secession and formed the Confederacy. The first six to declare secession had the highest proportions of slaves in their populations, a total of 49 percent.[17] The first seven with state legislatures to resolve for secession included split majorities for unionists Douglas and Bell in Georgia with 51% and Louisiana with 55%. Alabama had voted 46% for those unionists, Mississippi with 40%, Florida with 38%, Texas with 25%, and South Carolina cast Electoral College votes without a popular vote for president.[18] Of these, only Texas held a referendum on secession."b'Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln\'s March 4, 1861, inaugural address declared that his administration would not initiate a civil war. Speaking directly to the "Southern States", he attempted to calm their fears of any threats to slavery, reaffirming, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so."[19] After Confederate forces seized numerous federal forts within territory claimed by the Confederacy, efforts at compromise failed and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene, but none did, and none recognized the new Confederate States of America.'b"Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, the battle was inconclusive from 1861\xe2\x80\x931862. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal.[20] To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies, and seized New Orleans. The 1863 Union Siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William T. Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years, known as the Reconstruction Era."b'The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships and iron-clad ships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I, World War II and subsequent conflicts. It remains the deadliest war in American history. From 1861 to 1865, it is estimated that 620,000 to 750,000 soldiers died,[21] along with an undetermined number of civilians.[b] By one estimate, the war claimed the lives of 10 percent of all Northern males 20\xe2\x80\x9345 years old, and 30 percent of all Southern white males aged 18\xe2\x80\x9340.[23]'b'The causes of secession were complex and have been controversial since the war began, but most academic scholars\xc2\xa0identify\xc2\xa0slavery as a central cause of the war. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war.[24] Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to promote pro-slavery acts and policies.[25][26]'b'Slavery was a major cause of disunion.[27] Although there were opposing views even in the Union States,[28][29] most northern soldiers were largely indifferent on the subject of slavery,[30] while Confederates fought the war largely to protect a southern society of which slavery was an integral part.[31] From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with republicanism. The strategy of the anti-slavery forces was containment\xe2\x80\x94to stop the expansion and thus put slavery on a path to gradual extinction.[32] The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights.[33] Southern whites believed that the emancipation of slaves would destroy the South\'s economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population.[34] In particular, southerners feared a repeat of "the horrors of Santo Domingo", in which nearly all white people \xe2\x80\x93 including men, women, children, and even many sympathetic to abolition \xe2\x80\x93 were killed after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea, and proposes it contributed to the segregation in the Jim Crow era following emancipation.[35] These fears were exacerbated by the recent attempts of John Brown to instigate an armed slave rebellion in the South.'b'Slavery was illegal in much of the North, having been outlawed in the late 18th and early 19th centuries. It was also fading in the border states and in Southern cities, but it was expanding in the highly profitable cotton districts of the rural South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide.'b"Sectionalism refers to the different economies, social structure, customs and political values of the North and South.[36][37] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention which manifested Northern dissastisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern Presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for poor freedmen. In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations.[38]"b'Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[39][40]'b'Historically, southern slave-holding states, because of their low-cost manual labor, had little perceived need for mechanization and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.[41]'b'The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress.[42][43] The tariff issue was and is sometimes cited\xe2\x80\x93long after the war\xe2\x80\x93by Lost Cause historians and neo-Confederate apologists. In 1860\xe2\x80\x9361 none of the groups that proposed compromises to head off secession raised the tariff issue.[44] Pamphleteers North and South rarely mentioned the tariff,[45] and when some did, for instance, Matthew Fontaine Maury[46] and John Lothrop Motley,[47] they were generally writing for a foreign audience.'b'The South argued that each state had the right to secede\xe2\x80\x94leave the Union\xe2\x80\x94at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a perpetual union.[48] Historian James McPherson writes concerning states\' rights and other non-slavery explanations:'b"While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[49]"b'Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. It was over territories west of the Mississippi that the proslavery and antislavery forces collided.[50]'b'With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well.[51][52] Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the southern states the question of the territorial expansion of slavery westward again became explosive.[53] Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."[54][55]'b'By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[56] The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.[57]'b'The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance\xe2\x80\x94that slavery could be excluded in a territory as it was done in the Northwest Ordinance of 1787 at the discretion of Congress,[58] thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.[59]'b'Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty\xe2\x80\x94which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter.[60] The Kansas\xe2\x80\x93Nebraska Act of 1854 legislated this doctrine.[61] In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when southern senators began to leave.[62]'b'The fourth theory was advocated by Mississippi Senator Jefferson Davis,[63] one of state sovereignty ("states\' rights"),[64] also known as the "Calhoun doctrine",[65] named after the South Carolinian political theorist and statesman John C. Calhoun.[66] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[67] "States\' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[68] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power."[69][70] These four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the U.S. Constitution prior to the 1860 presidential election.[71]'b'Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in the sense that their country was a national republic based on the belief that all people had inalienable political liberty and personal rights which could serve as an important example to the rest of the world. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, the division and redivision of the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state.'b'In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure\xe2\x80\x94first in their political institutions, then in their civil life together.'b'Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy.[72] C. Vann Woodward said of the latter group,'b'A great slave society\xc2\xa0... had grown up and miraculously flourished in the heart of a thoroughly bourgeois and partly puritanical republic. It had renounced its bourgeois origins and elaborated and painfully rationalized its institutional, legal, metaphysical, and religious defenses\xc2\xa0... When the crisis came it chose to fight. It proved to be the death struggle of a society, which went down in ruins.[73]'b"Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin (1852)[74] and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.[75]"b'While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion\xc2\xa0... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence."[76] The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.[77]'b'The election of Abraham Lincoln in November 1860 was the final trigger for secession.[78] Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.'b'According to Lincoln, the people of the United States had shown that they can be successful in establishing and administering a republic, but a third challenge faced the nation, maintaining the republic, based on the people\'s vote. The people must now show: "successful maintenance [of the Republic] against a formidable internal attempt to overthrow it. It is now for them to demonstrate to the world that those who can fairly carry an election can also suppress a rebellion; that ballots are the rightful and peaceful successors of bullets; and that when ballots have fairly and constitutionally decided, there can be no successful appeal back to bullets; that there can be no successful appeal, except to ballots themselves, at succeeding elections. Such will be a great lesson of peace; teaching men that what they cannot take by an election, neither can they take it by a war".[79]'b'The election of Lincoln caused the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws and, even, secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860, and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states\' rights for slave owners in the South, but contained a complaint about states\' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.'b'Among the ordinances of secession passed by the individual states, those of three\xe2\x80\x94Texas, Alabama, and Virginia\xe2\x80\x94specifically mentioned the plight of the "slaveholding states" at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures.[80] However, at least four states\xe2\x80\x94South Carolina,[81] Mississippi,[82] Georgia,[83] and Texas[84]\xe2\x80\x94also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement\'s influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive slave clause of the Constitution.'b'These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861.[85] They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress".[86] One quarter of the U.S. Army\xe2\x80\x94the entire garrison in Texas\xe2\x80\x94was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.'b'As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts),[87] the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.'b"On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it.[88] It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would have voted in favor of it.[89] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[90]"b'On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[91] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[91]'b'The South sent delegations to Washington and offered to pay for the federal properties[which?] and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[92] Secretary of State William Seward, who at the time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed.[92] President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina\'s Fort Sumter.'b"Fort Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. fort's garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan, who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12\xe2\x80\x9313, forcing its capitulation."b'The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins said:'b"However, much of the North's attitude was based on the false belief that only a minority of Southerners were actually in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners really did favor secession, they might have hesitated at attempting the enormous task of conquering a united South.[95]"b'Lincoln called on all the states to send forces to recapture the fort and other federal properties. With the scale of the rebellion apparently small so far, Lincoln called for only 75,000 volunteers for 90\xc2\xa0days.[96] The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal.[97] On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.[98]'b'Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.[99]'b'Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. West Virginia then joined them as an additional border state after it separated from Virginia and became a state of the Union in 1863.'b"Maryland's territory surrounded the United States' capital of Washington, DC and could cut it off from the North.[100] It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53\xe2\x80\x9313) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war.[101] Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North.[102] Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened.[101][103] All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.[104]"b'In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. (See also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[105]'b'Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.[106]'b"After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving).[107] The inclusion of 24 secessionist counties[108] in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war.[109][110] Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000\xe2\x80\x9322,000 soldiers to both the Confederacy and the Union.[111]"b'A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[112]'b'The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy\'s soldier.[113]'b'As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[114] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.[115]'b'In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law\xe2\x80\x94conscription\xe2\x80\x94as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt.[116] The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.[117]'b"When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[118] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.[119]"b'In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[120] At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units.[121] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[122]'b'From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan\'s assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.[123]'b'Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:'b"Some historians emphasize that Civil War soldiers were driven by political ideology, holding firm beliefs about the importance of liberty, Union, or state rights, or about the need to protect or to destroy slavery. Others point to less overtly political reasons to fight, such as the defense of one's home and family, or the honor and brotherhood to be preserved when fighting alongside other men. Most historians agree that no matter what a soldier thought about when he went into the war, the experience of combat affected him profoundly and sometimes altered his reasons for continuing the fight.[124]"b"At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their own army where they were paid but not allowed to perform any military duties.[125] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[126]"b'The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396.[127][128] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[129] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.'b"By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible.[130] Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.[131]"b'In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[132]'b'The Civil War occurred during the early stages of the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union\'s naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[133] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union\'s own ironclad warships, they were unsuccessful.[134]'b"The Confederacy experimented with a submarine, which did not work well,[135] and with building an ironclad ship, the CSS Virginia, which was based on rebuilding a sunken Union ship, the Merrimack. On its first foray on March 8, 1862, the Virginia inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, the USS Monitor, arrived to challenge it in the Chesapeake Bay. The resulting three hour battle between the Ironclads was a draw, but it marked the worldwide transition to ironclad warships.[136] Not long after the battle the Confederacy was forced to scuttle the Virginia to prevent its capture, while the Union built many copies of the Monitor. Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Britain.[137]"b'British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were designed for speed and were so small that only a small amount of cotton went out.[138] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released.[139] The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[140]'b"Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[141] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.[142]"b'To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested.[134] After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.[143]'b'The 1862 Union strategy called for simultaneous advances along four axes:[144]'b'Ulysses Grant used river transport and Andrew Foote\'s gunboats of the Western Flotilla to threaten the Confederacy\'s "Gibraltar of the West" at Columbus, Kentucky. Though rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.[145]'b"In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[146] They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry (February 6, 1862) and Fort Donelson (February 11 to 16, 1862), and supplied Grant's forces as he moved into Tennessee. At Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory\xe2\x80\x94the first battle with the high casualty rates that would repeat over and over.[147] Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. On April 24, 1862, U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[148]"b'Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at Vicksburg, Mississippi in July 1863, and in the Union fully controlling the Mississippi River soon after.[149]'b'In one of the first highly visible battles, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces near Washington was repulsed.'b"Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign,[150][151][152] Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[153] The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[154] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops."b"Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[153][155] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[156]"b"When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[157] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker."b"Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863.[158] Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[159] Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[160] This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[161] However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant."b"While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[162] Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization."b'The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans,[163] which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.'b"General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state.[164] Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.[165]"b"The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged."b"The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh;[166] and the Battle of Vicksburg,[167] which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[168] driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy."b'Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[171] Roving Confederate bands such as Quantrill\'s Raiders terrorized the countryside, striking both military installations and civilian settlements.[172] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.'b'By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.[169]'b'Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union.[173] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[174]'b'After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him.[175] Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.'b'At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[176] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[177] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[178]'b"Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[179]"b"Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[180]"b"Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president.[181] Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.[182]"b'Leaving Atlanta, and his base of supplies, Sherman\'s army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman\'s army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee\'s army.[183]'b'Lee\'s army, thinned by desertion and casualties, was now much smaller than Grant\'s. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler\'s Creek.[184]'b"Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House.[185] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller."b"On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[186] On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William T. Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day.[1] On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department.[187] On June 23, Cherokee leader Stand Watie became the last Confederate General to surrender his forces.[188]"b"Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators.[189][190] The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.[191]"b'Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860\xe2\x80\x9362 crop failures in Europe made the North\'s grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half.[191] When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.[192]'b'Lincoln\'s foreign policy was deficient in 1861 in terms of appealing to European public opinion. Diplomats had to explain that United States was not committed to the ending of slavery, but instead they repeated legalistic arguments about the unconstitutionality of secession. Confederate spokesmen, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. In addition, the European aristocracy (the dominant factor in every major country) was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[193]'b'U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (CSS Alabama, CSS Shenandoah, CSS Tennessee, CSS Tallahassee, CSS Florida, and some others). The most famous, the CSS Alabama, did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for politicians in Britain, where the antislavery movement was powerful.[194]'b"War loomed in late 1861 between the U.S. and Britain over the Trent affair, involving the U.S. Navy's boarding of the British ship Trent and seizure of two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation between North and South\xe2\x80\x93 though even such an offer would have risked war with the U.S. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on this.[195]"b"The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.[196]"b'The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty.[197]'b"Historians have debated whether the Confederacy could have won the war. Most scholars, including James McPherson, argue that Confederate victory was at least possible.[198] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.[199]"b'Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[199] Lincoln was not a military dictator, and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.[203]'b'Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat.[204][205] Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back\xc2\xa0... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don\'t think the South ever had a chance to win that War."[206]'b'A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win."[207][208] Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederate army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict.[209] However, most historians reject the argument.[210] James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864\xe2\x80\x9365, he says most Confederate soldiers were fighting hard.[211] Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up\xe2\x80\x94some few deserters\xe2\x80\x94plenty tired of war, but the masses determined to fight it out."[212]'b"Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[213] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[214]"b'Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[215] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:'b'The North\'s victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[216]'b'Scholars have debated what the effects of the war were on political and economic power in the South.[217] The prevailing view is that the southern planter elite retained its powerful position in the South.[217] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[217]'b'The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths\xe2\x80\x94two-thirds by disease, and 50,000 civilians.[11] Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000.[21][218] The war accounted for more American deaths than in all other U.S. wars combined.[219]'b'Based on 1860 census figures, 8 percent of all white males aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[220][221] About 56,000 soldiers died in prison camps during the War.[222] An estimated 60,000 men lost limbs in the war.[223]'b'Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:[6]'b'In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[7]'b'Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle.[6] Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War.[224]:16 Notably, their mortality rate was significantly higher than white soldiers:'b'[We] find, according to the revised official data, that of the slightly over two millions troops in the United States Volunteers, over 316,000 died (from all causes), or 15.2 percent. Of the 67,000 Regular Army (white) troops, 8.6 percent, or not quite 6,000, died. Of the approximately 180,000 United States Colored Troops, however, over 36,000 died, or 20.5 percent. In other words, the mortality "rate" amongst the United States Colored Troops in the Civil War was thirty-five percent greater than that among other troops, notwithstanding the fact that the former were not enrolled until some eighteen months after the fighting began.[224]:16'b"Confederate records compiled by historian William F. Fox list 74,524 killed and died of wounds and 59,292 died of disease. Including Confederate estimates of battle losses where no records exist would bring the Confederate death toll to 94,000 killed and died of wounds. Fox complained, however, that records were incomplete, especially during the last year of the war, and that battlefield reports likely under-counted deaths (many men counted as wounded in battlefield reports subsequently died of their wounds). Thomas L. Livermore, using Fox's data, put the number of Confederate non-combat deaths at 166,000, using the official estimate of Union deaths from disease and accidents and a comparison of Union and Confederate enlistment records, for a total of 260,000 deaths.[6] However, this excludes the 30,000 deaths of Confederate troops in prisons, which would raise the minimum number of deaths to 290,000."b'The United States National Park Service uses the following figures in its official tally of war losses:[2]'b'Union: 853,838'b'Confederate: 914,660'b"While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, Superintendent of the 1870 Census, used census and Surgeon General data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 Census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.[218]"b'Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[22] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.'b"Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in an area where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor James Downs states that tens to hundreds of thousands of slaves died during the war from disease, starvation, exposure, or execution at the hands of the Confederates, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.[225]"b'Losses were far higher than during the recent defeat of Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Mini\xc3\xa9 balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.[226]'b"The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.[227]"b'The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century.[228] The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.'b'While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee\'s army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[229] Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal.[230] Lincoln\'s decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[231] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans\' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.[232]'b'The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army.[e] About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.[f]'b'During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[238] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.[239]'b'At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Fr\xc3\xa9mont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected.[240] But only the District of Columbia accepted Lincoln\'s gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat".[241] Lincoln laid the groundwork for public support in an open letter published in abolitionist Horace Greeley\'s newspaper.[242]'b'In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors\' Conference added support for the proclamation.[243] Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong\xc2\xa0... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling\xc2\xa0... I claim not to have controlled events, but confess plainly that events have controlled me."[244]'b"Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.[245]"b"Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[246] The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France.[247] By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.[248]"b'In Texas v. White, 74 U.S. 700 (1869) the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.[249]'b'Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877.[250] It comprised multiple complex methods to resolve the outstanding issues of the war\'s aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery\xe2\x80\x94and prevent semi-slavery status.[251]'b'President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson\'s work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended.[252] With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.'b'The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war\'s aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[253] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[254]'b'Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academe, leading to a proliferation of solid studies by non-scholars who are thoroughly familiar with the primary sources, pay close attention to battles and campaigns, and write for the large public readership, rather than the small scholarly community. Bruce Catton and Shelby Foote are among the best-known writers.[255][256] Practically every major figure in the war, both North and South, has had a serious biographical study.[257] Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:'b"God had chastised them and given them a special mission\xe2\x80\x94to maintain orthodoxy, strict biblicism, personal piety, and traditional race relations. Slavery, they insisted, had not been sinful. Rather, emancipation was a historical tragedy and the end of Reconstruction was a clear sign of God's favor.[258]"b'In sharp contrast, Black preachers interpreted the Civil War as:'b"God's gift of freedom. They appreciated opportunities to exercise their independence, to worship in their own way, to affirm their worth and dignity, and to proclaim the fatherhood of God and the brotherhood of man. Most of all, they could form their own churches, associations, and conventions. These institutions offered self-help and racial uplift, and provided places where the gospel of liberation could be proclaimed. As a result, black preachers continued to insist that God would protect and help him; God would be their rock in a stormy land.[259]"b'Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations.[260] Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[261] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing African-American progress to a white man\'s reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[262]'b"The interpretation of the Civil War presented by Charles A. Beard and Mary R. Beard in The Rise of American Civilization (1927) was highly influential among historians and the general public until the civil rights movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:"b'[A] social cataclysm in which the capitalists, laborers, and farmers of the North and West drove from power in the national government the planting aristocracy of the South.[263]'b'The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.[264]'b"The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861, but the oldest surviving monument is the Hazen monument, erected at Stones River near Murfreesboro, Tennessee, in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead in the Battle of Stones River. In the 1890s, the United States government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park in Tennessee and the Antietam National Battlefield in Maryland in 1890. The Shiloh National Military Park was established in 1894, followed by the Gettysburg National Military Park in 1895 and Vicksburg National Military Park in 1899. In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service.[265]"b'The modern Civil War battlefield preservation movement began in 1987 with the founding of the Association for the Preservation of Civil War Sites (APCWS), a grassroots organization created by Civil War historians and others to preserve battlefield land by acquiring it. In 1991, the original Civil War Trust was created in the mold of the Statue of Liberty/Ellis Island Foundation, but failed to attract corporate donors and soon helped manage the disbursement of U.S. Mint Civil War commemorative coin revenues designated for battlefield preservation. Although the two organizations joined forces on a number of battlefield acquisitions, ongoing conflicts prompted the boards of both organizations to facilitate a merger, which happened in 1999 with the creation of the Civil War Preservation Trust. In 2011, the organization was renamed The Civil War Trust. From 1987 through late 2017, The Trust and its predecessor organizations saved more than 40,000 acres at 126 Civil War battlefields and sites in 21 states.[266]'b"The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary. [267] Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as Birth of a Nation (1915), Gone with the Wind (1939), and more recently Lincoln (2012). Ken Burns produced a notable PBS series on television titled The Civil War (1990). It was digitally remastered and re-released in 2015."b'There were numerous technological innovations during the Civil War that had a great impact on 19th century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[268] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[269][270] It was also in this war when countries first used aerial warfare, in the form of reconnaissance balloons, to a significant effect.[271] It saw the first action involving steam-powered ironclad warships in naval warfare history.[272] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare, as well as the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[273]'b'General reference'b'Union'b'Confederacy'b'Ethnic articles'b'Topical articles'b'National articles'b'State articles'b'Memorials'b''Singular-value decomposition
b'The singular-value decomposition can be computed using the following observations:'b'Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.'b'Suppose M is a m \xc3\x97 n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the form'b'where'b'The diagonal entries \xcf\x83i of \xce\xa3 are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, \xce\xa3, is uniquely determined by M (though not the matrices U and V, see below).'b'In the special, yet common case when M is an m \xc3\x97 m real square matrix with positive determinant, U, V\xe2\x88\x97, and \xce\xa3 are real m \xc3\x97 m matrices as well, \xce\xa3 can be regarded as a scaling matrix, and U, V\xe2\x88\x97 can be viewed as rotation matrices. Thus the expression U\xce\xa3V\xe2\x88\x97 can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.'b"Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = V\xce\xa3V\xe2\x88\x97) with eigenvalue scale factors \xcf\x83i along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV\xe2\x88\x97). If the rotation is done first, M = P'R, then R is the same and P' = U\xce\xa3U\xe2\x88\x97 has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate."b'As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n \xc3\x97 n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m \xc3\x97 n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.'b'Since U and V\xe2\x88\x97 are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector \xcf\x83i Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U\xe2\x88\x97 and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U\xe2\x88\x97, V, and V\xe2\x88\x97 are orthonormal bases.'b'Consider the 4 \xc3\x97 5 matrix'b'A singular-value decomposition of this matrix is given by U\xce\xa3V\xe2\x88\x97'b'Notice \xce\xa3 is zero outside of the diagonal and one diagonal element is zero. Furthermore, because the matrices U and V\xe2\x88\x97 are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V\xe2\x88\x97 are real valued, each is an orthogonal matrix.'b'is also a valid singular-value decomposition.'b'In any singular-value decomposition'b'the diagonal entries of \xce\xa3 are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:'b'As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank\xe2\x80\x93nullity theorem cannot be the same dimension if m \xe2\x89\xa0 n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m \xe2\x88\x92 n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n \xe2\x88\x92 m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.'b'Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ei\xcf\x86 (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.'b'The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = U\xce\xa3V\xe2\x88\x97 is'b'where \xce\xa3+ is the pseudoinverse of \xce\xa3, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.'b"A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x\xe2\x88\x97A = 0, with x\xe2\x88\x97 denoting the conjugate transpose of x, is called a left null vector of A."b'A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.'b'Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.'b'As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \xce\xa3. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.'b'Here Ui and Vi are the i-th columns of the corresponding SVD matrices, \xcf\x83i are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero \xcf\x83i is exactly the rank of the matrix.'b"Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,"b'which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]'b'It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O \xe2\x88\x92 A. The solution is the product UV\xe2\x88\x97.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV\xe2\x88\x97 where I is the identity matrix, so that if A = U\xce\xa3V\xe2\x88\x97 then the product A = UV\xe2\x88\x97 amounts to replacing the singular values with ones.'b'A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,'b'This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.'b"The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules."b'The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]'b'The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.'b'The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the \xce\xa3 matrix is larger than one.'b'One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.'b'SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]'b"Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]"b'Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.'b'Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]'b'The singular-value decomposition is very general in the sense that it can be applied to any m \xc3\x97 n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.'b'Given an SVD of M, as described above, the following two relations hold:'b'The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:'b'In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU\xe2\x88\x97 for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU\xe2\x88\x97 is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each \xcf\x83i to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=U\xce\xa3U* is positive semidefinite and normal, and R=UV* is unitary.'b'An eigenvalue \xce\xbb of a matrix M is characterized by the algebraic relation Mu = \xce\xbbu. When M is Hermitian, a variational characterization is also available. Let M be a real n \xc3\x97 n symmetric matrix. Define'b'By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| \xe2\x89\xa4 1}. By the Lagrange multipliers theorem, u necessarily satisfies'b'where the nabla symbol, \xe2\x88\x87, is the del operator.'b'A short calculation shows the above leads to Mu = \xce\xbbu (symmetry of M is needed here). Therefore, \xce\xbb is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.'b'Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.'b'This section gives these two arguments for existence of singular-value decomposition.'b'Let M be an m \xc3\x97 n complex matrix. Since M\xe2\x88\x97M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n \xc3\x97 n matrix V such that'b'where D is diagonal and positive definite. Partition V appropriately so we can write'b'Therefore:'b'The second equation implies MV2 = 0. Also, since V is unitary:'b'where the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. Define'b'Then'b'We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, since'b'the columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:'b'For V1 we already have V2 to make it unitary. Now, define'b'which is the desired result:'b'Notice the argument could begin with diagonalizing MM\xe2\x88\x97 rather than M\xe2\x88\x97M (This shows directly that MM\xe2\x88\x97 and M\xe2\x88\x97M have the same non-zero eigenvalues).'b'The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.'b'Let M denote an m \xc3\x97 n matrix with real entries. Let Sm\xe2\x88\x921 and Sn\xe2\x88\x921 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the function'b'Consider the function \xcf\x83 restricted to Sm\xe2\x88\x921 \xc3\x97 Sn\xe2\x88\x921. Since both Sm\xe2\x88\x921 and Sn\xe2\x88\x921 are compact sets, their product is also compact. Furthermore, since \xcf\x83 is continuous, it attains a largest value for at least one pair of vectors u \xe2\x88\x88 Sm\xe2\x88\x921 and v \xe2\x88\x88 Sn\xe2\x88\x921. This largest value is denoted \xcf\x831 and the corresponding vectors are denoted u1 and v1. Since \xcf\x831 is the largest value of \xcf\x83(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.'b'Proof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:'b'After some algebra, this becomes'b'Plugging this into the pair of equations above, we have'b'This proves the statement.'b'More singular vectors and singular values can be found by maximizing \xcf\x83(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.'b'The passage from real to complex is similar to the eigenvalue case.'b'Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).'b'The linear transformation'b'has a particularly simple description with respect to these orthonormal bases: we have'b'where \xcf\x83i is the i-th diagonal entry of \xce\xa3, and T(Vi) = 0 for i > min(m,n).'b'The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T\xc2\xa0: Kn \xe2\x86\x92 Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.'b'To get a more visual flavour of singular values and SVD factorization \xe2\x80\x94 at least when working on real vector spaces \xe2\x80\x94 consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V\xe2\x88\x97 sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D \xe2\x88\x98 V\xe2\x88\x97 then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U \xe2\x88\x98 D \xe2\x88\x98 V\xe2\x88\x97 coincides with T.'b'The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m \xe2\x89\xa5 n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).'b'The first step can be done using Householder reflections for a cost of 4mn2 \xe2\x88\x92 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).'b'The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.'b'The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 \xc3\x97 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 \xc3\x97 2 eigenvalue methods (Golub & Van Loan 1996, \xc2\xa78.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).'b'There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or'b'The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M \xe2\x87\x92 Q R and LQ decomposition of R gives R \xe2\x87\x92 L P*. Thus, at every iteration, we have M \xe2\x87\x92 Q L P*, update M \xe2\x87\x90 L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.'b'In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m\xc3\x97n matrix M of rank r:'b"Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n\xc2\xa0\xe2\x89\xaa\xc2\xa0m. The matrix U'n is thus m\xc3\x97n, \xce\xa3n is n\xc3\x97n diagonal, and V is n\xc3\x97n."b'The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if\xc2\xa0n\xc2\xa0\xe2\x89\xaa\xc2\xa0m.'b'Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values \xce\xa3r are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r\xc2\xa0\xe2\x89\xaa\xc2\xa0n. The matrix Ur is thus m\xc3\x97r, \xce\xa3r is r\xc3\x97r diagonal, and Vr* is r\xc3\x97n.'b'Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values \xce\xa3t are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t\xe2\x89\xaar. The matrix Ut is thus m\xc3\x97t, \xce\xa3t is t\xc3\x97t diagonal, and Vt* is t\xc3\x97n.'b'The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]'b'The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spaces'b'But, in the matrix case, (M* M)\xc2\xbd is a normal matrix, so ||M* M||\xc2\xbd is the largest eigenvalue of (M* M)\xc2\xbd, i.e. the largest singular value of M.'b"The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)\xc2\xbd] (the eigenvalues of M* M are the squares of the singular values)."b'The singular values are related to another norm on the space of operators. Consider the Hilbert\xe2\x80\x93Schmidt inner product on the n \xc3\x97 n matrices, defined by'b'So the induced norm is'b'Since the trace is invariant under unitary equivalence, this shows'b'where \xcf\x83i are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert\xe2\x80\x93Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:'b'In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.'b'Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.'b'TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:'b'The factorization M = U\xce\xa3V\xe2\x88\x97 can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X,\xc2\xa0\xce\xbc), and a non-negative measurable f such that'b'This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such that'b'is an unitary operator.'b'As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write'b'and notice that U V* is still a partial isometry while VTf V* is positive.'b'The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero \xce\xbb in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M\xe2\x88\x97M. Applying the diagonalization result, the unitary image of its positive square root Tf\xc2\xa0 has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {\xcf\x83i}. For any \xcf\x88 \xe2\x88\x88 H,'b'where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. \xcf\x83i are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.'b'Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:'b'The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.'b'Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.'Method of moments (statistics)
b'In statistics, the method of moments is a method of estimation of population parameters. One starts with deriving equations that relate the population moments (i.e., the expected values of powers of the random variable under consideration) to the parameters of interest. Then a sample is drawn and the population moments are estimated from the sample. The equations are then solved for the parameters of interest, using the sample moments in place of the (unknown) population moments. This results in estimates of those parameters. The method of moments was introduced by Pafnuty Chebyshev in 1887.'b''b''b'The method of moments is fairly simple and yields consistent estimators (under very weak assumptions), though these estimators are often biased.'b"In some respects, when estimating parameters of a known family of probability distributions, this method was superseded by Fisher's method of maximum likelihood, because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased."b'However, in some cases the likelihood equations may be intractable without computers, whereas the method-of-moments estimators can be quickly and easily calculated by hand.'b'Estimates by the method of moments may be used as the first approximation to the solutions of the likelihood equations, and successive improved approximations may then be found by the Newton\xe2\x80\x93Raphson method. In this way the method of moments can assist in finding maximum likelihood estimates.'b'In some cases, infrequent with large samples but not so infrequent with small samples, the estimates given by the method of moments are outside of the parameter space; it does not make sense to rely on them then. That problem never arises in the method of maximum likelihood. Also, estimates by the method of moments are not necessarily sufficient statistics, i.e., they sometimes fail to take into account all relevant information in the sample.'b'When estimating other structural parameters (e.g., parameters of a utility function, instead of parameters of a known probability distribution), appropriate probability distributions may not be known, and moment-based estimates may be preferred to maximum likelihood estimation.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Explicit semantic analysis
b'In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf\xe2\x80\x93idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.[1]'b'ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization[2] and has been used by this pair of researchers to compute what they refer to as "semantic relatedness" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name "explicit semantic analysis" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.[1][3]'b''b''b'To perform the basic variant of ESA, one starts with a collection of texts, say, all Wikipedia articles; let the number of documents in the collection be N. These are all turned into "bags of words", i.e., term frequency histograms, stored in an inverted index. Using this inverted index, one can find for any word the set of Wikipedia articles containing this word; in the vocabulary of Egozi, Markovitch and Gabrilovitch, "each word appearing in the Wikipedia corpus can be seen as triggering each of the concepts it points to in the inverted index."[1]'b'The output of the inverted index for a single word query is a list of indexed documents (Wikipedia articles), each given a score depending on how often the word in question occurred in them (weighted by the total number of words in the document). Mathematically, this list is an N-dimensional vector of word-document scores, where a document not containing the query word has score zero. To compute the relatedness of two words, one compares the vectors (say u and v) by computing the cosine similarity,'b'and this gives numeric estimate of the semantic relatedness of the words. The scheme is extended from single words to multi-word texts by simply summing the vectors of all words in the text.[3]'b'ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically orthogonal concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of information retrieval systems when it is based not on Wikipedia, but on the Reuters corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".[4] To explain this observation, links have been shown between ESA and the generalized vector space model.[5] Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".[1]'b'Cross-language explicit semantic analysis (CL-ESA) is a multilingual generalization of ESA.[6] CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.'Latent semantic analysis
b'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]'b'An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).[2]'b''b''b'LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency\xe2\x80\x93inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.'b'This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.'b'After the construction of the occurrence matrix, LSA finds a low-rank approximation[4] to the term-document matrix. There could be various reasons for these approximations:'b'The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:'b'This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.'b'Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:'b'Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:'b'The matrix products giving us the term and document correlations then become'b'You can now do the following:'b'To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:'b'The new low-dimensional space typically can be used to:'b'Synonymy and polysemy are fundamental problems in natural language processing:'b'LSA has been used to assist in performing prior art searches for patents.[8]'b'The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words. These findings are referred to as the Semantic Proximity Effect.[9]'b'When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list. These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.[10]'b'Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.[11]'b"The SVD is typically computed using large matrix methods (for example, Lanczos methods) but may also be computed incrementally and with greatly reduced resources via a neural network-like approach, which does not require the large, full-rank matrix to be held in memory.[12] A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.[13] MATLAB and Python implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution. In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.[14]"b"Some of LSA's drawbacks include:"b'In semantic hashing [17] documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.'b'Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.[18]'b'LSI is also an application of correspondence analysis, a multivariate statistical technique developed by Jean-Paul Benz\xc3\xa9cri[19] in the early 1970s, to a contingency table built from word counts in documents.'b'Called "latent semantic indexing" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s. The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\xe2\x80\x99t share a specific word or words with the search criteria.'b'LSI overcomes two of the most problematic constraints of Boolean keyword queries: multiple words that have similar meanings (synonymy) and words that have more than one meaning (polysemy)[clarification needed]. Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems.[20] As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.'b'LSI is also used to perform automated document categorization. In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.[21] Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.[22] LSI uses example documents to establish the conceptual basis for each category. During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.'b'Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI. Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection of unstructured text.'b'Because it uses a strictly mathematical approach, LSI is inherently independent of language. This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri. LSI can also perform cross-linguistic concept searching and example-based categorization. For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.[citation needed]'b'LSI is not restricted to working only with words. It can also process arbitrary character strings. Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.[23]'b'LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).[24] This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion. LSI also deals effectively with sparse, ambiguous, and contradictory data.'b'Text does not need to be in sentence form for LSI to be effective. It can work with lists, free-form notes, email, Web-based content, etc. As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.'b'LSI has proven to be a useful solution to a number of conceptual matching problems.[25][26] The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.[27]'b'LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.'b'Some common local weighting functions[29] are defined in the following table.'b'Some common global weighting functions are defined in the following table.'b'In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A\xe2\x80\x94a measure of its unique dimensions \xe2\x89\xa4 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.'b'The SVD is then truncated to reduce the rank by keeping only the largest k \xc2\xab\xc2\xa0r diagonal entries in the singular value matrix S, where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A. This reduced set of matrices is often denoted with a modified formula such as:'b'Efficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.'b'Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons.'b'The computed Tk and Dk matrices define the term and document vector spaces, which with the computed singular values, Sk, embody the conceptual information derived from the document collection. The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.'b'The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index. By a simple transformation of the A = T S DT equation into the equivalent D = AT T S\xe2\x88\x921 equation, a new vector, d, for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S\xe2\x88\x921. The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.'b'A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored. These terms will have no impact on the global weights and learned correlations derived from the original collection of text. However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.'b'The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in. Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added. When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in [13]) be used.'b'It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems. As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.'b'LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.[32] Below are some other ways in which LSI is being used:'b'LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential. Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.[47]'b'Early challenges to LSI focused on scalability and performance. LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.[48] However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome. Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package.[49]'b'Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD. As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).[50] However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.[51]'b'Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain. The variance contained in the data can be viewed by plotting the singular values (S) in a scree plot. Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain. Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain. Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.[52][53][54]'b'Due to its cross-domain applications in Information Retrieval, Natural Language Processing (NLP), Cognitive Science and Computational Linguistics, LSA has been implemented to support many different kinds of applications.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Hierarchical Dirichlet process
b'In statistics and machine learning, the hierarchical Dirichlet process (HDP) is a nonparametric Bayesian approach to clustering grouped data.[1][2] It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. This method allows groups to share statistical strength via sharing of clusters across groups. The base distribution being drawn from a Dirichlet process is important, because draws from a Dirichlet process are atomic probability measures, and the atoms will appear in all group-level Dirichlet processes. Since each atom corresponds to a cluster, clusters are shared across all groups. It was developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei and published in the Journal of the American Statistical Association in 2006,[1] as a formalization and generalization of the infinite hidden Markov model published in 2002.[3]'b''b''b'Thus the set of atoms is shared across all groups, with each group having its own group-specific atom masses. Relating this representation back to the observed data, we see that each data item is described by a mixture model:'b'The HDP mixture model is a natural nonparametric generalization of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data.[1] Here each group is a document consisting of a bag of words, each cluster is a topic, and each document is a mixture of topics. The HDP is also a core component of the infinite hidden Markov model,[3] which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.[1] [4]'b'The HDP can be generalized in a number of directions. The Dirichlet processes can be replaced by Pitman-Yor processes, resulting in the Hierarchical Pitman-Yor process. The hierarchy can be deeper, with multiple levels of groups arranged in a hierarchy. Such an arrangement has been exploited in the sequence memoizer, a Bayesian nonparametric model for sequences which has a multi-level hierarchy of Pitman-Yor processes.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Mallet (software project)
b'MALLET is a Java "Machine Learning for Language Toolkit".'b''b''b'MALLET is an integrated collection of Java code useful for statistical natural language processing, document classification, cluster analysis, information extraction, topic modeling and other machine learning applications to text.'b'MALLET was developed primarily by Andrew McCallum, of the University of Massachusetts Amherst, with assistance from graduate students and faculty from both UMASS and the University of Pennsylvania.'b''Gensim
b'Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.'b''b''b'Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms,[1] hierarchical Dirichlet processes (HDP), latent semantic analysis (LSA, LSI, SVD) and latent Dirichlet allocation (LDA), including distributed parallel versions.[2]'b'Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation Scalability of Semantic Analysis in Natural Language Processing of Radim \xc5\x98eh\xc5\xaf\xc5\x99ek, the creator of Gensim.[3]'b'Gensim has been used and cited in over 800 commercial and academic applications, in a diverse array of disciplines from medicine to insurance claim analysis to patent search[4][5] The software has been covered in several new articles, podcasts and interviews since 2009.[6][7][8]'b'The open source code is developed and hosted on GitHub[9] and a public support forum is maintained on Google Groups[10] and Gitter.[11]'b'Gensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme.[12]'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Journal of Machine Learning Research
b'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Kevin Murphy (Google) and Bernhard Sch\xc3\xb6lkopf (Max Planck Institute for Intelligent Systems).'b''b''b'The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online.[2]'b'Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.[1]'b'In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007[3] and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems.'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'arXiv
b'arXiv (pronounced "archive")[2] is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million article milestone on October 3, 2008,[3][4] and hit a million by the end of 2014.[5][6] By October 2016 the submission rate had grown to more than 10,000 per month.[6][7]'b''b''b'The arXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side.[9] Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993.[6][10] The term e-print was quickly adopted to describe the articles.'b"It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org.[11] It is now hosted principally by Cornell, with eight mirrors around the world.[12]"b'Its existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access[13] and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.'b'The annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions.[14] This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Annual donations were envisaged to vary in size between $2,300 to $4,000, based on each institution\xe2\x80\x99s usage. As of 14\xc2\xa0January\xc2\xa02014[update], 174 institutions have pledged support for the period 2013\xe2\x80\x932017 on this basis, with a projected revenue from this source of approximately $340,000.[15]'b'In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv\'s operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence".[16] However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.'b'Although the arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic,[17] or reject submissions that are not scientific papers. The lists of moderators for many sections of the arXiv are publicly available,[18] but moderators for most of the physics sections remain unlisted.'b'Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines.[19] Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area.[17] New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.[20]'b'A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston\'s geometrization conjecture, including the Poincar\xc3\xa9 conjecture as a particular case, uploaded by Grigori Perelman in November 2002.[21] Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it\'s all there [on the arXiv]\xc2\xa0\xe2\x80\x93 let them go and read about it".[22] Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.[23]'b'While the arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat\'s last theorem using only high-school mathematics, they are "surprisingly rare".[24][better\xc2\xa0source\xc2\xa0needed] The arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them.[25]'b'Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.'b"The standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's front, a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front.[26] A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides TablearXiv,[27] a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv.[28] A full text and author search engine for arXiv is provided by Scientillion.[29] Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them."b'Files on arXiv can have a number of different copyright statuses:[30]'b'Some authors have voiced concern over the lack of transparency in the arXiv academic peer-review process.[31] Demetris Christopoulos from the National and Kapodistrian University of Athens likens arXiv to a non-declared Journal without a known editor in chief, without a specific written policy regarding submitted papers, and that applies hidden censorship to all papers that do not fall within established scientific dogma. [32]'International Standard Book Number
b'The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.'b'The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).'b'Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]'b'Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.'b''b''b'The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]'b'The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]'b'An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" \xe2\x80\x93 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN\xc2\xa00-340-01381-8; the check digit does not need to be re-calculated.'b'Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):'b'A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]'b'ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[15] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.'b'Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[16] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[17]'b'Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [18]'b'Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Biblioth\xc3\xa8que et Archives nationales du Qu\xc3\xa9bec.'b'Colombia: C\xc3\xa1mara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.'b'Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[19]'b'India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[20] There is no fee associated in getting ISBN in India.[21]'b'Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[22] The original national prefix 978-88 is reserved for publishing companies, starting at \xe2\x82\xac49 for a ten-codes block[23] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of \xe2\x82\xac25 for a single code.'b'Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]'b'Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[24][25][26]'b'Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.'b'New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[27]'b'Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.'b'Philippines: The National Library of the Philippines is responsible for ISBN registrations for Philippine publishers, authors, universities, institutions, and government departments that are responsible for publishing books. As of 2017[update], a fee of \xe2\x82\xb1120.00 per title was charged for the issuance of an ISBN.[28]'b'South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.'b'United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from \xc2\xa3120 (plus VAT) for the smallest block on a standard turnaround of ten days.[29]'b'United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[5] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125 for a single number. Access is immediate when requested via their website.[30]'b'Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.'b" The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[31] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0\xe2\x80\x935, 600\xe2\x80\x93621, 7, 80\xe2\x80\x9394, 950\xe2\x80\x93989, 9926\xe2\x80\x939989, and 99901\xe2\x80\x9399976.[32] Books published in rare languages typically have longer group identifiers.[33]"b'Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[34]'b'The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.'b'The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]'b'A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (\xe2\x82\xac1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[35] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.'b'Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.'b'By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[36] Here are some sample ISBN-10 codes, illustrating block length variations.'b'English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[37]'b'A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without \xe2\x80\x93 the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".'b'The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[38] \xe2\x80\x93 which is the last digit of the ten-digit ISBN \xe2\x80\x93 must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.'b'For example, for an ISBN-10 of 0-306-40615-2:'b'Formally, using modular arithmetic, we can say:'b"It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:"b'Formally, we can say:'b"The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN\xc2\xa0\xe2\x80\x93 the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[39]"b'In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).'b'Each of the first nine digits of the ten-digit ISBN\xe2\x80\x94excluding the check digit itself\xe2\x80\x94is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.'b'For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:'b'Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 \xe2\x80\x93 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)'b'For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:'b'Thus the check digit is 2.'b'It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:'b'The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.'b"The 2005 edition of the International ISBN Agency's official manual[40] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10."b'Formally, using modular arithmetic, we can say:'b'The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.'b'For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:'b'Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.'b'In general, the ISBN-13 check digit is calculated as follows.'b'Let'b'Then'b'This check system\xc2\xa0\xe2\x80\x93 similar to the UPC check digit formula\xc2\xa0\xe2\x80\x93 does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3\xc3\x976+1\xc3\x971 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3\xc3\x971+1\xc3\x976 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.'b'Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).'b'The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.'b'Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[41] For example, ISBN\xc2\xa00-590-76484-5 is shared by two books \xe2\x80\x93 Ninja gaiden\xc2\xae: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.'b'Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[42] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.'b'Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[43]'b'Currently the barcodes on a book\'s back cover (or inside a mass-market paperback book\'s front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[44] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).'b'Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[45] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.'b'Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[46]'b'Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.'Book sources
b'This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter. In Wikipedia, numbers preceded by "ISBN" link directly to this page.\n'b'This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). \n'b'Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).\n'b'Alabama\n'b'California\n'b'Colorado\n'b'Delaware\n'b'Florida\n'b'Georgia\n'b'Indiana\n'b'Iowa \n'b'Kansas\n'b'Kentucky\n'b'Massachusetts\n'b'Michigan\n'b'Minnesota\n'b'Missouri\n'b'Nebraska\n'b'New Jersey\n'b'New Mexico\n'b'New York\n'b'North Carolina\n'b'Ohio\n'b'Oklahoma\n'b'Oregon\n'b'Pennsylvania\n'b'Rhode Island\n'b'South Carolina\n'b'South Dakota\n'b'Tennessee\n'b'Texas\n'b'Utah\n'b'Washington state\n'b'Wisconsin\n'b'\n'b'Find your book on a site that compiles results from other online sites:\n'b'These sites allow you to search the catalogs of many individual booksellers:\n'b'\n'b'If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below \xe2\x80\x93 they are more likely to have sources appropriate for that language.\n'b'These links produce citations in various referencing styles.\n'b"You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.\n"b'You can also convert between 10 and 13 digit ISBN numbers with these tools:\n'b'\nEdit this page\n'b'\n'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'PubMed Central
b"PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. As one of the major research databases within the suite of resources that have been developed by the National Center for Biotechnology Information (NCBI), PubMed Central is much more than just a document repository. Submissions into PMC undergo an indexing and formatting procedure which results in enhanced metadata, medical ontology, and unique identifiers which all enrich the XML structured data for each article on deposit.[1] Content within PMC can easily be interlinked to many other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to freely discover, read and build upon this portfolio of biomedical knowledge.[2]"b'PubMed Central should not be confused with PubMed. These are two very different services at their core.[3] While PubMed is a searchable database of biomedical citations and abstracts, the full-text article referenced in the PubMed record will physically reside elsewhere. (Sometimes in print, sometimes online, sometimes free, sometimes behind a toll-wall accessible only to paying subscribers). PubMed Central is a free digital archive of articles, accessible to anyone from anywhere via a basic web browser. The full text of all PubMed Central articles is free to read, with varying provisions for reuse.'b'As of December\xc2\xa02016[update], the PMC archive contained over 4.1 million articles,[4] with contributions coming directly from publishers or authors depositing their own manuscripts into the repository per the NIH Public Access Policy. Older data shows that from Jan 2013 \xe2\x80\x93 Jan 2014 author-initiated deposits exceeded 103,000 papers during this 12-month period.[5] PMC also identifies about 4,000 journals which now participate in some capacity to automatically deposit their published content into the PMC repository.[6] Some participating publishers will delay the release of their articles on PubMed Central for a set time after publication, this is often referred to as an "embargo period", and can range from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common). However, PubMed Central is a key example of "systematic external distribution by a third party"[7] which is still prohibited by the contributor agreements of many publishers.'b''b''b'Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[8]'b'A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.'b'The National Library of Medicine "NLM Journal Publishing Tag Set" journal article markup language is freely available.[9] The Association of Learned and Professional Society Publishers comments that "it is likely to become the standard for preparing scholarly content for both books and journals".[10] A related DTD is available for books.[11] The Library of Congress and the British Library have announced support for the NLM DTD.[12] It has also been popular with journal service providers.[13]'b'With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[14] This includes NASA content, with the interface branded as "PubSpace".[15][16]'b'Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).'b'Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[17]'b'Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers\' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come "live" when the resources become available.'b'An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.'b'When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.'b'In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.'b'Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[18] to cautious concern by others.[19] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version-of-record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[20] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[21]'b'The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[22] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[23]'b'The change in procedure has received criticism.[24] The American Physiological Society has expressed reservations about the implementation of the policy.[25]'b'The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of "PMC" followed by a string of seven numbers. The format is:[26]'b'Authors applying for NIH awards must include the PMCID in their application.'PubMed
b'PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.'b'From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]'b'In addition to MEDLINE, PubMed provides access to:'b'Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]'b'Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]'b'As of 11\xc2\xa0July\xc2\xa02017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed\'s records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950s to 16% in 2016.[8] Other significant proportion of records correspond to \xe2\x80\x9cChemistry\xe2\x80\x9d (8.69%), \xe2\x80\x9cTherapy\xe2\x80\x9d (8.39%) and "Infection" (5%).'b'In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]'b"Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window."b"PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms."b'The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:'b'Likewise,'b"A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11] By default the results are sorted by Most Recent, but this changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.[12]"b'For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.[13][14]'b'When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).'b'Publication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.[15]'b'Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]'b'A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the \'Find related data\' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The \'related articles\' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.[18]'b'PubMed automatically links to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes).[19] This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.[19]'b'The PubMed optional facility "My NCBI" (with free registration) provides tools for'b'and a wide range of other options.[20] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[21]'b'LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[22] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March\xc2\xa02010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[23] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.'b'In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[24] In February 2018, PubMed Commons was discontinued due to the fact that "usage has remained minimal".[25][26]'b'PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[27] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[28]'b'askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[29]'b'A PMID (PubMed identifier or PubMed unique identifier)[30] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[31]'b'The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.'b'The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[32] As of October\xc2\xa02008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[33] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.'b'Lu[33] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:'b'As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[33] Without the need to store about 90\xc2\xa0GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[47]'b'Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950s to 16% in 2016.[48]'b'The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.[49]'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Natural-language processing
b'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language\xc2\xa0data.'b'Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.'b''b''b'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.'b'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.'b'Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".'b'During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.'b"Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks."b'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.'b'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.'b'In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'b'Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.'b'Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).'b'Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.'b'Systems based on machine-learning algorithms have many advantages over hand-produced rules:'b'The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.'b'Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.'b''Statistical model
b'A statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.'b'The assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.'b'A statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Ad\xc3\xa8r quoting Kenneth Bollen).[1]'b'All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.'b''b''b'Suppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: heighti\xc2\xa0= b0\xc2\xa0+ b1agei\xc2\xa0+ \xce\xb5i, where b0 is the intercept, b1 is a parameter that age is multiplied by in obtaining a prediction of height, \xce\xb5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.'b'An admissible model must be consistent with all the data points. Thus, a straight line (heighti\xc2\xa0= b0\xc2\xa0+ b1agei) cannot be the equation for a model of the data. The line cannot be the equation for a model, unless it exactly fits all the data points\xe2\x80\x94i.e. all the data points lie perfectly on the line. The error term, \xce\xb5i, must be included in the equation, so that the model is consistent with all the data points.'b'To do statistical inference, we would first need to assume some probability distributions for the \xce\xb5i. For instance, we might assume that the \xce\xb5i distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.'b'A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, \xce\xb5 is a stochastic variable; without that variable, the model would be deterministic.'b'Statistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).'b'There are three purposes for a statistical model, according to Konishi\xc2\xa0& Kitagawa.[4]'b'As an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that'b'In this example, the dimension, k, equals 2.'b'As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)'b'Parametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, "These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies".[5]'b'Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model'b'has, nested within it, the linear model'b'\xe2\x80\x94we constrain the parameter b2 to equal 0.'b'In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\xc2\xa01). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.'b'Models can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.'b'Konishi & Kitagawa state: "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models."[6] Relatedly, Sir David Cox has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".[7]'Bioinformatics
b'Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]'b''b''b'Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]'b'Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]'b'Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]'b'To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:'b'The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\xe2\x80\x93protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.'b'Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.'b'Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.'b'Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.'b'Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.'b'Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.'b'Since the Phage \xce\xa6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.'b'Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.'b'Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.'b'In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.'b'The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.'b'Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).'b'Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:'b'Future work endeavours to reconstruct the now more complex tree of life.'b'The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.'b'The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.'b'Many of these studies are based on the homology detection and protein families computation.[22]'b'Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]'b"With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]"b'In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.'b'Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]'b'With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]'b'Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.'b'The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.'b'Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]'b'Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.'b'For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.'b'Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.'b'Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.'b'Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.'b'The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]'b'Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]'b'Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \xe2\x80\x93 a.k.a. Mad Cow Disease \xe2\x80\x93 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]'b"One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably."b'One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]'b'Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.'b'Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\xe2\x80\x93protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.'b'Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.'b'Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\xe2\x80\x93protein interactions only based on these 3D shapes, without performing protein\xe2\x80\x93protein interaction experiments. A variety of methods have been developed to tackle the protein\xe2\x80\x93protein docking problem, though it seems that there is still much work to be done in this field.'b'Other interactions encountered in the field include Protein\xe2\x80\x93ligand (including drug) and protein\xe2\x80\x93peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.'b'The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:'b'The area of research draws from statistics and computational linguistics.'b"Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:"b'Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.'b'Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.'b'Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.'b'The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.'b'Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.'b'Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.'b'Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.'b'Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.'b'The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]'b'An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]'b'SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.'b'Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.'b'A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to'b'Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.'b'In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.'b'It was decided that the BioCompute paradigm would be in the form of digital \xe2\x80\x98lab notebooks\xe2\x80\x99 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]'b'In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \xe2\x80\x9cstandard trial use\xe2\x80\x9d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]'b'Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\xcf\x80 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\xcf\x80 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\xcf\x80 operating system.[51][52]'b"MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications."b'There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).'b''Probabilistic latent semantic analysis
b'Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.'b'Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.'b''b''b'Their parameters are learned using the EM algorithm.'b'PLSA may be used in a discriminative setting, via Fisher kernels.[1]'b'PLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.'b'It is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.[2]'b'This is an example of a latent class model (see references therein), and it is related[5][6] to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.[7]'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'David Blei
b'David M. Blei is a Professor in the Statistics and Computer Science departments at Columbia University. Prior to fall 2014 he was an Associate Professor in the Department of Computer Science at Princeton University. His work is primarily in machine learning.'b''b''b'His research interests include topic models and he was one of the original developers of latent Dirichlet allocation. As of October 25, 2017, his publications have been cited 50,850 times, giving him an h-index of 64.[1]'b'He was named Fellow of ACM "For contributions to the theory and practice of probabilistic topic modeling and Bayesian machine learning" in 2015.[2]'b''Andrew Ng
b"Andrew Yan-Tak Ng (Chinese: \xe5\x90\xb3\xe6\x81\xa9\xe9\x81\x94; born 1976) is a Chinese American computer scientist. He is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor (formerly associate professor) at Stanford University. Ng is also the co-founder and chairman of Coursera, an online education platform.[2]"b''b''b"Ng was born in the UK in 1976. His parents were both from Hong Kong. He spent time in Hong Kong and Singapore[1] and later graduated from Raffles Institution in Singapore in 1992. In 1997, he received his undergraduate degree in computer science from Carnegie Mellon University in Pittsburgh, Pennsylvania. Ng earned his master's degree from Massachusetts Institute of Technology in Cambridge, Massachusetts in 1998 and received his PhD from University of California, Berkeley in 2002. He started working at Stanford University during that year and currently lives in Palo Alto, California. He married Carol E. Reiley in 2014.[3]"b"Andrew was a professor at Stanford University Department of Computer Science and Department of Electrical Engineering. He became Director of the Stanford Artificial Intelligence Lab where he taught students and undertook research related to data mining and machine learning. From 2011 to 2012, he worked at Google, where he founded and led the Google Brain Deep Learning Project. In 2012, he co-founded Coursera to offer free online courses for everyone after over 100,000 students registered for Ng's popular course.[4] Today, several million people have taken the online course. In 2014, he joined[5] Baidu as Chief Scientist, and carried out research related to big data and A.I. In March 2017, he announced his resignation from Baidu.[6]"b'He soon afterwards launched Deeplearning.ai,[7] an online curriculum of classes. Then Ng launchedLanding.ai[8], bringing AI to manufacturing factories, announcing a partnership with FoxConn.[9]'b'In 2018, Ng unveiled the AI Fund,[10] raising $175 million to invest in new startups. He is also the chairman of Woebot and on the board of drive.ai.[11][12]'b'Ng researches primarily in machine learning and deep learning. His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world,[13][14] and the STAIR (STanford Artificial Intelligence Robot) project,[15] which resulted in ROS, a widely used open-source robotics software platform.'b'In 2011, Ng founded the Google Brain project at Google, which developed very large scale artificial neural networks using Google\'s distributed computer infrastructure.[16] Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, that learned to recognize higher-level concepts, such as cats, after watching only YouTube videos, and without ever having been told what a "cat" is.[17][18] The project\'s technology is currently also used in the Android Operating System\'s speech recognition system.[19]'b'He together with David M. Blei and Michael I. Jordan, coauthored the influential paper that introduced Latent Dirichlet allocation.[20]'b'Ng started the Stanford Engineering Everywhere (SEE) program, which in 2008 placed a number of Stanford courses online, for free. Ng taught one of these courses, Machine Learning, which consisted of video lectures by him, along with the student materials used in the Stanford CS229 class.'b'The "applied" version of the Stanford class (CS229a) was hosted on ml-class.org and started in October 2011, with over 100,000 students registered for its first iteration; the course featured quizzes and graded programming assignments and became one of the first successful MOOCs made by Stanford professors.[22] His work subsequently led to the founding of Coursera in 2012.'b"Ng is also the author or co-author of over 100 published papers in machine learning, robotics, and related fields. His work in computer vision and deep learning has been frequently featured in press releases and reviews.[23] In 2008, he was named to the MIT Technology Review TR35 as one of the top 35 innovators in the world under the age of 35.[24][25] Ng was awarded a Sloan Fellowship (2007). For his work in artificial intelligence, he is also a recipient of the Computers and Thought Award (2009). In 2013 at the age of 37, he was named one of Times 100 Most Influential People[26] and Fortune's 40 under 40.[27]"Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Dirichlet distribution
b'The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process.'b''b''b'The Dirichlet distribution of order K\xc2\xa0\xe2\x89\xa5\xc2\xa02 with parameters \xce\xb11, ..., \xce\xb1K >\xc2\xa00 has a probability density function with respect to Lebesgue measure on the Euclidean space RK\xe2\x88\x921 given by'b'The normalizing constant is the multivariate Beta function, which can be expressed in terms of the gamma function:'b'When \xce\xb1=1[2], the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard (K\xc2\xa0\xe2\x88\x92\xc2\xa01)-simplex, i.e. it is uniform over all points in its support. This particular distribution is known as the flat Dirichlet distribution. Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.'b'Let'b'Then[3][4]'b'Note that the matrix so defined is singular.'b'More generally, moments of Dirichlet-distributed random variables can be expressed as[5]'b'The mode of the distribution is[6] the vector (x1, ..., xK) with'b'The marginal distributions are beta distributions:[7]'b"The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations). This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet. Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one. This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties."b'Formally, this can be expressed as follows. Given a model'b'then the following holds:'b'This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector \xce\xb1 as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution.'b'In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models. See the section on applications below for more information.'b'In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out) is a Dirichlet-multinomial distribution. This distribution plays an important role in hierarchical Bayesian models, because when doing inference over such models using methods such as Gibbs sampling or variational Bayes, Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.'b'and'b'If'b'then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum,'b'The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series. It is given by Phillips[11] as'b'For K independently distributed Gamma distributions:'b'we have:[13]:402'b'Although the Xis are not independent from one another, they can be seen to be generated from a set of K independent gamma random variable.[13]:594 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.'b'Because the Dirichlet distribution is an exponential family distribution it has a conjugate prior. The conjugate prior is of the form:[14]'b'Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models. (Note that in many fields, such as in natural language processing, categorical variables are often imprecisely called "multinomial variables". Such a usage is liable to cause confusion, just as if Bernoulli distributions and binomial distributions were commonly conflated.)'b'Inference over hierarchical Bayesian models is often done using Gibbs sampling, and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable. This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution, conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters). One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.'b'and then set'b'The Jacobian now looks like'b'The determinant can be evaluated by noting that it remains unchanged if multiples of a row are added to another row, and adding each of the first K-1 rows to the bottom row to obtain'b'Substituting for x in the joint pdf and including the Jacobian, one obtains:'b'Which is equivalent to'b'Below is example Python code to draw the sample:'b'This formulation is correct regardless of how the Gamma distributions are parameterized (shape/scale vs. shape/rate) because they are equivalent when scale and rate equal 1.0.'b'and let'b'Finally, set'b'This iterative procedure corresponds closely to the "string cutting" intuition described below.'b'Below is example Python code to draw the sample:'b'Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value \xce\xb1 to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.'b'One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. The \xce\xb1/\xce\xb10 values specify the mean lengths of the cut pieces of string resulting from the distribution. The variance around this mean varies inversely with \xce\xb10.'b'Consider an urn containing balls of K different colors. Initially, the urn contains \xce\xb11 balls of color 1, \xce\xb12 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir(\xce\xb11,...,\xce\xb1K).[16]'b'For a formal proof, note that the proportions of the different colored balls form a bounded [0,1]K-valued martingale, hence by the martingale convergence theorem, these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.'b'Note that each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls.'Pachinko allocation
b'In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents. [1] The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive power than latent Dirichlet allocation.[2] While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The model is named for pachinko machines\xe2\x80\x94a game popular in Japan, in which metal balls bounce down around a complex collection of pins until they land in various bins at the bottom.[3]'b''b''b"Pachinko allocation was first described by Wei Li and Andrew McCallum in 2006.[3] The idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007.[4] In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP).[2] The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst."b'\nPAM connects words in V and topics in T with an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the interior levels and the leaves are words.'b'The probability of generating a whole corpus is the product of the probability for every document:'b''File:Topic model scheme.webm
b'https://creativecommons.org/licenses/by-sa/4.0 CC BY-SA 4.0 Creative Commons Attribution-Share Alike 4.0 truetrue'b'Click on a date/time to view the file as it appeared at that time.\n'b'The following other wikis use this file:\n'Pennsylvania Gazette
b"The Pennsylvania Gazette was one of the United States' most prominent newspapers from 1728, before the time period of the American Revolution, until 1800."b''b''b"The newspaper was first published in 1728 by Samuel Keimer and was the second newspaper to be published in Pennsylvania under the name The Universal Instructor in all Arts and Sciences: and Pennsylvania Gazette, alluding to Keimer's intention to print out a page of Ephraim Chambers' Cyclopaedia, or Universal Dictionary of Arts and Sciences in each copy.[1] On October 2, 1729, Benjamin Franklin and Hugh Meredith bought the paper and shortened its name, as well as dropping Keimer's grandiose plan to print out the Cyclopaedia.[1] Franklin not only printed the paper but also often contributed pieces to the paper under aliases. His newspaper soon became the most successful in the colonies."b'On August 6, 1741 Franklin published an editorial about deceased Andrew Hamilton, a lawyer and public figure in Philadelphia who had been a friend. The editorial praised the man highly and showed Franklin had held the man in high esteem.[2]'b'In 1752, Franklin published a third-person account of his pioneering kite experiment in The Pennsylvania Gazette, without mentioning that he himself had performed it.[3]'b'Primarily a publication for classified ads, merchants and individuals listed notices of employment, lost and found goods and items for sale; the newspaper also reprinted foreign news. Most entries involved stories of travel.[4] In the July 31, 1776 edition the front page lists military movements announced by John Hancock alongside the sale of a plantation in Chester County, Pa., a three-dollar reward for a horse that strayed from home, three pounds reward for the return of a fleeing 23-year-old Irish servant named Jane Stepberd, three pounds reward for runaway Negroe man Moses Graves and the sale of an unnamed "hearty Scotch Servant GIRL".'b"This newspaper, among other firsts, would print the first political cartoon in America, Join, or Die, authored by Franklin himself.[5] It ceased publication in 1800, ten years after Franklin's death.[6]"b'It is claimed that the publication later reemerged as the Saturday Evening Post in 1821.[7]'b'There are three known copies of the original issue, which are held by the Historical Society of Pennsylvania, the Library Company of Philadelphia, and the Wisconsin State Historical Society.[1]'b'Today, The Pennsylvania Gazette moniker is used by an unrelated bi-monthly alumni magazine of the University of Pennsylvania, which Franklin founded and served as a trustee.'b'Archives are available online for a fee.[6]'b'President of Pennsylvania (1785\xe2\x80\x931788), Ambassador to France (1779\xe2\x80\x931785)'Proceedings of the National Academy of Sciences of the United States of America
b'Proceedings of the National Academy of Sciences of the United States of America (PNAS) is the official scientific journal of the National Academy of Sciences, published since 1915. With broad coverage, spanning the biological, physical, and social sciences, the journal publishes original research alongside scientific reviews, commentaries, and letters. In 1999\xe2\x80\x932009, the last period for which data are available, PNAS was the second most cited journal across all fields of science.[1] PNAS is published weekly in print, and daily online in PNAS Early Edition.'b''b''b'PNAS was established by the National Academy of Sciences (NAS) in 1914, with its first issue published in 1915. The NAS itself had been founded in 1863 as a private institution, but chartered by the United States Congress, with the goal to "investigate, examine, experiment, and report upon any subject of science or art". By 1914 the Academy had been well established.'b"Prior to the inception of PNAS, the National Academy of Sciences published three volumes of organizational transactions, consisting mostly of minutes of meetings and annual reports. In accordance with the guiding principles established by astronomer George Ellery Hale, the foreign secretary of NAS in 1914, PNAS publishes brief first announcements of Academy members' and foreign associates' more important contributions to research and of work that appears to a member to be of particular importance.[2]"b'The following people have been editors-in-chief of the journal:'b'The first managing editor of the journal was mathematician Edwin Bidwell Wilson.'b'All research papers published in PNAS are peer-reviewed.[2] The standard mode is for papers to be submitted directly to PNAS rather than going through an Academy member. Members may handle the peer review process for up to 4 of their own papers per year\xe2\x80\x94this is an open review process because the member selects and communicates directly with the referees. These submissions and reviews, like all for PNAS, are evaluated for publication by the PNAS Editorial Board. Until July 1, 2010, members were allowed to communicate up to 2 papers from non-members to PNAS every year. The review process for these papers was anonymous in that the identities of the referees were not revealed to the authors. Referees were selected by the NAS member.[2][4][5] PNAS eliminated communicated submissions through NAS members as of July 1, 2010, while continuing to make the final decision on all PNAS papers.[6]'b'In 2003, PNAS issued an editorial stating its policy on publication of sensitive material in the life sciences.[7] PNAS stated that it would "continue to monitor submitted papers for material that may be deemed inappropriate and that could, if published, compromise the public welfare." This statement was in keeping with the efforts of several other journals.[8][9] In 2005 PNAS published an article titled "Analyzing a bioterror attack on the food supply: The case of botulinum toxin in milk"[10] despite objections raised by the U.S. Department of Health and Human Services.[11] The paper was published with a commentary by the president of the Academy at the time, Bruce Alberts, titled "Modeling attacks on the food supply".[12]'b'PNAS is widely read by researchers, particularly those involved in basic sciences, around the world. PNAS Online receives over 21 million hits per month.[13] The journal is notable for its policy of making research articles freely available online to everyone six months after publication (delayed open access), or immediately if authors have chosen the "open access" option (hybrid open access). Immediately free online access (without the six-month delay) is available to more than 100 developing countries[14] and for some categories of papers such as colloquia. Abstracts, tables of contents, and online supporting information are free. Anyone can sign up to receive free tables of contents by email.[15]'b'Because PNAS is self-sustaining and receives no direct funding from the U.S.\xc2\xa0government or the National Academy of Sciences, the journal charges authors publication fees and subscription fees to offset the cost of the editorial and publication process.'b'According to the Journal Citation Reports, the journal has a 2015 impact factor of 9.423.[16] PNAS is the second most cited scientific journal, with nearly 1.4\xc2\xa0million citations from 1999 to 2009 (the Journal of Biological Chemistry is the most cited journal over this period).[17]'b'PNAS has received occasional criticism for releasing papers to science journalists as much as a week before making them available to the general public; this practice is known as a news embargo.[18] According to critics, this allows mainstream news outlets to misrepresent or exaggerate the implications of experimental findings before the scientific community is able to respond.[19][20] Science writer Ed Yong, on the other hand, has argued that the real problem is not embargoes themselves, but the press releases issued by research institutes and universities.[18]'b'In January 2011, PNAS started considering manuscripts for exclusive online publication, "PNAS Plus" papers.[21] These have a larger maximum page limit (10 rather than 6 pages). Accompanying these papers both online and in print was a one- to two-page summary description written by the authors for a broad readership. Since mid-October 2012, PNAS Plus authors no longer need to submit author summaries and are instead asked to submit a 120-word-maximum statement about the significance of their paper. The significance statement will appear both online and in print.[22] Since July 15, 2013, the significance statement is required for all research articles.'b"In 2006 PNAS launched a new section of the journal dedicated to sustainability science, an emerging field of research dealing with the interactions between natural and social systems, and with how those interactions affect the challenge of sustainability: meeting the needs of present and future generations while substantially reducing poverty and conserving the planet's life support systems. See the Sustainability Science portal here."Richmond Times-Dispatch
b'The Richmond Times-Dispatch (RTD or TD for short) is the primary daily newspaper in Richmond, the capital of Virginia, United States. It is also the primary newspaper of record for the state of Virginia.[2][3][4]'b''b''b"The Times-Dispatch has the second-highest circulation of any Virginia newspaper, after Norfolk's The Virginian-Pilot.[5] In addition to the Richmond area (Petersburg, Chester, Hopewell, Colonial Heights and surrounding areas), the Times-Dispatch has substantial readership in Charlottesville, Lynchburg, and Waynesboro. As the primary paper of the state's capital, the Times-Dispatch serves as a newspaper of record for rural regions of the state that lack large local papers."b'Although the Richmond Compiler published in Virginia\'s capitol beginning in 1815, and merged with a later newspaper called The Times, the Times and Compiler failed in 1853, despite an attempt of former banker James A. Cowardin and William H. Davis to revive it several years before. In 1850, Cowardin and Davis established a rival newspaper called the Richmond Dispatch, and by 1852 the Dispatch bragged of having circulation three times as large as any other daily paper in the city, and advertising dominated even its front page. Cowardin began his only term in the Virginia House of Delegates (as a Whig) in 1853, but many thought the city\'s pre-eminent paper the Richmond Examiner.[6] John Hammersley bought half of the newspaper company in 1859, and continued as a joint publisher on the masthead until May 5, 1862, when no name appeared. By April 1861, the newspaper announced its circulation was \xe2\x80\x9cwithin a fraction of 13,000.\xe2\x80\x9d[7] The newspaper had been staunchly pro-slavery since 1852, and called Union soldiers "thieves and cut-throats".[8] Most of its wartime issues are now available online.[9] In 1864, Hammersley brought new presses from England, having run the Union blockade, although he sold half his interest to James W. Lewellen before his dangerous departure (presumably through Wilmington, North Carolina, the last Southern port open to Confederate vessels in 1864).'b'The Richmond Daily Dispatch published its last wartime issue on April 1, 1865; and its office was destroyed the next night during the fire set by Confederate soldiers as they left the city. However, it resumed publication on December 9, 1865, establishing a new office at 12th and Main Streets and accepting Henry K. Ellyson as part-owner as well as editor.[10] By 1866, the Dispatch was one of five papers "carrying prestige from ante bellum days" published in Richmond (of 7 newspapers). Although the newspaper initially opposed the Ku Klux Klan, the Richmond Dispatch accepted Klan advertising in 1868, as it fought Congressional Reconstruction and the Virginia Constitutional Convention of 1868. However, it later accepted the resulting state constitution (after anti-Confederate provisions were stripped) as well as allowing Negroes on juries and in the legislature. Ellyson briefly served as Richmond\'s mayor in 1870, selected by Richmond\'s city council appointed by Governor Gilbert C. Walker. After what some called the "Municipal War" because the prior appointed mayor George Chahoon refused to relinquish his office and mob violence and blockades, the Virginia Supreme Court declared Ellyson the mayor but awaited elections. After skullduggery concerning stolen ballots in the pro-Chahoon Jackson Ward and the election commission declared Ellyson the winner, he refused to serve under the resulting cloud, leading to yet another problematic election won by the Conservative Party candidate. The revived Dispatch later opposed former Confederate General William Mahone and his Readjuster Party.[11] After James Cowardin died in 1882, his son Charles took the helm (with Ellyson\'s assistance, and with Ellyson family members handling business operations), and the paper stopped supporting Negro rights, instead criticizing Del. John Mercer Langston with racial stereotypes.[12]'b"In 1886, Lewis Ginter founded the Richmond Daily Times. A year later, lawyer Joseph Bryan (1845-1908) bought the Daily Times from Ginter, beginning the paper's long association with the Bryan family. Bryan and Ginter had previously helped revitalize the Tanner & Delany Engine Company, transforming it into the Richmond Locomotive Works, which had 800 employees by 1893 and built 200 locomotives per year. In 1890, the Daily Times changed its name to the Richmond Times. In 1896, Bryan acquired the eight-year-old rival Manchester Leader and launched the Evening Leader. In 1899, the evening Richmond News was founded. John L. Williams, owner of the Dispatch, bought the News in 1900."b'By 1903, it was obvious Richmond was not big enough to support four papers. That year, Williams and Bryan agreed to merge Richmond\'s main newspapers. The morning papers merged to become the Richmond Times-Dispatch under Bryan\'s ownership, while the evening papers merged to become The Richmond News Leader under Williams\' ownership. Bryan bought the News Leader in 1908, but died later that year. (Joseph Bryan Park was donated by his widow, Isobel ("Belle") Stewart Bryan, and named for him).'b"His son John Stewart Bryan had given up his own legal career in 1900 to become a reporter working for the Dispatch and helped found the Associated Press and then became vice-president of the publishing company.[13] Upon his father's death, John Stewart Bryan became owner and publisher of the two papers, but in 1914 sold a controlling interest in the Times-Dispatch to three families. He hired Douglas Southall Freeman as editor of the News Leader in 1915, and remained in control until becoming President of the College of William and Mary in 1934 (and publishing a biography of his father the following year). John Stewart Bryan but reacquired the Times-Dispatch in 1940 when the two papers' business interests merged to form Richmond Newspapers, in which Bryan held a 54-percent interest. That conglomeration is now known as Media General. Other publishers in the Bryan family include D. Tennant Bryan and John Stewart Bryan III."b'On June 1, 1992, four days after its sponsored contestant Amanda Goad won the Scripps National Spelling Bee, the News Leader, which had been losing circulation for many years, ceased publication and was folded into the Times-Dispatch.'b"The Richmond Times-Dispatch drew national attention for its coverage of a December 21, 2004, attack by a suicide bomber on an American military base in Mosul, Iraq. The deadliest attack on an American military installation since the war began, the attack injured 69 people and killed 22, including two with the Virginia National Guard's Richmond-based 276th Engineer Battalion. Stories and photographs about the attack by a Times-Dispatch reporter embedded with the 276th were read, heard and seen across the nation."b'In 1990, The RTD borrowed an idea [14] from a local entrepreneur, Barry "Mad Dog" Gottlieb, to encourage a "Tacky Christmas Lights Tour," also known by locals as the "Tacky Light Tour". Every week, the RTD lists the addresses of houses where the most tacky Christmas lights can be found. This tradition has begun to spread to other cities, like Fairfax, Virginia (DC area) [15] as well as San Francisco and Los Angeles.'b"Diane Cantor, the wife of former House Majority Leader Republican Eric Cantor, sits on Media General's Board of Directors.[16] This drew some conflict-of-interest allegations because the RTD serves much of the congressman's 7th district, but no evidence surfaced that she was involved in the paper's content. Her association with the paper was noted at the end of Times-Dispatch stories about Rep. Cantor."b"On May 17, 2012, Media General [17] announced the sale of its newspaper division to BH Media, a subsidiary of Warren Buffett's Berkshire Hathaway company. The sale included all of Media General's newspapers except The Tampa Tribune and its associated publications. Berkshire Hathaway bought 63 newspapers for $142 million and, as part of the deal, offered Media General a $400 million term loan at 10.5 percent interest that will mature in 2020 and a $45 million revolving line of credit. Berkshire Hathaway received a seat on Media General's board of directors and an option to purchase a 19.9% stake in the company.[18] The deal closed on June 25, 2012."b"This also brought to a close Diane Cantor's relationship with the RTD."b'A prominent newspaper in the state, the Times-Dispatch frequently features commentary from important figures from around Virginia, such as officials and presidents from Virginia Commonwealth University, the College of William and Mary, and the University of Virginia. Former Richmond Mayor Douglas Wilder, who had articles published in the paper before he held that position, often outlined policies his administration was implementing. During the 2004 U.S. presidential campaign, its Commentary sections featured some pieces by Retired Admiral Roy Hoffmann, a founding member of the Swift Boat Veterans for Truth and resident of Richmond suburb Chesterfield, against Democratic candidate John Kerry.'b"Editorially, the Times-Dispatch has historically leaned conservative, leading the paper to frequently endorse candidates of the Republican Party. It supported many of former President George W. Bush's policies, including the 2003 invasion of Iraq and a flat income tax. However, the paper is not unilaterally conservative; for example, a 2005 editorial called for the then House Majority Leader Tom DeLay to relinquish his leadership position on ethical grounds. There are also some liberal syndicated columnists who appear frequently, especially Leonard Pitts."b'During the Civil Rights Movement, the Times-Dispatch, like nearly every major newspaper in Virginia, was an ardent supporter of segregation.[19]'b"In the 2016 presidential election, the Times-Dispatch endorsed Libertarian candidate Gary Johnson over major party candidates Donald Trump and Hillary Clinton. Clinton's running mate, Tim Kaine, is a Richmond resident who served as mayor of the city from 1998-2001. From at least 1980 until its Johnson endorsement in 2016, the Times-Dispatch had only endorsed Republican presidential candidates.[20]"b'Like most major papers, the sports section has MLB, NASCAR, MLS, NBA, NCAA, NFL, and NHL scores and results. The Times-Dispatch sports pages naturally focus on Richmond and Virginia professional and college teams. In addition to Richmond Flying Squirrels and Richmond Kickers coverage, readers can see in-depth coverage of the Washington Redskins in the fall and the Washington Nationals in the summer. "Virginians in the Pros" and similar features track all sorts of professional athletes who were born, lived in, or attended college in Virginia. Large automobile racing events like the Sprint Cup (at the Richmond International Raceway) are often given a separate preview guide.'b'Catering to the vast array of Virginia hunters, fishers, hikers, and outdoorsmen, somewhere between half a page to a whole page most days is dedicated to outdoors articles, written by Lee Graves, who succeeded Garvey Winegar in November 2003. The "Scoreboard," which features minor-league standings, Vegas betting, and other sports scores, also gives tide measurements, river levels, and skiing conditions, depending on the season.'b'Virginians have traditionally been highly supportive of high school athletics, and its flagship paper is a testament to that. Particular emphasis is given to American football and basketball; The Times-Dispatch ranks area teams in these sports, in the style of the NCAA polls, and generally updates them weekly. In the fall, Sunday editions have the scores of all high school football games played that weekend from across the state. Prep games are also receive above-average coverage in baseball, cross country, golf, lacrosse, soccer, softball, swimming, tennis, track and field, and volleyball. Stories are frequently done on notable prep athletes, such as those from foreign countries, those with disabilities, those who play a multitude of sports, or those who had little or no prior experience in a sport which they now excel in.'b'The business desk consists of six reporters; they cover technology, retail, energy, insurance, banking, economics, real estate, manufacturing, transportation and consumer issues. Unlike many newspapers, the Times-Dispatch produces a widely read Monday business section, Metro Business. It contains a center cover story on a regional business-related issue and is filled with events for the coming week, advice columnists and gadget reviews. In June 2006, the decision was made to remove the stock tables from the daily sections beginning July 15 and replace the numerous pages with a "Markets Review" section for subscribers who request it. The stock section was eliminated in 2009, as was the Sunday Real Estate section (both were cost-cutting moves). The Sunday Business section, which had been a showcase of general business-interest stories and features, has been rechristened Moneywise and now features primarily consumer-related coverage. Moneywise is also among select Sunday business sections nationwide that print Wall Street Journal Sunday pages.'b'On July 12, 2006, Richmond-based news magazine Style Weekly ran a cover story [21] titled "Truth and Consequences," a piece that took a look at the Times-Dispatch\'s operations as the paper settled into its first year with new management. The report described new editor Glenn Proctor, who took over Nov. 14, 2005, as an "inelegant, blunt and harsh critic \xe2\x80\x94 to the point of saying, repeatedly, that some reporters\' work \'sucks.\'" The piece described a newsroom teetering on the edge, preparing for promised changes \xe2\x80\x94 such as possible layoffs, fewer pages and combined sections \xe2\x80\x94 that eventually were realized. On April 2, 2009, the Times-Dispatch cut 90 jobs, laying off 59 workers, including 28 newsroom jobs. Proctor left the paper in 2011.'b'The front page of the Times-Dispatch\xe2\x80\x99s August 14, 2011 Sunday paper consisted entirely of a Wells Fargo advertisement, commemorating said bank\xe2\x80\x99s acquisition of Wachovia properties in Virginia.[22]'b'Notable columnists published include:'American Civil War
b'Union victory'b'2,200,000:[a]'b'750,000\xe2\x80\x931,000,000:[a][4]'b'110,000+ killed in action/died of wounds\n230,000+ accident/disease deaths[6][7]\n25,000\xe2\x80\x9330,000 died in Confederate prisons[2][6]'b'365,000+ total dead[8] 282,000+ wounded[7]\n181,193 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][9]'b'94,000+ killed in action/died of wounds[6]\n26,000\xe2\x80\x9331,000 died in Union prisons[7]'b'290,000+ total dead\n137,000+ wounded\n436,658 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][10]'b"The American Civil War (known by other names) was a civil war that was fought in the United States from 1861 to 1865. As a result of the long-standing controversy over slavery, war broke out in April 1861, when Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated. The nationalists of the Union proclaimed loyalty to the U.S. Constitution. They faced secessionists of the Confederate States, who advocated for states' rights to expand slavery."b'Among the 34 U.S. states in February 1861, seven Southern slave states individually declared their secession from the U.S. to form the Confederate States of America, or the South. The Confederacy grew to include eleven slave states. The Confederacy was never diplomatically recognized by the United States government, nor was it recognized by any foreign country (although the United Kingdom and France granted it belligerent status). The states that remained loyal to the U.S. (including the border states where slavery was legal) were known as the Union or the North.'b"The Union and Confederacy quickly raised volunteer and conscription armies that fought mostly in the South over four years. The Union finally won the war when General Robert E. Lee surrendered to General Ulysses S. Grant at the Battle of Appomattox Court House, followed by a series of surrenders by Confederate generals throughout the southern states. Four years of intense combat left 620,000 to 750,000 people dead, more than the number of U.S. military deaths in all other wars combined (at least until approximately the Vietnam War).[15] Much of the South's infrastructure was destroyed, especially the transportation systems, railroads, mills, and houses. The Confederacy collapsed, slavery was abolished, and 4 million slaves were freed. The Reconstruction Era (1863\xe2\x80\x931877) overlapped and followed the war, with the process of restoring national unity, strengthening the national government, and granting civil rights to freed slaves throughout the country. The Civil War is the most studied and written about episode in U.S. history.[16]"b''b''b"In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories. The Southern states viewed this as a violation of their constitutional rights and as the first step in a grander Republican plan to eventually abolish slavery. The three pro-Union candidates together received an overwhelming 82% majority of the votes cast nationally: Republican Lincoln's votes centered in the north, Democrat Stephen A. Douglas' votes were distributed nationally and Constitutional Unionist John Bell's votes centered in Tennessee, Kentucky, and Virginia. The Republican Party, dominant in the North, secured a plurality of the popular votes and a majority of the electoral votes nationally, so Lincoln was constitutionally elected president. He was the first Republican Party candidate to win the presidency. However, before his inauguration, seven slave states with cotton-based economies declared secession and formed the Confederacy. The first six to declare secession had the highest proportions of slaves in their populations, a total of 49 percent.[17] The first seven with state legislatures to resolve for secession included split majorities for unionists Douglas and Bell in Georgia with 51% and Louisiana with 55%. Alabama had voted 46% for those unionists, Mississippi with 40%, Florida with 38%, Texas with 25%, and South Carolina cast Electoral College votes without a popular vote for president.[18] Of these, only Texas held a referendum on secession."b'Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln\'s March 4, 1861, inaugural address declared that his administration would not initiate a civil war. Speaking directly to the "Southern States", he attempted to calm their fears of any threats to slavery, reaffirming, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so."[19] After Confederate forces seized numerous federal forts within territory claimed by the Confederacy, efforts at compromise failed and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene, but none did, and none recognized the new Confederate States of America.'b"Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, the battle was inconclusive from 1861\xe2\x80\x931862. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal.[20] To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies, and seized New Orleans. The 1863 Union Siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William T. Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years, known as the Reconstruction Era."b'The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships and iron-clad ships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I, World War II and subsequent conflicts. It remains the deadliest war in American history. From 1861 to 1865, it is estimated that 620,000 to 750,000 soldiers died,[21] along with an undetermined number of civilians.[b] By one estimate, the war claimed the lives of 10 percent of all Northern males 20\xe2\x80\x9345 years old, and 30 percent of all Southern white males aged 18\xe2\x80\x9340.[23]'b'The causes of secession were complex and have been controversial since the war began, but most academic scholars\xc2\xa0identify\xc2\xa0slavery as a central cause of the war. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war.[24] Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to promote pro-slavery acts and policies.[25][26]'b'Slavery was a major cause of disunion.[27] Although there were opposing views even in the Union States,[28][29] most northern soldiers were largely indifferent on the subject of slavery,[30] while Confederates fought the war largely to protect a southern society of which slavery was an integral part.[31] From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with republicanism. The strategy of the anti-slavery forces was containment\xe2\x80\x94to stop the expansion and thus put slavery on a path to gradual extinction.[32] The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights.[33] Southern whites believed that the emancipation of slaves would destroy the South\'s economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population.[34] In particular, southerners feared a repeat of "the horrors of Santo Domingo", in which nearly all white people \xe2\x80\x93 including men, women, children, and even many sympathetic to abolition \xe2\x80\x93 were killed after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea, and proposes it contributed to the segregation in the Jim Crow era following emancipation.[35] These fears were exacerbated by the recent attempts of John Brown to instigate an armed slave rebellion in the South.'b'Slavery was illegal in much of the North, having been outlawed in the late 18th and early 19th centuries. It was also fading in the border states and in Southern cities, but it was expanding in the highly profitable cotton districts of the rural South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide.'b"Sectionalism refers to the different economies, social structure, customs and political values of the North and South.[36][37] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention which manifested Northern dissastisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern Presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for poor freedmen. In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations.[38]"b'Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[39][40]'b'Historically, southern slave-holding states, because of their low-cost manual labor, had little perceived need for mechanization and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.[41]'b'The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress.[42][43] The tariff issue was and is sometimes cited\xe2\x80\x93long after the war\xe2\x80\x93by Lost Cause historians and neo-Confederate apologists. In 1860\xe2\x80\x9361 none of the groups that proposed compromises to head off secession raised the tariff issue.[44] Pamphleteers North and South rarely mentioned the tariff,[45] and when some did, for instance, Matthew Fontaine Maury[46] and John Lothrop Motley,[47] they were generally writing for a foreign audience.'b'The South argued that each state had the right to secede\xe2\x80\x94leave the Union\xe2\x80\x94at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a perpetual union.[48] Historian James McPherson writes concerning states\' rights and other non-slavery explanations:'b"While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[49]"b'Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. It was over territories west of the Mississippi that the proslavery and antislavery forces collided.[50]'b'With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well.[51][52] Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the southern states the question of the territorial expansion of slavery westward again became explosive.[53] Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."[54][55]'b'By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[56] The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.[57]'b'The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance\xe2\x80\x94that slavery could be excluded in a territory as it was done in the Northwest Ordinance of 1787 at the discretion of Congress,[58] thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.[59]'b'Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty\xe2\x80\x94which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter.[60] The Kansas\xe2\x80\x93Nebraska Act of 1854 legislated this doctrine.[61] In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when southern senators began to leave.[62]'b'The fourth theory was advocated by Mississippi Senator Jefferson Davis,[63] one of state sovereignty ("states\' rights"),[64] also known as the "Calhoun doctrine",[65] named after the South Carolinian political theorist and statesman John C. Calhoun.[66] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[67] "States\' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[68] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power."[69][70] These four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the U.S. Constitution prior to the 1860 presidential election.[71]'b'Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in the sense that their country was a national republic based on the belief that all people had inalienable political liberty and personal rights which could serve as an important example to the rest of the world. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, the division and redivision of the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state.'b'In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure\xe2\x80\x94first in their political institutions, then in their civil life together.'b'Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy.[72] C. Vann Woodward said of the latter group,'b'A great slave society\xc2\xa0... had grown up and miraculously flourished in the heart of a thoroughly bourgeois and partly puritanical republic. It had renounced its bourgeois origins and elaborated and painfully rationalized its institutional, legal, metaphysical, and religious defenses\xc2\xa0... When the crisis came it chose to fight. It proved to be the death struggle of a society, which went down in ruins.[73]'b"Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin (1852)[74] and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.[75]"b'While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion\xc2\xa0... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence."[76] The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.[77]'b'The election of Abraham Lincoln in November 1860 was the final trigger for secession.[78] Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.'b'According to Lincoln, the people of the United States had shown that they can be successful in establishing and administering a republic, but a third challenge faced the nation, maintaining the republic, based on the people\'s vote. The people must now show: "successful maintenance [of the Republic] against a formidable internal attempt to overthrow it. It is now for them to demonstrate to the world that those who can fairly carry an election can also suppress a rebellion; that ballots are the rightful and peaceful successors of bullets; and that when ballots have fairly and constitutionally decided, there can be no successful appeal back to bullets; that there can be no successful appeal, except to ballots themselves, at succeeding elections. Such will be a great lesson of peace; teaching men that what they cannot take by an election, neither can they take it by a war".[79]'b'The election of Lincoln caused the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws and, even, secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860, and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states\' rights for slave owners in the South, but contained a complaint about states\' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.'b'Among the ordinances of secession passed by the individual states, those of three\xe2\x80\x94Texas, Alabama, and Virginia\xe2\x80\x94specifically mentioned the plight of the "slaveholding states" at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures.[80] However, at least four states\xe2\x80\x94South Carolina,[81] Mississippi,[82] Georgia,[83] and Texas[84]\xe2\x80\x94also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement\'s influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive slave clause of the Constitution.'b'These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861.[85] They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress".[86] One quarter of the U.S. Army\xe2\x80\x94the entire garrison in Texas\xe2\x80\x94was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.'b'As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts),[87] the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.'b"On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it.[88] It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would have voted in favor of it.[89] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[90]"b'On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[91] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[91]'b'The South sent delegations to Washington and offered to pay for the federal properties[which?] and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[92] Secretary of State William Seward, who at the time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed.[92] President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina\'s Fort Sumter.'b"Fort Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. fort's garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan, who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12\xe2\x80\x9313, forcing its capitulation."b'The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins said:'b"However, much of the North's attitude was based on the false belief that only a minority of Southerners were actually in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners really did favor secession, they might have hesitated at attempting the enormous task of conquering a united South.[95]"b'Lincoln called on all the states to send forces to recapture the fort and other federal properties. With the scale of the rebellion apparently small so far, Lincoln called for only 75,000 volunteers for 90\xc2\xa0days.[96] The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal.[97] On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.[98]'b'Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.[99]'b'Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. West Virginia then joined them as an additional border state after it separated from Virginia and became a state of the Union in 1863.'b"Maryland's territory surrounded the United States' capital of Washington, DC and could cut it off from the North.[100] It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53\xe2\x80\x9313) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war.[101] Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North.[102] Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened.[101][103] All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.[104]"b'In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. (See also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[105]'b'Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.[106]'b"After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving).[107] The inclusion of 24 secessionist counties[108] in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war.[109][110] Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000\xe2\x80\x9322,000 soldiers to both the Confederacy and the Union.[111]"b'A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[112]'b'The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy\'s soldier.[113]'b'As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[114] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.[115]'b'In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law\xe2\x80\x94conscription\xe2\x80\x94as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt.[116] The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.[117]'b"When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[118] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.[119]"b'In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[120] At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units.[121] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[122]'b'From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan\'s assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.[123]'b'Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:'b"Some historians emphasize that Civil War soldiers were driven by political ideology, holding firm beliefs about the importance of liberty, Union, or state rights, or about the need to protect or to destroy slavery. Others point to less overtly political reasons to fight, such as the defense of one's home and family, or the honor and brotherhood to be preserved when fighting alongside other men. Most historians agree that no matter what a soldier thought about when he went into the war, the experience of combat affected him profoundly and sometimes altered his reasons for continuing the fight.[124]"b"At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their own army where they were paid but not allowed to perform any military duties.[125] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[126]"b'The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396.[127][128] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[129] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.'b"By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible.[130] Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.[131]"b'In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[132]'b'The Civil War occurred during the early stages of the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union\'s naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[133] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union\'s own ironclad warships, they were unsuccessful.[134]'b"The Confederacy experimented with a submarine, which did not work well,[135] and with building an ironclad ship, the CSS Virginia, which was based on rebuilding a sunken Union ship, the Merrimack. On its first foray on March 8, 1862, the Virginia inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, the USS Monitor, arrived to challenge it in the Chesapeake Bay. The resulting three hour battle between the Ironclads was a draw, but it marked the worldwide transition to ironclad warships.[136] Not long after the battle the Confederacy was forced to scuttle the Virginia to prevent its capture, while the Union built many copies of the Monitor. Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Britain.[137]"b'British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were designed for speed and were so small that only a small amount of cotton went out.[138] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released.[139] The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[140]'b"Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[141] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.[142]"b'To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested.[134] After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.[143]'b'The 1862 Union strategy called for simultaneous advances along four axes:[144]'b'Ulysses Grant used river transport and Andrew Foote\'s gunboats of the Western Flotilla to threaten the Confederacy\'s "Gibraltar of the West" at Columbus, Kentucky. Though rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.[145]'b"In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[146] They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry (February 6, 1862) and Fort Donelson (February 11 to 16, 1862), and supplied Grant's forces as he moved into Tennessee. At Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory\xe2\x80\x94the first battle with the high casualty rates that would repeat over and over.[147] Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. On April 24, 1862, U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[148]"b'Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at Vicksburg, Mississippi in July 1863, and in the Union fully controlling the Mississippi River soon after.[149]'b'In one of the first highly visible battles, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces near Washington was repulsed.'b"Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign,[150][151][152] Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[153] The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[154] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops."b"Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[153][155] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[156]"b"When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[157] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker."b"Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863.[158] Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[159] Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[160] This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[161] However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant."b"While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[162] Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization."b'The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans,[163] which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.'b"General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state.[164] Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.[165]"b"The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged."b"The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh;[166] and the Battle of Vicksburg,[167] which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[168] driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy."b'Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[171] Roving Confederate bands such as Quantrill\'s Raiders terrorized the countryside, striking both military installations and civilian settlements.[172] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.'b'By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.[169]'b'Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union.[173] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[174]'b'After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him.[175] Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.'b'At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[176] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[177] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[178]'b"Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[179]"b"Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[180]"b"Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president.[181] Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.[182]"b'Leaving Atlanta, and his base of supplies, Sherman\'s army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman\'s army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee\'s army.[183]'b'Lee\'s army, thinned by desertion and casualties, was now much smaller than Grant\'s. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler\'s Creek.[184]'b"Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House.[185] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller."b"On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[186] On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William T. Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day.[1] On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department.[187] On June 23, Cherokee leader Stand Watie became the last Confederate General to surrender his forces.[188]"b"Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators.[189][190] The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.[191]"b'Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860\xe2\x80\x9362 crop failures in Europe made the North\'s grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half.[191] When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.[192]'b'Lincoln\'s foreign policy was deficient in 1861 in terms of appealing to European public opinion. Diplomats had to explain that United States was not committed to the ending of slavery, but instead they repeated legalistic arguments about the unconstitutionality of secession. Confederate spokesmen, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. In addition, the European aristocracy (the dominant factor in every major country) was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[193]'b'U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (CSS Alabama, CSS Shenandoah, CSS Tennessee, CSS Tallahassee, CSS Florida, and some others). The most famous, the CSS Alabama, did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for politicians in Britain, where the antislavery movement was powerful.[194]'b"War loomed in late 1861 between the U.S. and Britain over the Trent affair, involving the U.S. Navy's boarding of the British ship Trent and seizure of two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation between North and South\xe2\x80\x93 though even such an offer would have risked war with the U.S. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on this.[195]"b"The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.[196]"b'The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty.[197]'b"Historians have debated whether the Confederacy could have won the war. Most scholars, including James McPherson, argue that Confederate victory was at least possible.[198] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.[199]"b'Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[199] Lincoln was not a military dictator, and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.[203]'b'Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat.[204][205] Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back\xc2\xa0... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don\'t think the South ever had a chance to win that War."[206]'b'A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win."[207][208] Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederate army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict.[209] However, most historians reject the argument.[210] James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864\xe2\x80\x9365, he says most Confederate soldiers were fighting hard.[211] Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up\xe2\x80\x94some few deserters\xe2\x80\x94plenty tired of war, but the masses determined to fight it out."[212]'b"Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[213] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[214]"b'Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[215] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:'b'The North\'s victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[216]'b'Scholars have debated what the effects of the war were on political and economic power in the South.[217] The prevailing view is that the southern planter elite retained its powerful position in the South.[217] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[217]'b'The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths\xe2\x80\x94two-thirds by disease, and 50,000 civilians.[11] Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000.[21][218] The war accounted for more American deaths than in all other U.S. wars combined.[219]'b'Based on 1860 census figures, 8 percent of all white males aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[220][221] About 56,000 soldiers died in prison camps during the War.[222] An estimated 60,000 men lost limbs in the war.[223]'b'Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:[6]'b'In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[7]'b'Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle.[6] Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War.[224]:16 Notably, their mortality rate was significantly higher than white soldiers:'b'[We] find, according to the revised official data, that of the slightly over two millions troops in the United States Volunteers, over 316,000 died (from all causes), or 15.2 percent. Of the 67,000 Regular Army (white) troops, 8.6 percent, or not quite 6,000, died. Of the approximately 180,000 United States Colored Troops, however, over 36,000 died, or 20.5 percent. In other words, the mortality "rate" amongst the United States Colored Troops in the Civil War was thirty-five percent greater than that among other troops, notwithstanding the fact that the former were not enrolled until some eighteen months after the fighting began.[224]:16'b"Confederate records compiled by historian William F. Fox list 74,524 killed and died of wounds and 59,292 died of disease. Including Confederate estimates of battle losses where no records exist would bring the Confederate death toll to 94,000 killed and died of wounds. Fox complained, however, that records were incomplete, especially during the last year of the war, and that battlefield reports likely under-counted deaths (many men counted as wounded in battlefield reports subsequently died of their wounds). Thomas L. Livermore, using Fox's data, put the number of Confederate non-combat deaths at 166,000, using the official estimate of Union deaths from disease and accidents and a comparison of Union and Confederate enlistment records, for a total of 260,000 deaths.[6] However, this excludes the 30,000 deaths of Confederate troops in prisons, which would raise the minimum number of deaths to 290,000."b'The United States National Park Service uses the following figures in its official tally of war losses:[2]'b'Union: 853,838'b'Confederate: 914,660'b"While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, Superintendent of the 1870 Census, used census and Surgeon General data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 Census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.[218]"b'Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[22] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.'b"Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in an area where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor James Downs states that tens to hundreds of thousands of slaves died during the war from disease, starvation, exposure, or execution at the hands of the Confederates, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.[225]"b'Losses were far higher than during the recent defeat of Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Mini\xc3\xa9 balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.[226]'b"The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.[227]"b'The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century.[228] The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.'b'While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee\'s army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[229] Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal.[230] Lincoln\'s decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[231] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans\' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.[232]'b'The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army.[e] About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.[f]'b'During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[238] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.[239]'b'At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Fr\xc3\xa9mont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected.[240] But only the District of Columbia accepted Lincoln\'s gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat".[241] Lincoln laid the groundwork for public support in an open letter published in abolitionist Horace Greeley\'s newspaper.[242]'b'In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors\' Conference added support for the proclamation.[243] Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong\xc2\xa0... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling\xc2\xa0... I claim not to have controlled events, but confess plainly that events have controlled me."[244]'b"Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.[245]"b"Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[246] The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France.[247] By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.[248]"b'In Texas v. White, 74 U.S. 700 (1869) the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.[249]'b'Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877.[250] It comprised multiple complex methods to resolve the outstanding issues of the war\'s aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery\xe2\x80\x94and prevent semi-slavery status.[251]'b'President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson\'s work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended.[252] With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.'b'The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war\'s aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[253] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[254]'b'Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academe, leading to a proliferation of solid studies by non-scholars who are thoroughly familiar with the primary sources, pay close attention to battles and campaigns, and write for the large public readership, rather than the small scholarly community. Bruce Catton and Shelby Foote are among the best-known writers.[255][256] Practically every major figure in the war, both North and South, has had a serious biographical study.[257] Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:'b"God had chastised them and given them a special mission\xe2\x80\x94to maintain orthodoxy, strict biblicism, personal piety, and traditional race relations. Slavery, they insisted, had not been sinful. Rather, emancipation was a historical tragedy and the end of Reconstruction was a clear sign of God's favor.[258]"b'In sharp contrast, Black preachers interpreted the Civil War as:'b"God's gift of freedom. They appreciated opportunities to exercise their independence, to worship in their own way, to affirm their worth and dignity, and to proclaim the fatherhood of God and the brotherhood of man. Most of all, they could form their own churches, associations, and conventions. These institutions offered self-help and racial uplift, and provided places where the gospel of liberation could be proclaimed. As a result, black preachers continued to insist that God would protect and help him; God would be their rock in a stormy land.[259]"b'Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations.[260] Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[261] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing African-American progress to a white man\'s reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[262]'b"The interpretation of the Civil War presented by Charles A. Beard and Mary R. Beard in The Rise of American Civilization (1927) was highly influential among historians and the general public until the civil rights movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:"b'[A] social cataclysm in which the capitalists, laborers, and farmers of the North and West drove from power in the national government the planting aristocracy of the South.[263]'b'The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.[264]'b"The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861, but the oldest surviving monument is the Hazen monument, erected at Stones River near Murfreesboro, Tennessee, in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead in the Battle of Stones River. In the 1890s, the United States government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park in Tennessee and the Antietam National Battlefield in Maryland in 1890. The Shiloh National Military Park was established in 1894, followed by the Gettysburg National Military Park in 1895 and Vicksburg National Military Park in 1899. In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service.[265]"b'The modern Civil War battlefield preservation movement began in 1987 with the founding of the Association for the Preservation of Civil War Sites (APCWS), a grassroots organization created by Civil War historians and others to preserve battlefield land by acquiring it. In 1991, the original Civil War Trust was created in the mold of the Statue of Liberty/Ellis Island Foundation, but failed to attract corporate donors and soon helped manage the disbursement of U.S. Mint Civil War commemorative coin revenues designated for battlefield preservation. Although the two organizations joined forces on a number of battlefield acquisitions, ongoing conflicts prompted the boards of both organizations to facilitate a merger, which happened in 1999 with the creation of the Civil War Preservation Trust. In 2011, the organization was renamed The Civil War Trust. From 1987 through late 2017, The Trust and its predecessor organizations saved more than 40,000 acres at 126 Civil War battlefields and sites in 21 states.[266]'b"The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary. [267] Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as Birth of a Nation (1915), Gone with the Wind (1939), and more recently Lincoln (2012). Ken Burns produced a notable PBS series on television titled The Civil War (1990). It was digitally remastered and re-released in 2015."b'There were numerous technological innovations during the Civil War that had a great impact on 19th century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[268] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[269][270] It was also in this war when countries first used aerial warfare, in the form of reconnaissance balloons, to a significant effect.[271] It saw the first action involving steam-powered ironclad warships in naval warfare history.[272] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare, as well as the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[273]'b'General reference'b'Union'b'Confederacy'b'Ethnic articles'b'Topical articles'b'National articles'b'State articles'b'Memorials'b''Singular-value decomposition
b'The singular-value decomposition can be computed using the following observations:'b'Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.'b'Suppose M is a m \xc3\x97 n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the form'b'where'b'The diagonal entries \xcf\x83i of \xce\xa3 are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, \xce\xa3, is uniquely determined by M (though not the matrices U and V, see below).'b'In the special, yet common case when M is an m \xc3\x97 m real square matrix with positive determinant, U, V\xe2\x88\x97, and \xce\xa3 are real m \xc3\x97 m matrices as well, \xce\xa3 can be regarded as a scaling matrix, and U, V\xe2\x88\x97 can be viewed as rotation matrices. Thus the expression U\xce\xa3V\xe2\x88\x97 can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.'b"Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = V\xce\xa3V\xe2\x88\x97) with eigenvalue scale factors \xcf\x83i along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV\xe2\x88\x97). If the rotation is done first, M = P'R, then R is the same and P' = U\xce\xa3U\xe2\x88\x97 has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate."b'As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n \xc3\x97 n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m \xc3\x97 n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.'b'Since U and V\xe2\x88\x97 are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector \xcf\x83i Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U\xe2\x88\x97 and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U\xe2\x88\x97, V, and V\xe2\x88\x97 are orthonormal bases.'b'Consider the 4 \xc3\x97 5 matrix'b'A singular-value decomposition of this matrix is given by U\xce\xa3V\xe2\x88\x97'b'Notice \xce\xa3 is zero outside of the diagonal and one diagonal element is zero. Furthermore, because the matrices U and V\xe2\x88\x97 are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V\xe2\x88\x97 are real valued, each is an orthogonal matrix.'b'is also a valid singular-value decomposition.'b'In any singular-value decomposition'b'the diagonal entries of \xce\xa3 are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:'b'As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank\xe2\x80\x93nullity theorem cannot be the same dimension if m \xe2\x89\xa0 n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m \xe2\x88\x92 n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n \xe2\x88\x92 m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.'b'Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ei\xcf\x86 (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.'b'The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = U\xce\xa3V\xe2\x88\x97 is'b'where \xce\xa3+ is the pseudoinverse of \xce\xa3, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.'b"A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x\xe2\x88\x97A = 0, with x\xe2\x88\x97 denoting the conjugate transpose of x, is called a left null vector of A."b'A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.'b'Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.'b'As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \xce\xa3. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.'b'Here Ui and Vi are the i-th columns of the corresponding SVD matrices, \xcf\x83i are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero \xcf\x83i is exactly the rank of the matrix.'b"Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,"b'which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]'b'It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O \xe2\x88\x92 A. The solution is the product UV\xe2\x88\x97.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV\xe2\x88\x97 where I is the identity matrix, so that if A = U\xce\xa3V\xe2\x88\x97 then the product A = UV\xe2\x88\x97 amounts to replacing the singular values with ones.'b'A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,'b'This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.'b"The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules."b'The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]'b'The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.'b'The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the \xce\xa3 matrix is larger than one.'b'One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.'b'SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]'b"Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]"b'Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.'b'Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]'b'The singular-value decomposition is very general in the sense that it can be applied to any m \xc3\x97 n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.'b'Given an SVD of M, as described above, the following two relations hold:'b'The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:'b'In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU\xe2\x88\x97 for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU\xe2\x88\x97 is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each \xcf\x83i to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=U\xce\xa3U* is positive semidefinite and normal, and R=UV* is unitary.'b'An eigenvalue \xce\xbb of a matrix M is characterized by the algebraic relation Mu = \xce\xbbu. When M is Hermitian, a variational characterization is also available. Let M be a real n \xc3\x97 n symmetric matrix. Define'b'By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| \xe2\x89\xa4 1}. By the Lagrange multipliers theorem, u necessarily satisfies'b'where the nabla symbol, \xe2\x88\x87, is the del operator.'b'A short calculation shows the above leads to Mu = \xce\xbbu (symmetry of M is needed here). Therefore, \xce\xbb is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.'b'Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.'b'This section gives these two arguments for existence of singular-value decomposition.'b'Let M be an m \xc3\x97 n complex matrix. Since M\xe2\x88\x97M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n \xc3\x97 n matrix V such that'b'where D is diagonal and positive definite. Partition V appropriately so we can write'b'Therefore:'b'The second equation implies MV2 = 0. Also, since V is unitary:'b'where the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. Define'b'Then'b'We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, since'b'the columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:'b'For V1 we already have V2 to make it unitary. Now, define'b'which is the desired result:'b'Notice the argument could begin with diagonalizing MM\xe2\x88\x97 rather than M\xe2\x88\x97M (This shows directly that MM\xe2\x88\x97 and M\xe2\x88\x97M have the same non-zero eigenvalues).'b'The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.'b'Let M denote an m \xc3\x97 n matrix with real entries. Let Sm\xe2\x88\x921 and Sn\xe2\x88\x921 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the function'b'Consider the function \xcf\x83 restricted to Sm\xe2\x88\x921 \xc3\x97 Sn\xe2\x88\x921. Since both Sm\xe2\x88\x921 and Sn\xe2\x88\x921 are compact sets, their product is also compact. Furthermore, since \xcf\x83 is continuous, it attains a largest value for at least one pair of vectors u \xe2\x88\x88 Sm\xe2\x88\x921 and v \xe2\x88\x88 Sn\xe2\x88\x921. This largest value is denoted \xcf\x831 and the corresponding vectors are denoted u1 and v1. Since \xcf\x831 is the largest value of \xcf\x83(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.'b'Proof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:'b'After some algebra, this becomes'b'Plugging this into the pair of equations above, we have'b'This proves the statement.'b'More singular vectors and singular values can be found by maximizing \xcf\x83(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.'b'The passage from real to complex is similar to the eigenvalue case.'b'Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).'b'The linear transformation'b'has a particularly simple description with respect to these orthonormal bases: we have'b'where \xcf\x83i is the i-th diagonal entry of \xce\xa3, and T(Vi) = 0 for i > min(m,n).'b'The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T\xc2\xa0: Kn \xe2\x86\x92 Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.'b'To get a more visual flavour of singular values and SVD factorization \xe2\x80\x94 at least when working on real vector spaces \xe2\x80\x94 consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V\xe2\x88\x97 sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D \xe2\x88\x98 V\xe2\x88\x97 then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U \xe2\x88\x98 D \xe2\x88\x98 V\xe2\x88\x97 coincides with T.'b'The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m \xe2\x89\xa5 n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).'b'The first step can be done using Householder reflections for a cost of 4mn2 \xe2\x88\x92 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).'b'The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.'b'The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 \xc3\x97 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 \xc3\x97 2 eigenvalue methods (Golub & Van Loan 1996, \xc2\xa78.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).'b'There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or'b'The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M \xe2\x87\x92 Q R and LQ decomposition of R gives R \xe2\x87\x92 L P*. Thus, at every iteration, we have M \xe2\x87\x92 Q L P*, update M \xe2\x87\x90 L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.'b'In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m\xc3\x97n matrix M of rank r:'b"Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n\xc2\xa0\xe2\x89\xaa\xc2\xa0m. The matrix U'n is thus m\xc3\x97n, \xce\xa3n is n\xc3\x97n diagonal, and V is n\xc3\x97n."b'The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if\xc2\xa0n\xc2\xa0\xe2\x89\xaa\xc2\xa0m.'b'Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values \xce\xa3r are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r\xc2\xa0\xe2\x89\xaa\xc2\xa0n. The matrix Ur is thus m\xc3\x97r, \xce\xa3r is r\xc3\x97r diagonal, and Vr* is r\xc3\x97n.'b'Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values \xce\xa3t are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t\xe2\x89\xaar. The matrix Ut is thus m\xc3\x97t, \xce\xa3t is t\xc3\x97t diagonal, and Vt* is t\xc3\x97n.'b'The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]'b'The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spaces'b'But, in the matrix case, (M* M)\xc2\xbd is a normal matrix, so ||M* M||\xc2\xbd is the largest eigenvalue of (M* M)\xc2\xbd, i.e. the largest singular value of M.'b"The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)\xc2\xbd] (the eigenvalues of M* M are the squares of the singular values)."b'The singular values are related to another norm on the space of operators. Consider the Hilbert\xe2\x80\x93Schmidt inner product on the n \xc3\x97 n matrices, defined by'b'So the induced norm is'b'Since the trace is invariant under unitary equivalence, this shows'b'where \xcf\x83i are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert\xe2\x80\x93Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:'b'In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.'b'Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.'b'TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:'b'The factorization M = U\xce\xa3V\xe2\x88\x97 can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X,\xc2\xa0\xce\xbc), and a non-negative measurable f such that'b'This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such that'b'is an unitary operator.'b'As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write'b'and notice that U V* is still a partial isometry while VTf V* is positive.'b'The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero \xce\xbb in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M\xe2\x88\x97M. Applying the diagonalization result, the unitary image of its positive square root Tf\xc2\xa0 has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {\xcf\x83i}. For any \xcf\x88 \xe2\x88\x88 H,'b'where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. \xcf\x83i are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.'b'Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:'b'The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.'b'Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.'Method of moments (statistics)
b'In statistics, the method of moments is a method of estimation of population parameters. One starts with deriving equations that relate the population moments (i.e., the expected values of powers of the random variable under consideration) to the parameters of interest. Then a sample is drawn and the population moments are estimated from the sample. The equations are then solved for the parameters of interest, using the sample moments in place of the (unknown) population moments. This results in estimates of those parameters. The method of moments was introduced by Pafnuty Chebyshev in 1887.'b''b''b'The method of moments is fairly simple and yields consistent estimators (under very weak assumptions), though these estimators are often biased.'b"In some respects, when estimating parameters of a known family of probability distributions, this method was superseded by Fisher's method of maximum likelihood, because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased."b'However, in some cases the likelihood equations may be intractable without computers, whereas the method-of-moments estimators can be quickly and easily calculated by hand.'b'Estimates by the method of moments may be used as the first approximation to the solutions of the likelihood equations, and successive improved approximations may then be found by the Newton\xe2\x80\x93Raphson method. In this way the method of moments can assist in finding maximum likelihood estimates.'b'In some cases, infrequent with large samples but not so infrequent with small samples, the estimates given by the method of moments are outside of the parameter space; it does not make sense to rely on them then. That problem never arises in the method of maximum likelihood. Also, estimates by the method of moments are not necessarily sufficient statistics, i.e., they sometimes fail to take into account all relevant information in the sample.'b'When estimating other structural parameters (e.g., parameters of a utility function, instead of parameters of a known probability distribution), appropriate probability distributions may not be known, and moment-based estimates may be preferred to maximum likelihood estimation.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Explicit semantic analysis
b'In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf\xe2\x80\x93idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.[1]'b'ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization[2] and has been used by this pair of researchers to compute what they refer to as "semantic relatedness" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name "explicit semantic analysis" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.[1][3]'b''b''b'To perform the basic variant of ESA, one starts with a collection of texts, say, all Wikipedia articles; let the number of documents in the collection be N. These are all turned into "bags of words", i.e., term frequency histograms, stored in an inverted index. Using this inverted index, one can find for any word the set of Wikipedia articles containing this word; in the vocabulary of Egozi, Markovitch and Gabrilovitch, "each word appearing in the Wikipedia corpus can be seen as triggering each of the concepts it points to in the inverted index."[1]'b'The output of the inverted index for a single word query is a list of indexed documents (Wikipedia articles), each given a score depending on how often the word in question occurred in them (weighted by the total number of words in the document). Mathematically, this list is an N-dimensional vector of word-document scores, where a document not containing the query word has score zero. To compute the relatedness of two words, one compares the vectors (say u and v) by computing the cosine similarity,'b'and this gives numeric estimate of the semantic relatedness of the words. The scheme is extended from single words to multi-word texts by simply summing the vectors of all words in the text.[3]'b'ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically orthogonal concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of information retrieval systems when it is based not on Wikipedia, but on the Reuters corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".[4] To explain this observation, links have been shown between ESA and the generalized vector space model.[5] Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".[1]'b'Cross-language explicit semantic analysis (CL-ESA) is a multilingual generalization of ESA.[6] CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.'Latent semantic analysis
b'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]'b'An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).[2]'b''b''b'LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency\xe2\x80\x93inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.'b'This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.'b'After the construction of the occurrence matrix, LSA finds a low-rank approximation[4] to the term-document matrix. There could be various reasons for these approximations:'b'The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:'b'This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.'b'Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:'b'Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:'b'The matrix products giving us the term and document correlations then become'b'You can now do the following:'b'To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:'b'The new low-dimensional space typically can be used to:'b'Synonymy and polysemy are fundamental problems in natural language processing:'b'LSA has been used to assist in performing prior art searches for patents.[8]'b'The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words. These findings are referred to as the Semantic Proximity Effect.[9]'b'When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list. These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.[10]'b'Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.[11]'b"The SVD is typically computed using large matrix methods (for example, Lanczos methods) but may also be computed incrementally and with greatly reduced resources via a neural network-like approach, which does not require the large, full-rank matrix to be held in memory.[12] A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.[13] MATLAB and Python implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution. In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.[14]"b"Some of LSA's drawbacks include:"b'In semantic hashing [17] documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.'b'Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.[18]'b'LSI is also an application of correspondence analysis, a multivariate statistical technique developed by Jean-Paul Benz\xc3\xa9cri[19] in the early 1970s, to a contingency table built from word counts in documents.'b'Called "latent semantic indexing" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s. The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\xe2\x80\x99t share a specific word or words with the search criteria.'b'LSI overcomes two of the most problematic constraints of Boolean keyword queries: multiple words that have similar meanings (synonymy) and words that have more than one meaning (polysemy)[clarification needed]. Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems.[20] As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.'b'LSI is also used to perform automated document categorization. In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.[21] Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.[22] LSI uses example documents to establish the conceptual basis for each category. During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.'b'Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI. Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection of unstructured text.'b'Because it uses a strictly mathematical approach, LSI is inherently independent of language. This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri. LSI can also perform cross-linguistic concept searching and example-based categorization. For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.[citation needed]'b'LSI is not restricted to working only with words. It can also process arbitrary character strings. Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.[23]'b'LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).[24] This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion. LSI also deals effectively with sparse, ambiguous, and contradictory data.'b'Text does not need to be in sentence form for LSI to be effective. It can work with lists, free-form notes, email, Web-based content, etc. As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.'b'LSI has proven to be a useful solution to a number of conceptual matching problems.[25][26] The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.[27]'b'LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.'b'Some common local weighting functions[29] are defined in the following table.'b'Some common global weighting functions are defined in the following table.'b'In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A\xe2\x80\x94a measure of its unique dimensions \xe2\x89\xa4 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.'b'The SVD is then truncated to reduce the rank by keeping only the largest k \xc2\xab\xc2\xa0r diagonal entries in the singular value matrix S, where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A. This reduced set of matrices is often denoted with a modified formula such as:'b'Efficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.'b'Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons.'b'The computed Tk and Dk matrices define the term and document vector spaces, which with the computed singular values, Sk, embody the conceptual information derived from the document collection. The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.'b'The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index. By a simple transformation of the A = T S DT equation into the equivalent D = AT T S\xe2\x88\x921 equation, a new vector, d, for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S\xe2\x88\x921. The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.'b'A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored. These terms will have no impact on the global weights and learned correlations derived from the original collection of text. However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.'b'The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in. Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added. When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in [13]) be used.'b'It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems. As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.'b'LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.[32] Below are some other ways in which LSI is being used:'b'LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential. Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.[47]'b'Early challenges to LSI focused on scalability and performance. LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.[48] However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome. Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package.[49]'b'Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD. As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).[50] However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.[51]'b'Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain. The variance contained in the data can be viewed by plotting the singular values (S) in a scree plot. Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain. Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain. Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.[52][53][54]'b'Due to its cross-domain applications in Information Retrieval, Natural Language Processing (NLP), Cognitive Science and Computational Linguistics, LSA has been implemented to support many different kinds of applications.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Hierarchical Dirichlet process
b'In statistics and machine learning, the hierarchical Dirichlet process (HDP) is a nonparametric Bayesian approach to clustering grouped data.[1][2] It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. This method allows groups to share statistical strength via sharing of clusters across groups. The base distribution being drawn from a Dirichlet process is important, because draws from a Dirichlet process are atomic probability measures, and the atoms will appear in all group-level Dirichlet processes. Since each atom corresponds to a cluster, clusters are shared across all groups. It was developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei and published in the Journal of the American Statistical Association in 2006,[1] as a formalization and generalization of the infinite hidden Markov model published in 2002.[3]'b''b''b'Thus the set of atoms is shared across all groups, with each group having its own group-specific atom masses. Relating this representation back to the observed data, we see that each data item is described by a mixture model:'b'The HDP mixture model is a natural nonparametric generalization of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data.[1] Here each group is a document consisting of a bag of words, each cluster is a topic, and each document is a mixture of topics. The HDP is also a core component of the infinite hidden Markov model,[3] which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.[1] [4]'b'The HDP can be generalized in a number of directions. The Dirichlet processes can be replaced by Pitman-Yor processes, resulting in the Hierarchical Pitman-Yor process. The hierarchy can be deeper, with multiple levels of groups arranged in a hierarchy. Such an arrangement has been exploited in the sequence memoizer, a Bayesian nonparametric model for sequences which has a multi-level hierarchy of Pitman-Yor processes.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Mallet (software project)
b'MALLET is a Java "Machine Learning for Language Toolkit".'b''b''b'MALLET is an integrated collection of Java code useful for statistical natural language processing, document classification, cluster analysis, information extraction, topic modeling and other machine learning applications to text.'b'MALLET was developed primarily by Andrew McCallum, of the University of Massachusetts Amherst, with assistance from graduate students and faculty from both UMASS and the University of Pennsylvania.'b''Gensim
b'Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.'b''b''b'Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms,[1] hierarchical Dirichlet processes (HDP), latent semantic analysis (LSA, LSI, SVD) and latent Dirichlet allocation (LDA), including distributed parallel versions.[2]'b'Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation Scalability of Semantic Analysis in Natural Language Processing of Radim \xc5\x98eh\xc5\xaf\xc5\x99ek, the creator of Gensim.[3]'b'Gensim has been used and cited in over 800 commercial and academic applications, in a diverse array of disciplines from medicine to insurance claim analysis to patent search[4][5] The software has been covered in several new articles, podcasts and interviews since 2009.[6][7][8]'b'The open source code is developed and hosted on GitHub[9] and a public support forum is maintained on Google Groups[10] and Gitter.[11]'b'Gensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme.[12]'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Journal of Machine Learning Research
b'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Kevin Murphy (Google) and Bernhard Sch\xc3\xb6lkopf (Max Planck Institute for Intelligent Systems).'b''b''b'The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online.[2]'b'Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.[1]'b'In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007[3] and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems.'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'arXiv
b'arXiv (pronounced "archive")[2] is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million article milestone on October 3, 2008,[3][4] and hit a million by the end of 2014.[5][6] By October 2016 the submission rate had grown to more than 10,000 per month.[6][7]'b''b''b'The arXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side.[9] Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993.[6][10] The term e-print was quickly adopted to describe the articles.'b"It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org.[11] It is now hosted principally by Cornell, with eight mirrors around the world.[12]"b'Its existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access[13] and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.'b'The annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions.[14] This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Annual donations were envisaged to vary in size between $2,300 to $4,000, based on each institution\xe2\x80\x99s usage. As of 14\xc2\xa0January\xc2\xa02014[update], 174 institutions have pledged support for the period 2013\xe2\x80\x932017 on this basis, with a projected revenue from this source of approximately $340,000.[15]'b'In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv\'s operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence".[16] However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.'b'Although the arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic,[17] or reject submissions that are not scientific papers. The lists of moderators for many sections of the arXiv are publicly available,[18] but moderators for most of the physics sections remain unlisted.'b'Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines.[19] Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area.[17] New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.[20]'b'A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston\'s geometrization conjecture, including the Poincar\xc3\xa9 conjecture as a particular case, uploaded by Grigori Perelman in November 2002.[21] Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it\'s all there [on the arXiv]\xc2\xa0\xe2\x80\x93 let them go and read about it".[22] Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.[23]'b'While the arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat\'s last theorem using only high-school mathematics, they are "surprisingly rare".[24][better\xc2\xa0source\xc2\xa0needed] The arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them.[25]'b'Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.'b"The standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's front, a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front.[26] A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides TablearXiv,[27] a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv.[28] A full text and author search engine for arXiv is provided by Scientillion.[29] Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them."b'Files on arXiv can have a number of different copyright statuses:[30]'b'Some authors have voiced concern over the lack of transparency in the arXiv academic peer-review process.[31] Demetris Christopoulos from the National and Kapodistrian University of Athens likens arXiv to a non-declared Journal without a known editor in chief, without a specific written policy regarding submitted papers, and that applies hidden censorship to all papers that do not fall within established scientific dogma. [32]'International Standard Book Number
b'The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.'b'The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).'b'Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]'b'Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.'b''b''b'The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]'b'The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]'b'An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" \xe2\x80\x93 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN\xc2\xa00-340-01381-8; the check digit does not need to be re-calculated.'b'Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):'b'A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]'b'ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[15] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.'b'Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[16] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[17]'b'Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [18]'b'Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Biblioth\xc3\xa8que et Archives nationales du Qu\xc3\xa9bec.'b'Colombia: C\xc3\xa1mara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.'b'Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[19]'b'India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[20] There is no fee associated in getting ISBN in India.[21]'b'Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[22] The original national prefix 978-88 is reserved for publishing companies, starting at \xe2\x82\xac49 for a ten-codes block[23] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of \xe2\x82\xac25 for a single code.'b'Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]'b'Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[24][25][26]'b'Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.'b'New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[27]'b'Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.'b'Philippines: The National Library of the Philippines is responsible for ISBN registrations for Philippine publishers, authors, universities, institutions, and government departments that are responsible for publishing books. As of 2017[update], a fee of \xe2\x82\xb1120.00 per title was charged for the issuance of an ISBN.[28]'b'South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.'b'United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from \xc2\xa3120 (plus VAT) for the smallest block on a standard turnaround of ten days.[29]'b'United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[5] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125 for a single number. Access is immediate when requested via their website.[30]'b'Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.'b" The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[31] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0\xe2\x80\x935, 600\xe2\x80\x93621, 7, 80\xe2\x80\x9394, 950\xe2\x80\x93989, 9926\xe2\x80\x939989, and 99901\xe2\x80\x9399976.[32] Books published in rare languages typically have longer group identifiers.[33]"b'Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[34]'b'The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.'b'The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]'b'A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (\xe2\x82\xac1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[35] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.'b'Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.'b'By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[36] Here are some sample ISBN-10 codes, illustrating block length variations.'b'English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[37]'b'A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without \xe2\x80\x93 the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".'b'The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[38] \xe2\x80\x93 which is the last digit of the ten-digit ISBN \xe2\x80\x93 must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.'b'For example, for an ISBN-10 of 0-306-40615-2:'b'Formally, using modular arithmetic, we can say:'b"It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:"b'Formally, we can say:'b"The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN\xc2\xa0\xe2\x80\x93 the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[39]"b'In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).'b'Each of the first nine digits of the ten-digit ISBN\xe2\x80\x94excluding the check digit itself\xe2\x80\x94is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.'b'For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:'b'Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 \xe2\x80\x93 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)'b'For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:'b'Thus the check digit is 2.'b'It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:'b'The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.'b"The 2005 edition of the International ISBN Agency's official manual[40] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10."b'Formally, using modular arithmetic, we can say:'b'The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.'b'For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:'b'Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.'b'In general, the ISBN-13 check digit is calculated as follows.'b'Let'b'Then'b'This check system\xc2\xa0\xe2\x80\x93 similar to the UPC check digit formula\xc2\xa0\xe2\x80\x93 does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3\xc3\x976+1\xc3\x971 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3\xc3\x971+1\xc3\x976 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.'b'Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).'b'The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.'b'Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[41] For example, ISBN\xc2\xa00-590-76484-5 is shared by two books \xe2\x80\x93 Ninja gaiden\xc2\xae: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.'b'Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[42] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.'b'Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[43]'b'Currently the barcodes on a book\'s back cover (or inside a mass-market paperback book\'s front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[44] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).'b'Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[45] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.'b'Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[46]'b'Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.'Book sources
b'This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter. In Wikipedia, numbers preceded by "ISBN" link directly to this page.\n'b'This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). \n'b'Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).\n'b'Alabama\n'b'California\n'b'Colorado\n'b'Delaware\n'b'Florida\n'b'Georgia\n'b'Indiana\n'b'Iowa \n'b'Kansas\n'b'Kentucky\n'b'Massachusetts\n'b'Michigan\n'b'Minnesota\n'b'Missouri\n'b'Nebraska\n'b'New Jersey\n'b'New Mexico\n'b'New York\n'b'North Carolina\n'b'Ohio\n'b'Oklahoma\n'b'Oregon\n'b'Pennsylvania\n'b'Rhode Island\n'b'South Carolina\n'b'South Dakota\n'b'Tennessee\n'b'Texas\n'b'Utah\n'b'Washington state\n'b'Wisconsin\n'b'\n'b'Find your book on a site that compiles results from other online sites:\n'b'These sites allow you to search the catalogs of many individual booksellers:\n'b'\n'b'If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below \xe2\x80\x93 they are more likely to have sources appropriate for that language.\n'b'These links produce citations in various referencing styles.\n'b"You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.\n"b'You can also convert between 10 and 13 digit ISBN numbers with these tools:\n'b'\nEdit this page\n'b'\n'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'PubMed Central
b"PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. As one of the major research databases within the suite of resources that have been developed by the National Center for Biotechnology Information (NCBI), PubMed Central is much more than just a document repository. Submissions into PMC undergo an indexing and formatting procedure which results in enhanced metadata, medical ontology, and unique identifiers which all enrich the XML structured data for each article on deposit.[1] Content within PMC can easily be interlinked to many other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to freely discover, read and build upon this portfolio of biomedical knowledge.[2]"b'PubMed Central should not be confused with PubMed. These are two very different services at their core.[3] While PubMed is a searchable database of biomedical citations and abstracts, the full-text article referenced in the PubMed record will physically reside elsewhere. (Sometimes in print, sometimes online, sometimes free, sometimes behind a toll-wall accessible only to paying subscribers). PubMed Central is a free digital archive of articles, accessible to anyone from anywhere via a basic web browser. The full text of all PubMed Central articles is free to read, with varying provisions for reuse.'b'As of December\xc2\xa02016[update], the PMC archive contained over 4.1 million articles,[4] with contributions coming directly from publishers or authors depositing their own manuscripts into the repository per the NIH Public Access Policy. Older data shows that from Jan 2013 \xe2\x80\x93 Jan 2014 author-initiated deposits exceeded 103,000 papers during this 12-month period.[5] PMC also identifies about 4,000 journals which now participate in some capacity to automatically deposit their published content into the PMC repository.[6] Some participating publishers will delay the release of their articles on PubMed Central for a set time after publication, this is often referred to as an "embargo period", and can range from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common). However, PubMed Central is a key example of "systematic external distribution by a third party"[7] which is still prohibited by the contributor agreements of many publishers.'b''b''b'Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[8]'b'A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.'b'The National Library of Medicine "NLM Journal Publishing Tag Set" journal article markup language is freely available.[9] The Association of Learned and Professional Society Publishers comments that "it is likely to become the standard for preparing scholarly content for both books and journals".[10] A related DTD is available for books.[11] The Library of Congress and the British Library have announced support for the NLM DTD.[12] It has also been popular with journal service providers.[13]'b'With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[14] This includes NASA content, with the interface branded as "PubSpace".[15][16]'b'Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).'b'Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[17]'b'Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers\' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come "live" when the resources become available.'b'An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.'b'When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.'b'In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.'b'Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[18] to cautious concern by others.[19] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version-of-record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[20] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[21]'b'The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[22] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[23]'b'The change in procedure has received criticism.[24] The American Physiological Society has expressed reservations about the implementation of the policy.[25]'b'The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of "PMC" followed by a string of seven numbers. The format is:[26]'b'Authors applying for NIH awards must include the PMCID in their application.'PubMed
b'PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.'b'From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]'b'In addition to MEDLINE, PubMed provides access to:'b'Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]'b'Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]'b'As of 11\xc2\xa0July\xc2\xa02017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed\'s records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950s to 16% in 2016.[8] Other significant proportion of records correspond to \xe2\x80\x9cChemistry\xe2\x80\x9d (8.69%), \xe2\x80\x9cTherapy\xe2\x80\x9d (8.39%) and "Infection" (5%).'b'In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]'b"Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window."b"PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms."b'The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:'b'Likewise,'b"A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11] By default the results are sorted by Most Recent, but this changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.[12]"b'For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.[13][14]'b'When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).'b'Publication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.[15]'b'Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]'b'A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the \'Find related data\' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The \'related articles\' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.[18]'b'PubMed automatically links to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes).[19] This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.[19]'b'The PubMed optional facility "My NCBI" (with free registration) provides tools for'b'and a wide range of other options.[20] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[21]'b'LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[22] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March\xc2\xa02010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[23] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.'b'In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[24] In February 2018, PubMed Commons was discontinued due to the fact that "usage has remained minimal".[25][26]'b'PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[27] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[28]'b'askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[29]'b'A PMID (PubMed identifier or PubMed unique identifier)[30] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[31]'b'The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.'b'The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[32] As of October\xc2\xa02008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[33] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.'b'Lu[33] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:'b'As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[33] Without the need to store about 90\xc2\xa0GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[47]'b'Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950s to 16% in 2016.[48]'b'The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.[49]'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Natural-language processing
b'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language\xc2\xa0data.'b'Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.'b''b''b'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.'b'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.'b'Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".'b'During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.'b"Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks."b'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.'b'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.'b'In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'b'Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.'b'Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).'b'Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.'b'Systems based on machine-learning algorithms have many advantages over hand-produced rules:'b'The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.'b'Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.'b''Statistical model
b'A statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.'b'The assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.'b'A statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Ad\xc3\xa8r quoting Kenneth Bollen).[1]'b'All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.'b''b''b'Suppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: heighti\xc2\xa0= b0\xc2\xa0+ b1agei\xc2\xa0+ \xce\xb5i, where b0 is the intercept, b1 is a parameter that age is multiplied by in obtaining a prediction of height, \xce\xb5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.'b'An admissible model must be consistent with all the data points. Thus, a straight line (heighti\xc2\xa0= b0\xc2\xa0+ b1agei) cannot be the equation for a model of the data. The line cannot be the equation for a model, unless it exactly fits all the data points\xe2\x80\x94i.e. all the data points lie perfectly on the line. The error term, \xce\xb5i, must be included in the equation, so that the model is consistent with all the data points.'b'To do statistical inference, we would first need to assume some probability distributions for the \xce\xb5i. For instance, we might assume that the \xce\xb5i distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.'b'A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, \xce\xb5 is a stochastic variable; without that variable, the model would be deterministic.'b'Statistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).'b'There are three purposes for a statistical model, according to Konishi\xc2\xa0& Kitagawa.[4]'b'As an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that'b'In this example, the dimension, k, equals 2.'b'As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)'b'Parametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, "These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies".[5]'b'Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model'b'has, nested within it, the linear model'b'\xe2\x80\x94we constrain the parameter b2 to equal 0.'b'In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\xc2\xa01). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.'b'Models can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.'b'Konishi & Kitagawa state: "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models."[6] Relatedly, Sir David Cox has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".[7]'Bioinformatics
b'Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]'b''b''b'Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]'b'Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]'b'Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]'b'To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:'b'The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\xe2\x80\x93protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.'b'Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.'b'Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.'b'Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.'b'Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.'b'Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.'b'Since the Phage \xce\xa6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.'b'Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.'b'Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.'b'In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.'b'The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.'b'Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).'b'Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:'b'Future work endeavours to reconstruct the now more complex tree of life.'b'The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.'b'The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.'b'Many of these studies are based on the homology detection and protein families computation.[22]'b'Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]'b"With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]"b'In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.'b'Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]'b'With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]'b'Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.'b'The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.'b'Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]'b'Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.'b'For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.'b'Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.'b'Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.'b'Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.'b'The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]'b'Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]'b'Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \xe2\x80\x93 a.k.a. Mad Cow Disease \xe2\x80\x93 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]'b"One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably."b'One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]'b'Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.'b'Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\xe2\x80\x93protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.'b'Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.'b'Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\xe2\x80\x93protein interactions only based on these 3D shapes, without performing protein\xe2\x80\x93protein interaction experiments. A variety of methods have been developed to tackle the protein\xe2\x80\x93protein docking problem, though it seems that there is still much work to be done in this field.'b'Other interactions encountered in the field include Protein\xe2\x80\x93ligand (including drug) and protein\xe2\x80\x93peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.'b'The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:'b'The area of research draws from statistics and computational linguistics.'b"Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:"b'Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.'b'Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.'b'Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.'b'The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.'b'Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.'b'Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.'b'Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.'b'Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.'b'The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]'b'An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]'b'SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.'b'Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.'b'A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to'b'Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.'b'In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.'b'It was decided that the BioCompute paradigm would be in the form of digital \xe2\x80\x98lab notebooks\xe2\x80\x99 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]'b'In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \xe2\x80\x9cstandard trial use\xe2\x80\x9d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]'b'Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\xcf\x80 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\xcf\x80 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\xcf\x80 operating system.[51][52]'b"MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications."b'There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).'b''Probabilistic latent semantic analysis
b'Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.'b'Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.'b''b''b'Their parameters are learned using the EM algorithm.'b'PLSA may be used in a discriminative setting, via Fisher kernels.[1]'b'PLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.'b'It is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.[2]'b'This is an example of a latent class model (see references therein), and it is related[5][6] to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.[7]'Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Natural-language processing
b'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language\xc2\xa0data.'b'Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.'b''b''b'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.'b'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.'b'Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".'b'During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.'b"Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks."b'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.'b'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.'b'In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'b'Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.'b'Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).'b'Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.'b'Systems based on machine-learning algorithms have many advantages over hand-produced rules:'b'The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.'b'Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.'b''Statistical model
b'A statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.'b'The assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.'b'A statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Ad\xc3\xa8r quoting Kenneth Bollen).[1]'b'All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.'b''b''b'Suppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: heighti\xc2\xa0= b0\xc2\xa0+ b1agei\xc2\xa0+ \xce\xb5i, where b0 is the intercept, b1 is a parameter that age is multiplied by in obtaining a prediction of height, \xce\xb5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.'b'An admissible model must be consistent with all the data points. Thus, a straight line (heighti\xc2\xa0= b0\xc2\xa0+ b1agei) cannot be the equation for a model of the data. The line cannot be the equation for a model, unless it exactly fits all the data points\xe2\x80\x94i.e. all the data points lie perfectly on the line. The error term, \xce\xb5i, must be included in the equation, so that the model is consistent with all the data points.'b'To do statistical inference, we would first need to assume some probability distributions for the \xce\xb5i. For instance, we might assume that the \xce\xb5i distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.'b'A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, \xce\xb5 is a stochastic variable; without that variable, the model would be deterministic.'b'Statistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).'b'There are three purposes for a statistical model, according to Konishi\xc2\xa0& Kitagawa.[4]'b'As an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that'b'In this example, the dimension, k, equals 2.'b'As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)'b'Parametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, "These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies".[5]'b'Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model'b'has, nested within it, the linear model'b'\xe2\x80\x94we constrain the parameter b2 to equal 0.'b'In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\xc2\xa01). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.'b'Models can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.'b'Konishi & Kitagawa state: "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models."[6] Relatedly, Sir David Cox has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".[7]'Bioinformatics
b'Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]'b''b''b'Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]'b'Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]'b'Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]'b'To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:'b'The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\xe2\x80\x93protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.'b'Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.'b'Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.'b'Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.'b'Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.'b'Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.'b'Since the Phage \xce\xa6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.'b'Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.'b'Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.'b'In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.'b'The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.'b'Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).'b'Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:'b'Future work endeavours to reconstruct the now more complex tree of life.'b'The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.'b'The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.'b'Many of these studies are based on the homology detection and protein families computation.[22]'b'Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]'b"With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]"b'In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.'b'Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]'b'With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]'b'Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.'b'The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.'b'Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]'b'Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.'b'For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.'b'Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.'b'Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.'b'Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.'b'The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]'b'Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]'b'Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \xe2\x80\x93 a.k.a. Mad Cow Disease \xe2\x80\x93 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]'b"One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably."b'One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]'b'Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.'b'Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\xe2\x80\x93protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.'b'Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.'b'Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\xe2\x80\x93protein interactions only based on these 3D shapes, without performing protein\xe2\x80\x93protein interaction experiments. A variety of methods have been developed to tackle the protein\xe2\x80\x93protein docking problem, though it seems that there is still much work to be done in this field.'b'Other interactions encountered in the field include Protein\xe2\x80\x93ligand (including drug) and protein\xe2\x80\x93peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.'b'The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:'b'The area of research draws from statistics and computational linguistics.'b"Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:"b'Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.'b'Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.'b'Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.'b'The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.'b'Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.'b'Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.'b'Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.'b'Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.'b'The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]'b'An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]'b'SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.'b'Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.'b'A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to'b'Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.'b'In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.'b'It was decided that the BioCompute paradigm would be in the form of digital \xe2\x80\x98lab notebooks\xe2\x80\x99 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]'b'In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \xe2\x80\x9cstandard trial use\xe2\x80\x9d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]'b'Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\xcf\x80 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\xcf\x80 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\xcf\x80 operating system.[51][52]'b"MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications."b'There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).'b''Probabilistic latent semantic analysis
b'Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.'b'Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.'b''b''b'Their parameters are learned using the EM algorithm.'b'PLSA may be used in a discriminative setting, via Fisher kernels.[1]'b'PLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.'b'It is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.[2]'b'This is an example of a latent class model (see references therein), and it is related[5][6] to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.[7]'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'David Blei
b'David M. Blei is a Professor in the Statistics and Computer Science departments at Columbia University. Prior to fall 2014 he was an Associate Professor in the Department of Computer Science at Princeton University. His work is primarily in machine learning.'b''b''b'His research interests include topic models and he was one of the original developers of latent Dirichlet allocation. As of October 25, 2017, his publications have been cited 50,850 times, giving him an h-index of 64.[1]'b'He was named Fellow of ACM "For contributions to the theory and practice of probabilistic topic modeling and Bayesian machine learning" in 2015.[2]'b''Andrew Ng
b"Andrew Yan-Tak Ng (Chinese: \xe5\x90\xb3\xe6\x81\xa9\xe9\x81\x94; born 1976) is a Chinese American computer scientist. He is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor (formerly associate professor) at Stanford University. Ng is also the co-founder and chairman of Coursera, an online education platform.[2]"b''b''b"Ng was born in the UK in 1976. His parents were both from Hong Kong. He spent time in Hong Kong and Singapore[1] and later graduated from Raffles Institution in Singapore in 1992. In 1997, he received his undergraduate degree in computer science from Carnegie Mellon University in Pittsburgh, Pennsylvania. Ng earned his master's degree from Massachusetts Institute of Technology in Cambridge, Massachusetts in 1998 and received his PhD from University of California, Berkeley in 2002. He started working at Stanford University during that year and currently lives in Palo Alto, California. He married Carol E. Reiley in 2014.[3]"b"Andrew was a professor at Stanford University Department of Computer Science and Department of Electrical Engineering. He became Director of the Stanford Artificial Intelligence Lab where he taught students and undertook research related to data mining and machine learning. From 2011 to 2012, he worked at Google, where he founded and led the Google Brain Deep Learning Project. In 2012, he co-founded Coursera to offer free online courses for everyone after over 100,000 students registered for Ng's popular course.[4] Today, several million people have taken the online course. In 2014, he joined[5] Baidu as Chief Scientist, and carried out research related to big data and A.I. In March 2017, he announced his resignation from Baidu.[6]"b'He soon afterwards launched Deeplearning.ai,[7] an online curriculum of classes. Then Ng launchedLanding.ai[8], bringing AI to manufacturing factories, announcing a partnership with FoxConn.[9]'b'In 2018, Ng unveiled the AI Fund,[10] raising $175 million to invest in new startups. He is also the chairman of Woebot and on the board of drive.ai.[11][12]'b'Ng researches primarily in machine learning and deep learning. His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world,[13][14] and the STAIR (STanford Artificial Intelligence Robot) project,[15] which resulted in ROS, a widely used open-source robotics software platform.'b'In 2011, Ng founded the Google Brain project at Google, which developed very large scale artificial neural networks using Google\'s distributed computer infrastructure.[16] Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, that learned to recognize higher-level concepts, such as cats, after watching only YouTube videos, and without ever having been told what a "cat" is.[17][18] The project\'s technology is currently also used in the Android Operating System\'s speech recognition system.[19]'b'He together with David M. Blei and Michael I. Jordan, coauthored the influential paper that introduced Latent Dirichlet allocation.[20]'b'Ng started the Stanford Engineering Everywhere (SEE) program, which in 2008 placed a number of Stanford courses online, for free. Ng taught one of these courses, Machine Learning, which consisted of video lectures by him, along with the student materials used in the Stanford CS229 class.'b'The "applied" version of the Stanford class (CS229a) was hosted on ml-class.org and started in October 2011, with over 100,000 students registered for its first iteration; the course featured quizzes and graded programming assignments and became one of the first successful MOOCs made by Stanford professors.[22] His work subsequently led to the founding of Coursera in 2012.'b"Ng is also the author or co-author of over 100 published papers in machine learning, robotics, and related fields. His work in computer vision and deep learning has been frequently featured in press releases and reviews.[23] In 2008, he was named to the MIT Technology Review TR35 as one of the top 35 innovators in the world under the age of 35.[24][25] Ng was awarded a Sloan Fellowship (2007). For his work in artificial intelligence, he is also a recipient of the Computers and Thought Award (2009). In 2013 at the age of 37, he was named one of Times 100 Most Influential People[26] and Fortune's 40 under 40.[27]"Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Dirichlet distribution
b'The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process.'b''b''b'The Dirichlet distribution of order K\xc2\xa0\xe2\x89\xa5\xc2\xa02 with parameters \xce\xb11, ..., \xce\xb1K >\xc2\xa00 has a probability density function with respect to Lebesgue measure on the Euclidean space RK\xe2\x88\x921 given by'b'The normalizing constant is the multivariate Beta function, which can be expressed in terms of the gamma function:'b'When \xce\xb1=1[2], the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard (K\xc2\xa0\xe2\x88\x92\xc2\xa01)-simplex, i.e. it is uniform over all points in its support. This particular distribution is known as the flat Dirichlet distribution. Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.'b'Let'b'Then[3][4]'b'Note that the matrix so defined is singular.'b'More generally, moments of Dirichlet-distributed random variables can be expressed as[5]'b'The mode of the distribution is[6] the vector (x1, ..., xK) with'b'The marginal distributions are beta distributions:[7]'b"The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations). This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet. Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one. This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties."b'Formally, this can be expressed as follows. Given a model'b'then the following holds:'b'This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector \xce\xb1 as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution.'b'In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models. See the section on applications below for more information.'b'In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out) is a Dirichlet-multinomial distribution. This distribution plays an important role in hierarchical Bayesian models, because when doing inference over such models using methods such as Gibbs sampling or variational Bayes, Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.'b'and'b'If'b'then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum,'b'The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series. It is given by Phillips[11] as'b'For K independently distributed Gamma distributions:'b'we have:[13]:402'b'Although the Xis are not independent from one another, they can be seen to be generated from a set of K independent gamma random variable.[13]:594 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.'b'Because the Dirichlet distribution is an exponential family distribution it has a conjugate prior. The conjugate prior is of the form:[14]'b'Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models. (Note that in many fields, such as in natural language processing, categorical variables are often imprecisely called "multinomial variables". Such a usage is liable to cause confusion, just as if Bernoulli distributions and binomial distributions were commonly conflated.)'b'Inference over hierarchical Bayesian models is often done using Gibbs sampling, and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable. This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution, conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters). One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.'b'and then set'b'The Jacobian now looks like'b'The determinant can be evaluated by noting that it remains unchanged if multiples of a row are added to another row, and adding each of the first K-1 rows to the bottom row to obtain'b'Substituting for x in the joint pdf and including the Jacobian, one obtains:'b'Which is equivalent to'b'Below is example Python code to draw the sample:'b'This formulation is correct regardless of how the Gamma distributions are parameterized (shape/scale vs. shape/rate) because they are equivalent when scale and rate equal 1.0.'b'and let'b'Finally, set'b'This iterative procedure corresponds closely to the "string cutting" intuition described below.'b'Below is example Python code to draw the sample:'b'Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value \xce\xb1 to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.'b'One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. The \xce\xb1/\xce\xb10 values specify the mean lengths of the cut pieces of string resulting from the distribution. The variance around this mean varies inversely with \xce\xb10.'b'Consider an urn containing balls of K different colors. Initially, the urn contains \xce\xb11 balls of color 1, \xce\xb12 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir(\xce\xb11,...,\xce\xb1K).[16]'b'For a formal proof, note that the proportions of the different colored balls form a bounded [0,1]K-valued martingale, hence by the martingale convergence theorem, these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.'b'Note that each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls.'Pachinko allocation
b'In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents. [1] The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive power than latent Dirichlet allocation.[2] While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The model is named for pachinko machines\xe2\x80\x94a game popular in Japan, in which metal balls bounce down around a complex collection of pins until they land in various bins at the bottom.[3]'b''b''b"Pachinko allocation was first described by Wei Li and Andrew McCallum in 2006.[3] The idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007.[4] In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP).[2] The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst."b'\nPAM connects words in V and topics in T with an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the interior levels and the leaves are words.'b'The probability of generating a whole corpus is the product of the probability for every document:'b''File:Topic model scheme.webm
b'https://creativecommons.org/licenses/by-sa/4.0 CC BY-SA 4.0 Creative Commons Attribution-Share Alike 4.0 truetrue'b'Click on a date/time to view the file as it appeared at that time.\n'b'The following other wikis use this file:\n'Pennsylvania Gazette
b"The Pennsylvania Gazette was one of the United States' most prominent newspapers from 1728, before the time period of the American Revolution, until 1800."b''b''b"The newspaper was first published in 1728 by Samuel Keimer and was the second newspaper to be published in Pennsylvania under the name The Universal Instructor in all Arts and Sciences: and Pennsylvania Gazette, alluding to Keimer's intention to print out a page of Ephraim Chambers' Cyclopaedia, or Universal Dictionary of Arts and Sciences in each copy.[1] On October 2, 1729, Benjamin Franklin and Hugh Meredith bought the paper and shortened its name, as well as dropping Keimer's grandiose plan to print out the Cyclopaedia.[1] Franklin not only printed the paper but also often contributed pieces to the paper under aliases. His newspaper soon became the most successful in the colonies."b'On August 6, 1741 Franklin published an editorial about deceased Andrew Hamilton, a lawyer and public figure in Philadelphia who had been a friend. The editorial praised the man highly and showed Franklin had held the man in high esteem.[2]'b'In 1752, Franklin published a third-person account of his pioneering kite experiment in The Pennsylvania Gazette, without mentioning that he himself had performed it.[3]'b'Primarily a publication for classified ads, merchants and individuals listed notices of employment, lost and found goods and items for sale; the newspaper also reprinted foreign news. Most entries involved stories of travel.[4] In the July 31, 1776 edition the front page lists military movements announced by John Hancock alongside the sale of a plantation in Chester County, Pa., a three-dollar reward for a horse that strayed from home, three pounds reward for the return of a fleeing 23-year-old Irish servant named Jane Stepberd, three pounds reward for runaway Negroe man Moses Graves and the sale of an unnamed "hearty Scotch Servant GIRL".'b"This newspaper, among other firsts, would print the first political cartoon in America, Join, or Die, authored by Franklin himself.[5] It ceased publication in 1800, ten years after Franklin's death.[6]"b'It is claimed that the publication later reemerged as the Saturday Evening Post in 1821.[7]'b'There are three known copies of the original issue, which are held by the Historical Society of Pennsylvania, the Library Company of Philadelphia, and the Wisconsin State Historical Society.[1]'b'Today, The Pennsylvania Gazette moniker is used by an unrelated bi-monthly alumni magazine of the University of Pennsylvania, which Franklin founded and served as a trustee.'b'Archives are available online for a fee.[6]'b'President of Pennsylvania (1785\xe2\x80\x931788), Ambassador to France (1779\xe2\x80\x931785)'Proceedings of the National Academy of Sciences of the United States of America
b'Proceedings of the National Academy of Sciences of the United States of America (PNAS) is the official scientific journal of the National Academy of Sciences, published since 1915. With broad coverage, spanning the biological, physical, and social sciences, the journal publishes original research alongside scientific reviews, commentaries, and letters. In 1999\xe2\x80\x932009, the last period for which data are available, PNAS was the second most cited journal across all fields of science.[1] PNAS is published weekly in print, and daily online in PNAS Early Edition.'b''b''b'PNAS was established by the National Academy of Sciences (NAS) in 1914, with its first issue published in 1915. The NAS itself had been founded in 1863 as a private institution, but chartered by the United States Congress, with the goal to "investigate, examine, experiment, and report upon any subject of science or art". By 1914 the Academy had been well established.'b"Prior to the inception of PNAS, the National Academy of Sciences published three volumes of organizational transactions, consisting mostly of minutes of meetings and annual reports. In accordance with the guiding principles established by astronomer George Ellery Hale, the foreign secretary of NAS in 1914, PNAS publishes brief first announcements of Academy members' and foreign associates' more important contributions to research and of work that appears to a member to be of particular importance.[2]"b'The following people have been editors-in-chief of the journal:'b'The first managing editor of the journal was mathematician Edwin Bidwell Wilson.'b'All research papers published in PNAS are peer-reviewed.[2] The standard mode is for papers to be submitted directly to PNAS rather than going through an Academy member. Members may handle the peer review process for up to 4 of their own papers per year\xe2\x80\x94this is an open review process because the member selects and communicates directly with the referees. These submissions and reviews, like all for PNAS, are evaluated for publication by the PNAS Editorial Board. Until July 1, 2010, members were allowed to communicate up to 2 papers from non-members to PNAS every year. The review process for these papers was anonymous in that the identities of the referees were not revealed to the authors. Referees were selected by the NAS member.[2][4][5] PNAS eliminated communicated submissions through NAS members as of July 1, 2010, while continuing to make the final decision on all PNAS papers.[6]'b'In 2003, PNAS issued an editorial stating its policy on publication of sensitive material in the life sciences.[7] PNAS stated that it would "continue to monitor submitted papers for material that may be deemed inappropriate and that could, if published, compromise the public welfare." This statement was in keeping with the efforts of several other journals.[8][9] In 2005 PNAS published an article titled "Analyzing a bioterror attack on the food supply: The case of botulinum toxin in milk"[10] despite objections raised by the U.S. Department of Health and Human Services.[11] The paper was published with a commentary by the president of the Academy at the time, Bruce Alberts, titled "Modeling attacks on the food supply".[12]'b'PNAS is widely read by researchers, particularly those involved in basic sciences, around the world. PNAS Online receives over 21 million hits per month.[13] The journal is notable for its policy of making research articles freely available online to everyone six months after publication (delayed open access), or immediately if authors have chosen the "open access" option (hybrid open access). Immediately free online access (without the six-month delay) is available to more than 100 developing countries[14] and for some categories of papers such as colloquia. Abstracts, tables of contents, and online supporting information are free. Anyone can sign up to receive free tables of contents by email.[15]'b'Because PNAS is self-sustaining and receives no direct funding from the U.S.\xc2\xa0government or the National Academy of Sciences, the journal charges authors publication fees and subscription fees to offset the cost of the editorial and publication process.'b'According to the Journal Citation Reports, the journal has a 2015 impact factor of 9.423.[16] PNAS is the second most cited scientific journal, with nearly 1.4\xc2\xa0million citations from 1999 to 2009 (the Journal of Biological Chemistry is the most cited journal over this period).[17]'b'PNAS has received occasional criticism for releasing papers to science journalists as much as a week before making them available to the general public; this practice is known as a news embargo.[18] According to critics, this allows mainstream news outlets to misrepresent or exaggerate the implications of experimental findings before the scientific community is able to respond.[19][20] Science writer Ed Yong, on the other hand, has argued that the real problem is not embargoes themselves, but the press releases issued by research institutes and universities.[18]'b'In January 2011, PNAS started considering manuscripts for exclusive online publication, "PNAS Plus" papers.[21] These have a larger maximum page limit (10 rather than 6 pages). Accompanying these papers both online and in print was a one- to two-page summary description written by the authors for a broad readership. Since mid-October 2012, PNAS Plus authors no longer need to submit author summaries and are instead asked to submit a 120-word-maximum statement about the significance of their paper. The significance statement will appear both online and in print.[22] Since July 15, 2013, the significance statement is required for all research articles.'b"In 2006 PNAS launched a new section of the journal dedicated to sustainability science, an emerging field of research dealing with the interactions between natural and social systems, and with how those interactions affect the challenge of sustainability: meeting the needs of present and future generations while substantially reducing poverty and conserving the planet's life support systems. See the Sustainability Science portal here."Richmond Times-Dispatch
b'The Richmond Times-Dispatch (RTD or TD for short) is the primary daily newspaper in Richmond, the capital of Virginia, United States. It is also the primary newspaper of record for the state of Virginia.[2][3][4]'b''b''b"The Times-Dispatch has the second-highest circulation of any Virginia newspaper, after Norfolk's The Virginian-Pilot.[5] In addition to the Richmond area (Petersburg, Chester, Hopewell, Colonial Heights and surrounding areas), the Times-Dispatch has substantial readership in Charlottesville, Lynchburg, and Waynesboro. As the primary paper of the state's capital, the Times-Dispatch serves as a newspaper of record for rural regions of the state that lack large local papers."b'Although the Richmond Compiler published in Virginia\'s capitol beginning in 1815, and merged with a later newspaper called The Times, the Times and Compiler failed in 1853, despite an attempt of former banker James A. Cowardin and William H. Davis to revive it several years before. In 1850, Cowardin and Davis established a rival newspaper called the Richmond Dispatch, and by 1852 the Dispatch bragged of having circulation three times as large as any other daily paper in the city, and advertising dominated even its front page. Cowardin began his only term in the Virginia House of Delegates (as a Whig) in 1853, but many thought the city\'s pre-eminent paper the Richmond Examiner.[6] John Hammersley bought half of the newspaper company in 1859, and continued as a joint publisher on the masthead until May 5, 1862, when no name appeared. By April 1861, the newspaper announced its circulation was \xe2\x80\x9cwithin a fraction of 13,000.\xe2\x80\x9d[7] The newspaper had been staunchly pro-slavery since 1852, and called Union soldiers "thieves and cut-throats".[8] Most of its wartime issues are now available online.[9] In 1864, Hammersley brought new presses from England, having run the Union blockade, although he sold half his interest to James W. Lewellen before his dangerous departure (presumably through Wilmington, North Carolina, the last Southern port open to Confederate vessels in 1864).'b'The Richmond Daily Dispatch published its last wartime issue on April 1, 1865; and its office was destroyed the next night during the fire set by Confederate soldiers as they left the city. However, it resumed publication on December 9, 1865, establishing a new office at 12th and Main Streets and accepting Henry K. Ellyson as part-owner as well as editor.[10] By 1866, the Dispatch was one of five papers "carrying prestige from ante bellum days" published in Richmond (of 7 newspapers). Although the newspaper initially opposed the Ku Klux Klan, the Richmond Dispatch accepted Klan advertising in 1868, as it fought Congressional Reconstruction and the Virginia Constitutional Convention of 1868. However, it later accepted the resulting state constitution (after anti-Confederate provisions were stripped) as well as allowing Negroes on juries and in the legislature. Ellyson briefly served as Richmond\'s mayor in 1870, selected by Richmond\'s city council appointed by Governor Gilbert C. Walker. After what some called the "Municipal War" because the prior appointed mayor George Chahoon refused to relinquish his office and mob violence and blockades, the Virginia Supreme Court declared Ellyson the mayor but awaited elections. After skullduggery concerning stolen ballots in the pro-Chahoon Jackson Ward and the election commission declared Ellyson the winner, he refused to serve under the resulting cloud, leading to yet another problematic election won by the Conservative Party candidate. The revived Dispatch later opposed former Confederate General William Mahone and his Readjuster Party.[11] After James Cowardin died in 1882, his son Charles took the helm (with Ellyson\'s assistance, and with Ellyson family members handling business operations), and the paper stopped supporting Negro rights, instead criticizing Del. John Mercer Langston with racial stereotypes.[12]'b"In 1886, Lewis Ginter founded the Richmond Daily Times. A year later, lawyer Joseph Bryan (1845-1908) bought the Daily Times from Ginter, beginning the paper's long association with the Bryan family. Bryan and Ginter had previously helped revitalize the Tanner & Delany Engine Company, transforming it into the Richmond Locomotive Works, which had 800 employees by 1893 and built 200 locomotives per year. In 1890, the Daily Times changed its name to the Richmond Times. In 1896, Bryan acquired the eight-year-old rival Manchester Leader and launched the Evening Leader. In 1899, the evening Richmond News was founded. John L. Williams, owner of the Dispatch, bought the News in 1900."b'By 1903, it was obvious Richmond was not big enough to support four papers. That year, Williams and Bryan agreed to merge Richmond\'s main newspapers. The morning papers merged to become the Richmond Times-Dispatch under Bryan\'s ownership, while the evening papers merged to become The Richmond News Leader under Williams\' ownership. Bryan bought the News Leader in 1908, but died later that year. (Joseph Bryan Park was donated by his widow, Isobel ("Belle") Stewart Bryan, and named for him).'b"His son John Stewart Bryan had given up his own legal career in 1900 to become a reporter working for the Dispatch and helped found the Associated Press and then became vice-president of the publishing company.[13] Upon his father's death, John Stewart Bryan became owner and publisher of the two papers, but in 1914 sold a controlling interest in the Times-Dispatch to three families. He hired Douglas Southall Freeman as editor of the News Leader in 1915, and remained in control until becoming President of the College of William and Mary in 1934 (and publishing a biography of his father the following year). John Stewart Bryan but reacquired the Times-Dispatch in 1940 when the two papers' business interests merged to form Richmond Newspapers, in which Bryan held a 54-percent interest. That conglomeration is now known as Media General. Other publishers in the Bryan family include D. Tennant Bryan and John Stewart Bryan III."b'On June 1, 1992, four days after its sponsored contestant Amanda Goad won the Scripps National Spelling Bee, the News Leader, which had been losing circulation for many years, ceased publication and was folded into the Times-Dispatch.'b"The Richmond Times-Dispatch drew national attention for its coverage of a December 21, 2004, attack by a suicide bomber on an American military base in Mosul, Iraq. The deadliest attack on an American military installation since the war began, the attack injured 69 people and killed 22, including two with the Virginia National Guard's Richmond-based 276th Engineer Battalion. Stories and photographs about the attack by a Times-Dispatch reporter embedded with the 276th were read, heard and seen across the nation."b'In 1990, The RTD borrowed an idea [14] from a local entrepreneur, Barry "Mad Dog" Gottlieb, to encourage a "Tacky Christmas Lights Tour," also known by locals as the "Tacky Light Tour". Every week, the RTD lists the addresses of houses where the most tacky Christmas lights can be found. This tradition has begun to spread to other cities, like Fairfax, Virginia (DC area) [15] as well as San Francisco and Los Angeles.'b"Diane Cantor, the wife of former House Majority Leader Republican Eric Cantor, sits on Media General's Board of Directors.[16] This drew some conflict-of-interest allegations because the RTD serves much of the congressman's 7th district, but no evidence surfaced that she was involved in the paper's content. Her association with the paper was noted at the end of Times-Dispatch stories about Rep. Cantor."b"On May 17, 2012, Media General [17] announced the sale of its newspaper division to BH Media, a subsidiary of Warren Buffett's Berkshire Hathaway company. The sale included all of Media General's newspapers except The Tampa Tribune and its associated publications. Berkshire Hathaway bought 63 newspapers for $142 million and, as part of the deal, offered Media General a $400 million term loan at 10.5 percent interest that will mature in 2020 and a $45 million revolving line of credit. Berkshire Hathaway received a seat on Media General's board of directors and an option to purchase a 19.9% stake in the company.[18] The deal closed on June 25, 2012."b"This also brought to a close Diane Cantor's relationship with the RTD."b'A prominent newspaper in the state, the Times-Dispatch frequently features commentary from important figures from around Virginia, such as officials and presidents from Virginia Commonwealth University, the College of William and Mary, and the University of Virginia. Former Richmond Mayor Douglas Wilder, who had articles published in the paper before he held that position, often outlined policies his administration was implementing. During the 2004 U.S. presidential campaign, its Commentary sections featured some pieces by Retired Admiral Roy Hoffmann, a founding member of the Swift Boat Veterans for Truth and resident of Richmond suburb Chesterfield, against Democratic candidate John Kerry.'b"Editorially, the Times-Dispatch has historically leaned conservative, leading the paper to frequently endorse candidates of the Republican Party. It supported many of former President George W. Bush's policies, including the 2003 invasion of Iraq and a flat income tax. However, the paper is not unilaterally conservative; for example, a 2005 editorial called for the then House Majority Leader Tom DeLay to relinquish his leadership position on ethical grounds. There are also some liberal syndicated columnists who appear frequently, especially Leonard Pitts."b'During the Civil Rights Movement, the Times-Dispatch, like nearly every major newspaper in Virginia, was an ardent supporter of segregation.[19]'b"In the 2016 presidential election, the Times-Dispatch endorsed Libertarian candidate Gary Johnson over major party candidates Donald Trump and Hillary Clinton. Clinton's running mate, Tim Kaine, is a Richmond resident who served as mayor of the city from 1998-2001. From at least 1980 until its Johnson endorsement in 2016, the Times-Dispatch had only endorsed Republican presidential candidates.[20]"b'Like most major papers, the sports section has MLB, NASCAR, MLS, NBA, NCAA, NFL, and NHL scores and results. The Times-Dispatch sports pages naturally focus on Richmond and Virginia professional and college teams. In addition to Richmond Flying Squirrels and Richmond Kickers coverage, readers can see in-depth coverage of the Washington Redskins in the fall and the Washington Nationals in the summer. "Virginians in the Pros" and similar features track all sorts of professional athletes who were born, lived in, or attended college in Virginia. Large automobile racing events like the Sprint Cup (at the Richmond International Raceway) are often given a separate preview guide.'b'Catering to the vast array of Virginia hunters, fishers, hikers, and outdoorsmen, somewhere between half a page to a whole page most days is dedicated to outdoors articles, written by Lee Graves, who succeeded Garvey Winegar in November 2003. The "Scoreboard," which features minor-league standings, Vegas betting, and other sports scores, also gives tide measurements, river levels, and skiing conditions, depending on the season.'b'Virginians have traditionally been highly supportive of high school athletics, and its flagship paper is a testament to that. Particular emphasis is given to American football and basketball; The Times-Dispatch ranks area teams in these sports, in the style of the NCAA polls, and generally updates them weekly. In the fall, Sunday editions have the scores of all high school football games played that weekend from across the state. Prep games are also receive above-average coverage in baseball, cross country, golf, lacrosse, soccer, softball, swimming, tennis, track and field, and volleyball. Stories are frequently done on notable prep athletes, such as those from foreign countries, those with disabilities, those who play a multitude of sports, or those who had little or no prior experience in a sport which they now excel in.'b'The business desk consists of six reporters; they cover technology, retail, energy, insurance, banking, economics, real estate, manufacturing, transportation and consumer issues. Unlike many newspapers, the Times-Dispatch produces a widely read Monday business section, Metro Business. It contains a center cover story on a regional business-related issue and is filled with events for the coming week, advice columnists and gadget reviews. In June 2006, the decision was made to remove the stock tables from the daily sections beginning July 15 and replace the numerous pages with a "Markets Review" section for subscribers who request it. The stock section was eliminated in 2009, as was the Sunday Real Estate section (both were cost-cutting moves). The Sunday Business section, which had been a showcase of general business-interest stories and features, has been rechristened Moneywise and now features primarily consumer-related coverage. Moneywise is also among select Sunday business sections nationwide that print Wall Street Journal Sunday pages.'b'On July 12, 2006, Richmond-based news magazine Style Weekly ran a cover story [21] titled "Truth and Consequences," a piece that took a look at the Times-Dispatch\'s operations as the paper settled into its first year with new management. The report described new editor Glenn Proctor, who took over Nov. 14, 2005, as an "inelegant, blunt and harsh critic \xe2\x80\x94 to the point of saying, repeatedly, that some reporters\' work \'sucks.\'" The piece described a newsroom teetering on the edge, preparing for promised changes \xe2\x80\x94 such as possible layoffs, fewer pages and combined sections \xe2\x80\x94 that eventually were realized. On April 2, 2009, the Times-Dispatch cut 90 jobs, laying off 59 workers, including 28 newsroom jobs. Proctor left the paper in 2011.'b'The front page of the Times-Dispatch\xe2\x80\x99s August 14, 2011 Sunday paper consisted entirely of a Wells Fargo advertisement, commemorating said bank\xe2\x80\x99s acquisition of Wachovia properties in Virginia.[22]'b'Notable columnists published include:'American Civil War
b'Union victory'b'2,200,000:[a]'b'750,000\xe2\x80\x931,000,000:[a][4]'b'110,000+ killed in action/died of wounds\n230,000+ accident/disease deaths[6][7]\n25,000\xe2\x80\x9330,000 died in Confederate prisons[2][6]'b'365,000+ total dead[8] 282,000+ wounded[7]\n181,193 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][9]'b'94,000+ killed in action/died of wounds[6]\n26,000\xe2\x80\x9331,000 died in Union prisons[7]'b'290,000+ total dead\n137,000+ wounded\n436,658 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][10]'b"The American Civil War (known by other names) was a civil war that was fought in the United States from 1861 to 1865. As a result of the long-standing controversy over slavery, war broke out in April 1861, when Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated. The nationalists of the Union proclaimed loyalty to the U.S. Constitution. They faced secessionists of the Confederate States, who advocated for states' rights to expand slavery."b'Among the 34 U.S. states in February 1861, seven Southern slave states individually declared their secession from the U.S. to form the Confederate States of America, or the South. The Confederacy grew to include eleven slave states. The Confederacy was never diplomatically recognized by the United States government, nor was it recognized by any foreign country (although the United Kingdom and France granted it belligerent status). The states that remained loyal to the U.S. (including the border states where slavery was legal) were known as the Union or the North.'b"The Union and Confederacy quickly raised volunteer and conscription armies that fought mostly in the South over four years. The Union finally won the war when General Robert E. Lee surrendered to General Ulysses S. Grant at the Battle of Appomattox Court House, followed by a series of surrenders by Confederate generals throughout the southern states. Four years of intense combat left 620,000 to 750,000 people dead, more than the number of U.S. military deaths in all other wars combined (at least until approximately the Vietnam War).[15] Much of the South's infrastructure was destroyed, especially the transportation systems, railroads, mills, and houses. The Confederacy collapsed, slavery was abolished, and 4 million slaves were freed. The Reconstruction Era (1863\xe2\x80\x931877) overlapped and followed the war, with the process of restoring national unity, strengthening the national government, and granting civil rights to freed slaves throughout the country. The Civil War is the most studied and written about episode in U.S. history.[16]"b''b''b"In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories. The Southern states viewed this as a violation of their constitutional rights and as the first step in a grander Republican plan to eventually abolish slavery. The three pro-Union candidates together received an overwhelming 82% majority of the votes cast nationally: Republican Lincoln's votes centered in the north, Democrat Stephen A. Douglas' votes were distributed nationally and Constitutional Unionist John Bell's votes centered in Tennessee, Kentucky, and Virginia. The Republican Party, dominant in the North, secured a plurality of the popular votes and a majority of the electoral votes nationally, so Lincoln was constitutionally elected president. He was the first Republican Party candidate to win the presidency. However, before his inauguration, seven slave states with cotton-based economies declared secession and formed the Confederacy. The first six to declare secession had the highest proportions of slaves in their populations, a total of 49 percent.[17] The first seven with state legislatures to resolve for secession included split majorities for unionists Douglas and Bell in Georgia with 51% and Louisiana with 55%. Alabama had voted 46% for those unionists, Mississippi with 40%, Florida with 38%, Texas with 25%, and South Carolina cast Electoral College votes without a popular vote for president.[18] Of these, only Texas held a referendum on secession."b'Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln\'s March 4, 1861, inaugural address declared that his administration would not initiate a civil war. Speaking directly to the "Southern States", he attempted to calm their fears of any threats to slavery, reaffirming, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so."[19] After Confederate forces seized numerous federal forts within territory claimed by the Confederacy, efforts at compromise failed and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene, but none did, and none recognized the new Confederate States of America.'b"Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, the battle was inconclusive from 1861\xe2\x80\x931862. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal.[20] To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies, and seized New Orleans. The 1863 Union Siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William T. Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years, known as the Reconstruction Era."b'The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships and iron-clad ships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I, World War II and subsequent conflicts. It remains the deadliest war in American history. From 1861 to 1865, it is estimated that 620,000 to 750,000 soldiers died,[21] along with an undetermined number of civilians.[b] By one estimate, the war claimed the lives of 10 percent of all Northern males 20\xe2\x80\x9345 years old, and 30 percent of all Southern white males aged 18\xe2\x80\x9340.[23]'b'The causes of secession were complex and have been controversial since the war began, but most academic scholars\xc2\xa0identify\xc2\xa0slavery as a central cause of the war. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war.[24] Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to promote pro-slavery acts and policies.[25][26]'b'Slavery was a major cause of disunion.[27] Although there were opposing views even in the Union States,[28][29] most northern soldiers were largely indifferent on the subject of slavery,[30] while Confederates fought the war largely to protect a southern society of which slavery was an integral part.[31] From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with republicanism. The strategy of the anti-slavery forces was containment\xe2\x80\x94to stop the expansion and thus put slavery on a path to gradual extinction.[32] The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights.[33] Southern whites believed that the emancipation of slaves would destroy the South\'s economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population.[34] In particular, southerners feared a repeat of "the horrors of Santo Domingo", in which nearly all white people \xe2\x80\x93 including men, women, children, and even many sympathetic to abolition \xe2\x80\x93 were killed after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea, and proposes it contributed to the segregation in the Jim Crow era following emancipation.[35] These fears were exacerbated by the recent attempts of John Brown to instigate an armed slave rebellion in the South.'b'Slavery was illegal in much of the North, having been outlawed in the late 18th and early 19th centuries. It was also fading in the border states and in Southern cities, but it was expanding in the highly profitable cotton districts of the rural South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide.'b"Sectionalism refers to the different economies, social structure, customs and political values of the North and South.[36][37] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention which manifested Northern dissastisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern Presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for poor freedmen. In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations.[38]"b'Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[39][40]'b'Historically, southern slave-holding states, because of their low-cost manual labor, had little perceived need for mechanization and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.[41]'b'The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress.[42][43] The tariff issue was and is sometimes cited\xe2\x80\x93long after the war\xe2\x80\x93by Lost Cause historians and neo-Confederate apologists. In 1860\xe2\x80\x9361 none of the groups that proposed compromises to head off secession raised the tariff issue.[44] Pamphleteers North and South rarely mentioned the tariff,[45] and when some did, for instance, Matthew Fontaine Maury[46] and John Lothrop Motley,[47] they were generally writing for a foreign audience.'b'The South argued that each state had the right to secede\xe2\x80\x94leave the Union\xe2\x80\x94at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a perpetual union.[48] Historian James McPherson writes concerning states\' rights and other non-slavery explanations:'b"While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[49]"b'Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. It was over territories west of the Mississippi that the proslavery and antislavery forces collided.[50]'b'With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well.[51][52] Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the southern states the question of the territorial expansion of slavery westward again became explosive.[53] Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."[54][55]'b'By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[56] The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.[57]'b'The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance\xe2\x80\x94that slavery could be excluded in a territory as it was done in the Northwest Ordinance of 1787 at the discretion of Congress,[58] thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.[59]'b'Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty\xe2\x80\x94which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter.[60] The Kansas\xe2\x80\x93Nebraska Act of 1854 legislated this doctrine.[61] In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when southern senators began to leave.[62]'b'The fourth theory was advocated by Mississippi Senator Jefferson Davis,[63] one of state sovereignty ("states\' rights"),[64] also known as the "Calhoun doctrine",[65] named after the South Carolinian political theorist and statesman John C. Calhoun.[66] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[67] "States\' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[68] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power."[69][70] These four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the U.S. Constitution prior to the 1860 presidential election.[71]'b'Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in the sense that their country was a national republic based on the belief that all people had inalienable political liberty and personal rights which could serve as an important example to the rest of the world. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, the division and redivision of the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state.'b'In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure\xe2\x80\x94first in their political institutions, then in their civil life together.'b'Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy.[72] C. Vann Woodward said of the latter group,'b'A great slave society\xc2\xa0... had grown up and miraculously flourished in the heart of a thoroughly bourgeois and partly puritanical republic. It had renounced its bourgeois origins and elaborated and painfully rationalized its institutional, legal, metaphysical, and religious defenses\xc2\xa0... When the crisis came it chose to fight. It proved to be the death struggle of a society, which went down in ruins.[73]'b"Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin (1852)[74] and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.[75]"b'While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion\xc2\xa0... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence."[76] The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.[77]'b'The election of Abraham Lincoln in November 1860 was the final trigger for secession.[78] Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.'b'According to Lincoln, the people of the United States had shown that they can be successful in establishing and administering a republic, but a third challenge faced the nation, maintaining the republic, based on the people\'s vote. The people must now show: "successful maintenance [of the Republic] against a formidable internal attempt to overthrow it. It is now for them to demonstrate to the world that those who can fairly carry an election can also suppress a rebellion; that ballots are the rightful and peaceful successors of bullets; and that when ballots have fairly and constitutionally decided, there can be no successful appeal back to bullets; that there can be no successful appeal, except to ballots themselves, at succeeding elections. Such will be a great lesson of peace; teaching men that what they cannot take by an election, neither can they take it by a war".[79]'b'The election of Lincoln caused the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws and, even, secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860, and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states\' rights for slave owners in the South, but contained a complaint about states\' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.'b'Among the ordinances of secession passed by the individual states, those of three\xe2\x80\x94Texas, Alabama, and Virginia\xe2\x80\x94specifically mentioned the plight of the "slaveholding states" at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures.[80] However, at least four states\xe2\x80\x94South Carolina,[81] Mississippi,[82] Georgia,[83] and Texas[84]\xe2\x80\x94also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement\'s influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive slave clause of the Constitution.'b'These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861.[85] They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress".[86] One quarter of the U.S. Army\xe2\x80\x94the entire garrison in Texas\xe2\x80\x94was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.'b'As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts),[87] the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.'b"On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it.[88] It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would have voted in favor of it.[89] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[90]"b'On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[91] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[91]'b'The South sent delegations to Washington and offered to pay for the federal properties[which?] and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[92] Secretary of State William Seward, who at the time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed.[92] President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina\'s Fort Sumter.'b"Fort Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. fort's garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan, who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12\xe2\x80\x9313, forcing its capitulation."b'The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins said:'b"However, much of the North's attitude was based on the false belief that only a minority of Southerners were actually in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners really did favor secession, they might have hesitated at attempting the enormous task of conquering a united South.[95]"b'Lincoln called on all the states to send forces to recapture the fort and other federal properties. With the scale of the rebellion apparently small so far, Lincoln called for only 75,000 volunteers for 90\xc2\xa0days.[96] The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal.[97] On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.[98]'b'Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.[99]'b'Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. West Virginia then joined them as an additional border state after it separated from Virginia and became a state of the Union in 1863.'b"Maryland's territory surrounded the United States' capital of Washington, DC and could cut it off from the North.[100] It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53\xe2\x80\x9313) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war.[101] Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North.[102] Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened.[101][103] All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.[104]"b'In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. (See also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[105]'b'Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.[106]'b"After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving).[107] The inclusion of 24 secessionist counties[108] in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war.[109][110] Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000\xe2\x80\x9322,000 soldiers to both the Confederacy and the Union.[111]"b'A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[112]'b'The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy\'s soldier.[113]'b'As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[114] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.[115]'b'In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law\xe2\x80\x94conscription\xe2\x80\x94as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt.[116] The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.[117]'b"When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[118] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.[119]"b'In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[120] At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units.[121] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[122]'b'From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan\'s assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.[123]'b'Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:'b"Some historians emphasize that Civil War soldiers were driven by political ideology, holding firm beliefs about the importance of liberty, Union, or state rights, or about the need to protect or to destroy slavery. Others point to less overtly political reasons to fight, such as the defense of one's home and family, or the honor and brotherhood to be preserved when fighting alongside other men. Most historians agree that no matter what a soldier thought about when he went into the war, the experience of combat affected him profoundly and sometimes altered his reasons for continuing the fight.[124]"b"At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their own army where they were paid but not allowed to perform any military duties.[125] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[126]"b'The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396.[127][128] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[129] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.'b"By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible.[130] Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.[131]"b'In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[132]'b'The Civil War occurred during the early stages of the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union\'s naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[133] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union\'s own ironclad warships, they were unsuccessful.[134]'b"The Confederacy experimented with a submarine, which did not work well,[135] and with building an ironclad ship, the CSS Virginia, which was based on rebuilding a sunken Union ship, the Merrimack. On its first foray on March 8, 1862, the Virginia inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, the USS Monitor, arrived to challenge it in the Chesapeake Bay. The resulting three hour battle between the Ironclads was a draw, but it marked the worldwide transition to ironclad warships.[136] Not long after the battle the Confederacy was forced to scuttle the Virginia to prevent its capture, while the Union built many copies of the Monitor. Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Britain.[137]"b'British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were designed for speed and were so small that only a small amount of cotton went out.[138] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released.[139] The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[140]'b"Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[141] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.[142]"b'To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested.[134] After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.[143]'b'The 1862 Union strategy called for simultaneous advances along four axes:[144]'b'Ulysses Grant used river transport and Andrew Foote\'s gunboats of the Western Flotilla to threaten the Confederacy\'s "Gibraltar of the West" at Columbus, Kentucky. Though rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.[145]'b"In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[146] They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry (February 6, 1862) and Fort Donelson (February 11 to 16, 1862), and supplied Grant's forces as he moved into Tennessee. At Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory\xe2\x80\x94the first battle with the high casualty rates that would repeat over and over.[147] Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. On April 24, 1862, U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[148]"b'Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at Vicksburg, Mississippi in July 1863, and in the Union fully controlling the Mississippi River soon after.[149]'b'In one of the first highly visible battles, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces near Washington was repulsed.'b"Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign,[150][151][152] Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[153] The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[154] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops."b"Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[153][155] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[156]"b"When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[157] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker."b"Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863.[158] Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[159] Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[160] This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[161] However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant."b"While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[162] Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization."b'The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans,[163] which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.'b"General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state.[164] Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.[165]"b"The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged."b"The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh;[166] and the Battle of Vicksburg,[167] which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[168] driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy."b'Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[171] Roving Confederate bands such as Quantrill\'s Raiders terrorized the countryside, striking both military installations and civilian settlements.[172] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.'b'By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.[169]'b'Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union.[173] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[174]'b'After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him.[175] Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.'b'At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[176] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[177] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[178]'b"Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[179]"b"Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[180]"b"Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president.[181] Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.[182]"b'Leaving Atlanta, and his base of supplies, Sherman\'s army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman\'s army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee\'s army.[183]'b'Lee\'s army, thinned by desertion and casualties, was now much smaller than Grant\'s. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler\'s Creek.[184]'b"Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House.[185] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller."b"On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[186] On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William T. Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day.[1] On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department.[187] On June 23, Cherokee leader Stand Watie became the last Confederate General to surrender his forces.[188]"b"Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators.[189][190] The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.[191]"b'Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860\xe2\x80\x9362 crop failures in Europe made the North\'s grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half.[191] When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.[192]'b'Lincoln\'s foreign policy was deficient in 1861 in terms of appealing to European public opinion. Diplomats had to explain that United States was not committed to the ending of slavery, but instead they repeated legalistic arguments about the unconstitutionality of secession. Confederate spokesmen, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. In addition, the European aristocracy (the dominant factor in every major country) was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[193]'b'U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (CSS Alabama, CSS Shenandoah, CSS Tennessee, CSS Tallahassee, CSS Florida, and some others). The most famous, the CSS Alabama, did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for politicians in Britain, where the antislavery movement was powerful.[194]'b"War loomed in late 1861 between the U.S. and Britain over the Trent affair, involving the U.S. Navy's boarding of the British ship Trent and seizure of two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation between North and South\xe2\x80\x93 though even such an offer would have risked war with the U.S. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on this.[195]"b"The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.[196]"b'The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty.[197]'b"Historians have debated whether the Confederacy could have won the war. Most scholars, including James McPherson, argue that Confederate victory was at least possible.[198] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.[199]"b'Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[199] Lincoln was not a military dictator, and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.[203]'b'Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat.[204][205] Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back\xc2\xa0... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don\'t think the South ever had a chance to win that War."[206]'b'A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win."[207][208] Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederate army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict.[209] However, most historians reject the argument.[210] James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864\xe2\x80\x9365, he says most Confederate soldiers were fighting hard.[211] Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up\xe2\x80\x94some few deserters\xe2\x80\x94plenty tired of war, but the masses determined to fight it out."[212]'b"Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[213] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[214]"b'Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[215] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:'b'The North\'s victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[216]'b'Scholars have debated what the effects of the war were on political and economic power in the South.[217] The prevailing view is that the southern planter elite retained its powerful position in the South.[217] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[217]'b'The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths\xe2\x80\x94two-thirds by disease, and 50,000 civilians.[11] Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000.[21][218] The war accounted for more American deaths than in all other U.S. wars combined.[219]'b'Based on 1860 census figures, 8 percent of all white males aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[220][221] About 56,000 soldiers died in prison camps during the War.[222] An estimated 60,000 men lost limbs in the war.[223]'b'Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:[6]'b'In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[7]'b'Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle.[6] Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War.[224]:16 Notably, their mortality rate was significantly higher than white soldiers:'b'[We] find, according to the revised official data, that of the slightly over two millions troops in the United States Volunteers, over 316,000 died (from all causes), or 15.2 percent. Of the 67,000 Regular Army (white) troops, 8.6 percent, or not quite 6,000, died. Of the approximately 180,000 United States Colored Troops, however, over 36,000 died, or 20.5 percent. In other words, the mortality "rate" amongst the United States Colored Troops in the Civil War was thirty-five percent greater than that among other troops, notwithstanding the fact that the former were not enrolled until some eighteen months after the fighting began.[224]:16'b"Confederate records compiled by historian William F. Fox list 74,524 killed and died of wounds and 59,292 died of disease. Including Confederate estimates of battle losses where no records exist would bring the Confederate death toll to 94,000 killed and died of wounds. Fox complained, however, that records were incomplete, especially during the last year of the war, and that battlefield reports likely under-counted deaths (many men counted as wounded in battlefield reports subsequently died of their wounds). Thomas L. Livermore, using Fox's data, put the number of Confederate non-combat deaths at 166,000, using the official estimate of Union deaths from disease and accidents and a comparison of Union and Confederate enlistment records, for a total of 260,000 deaths.[6] However, this excludes the 30,000 deaths of Confederate troops in prisons, which would raise the minimum number of deaths to 290,000."b'The United States National Park Service uses the following figures in its official tally of war losses:[2]'b'Union: 853,838'b'Confederate: 914,660'b"While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, Superintendent of the 1870 Census, used census and Surgeon General data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 Census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.[218]"b'Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[22] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.'b"Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in an area where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor James Downs states that tens to hundreds of thousands of slaves died during the war from disease, starvation, exposure, or execution at the hands of the Confederates, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.[225]"b'Losses were far higher than during the recent defeat of Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Mini\xc3\xa9 balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.[226]'b"The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.[227]"b'The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century.[228] The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.'b'While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee\'s army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[229] Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal.[230] Lincoln\'s decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[231] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans\' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.[232]'b'The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army.[e] About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.[f]'b'During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[238] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.[239]'b'At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Fr\xc3\xa9mont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected.[240] But only the District of Columbia accepted Lincoln\'s gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat".[241] Lincoln laid the groundwork for public support in an open letter published in abolitionist Horace Greeley\'s newspaper.[242]'b'In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors\' Conference added support for the proclamation.[243] Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong\xc2\xa0... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling\xc2\xa0... I claim not to have controlled events, but confess plainly that events have controlled me."[244]'b"Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.[245]"b"Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[246] The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France.[247] By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.[248]"b'In Texas v. White, 74 U.S. 700 (1869) the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.[249]'b'Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877.[250] It comprised multiple complex methods to resolve the outstanding issues of the war\'s aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery\xe2\x80\x94and prevent semi-slavery status.[251]'b'President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson\'s work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended.[252] With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.'b'The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war\'s aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[253] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[254]'b'Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academe, leading to a proliferation of solid studies by non-scholars who are thoroughly familiar with the primary sources, pay close attention to battles and campaigns, and write for the large public readership, rather than the small scholarly community. Bruce Catton and Shelby Foote are among the best-known writers.[255][256] Practically every major figure in the war, both North and South, has had a serious biographical study.[257] Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:'b"God had chastised them and given them a special mission\xe2\x80\x94to maintain orthodoxy, strict biblicism, personal piety, and traditional race relations. Slavery, they insisted, had not been sinful. Rather, emancipation was a historical tragedy and the end of Reconstruction was a clear sign of God's favor.[258]"b'In sharp contrast, Black preachers interpreted the Civil War as:'b"God's gift of freedom. They appreciated opportunities to exercise their independence, to worship in their own way, to affirm their worth and dignity, and to proclaim the fatherhood of God and the brotherhood of man. Most of all, they could form their own churches, associations, and conventions. These institutions offered self-help and racial uplift, and provided places where the gospel of liberation could be proclaimed. As a result, black preachers continued to insist that God would protect and help him; God would be their rock in a stormy land.[259]"b'Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations.[260] Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[261] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing African-American progress to a white man\'s reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[262]'b"The interpretation of the Civil War presented by Charles A. Beard and Mary R. Beard in The Rise of American Civilization (1927) was highly influential among historians and the general public until the civil rights movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:"b'[A] social cataclysm in which the capitalists, laborers, and farmers of the North and West drove from power in the national government the planting aristocracy of the South.[263]'b'The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.[264]'b"The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861, but the oldest surviving monument is the Hazen monument, erected at Stones River near Murfreesboro, Tennessee, in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead in the Battle of Stones River. In the 1890s, the United States government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park in Tennessee and the Antietam National Battlefield in Maryland in 1890. The Shiloh National Military Park was established in 1894, followed by the Gettysburg National Military Park in 1895 and Vicksburg National Military Park in 1899. In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service.[265]"b'The modern Civil War battlefield preservation movement began in 1987 with the founding of the Association for the Preservation of Civil War Sites (APCWS), a grassroots organization created by Civil War historians and others to preserve battlefield land by acquiring it. In 1991, the original Civil War Trust was created in the mold of the Statue of Liberty/Ellis Island Foundation, but failed to attract corporate donors and soon helped manage the disbursement of U.S. Mint Civil War commemorative coin revenues designated for battlefield preservation. Although the two organizations joined forces on a number of battlefield acquisitions, ongoing conflicts prompted the boards of both organizations to facilitate a merger, which happened in 1999 with the creation of the Civil War Preservation Trust. In 2011, the organization was renamed The Civil War Trust. From 1987 through late 2017, The Trust and its predecessor organizations saved more than 40,000 acres at 126 Civil War battlefields and sites in 21 states.[266]'b"The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary. [267] Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as Birth of a Nation (1915), Gone with the Wind (1939), and more recently Lincoln (2012). Ken Burns produced a notable PBS series on television titled The Civil War (1990). It was digitally remastered and re-released in 2015."b'There were numerous technological innovations during the Civil War that had a great impact on 19th century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[268] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[269][270] It was also in this war when countries first used aerial warfare, in the form of reconnaissance balloons, to a significant effect.[271] It saw the first action involving steam-powered ironclad warships in naval warfare history.[272] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare, as well as the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[273]'b'General reference'b'Union'b'Confederacy'b'Ethnic articles'b'Topical articles'b'National articles'b'State articles'b'Memorials'b''Singular-value decomposition
b'The singular-value decomposition can be computed using the following observations:'b'Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.'b'Suppose M is a m \xc3\x97 n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the form'b'where'b'The diagonal entries \xcf\x83i of \xce\xa3 are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, \xce\xa3, is uniquely determined by M (though not the matrices U and V, see below).'b'In the special, yet common case when M is an m \xc3\x97 m real square matrix with positive determinant, U, V\xe2\x88\x97, and \xce\xa3 are real m \xc3\x97 m matrices as well, \xce\xa3 can be regarded as a scaling matrix, and U, V\xe2\x88\x97 can be viewed as rotation matrices. Thus the expression U\xce\xa3V\xe2\x88\x97 can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.'b"Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = V\xce\xa3V\xe2\x88\x97) with eigenvalue scale factors \xcf\x83i along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV\xe2\x88\x97). If the rotation is done first, M = P'R, then R is the same and P' = U\xce\xa3U\xe2\x88\x97 has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate."b'As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n \xc3\x97 n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m \xc3\x97 n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.'b'Since U and V\xe2\x88\x97 are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector \xcf\x83i Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U\xe2\x88\x97 and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U\xe2\x88\x97, V, and V\xe2\x88\x97 are orthonormal bases.'b'Consider the 4 \xc3\x97 5 matrix'b'A singular-value decomposition of this matrix is given by U\xce\xa3V\xe2\x88\x97'b'Notice \xce\xa3 is zero outside of the diagonal and one diagonal element is zero. Furthermore, because the matrices U and V\xe2\x88\x97 are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V\xe2\x88\x97 are real valued, each is an orthogonal matrix.'b'is also a valid singular-value decomposition.'b'In any singular-value decomposition'b'the diagonal entries of \xce\xa3 are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:'b'As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank\xe2\x80\x93nullity theorem cannot be the same dimension if m \xe2\x89\xa0 n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m \xe2\x88\x92 n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n \xe2\x88\x92 m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.'b'Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ei\xcf\x86 (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.'b'The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = U\xce\xa3V\xe2\x88\x97 is'b'where \xce\xa3+ is the pseudoinverse of \xce\xa3, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.'b"A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x\xe2\x88\x97A = 0, with x\xe2\x88\x97 denoting the conjugate transpose of x, is called a left null vector of A."b'A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.'b'Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.'b'As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \xce\xa3. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.'b'Here Ui and Vi are the i-th columns of the corresponding SVD matrices, \xcf\x83i are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero \xcf\x83i is exactly the rank of the matrix.'b"Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,"b'which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]'b'It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O \xe2\x88\x92 A. The solution is the product UV\xe2\x88\x97.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV\xe2\x88\x97 where I is the identity matrix, so that if A = U\xce\xa3V\xe2\x88\x97 then the product A = UV\xe2\x88\x97 amounts to replacing the singular values with ones.'b'A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,'b'This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.'b"The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules."b'The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]'b'The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.'b'The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the \xce\xa3 matrix is larger than one.'b'One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.'b'SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]'b"Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]"b'Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.'b'Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]'b'The singular-value decomposition is very general in the sense that it can be applied to any m \xc3\x97 n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.'b'Given an SVD of M, as described above, the following two relations hold:'b'The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:'b'In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU\xe2\x88\x97 for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU\xe2\x88\x97 is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each \xcf\x83i to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=U\xce\xa3U* is positive semidefinite and normal, and R=UV* is unitary.'b'An eigenvalue \xce\xbb of a matrix M is characterized by the algebraic relation Mu = \xce\xbbu. When M is Hermitian, a variational characterization is also available. Let M be a real n \xc3\x97 n symmetric matrix. Define'b'By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| \xe2\x89\xa4 1}. By the Lagrange multipliers theorem, u necessarily satisfies'b'where the nabla symbol, \xe2\x88\x87, is the del operator.'b'A short calculation shows the above leads to Mu = \xce\xbbu (symmetry of M is needed here). Therefore, \xce\xbb is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.'b'Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.'b'This section gives these two arguments for existence of singular-value decomposition.'b'Let M be an m \xc3\x97 n complex matrix. Since M\xe2\x88\x97M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n \xc3\x97 n matrix V such that'b'where D is diagonal and positive definite. Partition V appropriately so we can write'b'Therefore:'b'The second equation implies MV2 = 0. Also, since V is unitary:'b'where the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. Define'b'Then'b'We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, since'b'the columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:'b'For V1 we already have V2 to make it unitary. Now, define'b'which is the desired result:'b'Notice the argument could begin with diagonalizing MM\xe2\x88\x97 rather than M\xe2\x88\x97M (This shows directly that MM\xe2\x88\x97 and M\xe2\x88\x97M have the same non-zero eigenvalues).'b'The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.'b'Let M denote an m \xc3\x97 n matrix with real entries. Let Sm\xe2\x88\x921 and Sn\xe2\x88\x921 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the function'b'Consider the function \xcf\x83 restricted to Sm\xe2\x88\x921 \xc3\x97 Sn\xe2\x88\x921. Since both Sm\xe2\x88\x921 and Sn\xe2\x88\x921 are compact sets, their product is also compact. Furthermore, since \xcf\x83 is continuous, it attains a largest value for at least one pair of vectors u \xe2\x88\x88 Sm\xe2\x88\x921 and v \xe2\x88\x88 Sn\xe2\x88\x921. This largest value is denoted \xcf\x831 and the corresponding vectors are denoted u1 and v1. Since \xcf\x831 is the largest value of \xcf\x83(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.'b'Proof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:'b'After some algebra, this becomes'b'Plugging this into the pair of equations above, we have'b'This proves the statement.'b'More singular vectors and singular values can be found by maximizing \xcf\x83(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.'b'The passage from real to complex is similar to the eigenvalue case.'b'Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).'b'The linear transformation'b'has a particularly simple description with respect to these orthonormal bases: we have'b'where \xcf\x83i is the i-th diagonal entry of \xce\xa3, and T(Vi) = 0 for i > min(m,n).'b'The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T\xc2\xa0: Kn \xe2\x86\x92 Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.'b'To get a more visual flavour of singular values and SVD factorization \xe2\x80\x94 at least when working on real vector spaces \xe2\x80\x94 consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V\xe2\x88\x97 sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D \xe2\x88\x98 V\xe2\x88\x97 then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U \xe2\x88\x98 D \xe2\x88\x98 V\xe2\x88\x97 coincides with T.'b'The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m \xe2\x89\xa5 n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).'b'The first step can be done using Householder reflections for a cost of 4mn2 \xe2\x88\x92 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).'b'The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.'b'The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 \xc3\x97 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 \xc3\x97 2 eigenvalue methods (Golub & Van Loan 1996, \xc2\xa78.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).'b'There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or'b'The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M \xe2\x87\x92 Q R and LQ decomposition of R gives R \xe2\x87\x92 L P*. Thus, at every iteration, we have M \xe2\x87\x92 Q L P*, update M \xe2\x87\x90 L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.'b'In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m\xc3\x97n matrix M of rank r:'b"Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n\xc2\xa0\xe2\x89\xaa\xc2\xa0m. The matrix U'n is thus m\xc3\x97n, \xce\xa3n is n\xc3\x97n diagonal, and V is n\xc3\x97n."b'The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if\xc2\xa0n\xc2\xa0\xe2\x89\xaa\xc2\xa0m.'b'Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values \xce\xa3r are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r\xc2\xa0\xe2\x89\xaa\xc2\xa0n. The matrix Ur is thus m\xc3\x97r, \xce\xa3r is r\xc3\x97r diagonal, and Vr* is r\xc3\x97n.'b'Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values \xce\xa3t are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t\xe2\x89\xaar. The matrix Ut is thus m\xc3\x97t, \xce\xa3t is t\xc3\x97t diagonal, and Vt* is t\xc3\x97n.'b'The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]'b'The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spaces'b'But, in the matrix case, (M* M)\xc2\xbd is a normal matrix, so ||M* M||\xc2\xbd is the largest eigenvalue of (M* M)\xc2\xbd, i.e. the largest singular value of M.'b"The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)\xc2\xbd] (the eigenvalues of M* M are the squares of the singular values)."b'The singular values are related to another norm on the space of operators. Consider the Hilbert\xe2\x80\x93Schmidt inner product on the n \xc3\x97 n matrices, defined by'b'So the induced norm is'b'Since the trace is invariant under unitary equivalence, this shows'b'where \xcf\x83i are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert\xe2\x80\x93Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:'b'In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.'b'Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.'b'TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:'b'The factorization M = U\xce\xa3V\xe2\x88\x97 can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X,\xc2\xa0\xce\xbc), and a non-negative measurable f such that'b'This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such that'b'is an unitary operator.'b'As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write'b'and notice that U V* is still a partial isometry while VTf V* is positive.'b'The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero \xce\xbb in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M\xe2\x88\x97M. Applying the diagonalization result, the unitary image of its positive square root Tf\xc2\xa0 has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {\xcf\x83i}. For any \xcf\x88 \xe2\x88\x88 H,'b'where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. \xcf\x83i are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.'b'Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:'b'The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.'b'Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.'Method of moments (statistics)
b'In statistics, the method of moments is a method of estimation of population parameters. One starts with deriving equations that relate the population moments (i.e., the expected values of powers of the random variable under consideration) to the parameters of interest. Then a sample is drawn and the population moments are estimated from the sample. The equations are then solved for the parameters of interest, using the sample moments in place of the (unknown) population moments. This results in estimates of those parameters. The method of moments was introduced by Pafnuty Chebyshev in 1887.'b''b''b'The method of moments is fairly simple and yields consistent estimators (under very weak assumptions), though these estimators are often biased.'b"In some respects, when estimating parameters of a known family of probability distributions, this method was superseded by Fisher's method of maximum likelihood, because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased."b'However, in some cases the likelihood equations may be intractable without computers, whereas the method-of-moments estimators can be quickly and easily calculated by hand.'b'Estimates by the method of moments may be used as the first approximation to the solutions of the likelihood equations, and successive improved approximations may then be found by the Newton\xe2\x80\x93Raphson method. In this way the method of moments can assist in finding maximum likelihood estimates.'b'In some cases, infrequent with large samples but not so infrequent with small samples, the estimates given by the method of moments are outside of the parameter space; it does not make sense to rely on them then. That problem never arises in the method of maximum likelihood. Also, estimates by the method of moments are not necessarily sufficient statistics, i.e., they sometimes fail to take into account all relevant information in the sample.'b'When estimating other structural parameters (e.g., parameters of a utility function, instead of parameters of a known probability distribution), appropriate probability distributions may not be known, and moment-based estimates may be preferred to maximum likelihood estimation.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Explicit semantic analysis
b'In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf\xe2\x80\x93idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.[1]'b'ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization[2] and has been used by this pair of researchers to compute what they refer to as "semantic relatedness" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name "explicit semantic analysis" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.[1][3]'b''b''b'To perform the basic variant of ESA, one starts with a collection of texts, say, all Wikipedia articles; let the number of documents in the collection be N. These are all turned into "bags of words", i.e., term frequency histograms, stored in an inverted index. Using this inverted index, one can find for any word the set of Wikipedia articles containing this word; in the vocabulary of Egozi, Markovitch and Gabrilovitch, "each word appearing in the Wikipedia corpus can be seen as triggering each of the concepts it points to in the inverted index."[1]'b'The output of the inverted index for a single word query is a list of indexed documents (Wikipedia articles), each given a score depending on how often the word in question occurred in them (weighted by the total number of words in the document). Mathematically, this list is an N-dimensional vector of word-document scores, where a document not containing the query word has score zero. To compute the relatedness of two words, one compares the vectors (say u and v) by computing the cosine similarity,'b'and this gives numeric estimate of the semantic relatedness of the words. The scheme is extended from single words to multi-word texts by simply summing the vectors of all words in the text.[3]'b'ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically orthogonal concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of information retrieval systems when it is based not on Wikipedia, but on the Reuters corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".[4] To explain this observation, links have been shown between ESA and the generalized vector space model.[5] Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".[1]'b'Cross-language explicit semantic analysis (CL-ESA) is a multilingual generalization of ESA.[6] CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.'Latent semantic analysis
b'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]'b'An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).[2]'b''b''b'LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency\xe2\x80\x93inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.'b'This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.'b'After the construction of the occurrence matrix, LSA finds a low-rank approximation[4] to the term-document matrix. There could be various reasons for these approximations:'b'The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:'b'This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.'b'Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:'b'Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:'b'The matrix products giving us the term and document correlations then become'b'You can now do the following:'b'To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:'b'The new low-dimensional space typically can be used to:'b'Synonymy and polysemy are fundamental problems in natural language processing:'b'LSA has been used to assist in performing prior art searches for patents.[8]'b'The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words. These findings are referred to as the Semantic Proximity Effect.[9]'b'When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list. These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.[10]'b'Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.[11]'b"The SVD is typically computed using large matrix methods (for example, Lanczos methods) but may also be computed incrementally and with greatly reduced resources via a neural network-like approach, which does not require the large, full-rank matrix to be held in memory.[12] A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.[13] MATLAB and Python implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution. In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.[14]"b"Some of LSA's drawbacks include:"b'In semantic hashing [17] documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.'b'Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.[18]'b'LSI is also an application of correspondence analysis, a multivariate statistical technique developed by Jean-Paul Benz\xc3\xa9cri[19] in the early 1970s, to a contingency table built from word counts in documents.'b'Called "latent semantic indexing" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s. The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\xe2\x80\x99t share a specific word or words with the search criteria.'b'LSI overcomes two of the most problematic constraints of Boolean keyword queries: multiple words that have similar meanings (synonymy) and words that have more than one meaning (polysemy)[clarification needed]. Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems.[20] As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.'b'LSI is also used to perform automated document categorization. In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.[21] Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.[22] LSI uses example documents to establish the conceptual basis for each category. During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.'b'Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI. Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection of unstructured text.'b'Because it uses a strictly mathematical approach, LSI is inherently independent of language. This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri. LSI can also perform cross-linguistic concept searching and example-based categorization. For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.[citation needed]'b'LSI is not restricted to working only with words. It can also process arbitrary character strings. Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.[23]'b'LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).[24] This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion. LSI also deals effectively with sparse, ambiguous, and contradictory data.'b'Text does not need to be in sentence form for LSI to be effective. It can work with lists, free-form notes, email, Web-based content, etc. As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.'b'LSI has proven to be a useful solution to a number of conceptual matching problems.[25][26] The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.[27]'b'LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.'b'Some common local weighting functions[29] are defined in the following table.'b'Some common global weighting functions are defined in the following table.'b'In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A\xe2\x80\x94a measure of its unique dimensions \xe2\x89\xa4 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.'b'The SVD is then truncated to reduce the rank by keeping only the largest k \xc2\xab\xc2\xa0r diagonal entries in the singular value matrix S, where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A. This reduced set of matrices is often denoted with a modified formula such as:'b'Efficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.'b'Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons.'b'The computed Tk and Dk matrices define the term and document vector spaces, which with the computed singular values, Sk, embody the conceptual information derived from the document collection. The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.'b'The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index. By a simple transformation of the A = T S DT equation into the equivalent D = AT T S\xe2\x88\x921 equation, a new vector, d, for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S\xe2\x88\x921. The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.'b'A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored. These terms will have no impact on the global weights and learned correlations derived from the original collection of text. However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.'b'The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in. Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added. When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in [13]) be used.'b'It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems. As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.'b'LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.[32] Below are some other ways in which LSI is being used:'b'LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential. Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.[47]'b'Early challenges to LSI focused on scalability and performance. LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.[48] However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome. Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package.[49]'b'Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD. As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).[50] However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.[51]'b'Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain. The variance contained in the data can be viewed by plotting the singular values (S) in a scree plot. Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain. Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain. Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.[52][53][54]'b'Due to its cross-domain applications in Information Retrieval, Natural Language Processing (NLP), Cognitive Science and Computational Linguistics, LSA has been implemented to support many different kinds of applications.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Hierarchical Dirichlet process
b'In statistics and machine learning, the hierarchical Dirichlet process (HDP) is a nonparametric Bayesian approach to clustering grouped data.[1][2] It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. This method allows groups to share statistical strength via sharing of clusters across groups. The base distribution being drawn from a Dirichlet process is important, because draws from a Dirichlet process are atomic probability measures, and the atoms will appear in all group-level Dirichlet processes. Since each atom corresponds to a cluster, clusters are shared across all groups. It was developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei and published in the Journal of the American Statistical Association in 2006,[1] as a formalization and generalization of the infinite hidden Markov model published in 2002.[3]'b''b''b'Thus the set of atoms is shared across all groups, with each group having its own group-specific atom masses. Relating this representation back to the observed data, we see that each data item is described by a mixture model:'b'The HDP mixture model is a natural nonparametric generalization of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data.[1] Here each group is a document consisting of a bag of words, each cluster is a topic, and each document is a mixture of topics. The HDP is also a core component of the infinite hidden Markov model,[3] which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.[1] [4]'b'The HDP can be generalized in a number of directions. The Dirichlet processes can be replaced by Pitman-Yor processes, resulting in the Hierarchical Pitman-Yor process. The hierarchy can be deeper, with multiple levels of groups arranged in a hierarchy. Such an arrangement has been exploited in the sequence memoizer, a Bayesian nonparametric model for sequences which has a multi-level hierarchy of Pitman-Yor processes.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Mallet (software project)
b'MALLET is a Java "Machine Learning for Language Toolkit".'b''b''b'MALLET is an integrated collection of Java code useful for statistical natural language processing, document classification, cluster analysis, information extraction, topic modeling and other machine learning applications to text.'b'MALLET was developed primarily by Andrew McCallum, of the University of Massachusetts Amherst, with assistance from graduate students and faculty from both UMASS and the University of Pennsylvania.'b''Gensim
b'Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.'b''b''b'Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms,[1] hierarchical Dirichlet processes (HDP), latent semantic analysis (LSA, LSI, SVD) and latent Dirichlet allocation (LDA), including distributed parallel versions.[2]'b'Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation Scalability of Semantic Analysis in Natural Language Processing of Radim \xc5\x98eh\xc5\xaf\xc5\x99ek, the creator of Gensim.[3]'b'Gensim has been used and cited in over 800 commercial and academic applications, in a diverse array of disciplines from medicine to insurance claim analysis to patent search[4][5] The software has been covered in several new articles, podcasts and interviews since 2009.[6][7][8]'b'The open source code is developed and hosted on GitHub[9] and a public support forum is maintained on Google Groups[10] and Gitter.[11]'b'Gensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme.[12]'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Journal of Machine Learning Research
b'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Kevin Murphy (Google) and Bernhard Sch\xc3\xb6lkopf (Max Planck Institute for Intelligent Systems).'b''b''b'The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online.[2]'b'Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.[1]'b'In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007[3] and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems.'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'arXiv
b'arXiv (pronounced "archive")[2] is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million article milestone on October 3, 2008,[3][4] and hit a million by the end of 2014.[5][6] By October 2016 the submission rate had grown to more than 10,000 per month.[6][7]'b''b''b'The arXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side.[9] Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993.[6][10] The term e-print was quickly adopted to describe the articles.'b"It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org.[11] It is now hosted principally by Cornell, with eight mirrors around the world.[12]"b'Its existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access[13] and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.'b'The annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions.[14] This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Annual donations were envisaged to vary in size between $2,300 to $4,000, based on each institution\xe2\x80\x99s usage. As of 14\xc2\xa0January\xc2\xa02014[update], 174 institutions have pledged support for the period 2013\xe2\x80\x932017 on this basis, with a projected revenue from this source of approximately $340,000.[15]'b'In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv\'s operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence".[16] However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.'b'Although the arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic,[17] or reject submissions that are not scientific papers. The lists of moderators for many sections of the arXiv are publicly available,[18] but moderators for most of the physics sections remain unlisted.'b'Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines.[19] Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area.[17] New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.[20]'b'A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston\'s geometrization conjecture, including the Poincar\xc3\xa9 conjecture as a particular case, uploaded by Grigori Perelman in November 2002.[21] Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it\'s all there [on the arXiv]\xc2\xa0\xe2\x80\x93 let them go and read about it".[22] Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.[23]'b'While the arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat\'s last theorem using only high-school mathematics, they are "surprisingly rare".[24][better\xc2\xa0source\xc2\xa0needed] The arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them.[25]'b'Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.'b"The standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's front, a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front.[26] A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides TablearXiv,[27] a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv.[28] A full text and author search engine for arXiv is provided by Scientillion.[29] Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them."b'Files on arXiv can have a number of different copyright statuses:[30]'b'Some authors have voiced concern over the lack of transparency in the arXiv academic peer-review process.[31] Demetris Christopoulos from the National and Kapodistrian University of Athens likens arXiv to a non-declared Journal without a known editor in chief, without a specific written policy regarding submitted papers, and that applies hidden censorship to all papers that do not fall within established scientific dogma. [32]'International Standard Book Number
b'The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.'b'The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).'b'Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]'b'Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.'b''b''b'The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]'b'The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]'b'An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" \xe2\x80\x93 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN\xc2\xa00-340-01381-8; the check digit does not need to be re-calculated.'b'Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):'b'A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]'b'ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[15] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.'b'Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[16] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[17]'b'Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [18]'b'Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Biblioth\xc3\xa8que et Archives nationales du Qu\xc3\xa9bec.'b'Colombia: C\xc3\xa1mara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.'b'Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[19]'b'India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[20] There is no fee associated in getting ISBN in India.[21]'b'Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[22] The original national prefix 978-88 is reserved for publishing companies, starting at \xe2\x82\xac49 for a ten-codes block[23] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of \xe2\x82\xac25 for a single code.'b'Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]'b'Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[24][25][26]'b'Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.'b'New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[27]'b'Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.'b'Philippines: The National Library of the Philippines is responsible for ISBN registrations for Philippine publishers, authors, universities, institutions, and government departments that are responsible for publishing books. As of 2017[update], a fee of \xe2\x82\xb1120.00 per title was charged for the issuance of an ISBN.[28]'b'South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.'b'United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from \xc2\xa3120 (plus VAT) for the smallest block on a standard turnaround of ten days.[29]'b'United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[5] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125 for a single number. Access is immediate when requested via their website.[30]'b'Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.'b" The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[31] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0\xe2\x80\x935, 600\xe2\x80\x93621, 7, 80\xe2\x80\x9394, 950\xe2\x80\x93989, 9926\xe2\x80\x939989, and 99901\xe2\x80\x9399976.[32] Books published in rare languages typically have longer group identifiers.[33]"b'Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[34]'b'The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.'b'The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]'b'A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (\xe2\x82\xac1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[35] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.'b'Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.'b'By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[36] Here are some sample ISBN-10 codes, illustrating block length variations.'b'English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[37]'b'A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without \xe2\x80\x93 the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".'b'The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[38] \xe2\x80\x93 which is the last digit of the ten-digit ISBN \xe2\x80\x93 must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.'b'For example, for an ISBN-10 of 0-306-40615-2:'b'Formally, using modular arithmetic, we can say:'b"It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:"b'Formally, we can say:'b"The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN\xc2\xa0\xe2\x80\x93 the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[39]"b'In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).'b'Each of the first nine digits of the ten-digit ISBN\xe2\x80\x94excluding the check digit itself\xe2\x80\x94is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.'b'For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:'b'Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 \xe2\x80\x93 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)'b'For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:'b'Thus the check digit is 2.'b'It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:'b'The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.'b"The 2005 edition of the International ISBN Agency's official manual[40] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10."b'Formally, using modular arithmetic, we can say:'b'The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.'b'For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:'b'Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.'b'In general, the ISBN-13 check digit is calculated as follows.'b'Let'b'Then'b'This check system\xc2\xa0\xe2\x80\x93 similar to the UPC check digit formula\xc2\xa0\xe2\x80\x93 does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3\xc3\x976+1\xc3\x971 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3\xc3\x971+1\xc3\x976 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.'b'Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).'b'The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.'b'Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[41] For example, ISBN\xc2\xa00-590-76484-5 is shared by two books \xe2\x80\x93 Ninja gaiden\xc2\xae: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.'b'Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[42] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.'b'Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[43]'b'Currently the barcodes on a book\'s back cover (or inside a mass-market paperback book\'s front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[44] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).'b'Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[45] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.'b'Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[46]'b'Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.'Book sources
b'This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter. In Wikipedia, numbers preceded by "ISBN" link directly to this page.\n'b'This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). \n'b'Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).\n'b'Alabama\n'b'California\n'b'Colorado\n'b'Delaware\n'b'Florida\n'b'Georgia\n'b'Indiana\n'b'Iowa \n'b'Kansas\n'b'Kentucky\n'b'Massachusetts\n'b'Michigan\n'b'Minnesota\n'b'Missouri\n'b'Nebraska\n'b'New Jersey\n'b'New Mexico\n'b'New York\n'b'North Carolina\n'b'Ohio\n'b'Oklahoma\n'b'Oregon\n'b'Pennsylvania\n'b'Rhode Island\n'b'South Carolina\n'b'South Dakota\n'b'Tennessee\n'b'Texas\n'b'Utah\n'b'Washington state\n'b'Wisconsin\n'b'\n'b'Find your book on a site that compiles results from other online sites:\n'b'These sites allow you to search the catalogs of many individual booksellers:\n'b'\n'b'If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below \xe2\x80\x93 they are more likely to have sources appropriate for that language.\n'b'These links produce citations in various referencing styles.\n'b"You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.\n"b'You can also convert between 10 and 13 digit ISBN numbers with these tools:\n'b'\nEdit this page\n'b'\n'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'PubMed Central
b"PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. As one of the major research databases within the suite of resources that have been developed by the National Center for Biotechnology Information (NCBI), PubMed Central is much more than just a document repository. Submissions into PMC undergo an indexing and formatting procedure which results in enhanced metadata, medical ontology, and unique identifiers which all enrich the XML structured data for each article on deposit.[1] Content within PMC can easily be interlinked to many other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to freely discover, read and build upon this portfolio of biomedical knowledge.[2]"b'PubMed Central should not be confused with PubMed. These are two very different services at their core.[3] While PubMed is a searchable database of biomedical citations and abstracts, the full-text article referenced in the PubMed record will physically reside elsewhere. (Sometimes in print, sometimes online, sometimes free, sometimes behind a toll-wall accessible only to paying subscribers). PubMed Central is a free digital archive of articles, accessible to anyone from anywhere via a basic web browser. The full text of all PubMed Central articles is free to read, with varying provisions for reuse.'b'As of December\xc2\xa02016[update], the PMC archive contained over 4.1 million articles,[4] with contributions coming directly from publishers or authors depositing their own manuscripts into the repository per the NIH Public Access Policy. Older data shows that from Jan 2013 \xe2\x80\x93 Jan 2014 author-initiated deposits exceeded 103,000 papers during this 12-month period.[5] PMC also identifies about 4,000 journals which now participate in some capacity to automatically deposit their published content into the PMC repository.[6] Some participating publishers will delay the release of their articles on PubMed Central for a set time after publication, this is often referred to as an "embargo period", and can range from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common). However, PubMed Central is a key example of "systematic external distribution by a third party"[7] which is still prohibited by the contributor agreements of many publishers.'b''b''b'Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[8]'b'A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.'b'The National Library of Medicine "NLM Journal Publishing Tag Set" journal article markup language is freely available.[9] The Association of Learned and Professional Society Publishers comments that "it is likely to become the standard for preparing scholarly content for both books and journals".[10] A related DTD is available for books.[11] The Library of Congress and the British Library have announced support for the NLM DTD.[12] It has also been popular with journal service providers.[13]'b'With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[14] This includes NASA content, with the interface branded as "PubSpace".[15][16]'b'Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).'b'Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[17]'b'Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers\' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come "live" when the resources become available.'b'An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.'b'When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.'b'In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.'b'Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[18] to cautious concern by others.[19] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version-of-record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[20] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[21]'b'The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[22] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[23]'b'The change in procedure has received criticism.[24] The American Physiological Society has expressed reservations about the implementation of the policy.[25]'b'The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of "PMC" followed by a string of seven numbers. The format is:[26]'b'Authors applying for NIH awards must include the PMCID in their application.'PubMed
b'PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.'b'From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]'b'In addition to MEDLINE, PubMed provides access to:'b'Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]'b'Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]'b'As of 11\xc2\xa0July\xc2\xa02017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed\'s records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950s to 16% in 2016.[8] Other significant proportion of records correspond to \xe2\x80\x9cChemistry\xe2\x80\x9d (8.69%), \xe2\x80\x9cTherapy\xe2\x80\x9d (8.39%) and "Infection" (5%).'b'In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]'b"Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window."b"PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms."b'The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:'b'Likewise,'b"A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11] By default the results are sorted by Most Recent, but this changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.[12]"b'For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.[13][14]'b'When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).'b'Publication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.[15]'b'Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]'b'A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the \'Find related data\' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The \'related articles\' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.[18]'b'PubMed automatically links to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes).[19] This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.[19]'b'The PubMed optional facility "My NCBI" (with free registration) provides tools for'b'and a wide range of other options.[20] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[21]'b'LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[22] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March\xc2\xa02010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[23] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.'b'In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[24] In February 2018, PubMed Commons was discontinued due to the fact that "usage has remained minimal".[25][26]'b'PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[27] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[28]'b'askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[29]'b'A PMID (PubMed identifier or PubMed unique identifier)[30] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[31]'b'The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.'b'The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[32] As of October\xc2\xa02008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[33] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.'b'Lu[33] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:'b'As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[33] Without the need to store about 90\xc2\xa0GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[47]'b'Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950s to 16% in 2016.[48]'b'The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.[49]'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Topic model
In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 17281800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 18292008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time.Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]Machine learning
Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4]  such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing's proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The biasvariance decomposition is one way to quantify generalization error.For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value.Association rule learning is a method for discovering interesting relations between variables in large databases.An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]Applications for machine learning include:In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a models diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROCs associated Area Under the Curve (AUC).Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]Software suites containing a variety of machine learning algorithms include the following:Natural-language processing
Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural languagedata.Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.Systems based on machine-learning algorithms have many advantages over hand-produced rules:The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.Statistical model
Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Natural-language processing
b'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language\xc2\xa0data.'b'Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.'b''b''b'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.'b'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.'b'Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".'b'During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.'b"Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks."b'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.'b'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.'b'In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'b'Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.'b'Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).'b'Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.'b'Systems based on machine-learning algorithms have many advantages over hand-produced rules:'b'The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.'b'Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.'b''Statistical model
b'A statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.'b'The assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.'b'A statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Ad\xc3\xa8r quoting Kenneth Bollen).[1]'b'All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.'b''b''b'Suppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: heighti\xc2\xa0= b0\xc2\xa0+ b1agei\xc2\xa0+ \xce\xb5i, where b0 is the intercept, b1 is a parameter that age is multiplied by in obtaining a prediction of height, \xce\xb5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.'b'An admissible model must be consistent with all the data points. Thus, a straight line (heighti\xc2\xa0= b0\xc2\xa0+ b1agei) cannot be the equation for a model of the data. The line cannot be the equation for a model, unless it exactly fits all the data points\xe2\x80\x94i.e. all the data points lie perfectly on the line. The error term, \xce\xb5i, must be included in the equation, so that the model is consistent with all the data points.'b'To do statistical inference, we would first need to assume some probability distributions for the \xce\xb5i. For instance, we might assume that the \xce\xb5i distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.'b'A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, \xce\xb5 is a stochastic variable; without that variable, the model would be deterministic.'b'Statistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).'b'There are three purposes for a statistical model, according to Konishi\xc2\xa0& Kitagawa.[4]'b'As an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that'b'In this example, the dimension, k, equals 2.'b'As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)'b'Parametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, "These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies".[5]'b'Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model'b'has, nested within it, the linear model'b'\xe2\x80\x94we constrain the parameter b2 to equal 0.'b'In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\xc2\xa01). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.'b'Models can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.'b'Konishi & Kitagawa state: "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models."[6] Relatedly, Sir David Cox has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".[7]'Bioinformatics
b'Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]'b''b''b'Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]'b'Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]'b'Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]'b'To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:'b'The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\xe2\x80\x93protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.'b'Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.'b'Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.'b'Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.'b'Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.'b'Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.'b'Since the Phage \xce\xa6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.'b'Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.'b'Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.'b'In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.'b'The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.'b'Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).'b'Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:'b'Future work endeavours to reconstruct the now more complex tree of life.'b'The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.'b'The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.'b'Many of these studies are based on the homology detection and protein families computation.[22]'b'Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]'b"With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]"b'In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.'b'Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]'b'With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]'b'Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.'b'The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.'b'Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]'b'Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.'b'For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.'b'Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.'b'Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.'b'Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.'b'The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]'b'Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]'b'Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \xe2\x80\x93 a.k.a. Mad Cow Disease \xe2\x80\x93 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]'b"One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably."b'One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]'b'Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.'b'Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\xe2\x80\x93protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.'b'Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.'b'Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\xe2\x80\x93protein interactions only based on these 3D shapes, without performing protein\xe2\x80\x93protein interaction experiments. A variety of methods have been developed to tackle the protein\xe2\x80\x93protein docking problem, though it seems that there is still much work to be done in this field.'b'Other interactions encountered in the field include Protein\xe2\x80\x93ligand (including drug) and protein\xe2\x80\x93peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.'b'The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:'b'The area of research draws from statistics and computational linguistics.'b"Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:"b'Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.'b'Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.'b'Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.'b'The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.'b'Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.'b'Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.'b'Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.'b'Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.'b'The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]'b'An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]'b'SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.'b'Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.'b'A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to'b'Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.'b'In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.'b'It was decided that the BioCompute paradigm would be in the form of digital \xe2\x80\x98lab notebooks\xe2\x80\x99 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]'b'In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \xe2\x80\x9cstandard trial use\xe2\x80\x9d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]'b'Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\xcf\x80 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\xcf\x80 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\xcf\x80 operating system.[51][52]'b"MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications."b'There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).'b''Probabilistic latent semantic analysis
b'Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.'b'Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.'b''b''b'Their parameters are learned using the EM algorithm.'b'PLSA may be used in a discriminative setting, via Fisher kernels.[1]'b'PLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.'b'It is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.[2]'b'This is an example of a latent class model (see references therein), and it is related[5][6] to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.[7]'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'David Blei
b'David M. Blei is a Professor in the Statistics and Computer Science departments at Columbia University. Prior to fall 2014 he was an Associate Professor in the Department of Computer Science at Princeton University. His work is primarily in machine learning.'b''b''b'His research interests include topic models and he was one of the original developers of latent Dirichlet allocation. As of October 25, 2017, his publications have been cited 50,850 times, giving him an h-index of 64.[1]'b'He was named Fellow of ACM "For contributions to the theory and practice of probabilistic topic modeling and Bayesian machine learning" in 2015.[2]'b''Andrew Ng
b"Andrew Yan-Tak Ng (Chinese: \xe5\x90\xb3\xe6\x81\xa9\xe9\x81\x94; born 1976) is a Chinese American computer scientist. He is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor (formerly associate professor) at Stanford University. Ng is also the co-founder and chairman of Coursera, an online education platform.[2]"b''b''b"Ng was born in the UK in 1976. His parents were both from Hong Kong. He spent time in Hong Kong and Singapore[1] and later graduated from Raffles Institution in Singapore in 1992. In 1997, he received his undergraduate degree in computer science from Carnegie Mellon University in Pittsburgh, Pennsylvania. Ng earned his master's degree from Massachusetts Institute of Technology in Cambridge, Massachusetts in 1998 and received his PhD from University of California, Berkeley in 2002. He started working at Stanford University during that year and currently lives in Palo Alto, California. He married Carol E. Reiley in 2014.[3]"b"Andrew was a professor at Stanford University Department of Computer Science and Department of Electrical Engineering. He became Director of the Stanford Artificial Intelligence Lab where he taught students and undertook research related to data mining and machine learning. From 2011 to 2012, he worked at Google, where he founded and led the Google Brain Deep Learning Project. In 2012, he co-founded Coursera to offer free online courses for everyone after over 100,000 students registered for Ng's popular course.[4] Today, several million people have taken the online course. In 2014, he joined[5] Baidu as Chief Scientist, and carried out research related to big data and A.I. In March 2017, he announced his resignation from Baidu.[6]"b'He soon afterwards launched Deeplearning.ai,[7] an online curriculum of classes. Then Ng launchedLanding.ai[8], bringing AI to manufacturing factories, announcing a partnership with FoxConn.[9]'b'In 2018, Ng unveiled the AI Fund,[10] raising $175 million to invest in new startups. He is also the chairman of Woebot and on the board of drive.ai.[11][12]'b'Ng researches primarily in machine learning and deep learning. His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world,[13][14] and the STAIR (STanford Artificial Intelligence Robot) project,[15] which resulted in ROS, a widely used open-source robotics software platform.'b'In 2011, Ng founded the Google Brain project at Google, which developed very large scale artificial neural networks using Google\'s distributed computer infrastructure.[16] Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, that learned to recognize higher-level concepts, such as cats, after watching only YouTube videos, and without ever having been told what a "cat" is.[17][18] The project\'s technology is currently also used in the Android Operating System\'s speech recognition system.[19]'b'He together with David M. Blei and Michael I. Jordan, coauthored the influential paper that introduced Latent Dirichlet allocation.[20]'b'Ng started the Stanford Engineering Everywhere (SEE) program, which in 2008 placed a number of Stanford courses online, for free. Ng taught one of these courses, Machine Learning, which consisted of video lectures by him, along with the student materials used in the Stanford CS229 class.'b'The "applied" version of the Stanford class (CS229a) was hosted on ml-class.org and started in October 2011, with over 100,000 students registered for its first iteration; the course featured quizzes and graded programming assignments and became one of the first successful MOOCs made by Stanford professors.[22] His work subsequently led to the founding of Coursera in 2012.'b"Ng is also the author or co-author of over 100 published papers in machine learning, robotics, and related fields. His work in computer vision and deep learning has been frequently featured in press releases and reviews.[23] In 2008, he was named to the MIT Technology Review TR35 as one of the top 35 innovators in the world under the age of 35.[24][25] Ng was awarded a Sloan Fellowship (2007). For his work in artificial intelligence, he is also a recipient of the Computers and Thought Award (2009). In 2013 at the age of 37, he was named one of Times 100 Most Influential People[26] and Fortune's 40 under 40.[27]"Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Dirichlet distribution
b'The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process.'b''b''b'The Dirichlet distribution of order K\xc2\xa0\xe2\x89\xa5\xc2\xa02 with parameters \xce\xb11, ..., \xce\xb1K >\xc2\xa00 has a probability density function with respect to Lebesgue measure on the Euclidean space RK\xe2\x88\x921 given by'b'The normalizing constant is the multivariate Beta function, which can be expressed in terms of the gamma function:'b'When \xce\xb1=1[2], the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard (K\xc2\xa0\xe2\x88\x92\xc2\xa01)-simplex, i.e. it is uniform over all points in its support. This particular distribution is known as the flat Dirichlet distribution. Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.'b'Let'b'Then[3][4]'b'Note that the matrix so defined is singular.'b'More generally, moments of Dirichlet-distributed random variables can be expressed as[5]'b'The mode of the distribution is[6] the vector (x1, ..., xK) with'b'The marginal distributions are beta distributions:[7]'b"The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations). This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet. Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one. This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties."b'Formally, this can be expressed as follows. Given a model'b'then the following holds:'b'This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector \xce\xb1 as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution.'b'In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models. See the section on applications below for more information.'b'In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out) is a Dirichlet-multinomial distribution. This distribution plays an important role in hierarchical Bayesian models, because when doing inference over such models using methods such as Gibbs sampling or variational Bayes, Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.'b'and'b'If'b'then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum,'b'The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series. It is given by Phillips[11] as'b'For K independently distributed Gamma distributions:'b'we have:[13]:402'b'Although the Xis are not independent from one another, they can be seen to be generated from a set of K independent gamma random variable.[13]:594 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.'b'Because the Dirichlet distribution is an exponential family distribution it has a conjugate prior. The conjugate prior is of the form:[14]'b'Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models. (Note that in many fields, such as in natural language processing, categorical variables are often imprecisely called "multinomial variables". Such a usage is liable to cause confusion, just as if Bernoulli distributions and binomial distributions were commonly conflated.)'b'Inference over hierarchical Bayesian models is often done using Gibbs sampling, and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable. This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution, conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters). One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.'b'and then set'b'The Jacobian now looks like'b'The determinant can be evaluated by noting that it remains unchanged if multiples of a row are added to another row, and adding each of the first K-1 rows to the bottom row to obtain'b'Substituting for x in the joint pdf and including the Jacobian, one obtains:'b'Which is equivalent to'b'Below is example Python code to draw the sample:'b'This formulation is correct regardless of how the Gamma distributions are parameterized (shape/scale vs. shape/rate) because they are equivalent when scale and rate equal 1.0.'b'and let'b'Finally, set'b'This iterative procedure corresponds closely to the "string cutting" intuition described below.'b'Below is example Python code to draw the sample:'b'Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value \xce\xb1 to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.'b'One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. The \xce\xb1/\xce\xb10 values specify the mean lengths of the cut pieces of string resulting from the distribution. The variance around this mean varies inversely with \xce\xb10.'b'Consider an urn containing balls of K different colors. Initially, the urn contains \xce\xb11 balls of color 1, \xce\xb12 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir(\xce\xb11,...,\xce\xb1K).[16]'b'For a formal proof, note that the proportions of the different colored balls form a bounded [0,1]K-valued martingale, hence by the martingale convergence theorem, these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.'b'Note that each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls.'Pachinko allocation
b'In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents. [1] The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive power than latent Dirichlet allocation.[2] While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The model is named for pachinko machines\xe2\x80\x94a game popular in Japan, in which metal balls bounce down around a complex collection of pins until they land in various bins at the bottom.[3]'b''b''b"Pachinko allocation was first described by Wei Li and Andrew McCallum in 2006.[3] The idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007.[4] In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP).[2] The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst."b'\nPAM connects words in V and topics in T with an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the interior levels and the leaves are words.'b'The probability of generating a whole corpus is the product of the probability for every document:'b''File:Topic model scheme.webm
b'https://creativecommons.org/licenses/by-sa/4.0 CC BY-SA 4.0 Creative Commons Attribution-Share Alike 4.0 truetrue'b'Click on a date/time to view the file as it appeared at that time.\n'b'The following other wikis use this file:\n'Pennsylvania Gazette
b"The Pennsylvania Gazette was one of the United States' most prominent newspapers from 1728, before the time period of the American Revolution, until 1800."b''b''b"The newspaper was first published in 1728 by Samuel Keimer and was the second newspaper to be published in Pennsylvania under the name The Universal Instructor in all Arts and Sciences: and Pennsylvania Gazette, alluding to Keimer's intention to print out a page of Ephraim Chambers' Cyclopaedia, or Universal Dictionary of Arts and Sciences in each copy.[1] On October 2, 1729, Benjamin Franklin and Hugh Meredith bought the paper and shortened its name, as well as dropping Keimer's grandiose plan to print out the Cyclopaedia.[1] Franklin not only printed the paper but also often contributed pieces to the paper under aliases. His newspaper soon became the most successful in the colonies."b'On August 6, 1741 Franklin published an editorial about deceased Andrew Hamilton, a lawyer and public figure in Philadelphia who had been a friend. The editorial praised the man highly and showed Franklin had held the man in high esteem.[2]'b'In 1752, Franklin published a third-person account of his pioneering kite experiment in The Pennsylvania Gazette, without mentioning that he himself had performed it.[3]'b'Primarily a publication for classified ads, merchants and individuals listed notices of employment, lost and found goods and items for sale; the newspaper also reprinted foreign news. Most entries involved stories of travel.[4] In the July 31, 1776 edition the front page lists military movements announced by John Hancock alongside the sale of a plantation in Chester County, Pa., a three-dollar reward for a horse that strayed from home, three pounds reward for the return of a fleeing 23-year-old Irish servant named Jane Stepberd, three pounds reward for runaway Negroe man Moses Graves and the sale of an unnamed "hearty Scotch Servant GIRL".'b"This newspaper, among other firsts, would print the first political cartoon in America, Join, or Die, authored by Franklin himself.[5] It ceased publication in 1800, ten years after Franklin's death.[6]"b'It is claimed that the publication later reemerged as the Saturday Evening Post in 1821.[7]'b'There are three known copies of the original issue, which are held by the Historical Society of Pennsylvania, the Library Company of Philadelphia, and the Wisconsin State Historical Society.[1]'b'Today, The Pennsylvania Gazette moniker is used by an unrelated bi-monthly alumni magazine of the University of Pennsylvania, which Franklin founded and served as a trustee.'b'Archives are available online for a fee.[6]'b'President of Pennsylvania (1785\xe2\x80\x931788), Ambassador to France (1779\xe2\x80\x931785)'Proceedings of the National Academy of Sciences of the United States of America
b'Proceedings of the National Academy of Sciences of the United States of America (PNAS) is the official scientific journal of the National Academy of Sciences, published since 1915. With broad coverage, spanning the biological, physical, and social sciences, the journal publishes original research alongside scientific reviews, commentaries, and letters. In 1999\xe2\x80\x932009, the last period for which data are available, PNAS was the second most cited journal across all fields of science.[1] PNAS is published weekly in print, and daily online in PNAS Early Edition.'b''b''b'PNAS was established by the National Academy of Sciences (NAS) in 1914, with its first issue published in 1915. The NAS itself had been founded in 1863 as a private institution, but chartered by the United States Congress, with the goal to "investigate, examine, experiment, and report upon any subject of science or art". By 1914 the Academy had been well established.'b"Prior to the inception of PNAS, the National Academy of Sciences published three volumes of organizational transactions, consisting mostly of minutes of meetings and annual reports. In accordance with the guiding principles established by astronomer George Ellery Hale, the foreign secretary of NAS in 1914, PNAS publishes brief first announcements of Academy members' and foreign associates' more important contributions to research and of work that appears to a member to be of particular importance.[2]"b'The following people have been editors-in-chief of the journal:'b'The first managing editor of the journal was mathematician Edwin Bidwell Wilson.'b'All research papers published in PNAS are peer-reviewed.[2] The standard mode is for papers to be submitted directly to PNAS rather than going through an Academy member. Members may handle the peer review process for up to 4 of their own papers per year\xe2\x80\x94this is an open review process because the member selects and communicates directly with the referees. These submissions and reviews, like all for PNAS, are evaluated for publication by the PNAS Editorial Board. Until July 1, 2010, members were allowed to communicate up to 2 papers from non-members to PNAS every year. The review process for these papers was anonymous in that the identities of the referees were not revealed to the authors. Referees were selected by the NAS member.[2][4][5] PNAS eliminated communicated submissions through NAS members as of July 1, 2010, while continuing to make the final decision on all PNAS papers.[6]'b'In 2003, PNAS issued an editorial stating its policy on publication of sensitive material in the life sciences.[7] PNAS stated that it would "continue to monitor submitted papers for material that may be deemed inappropriate and that could, if published, compromise the public welfare." This statement was in keeping with the efforts of several other journals.[8][9] In 2005 PNAS published an article titled "Analyzing a bioterror attack on the food supply: The case of botulinum toxin in milk"[10] despite objections raised by the U.S. Department of Health and Human Services.[11] The paper was published with a commentary by the president of the Academy at the time, Bruce Alberts, titled "Modeling attacks on the food supply".[12]'b'PNAS is widely read by researchers, particularly those involved in basic sciences, around the world. PNAS Online receives over 21 million hits per month.[13] The journal is notable for its policy of making research articles freely available online to everyone six months after publication (delayed open access), or immediately if authors have chosen the "open access" option (hybrid open access). Immediately free online access (without the six-month delay) is available to more than 100 developing countries[14] and for some categories of papers such as colloquia. Abstracts, tables of contents, and online supporting information are free. Anyone can sign up to receive free tables of contents by email.[15]'b'Because PNAS is self-sustaining and receives no direct funding from the U.S.\xc2\xa0government or the National Academy of Sciences, the journal charges authors publication fees and subscription fees to offset the cost of the editorial and publication process.'b'According to the Journal Citation Reports, the journal has a 2015 impact factor of 9.423.[16] PNAS is the second most cited scientific journal, with nearly 1.4\xc2\xa0million citations from 1999 to 2009 (the Journal of Biological Chemistry is the most cited journal over this period).[17]'b'PNAS has received occasional criticism for releasing papers to science journalists as much as a week before making them available to the general public; this practice is known as a news embargo.[18] According to critics, this allows mainstream news outlets to misrepresent or exaggerate the implications of experimental findings before the scientific community is able to respond.[19][20] Science writer Ed Yong, on the other hand, has argued that the real problem is not embargoes themselves, but the press releases issued by research institutes and universities.[18]'b'In January 2011, PNAS started considering manuscripts for exclusive online publication, "PNAS Plus" papers.[21] These have a larger maximum page limit (10 rather than 6 pages). Accompanying these papers both online and in print was a one- to two-page summary description written by the authors for a broad readership. Since mid-October 2012, PNAS Plus authors no longer need to submit author summaries and are instead asked to submit a 120-word-maximum statement about the significance of their paper. The significance statement will appear both online and in print.[22] Since July 15, 2013, the significance statement is required for all research articles.'b"In 2006 PNAS launched a new section of the journal dedicated to sustainability science, an emerging field of research dealing with the interactions between natural and social systems, and with how those interactions affect the challenge of sustainability: meeting the needs of present and future generations while substantially reducing poverty and conserving the planet's life support systems. See the Sustainability Science portal here."Richmond Times-Dispatch
b'The Richmond Times-Dispatch (RTD or TD for short) is the primary daily newspaper in Richmond, the capital of Virginia, United States. It is also the primary newspaper of record for the state of Virginia.[2][3][4]'b''b''b"The Times-Dispatch has the second-highest circulation of any Virginia newspaper, after Norfolk's The Virginian-Pilot.[5] In addition to the Richmond area (Petersburg, Chester, Hopewell, Colonial Heights and surrounding areas), the Times-Dispatch has substantial readership in Charlottesville, Lynchburg, and Waynesboro. As the primary paper of the state's capital, the Times-Dispatch serves as a newspaper of record for rural regions of the state that lack large local papers."b'Although the Richmond Compiler published in Virginia\'s capitol beginning in 1815, and merged with a later newspaper called The Times, the Times and Compiler failed in 1853, despite an attempt of former banker James A. Cowardin and William H. Davis to revive it several years before. In 1850, Cowardin and Davis established a rival newspaper called the Richmond Dispatch, and by 1852 the Dispatch bragged of having circulation three times as large as any other daily paper in the city, and advertising dominated even its front page. Cowardin began his only term in the Virginia House of Delegates (as a Whig) in 1853, but many thought the city\'s pre-eminent paper the Richmond Examiner.[6] John Hammersley bought half of the newspaper company in 1859, and continued as a joint publisher on the masthead until May 5, 1862, when no name appeared. By April 1861, the newspaper announced its circulation was \xe2\x80\x9cwithin a fraction of 13,000.\xe2\x80\x9d[7] The newspaper had been staunchly pro-slavery since 1852, and called Union soldiers "thieves and cut-throats".[8] Most of its wartime issues are now available online.[9] In 1864, Hammersley brought new presses from England, having run the Union blockade, although he sold half his interest to James W. Lewellen before his dangerous departure (presumably through Wilmington, North Carolina, the last Southern port open to Confederate vessels in 1864).'b'The Richmond Daily Dispatch published its last wartime issue on April 1, 1865; and its office was destroyed the next night during the fire set by Confederate soldiers as they left the city. However, it resumed publication on December 9, 1865, establishing a new office at 12th and Main Streets and accepting Henry K. Ellyson as part-owner as well as editor.[10] By 1866, the Dispatch was one of five papers "carrying prestige from ante bellum days" published in Richmond (of 7 newspapers). Although the newspaper initially opposed the Ku Klux Klan, the Richmond Dispatch accepted Klan advertising in 1868, as it fought Congressional Reconstruction and the Virginia Constitutional Convention of 1868. However, it later accepted the resulting state constitution (after anti-Confederate provisions were stripped) as well as allowing Negroes on juries and in the legislature. Ellyson briefly served as Richmond\'s mayor in 1870, selected by Richmond\'s city council appointed by Governor Gilbert C. Walker. After what some called the "Municipal War" because the prior appointed mayor George Chahoon refused to relinquish his office and mob violence and blockades, the Virginia Supreme Court declared Ellyson the mayor but awaited elections. After skullduggery concerning stolen ballots in the pro-Chahoon Jackson Ward and the election commission declared Ellyson the winner, he refused to serve under the resulting cloud, leading to yet another problematic election won by the Conservative Party candidate. The revived Dispatch later opposed former Confederate General William Mahone and his Readjuster Party.[11] After James Cowardin died in 1882, his son Charles took the helm (with Ellyson\'s assistance, and with Ellyson family members handling business operations), and the paper stopped supporting Negro rights, instead criticizing Del. John Mercer Langston with racial stereotypes.[12]'b"In 1886, Lewis Ginter founded the Richmond Daily Times. A year later, lawyer Joseph Bryan (1845-1908) bought the Daily Times from Ginter, beginning the paper's long association with the Bryan family. Bryan and Ginter had previously helped revitalize the Tanner & Delany Engine Company, transforming it into the Richmond Locomotive Works, which had 800 employees by 1893 and built 200 locomotives per year. In 1890, the Daily Times changed its name to the Richmond Times. In 1896, Bryan acquired the eight-year-old rival Manchester Leader and launched the Evening Leader. In 1899, the evening Richmond News was founded. John L. Williams, owner of the Dispatch, bought the News in 1900."b'By 1903, it was obvious Richmond was not big enough to support four papers. That year, Williams and Bryan agreed to merge Richmond\'s main newspapers. The morning papers merged to become the Richmond Times-Dispatch under Bryan\'s ownership, while the evening papers merged to become The Richmond News Leader under Williams\' ownership. Bryan bought the News Leader in 1908, but died later that year. (Joseph Bryan Park was donated by his widow, Isobel ("Belle") Stewart Bryan, and named for him).'b"His son John Stewart Bryan had given up his own legal career in 1900 to become a reporter working for the Dispatch and helped found the Associated Press and then became vice-president of the publishing company.[13] Upon his father's death, John Stewart Bryan became owner and publisher of the two papers, but in 1914 sold a controlling interest in the Times-Dispatch to three families. He hired Douglas Southall Freeman as editor of the News Leader in 1915, and remained in control until becoming President of the College of William and Mary in 1934 (and publishing a biography of his father the following year). John Stewart Bryan but reacquired the Times-Dispatch in 1940 when the two papers' business interests merged to form Richmond Newspapers, in which Bryan held a 54-percent interest. That conglomeration is now known as Media General. Other publishers in the Bryan family include D. Tennant Bryan and John Stewart Bryan III."b'On June 1, 1992, four days after its sponsored contestant Amanda Goad won the Scripps National Spelling Bee, the News Leader, which had been losing circulation for many years, ceased publication and was folded into the Times-Dispatch.'b"The Richmond Times-Dispatch drew national attention for its coverage of a December 21, 2004, attack by a suicide bomber on an American military base in Mosul, Iraq. The deadliest attack on an American military installation since the war began, the attack injured 69 people and killed 22, including two with the Virginia National Guard's Richmond-based 276th Engineer Battalion. Stories and photographs about the attack by a Times-Dispatch reporter embedded with the 276th were read, heard and seen across the nation."b'In 1990, The RTD borrowed an idea [14] from a local entrepreneur, Barry "Mad Dog" Gottlieb, to encourage a "Tacky Christmas Lights Tour," also known by locals as the "Tacky Light Tour". Every week, the RTD lists the addresses of houses where the most tacky Christmas lights can be found. This tradition has begun to spread to other cities, like Fairfax, Virginia (DC area) [15] as well as San Francisco and Los Angeles.'b"Diane Cantor, the wife of former House Majority Leader Republican Eric Cantor, sits on Media General's Board of Directors.[16] This drew some conflict-of-interest allegations because the RTD serves much of the congressman's 7th district, but no evidence surfaced that she was involved in the paper's content. Her association with the paper was noted at the end of Times-Dispatch stories about Rep. Cantor."b"On May 17, 2012, Media General [17] announced the sale of its newspaper division to BH Media, a subsidiary of Warren Buffett's Berkshire Hathaway company. The sale included all of Media General's newspapers except The Tampa Tribune and its associated publications. Berkshire Hathaway bought 63 newspapers for $142 million and, as part of the deal, offered Media General a $400 million term loan at 10.5 percent interest that will mature in 2020 and a $45 million revolving line of credit. Berkshire Hathaway received a seat on Media General's board of directors and an option to purchase a 19.9% stake in the company.[18] The deal closed on June 25, 2012."b"This also brought to a close Diane Cantor's relationship with the RTD."b'A prominent newspaper in the state, the Times-Dispatch frequently features commentary from important figures from around Virginia, such as officials and presidents from Virginia Commonwealth University, the College of William and Mary, and the University of Virginia. Former Richmond Mayor Douglas Wilder, who had articles published in the paper before he held that position, often outlined policies his administration was implementing. During the 2004 U.S. presidential campaign, its Commentary sections featured some pieces by Retired Admiral Roy Hoffmann, a founding member of the Swift Boat Veterans for Truth and resident of Richmond suburb Chesterfield, against Democratic candidate John Kerry.'b"Editorially, the Times-Dispatch has historically leaned conservative, leading the paper to frequently endorse candidates of the Republican Party. It supported many of former President George W. Bush's policies, including the 2003 invasion of Iraq and a flat income tax. However, the paper is not unilaterally conservative; for example, a 2005 editorial called for the then House Majority Leader Tom DeLay to relinquish his leadership position on ethical grounds. There are also some liberal syndicated columnists who appear frequently, especially Leonard Pitts."b'During the Civil Rights Movement, the Times-Dispatch, like nearly every major newspaper in Virginia, was an ardent supporter of segregation.[19]'b"In the 2016 presidential election, the Times-Dispatch endorsed Libertarian candidate Gary Johnson over major party candidates Donald Trump and Hillary Clinton. Clinton's running mate, Tim Kaine, is a Richmond resident who served as mayor of the city from 1998-2001. From at least 1980 until its Johnson endorsement in 2016, the Times-Dispatch had only endorsed Republican presidential candidates.[20]"b'Like most major papers, the sports section has MLB, NASCAR, MLS, NBA, NCAA, NFL, and NHL scores and results. The Times-Dispatch sports pages naturally focus on Richmond and Virginia professional and college teams. In addition to Richmond Flying Squirrels and Richmond Kickers coverage, readers can see in-depth coverage of the Washington Redskins in the fall and the Washington Nationals in the summer. "Virginians in the Pros" and similar features track all sorts of professional athletes who were born, lived in, or attended college in Virginia. Large automobile racing events like the Sprint Cup (at the Richmond International Raceway) are often given a separate preview guide.'b'Catering to the vast array of Virginia hunters, fishers, hikers, and outdoorsmen, somewhere between half a page to a whole page most days is dedicated to outdoors articles, written by Lee Graves, who succeeded Garvey Winegar in November 2003. The "Scoreboard," which features minor-league standings, Vegas betting, and other sports scores, also gives tide measurements, river levels, and skiing conditions, depending on the season.'b'Virginians have traditionally been highly supportive of high school athletics, and its flagship paper is a testament to that. Particular emphasis is given to American football and basketball; The Times-Dispatch ranks area teams in these sports, in the style of the NCAA polls, and generally updates them weekly. In the fall, Sunday editions have the scores of all high school football games played that weekend from across the state. Prep games are also receive above-average coverage in baseball, cross country, golf, lacrosse, soccer, softball, swimming, tennis, track and field, and volleyball. Stories are frequently done on notable prep athletes, such as those from foreign countries, those with disabilities, those who play a multitude of sports, or those who had little or no prior experience in a sport which they now excel in.'b'The business desk consists of six reporters; they cover technology, retail, energy, insurance, banking, economics, real estate, manufacturing, transportation and consumer issues. Unlike many newspapers, the Times-Dispatch produces a widely read Monday business section, Metro Business. It contains a center cover story on a regional business-related issue and is filled with events for the coming week, advice columnists and gadget reviews. In June 2006, the decision was made to remove the stock tables from the daily sections beginning July 15 and replace the numerous pages with a "Markets Review" section for subscribers who request it. The stock section was eliminated in 2009, as was the Sunday Real Estate section (both were cost-cutting moves). The Sunday Business section, which had been a showcase of general business-interest stories and features, has been rechristened Moneywise and now features primarily consumer-related coverage. Moneywise is also among select Sunday business sections nationwide that print Wall Street Journal Sunday pages.'b'On July 12, 2006, Richmond-based news magazine Style Weekly ran a cover story [21] titled "Truth and Consequences," a piece that took a look at the Times-Dispatch\'s operations as the paper settled into its first year with new management. The report described new editor Glenn Proctor, who took over Nov. 14, 2005, as an "inelegant, blunt and harsh critic \xe2\x80\x94 to the point of saying, repeatedly, that some reporters\' work \'sucks.\'" The piece described a newsroom teetering on the edge, preparing for promised changes \xe2\x80\x94 such as possible layoffs, fewer pages and combined sections \xe2\x80\x94 that eventually were realized. On April 2, 2009, the Times-Dispatch cut 90 jobs, laying off 59 workers, including 28 newsroom jobs. Proctor left the paper in 2011.'b'The front page of the Times-Dispatch\xe2\x80\x99s August 14, 2011 Sunday paper consisted entirely of a Wells Fargo advertisement, commemorating said bank\xe2\x80\x99s acquisition of Wachovia properties in Virginia.[22]'b'Notable columnists published include:'American Civil War
b'Union victory'b'2,200,000:[a]'b'750,000\xe2\x80\x931,000,000:[a][4]'b'110,000+ killed in action/died of wounds\n230,000+ accident/disease deaths[6][7]\n25,000\xe2\x80\x9330,000 died in Confederate prisons[2][6]'b'365,000+ total dead[8] 282,000+ wounded[7]\n181,193 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][9]'b'94,000+ killed in action/died of wounds[6]\n26,000\xe2\x80\x9331,000 died in Union prisons[7]'b'290,000+ total dead\n137,000+ wounded\n436,658 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][10]'b"The American Civil War (known by other names) was a civil war that was fought in the United States from 1861 to 1865. As a result of the long-standing controversy over slavery, war broke out in April 1861, when Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated. The nationalists of the Union proclaimed loyalty to the U.S. Constitution. They faced secessionists of the Confederate States, who advocated for states' rights to expand slavery."b'Among the 34 U.S. states in February 1861, seven Southern slave states individually declared their secession from the U.S. to form the Confederate States of America, or the South. The Confederacy grew to include eleven slave states. The Confederacy was never diplomatically recognized by the United States government, nor was it recognized by any foreign country (although the United Kingdom and France granted it belligerent status). The states that remained loyal to the U.S. (including the border states where slavery was legal) were known as the Union or the North.'b"The Union and Confederacy quickly raised volunteer and conscription armies that fought mostly in the South over four years. The Union finally won the war when General Robert E. Lee surrendered to General Ulysses S. Grant at the Battle of Appomattox Court House, followed by a series of surrenders by Confederate generals throughout the southern states. Four years of intense combat left 620,000 to 750,000 people dead, more than the number of U.S. military deaths in all other wars combined (at least until approximately the Vietnam War).[15] Much of the South's infrastructure was destroyed, especially the transportation systems, railroads, mills, and houses. The Confederacy collapsed, slavery was abolished, and 4 million slaves were freed. The Reconstruction Era (1863\xe2\x80\x931877) overlapped and followed the war, with the process of restoring national unity, strengthening the national government, and granting civil rights to freed slaves throughout the country. The Civil War is the most studied and written about episode in U.S. history.[16]"b''b''b"In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories. The Southern states viewed this as a violation of their constitutional rights and as the first step in a grander Republican plan to eventually abolish slavery. The three pro-Union candidates together received an overwhelming 82% majority of the votes cast nationally: Republican Lincoln's votes centered in the north, Democrat Stephen A. Douglas' votes were distributed nationally and Constitutional Unionist John Bell's votes centered in Tennessee, Kentucky, and Virginia. The Republican Party, dominant in the North, secured a plurality of the popular votes and a majority of the electoral votes nationally, so Lincoln was constitutionally elected president. He was the first Republican Party candidate to win the presidency. However, before his inauguration, seven slave states with cotton-based economies declared secession and formed the Confederacy. The first six to declare secession had the highest proportions of slaves in their populations, a total of 49 percent.[17] The first seven with state legislatures to resolve for secession included split majorities for unionists Douglas and Bell in Georgia with 51% and Louisiana with 55%. Alabama had voted 46% for those unionists, Mississippi with 40%, Florida with 38%, Texas with 25%, and South Carolina cast Electoral College votes without a popular vote for president.[18] Of these, only Texas held a referendum on secession."b'Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln\'s March 4, 1861, inaugural address declared that his administration would not initiate a civil war. Speaking directly to the "Southern States", he attempted to calm their fears of any threats to slavery, reaffirming, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so."[19] After Confederate forces seized numerous federal forts within territory claimed by the Confederacy, efforts at compromise failed and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene, but none did, and none recognized the new Confederate States of America.'b"Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, the battle was inconclusive from 1861\xe2\x80\x931862. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal.[20] To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies, and seized New Orleans. The 1863 Union Siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William T. Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years, known as the Reconstruction Era."b'The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships and iron-clad ships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I, World War II and subsequent conflicts. It remains the deadliest war in American history. From 1861 to 1865, it is estimated that 620,000 to 750,000 soldiers died,[21] along with an undetermined number of civilians.[b] By one estimate, the war claimed the lives of 10 percent of all Northern males 20\xe2\x80\x9345 years old, and 30 percent of all Southern white males aged 18\xe2\x80\x9340.[23]'b'The causes of secession were complex and have been controversial since the war began, but most academic scholars\xc2\xa0identify\xc2\xa0slavery as a central cause of the war. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war.[24] Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to promote pro-slavery acts and policies.[25][26]'b'Slavery was a major cause of disunion.[27] Although there were opposing views even in the Union States,[28][29] most northern soldiers were largely indifferent on the subject of slavery,[30] while Confederates fought the war largely to protect a southern society of which slavery was an integral part.[31] From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with republicanism. The strategy of the anti-slavery forces was containment\xe2\x80\x94to stop the expansion and thus put slavery on a path to gradual extinction.[32] The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights.[33] Southern whites believed that the emancipation of slaves would destroy the South\'s economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population.[34] In particular, southerners feared a repeat of "the horrors of Santo Domingo", in which nearly all white people \xe2\x80\x93 including men, women, children, and even many sympathetic to abolition \xe2\x80\x93 were killed after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea, and proposes it contributed to the segregation in the Jim Crow era following emancipation.[35] These fears were exacerbated by the recent attempts of John Brown to instigate an armed slave rebellion in the South.'b'Slavery was illegal in much of the North, having been outlawed in the late 18th and early 19th centuries. It was also fading in the border states and in Southern cities, but it was expanding in the highly profitable cotton districts of the rural South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide.'b"Sectionalism refers to the different economies, social structure, customs and political values of the North and South.[36][37] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention which manifested Northern dissastisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern Presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for poor freedmen. In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations.[38]"b'Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[39][40]'b'Historically, southern slave-holding states, because of their low-cost manual labor, had little perceived need for mechanization and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.[41]'b'The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress.[42][43] The tariff issue was and is sometimes cited\xe2\x80\x93long after the war\xe2\x80\x93by Lost Cause historians and neo-Confederate apologists. In 1860\xe2\x80\x9361 none of the groups that proposed compromises to head off secession raised the tariff issue.[44] Pamphleteers North and South rarely mentioned the tariff,[45] and when some did, for instance, Matthew Fontaine Maury[46] and John Lothrop Motley,[47] they were generally writing for a foreign audience.'b'The South argued that each state had the right to secede\xe2\x80\x94leave the Union\xe2\x80\x94at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a perpetual union.[48] Historian James McPherson writes concerning states\' rights and other non-slavery explanations:'b"While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[49]"b'Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. It was over territories west of the Mississippi that the proslavery and antislavery forces collided.[50]'b'With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well.[51][52] Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the southern states the question of the territorial expansion of slavery westward again became explosive.[53] Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."[54][55]'b'By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[56] The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.[57]'b'The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance\xe2\x80\x94that slavery could be excluded in a territory as it was done in the Northwest Ordinance of 1787 at the discretion of Congress,[58] thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.[59]'b'Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty\xe2\x80\x94which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter.[60] The Kansas\xe2\x80\x93Nebraska Act of 1854 legislated this doctrine.[61] In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when southern senators began to leave.[62]'b'The fourth theory was advocated by Mississippi Senator Jefferson Davis,[63] one of state sovereignty ("states\' rights"),[64] also known as the "Calhoun doctrine",[65] named after the South Carolinian political theorist and statesman John C. Calhoun.[66] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[67] "States\' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[68] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power."[69][70] These four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the U.S. Constitution prior to the 1860 presidential election.[71]'b'Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in the sense that their country was a national republic based on the belief that all people had inalienable political liberty and personal rights which could serve as an important example to the rest of the world. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, the division and redivision of the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state.'b'In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure\xe2\x80\x94first in their political institutions, then in their civil life together.'b'Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy.[72] C. Vann Woodward said of the latter group,'b'A great slave society\xc2\xa0... had grown up and miraculously flourished in the heart of a thoroughly bourgeois and partly puritanical republic. It had renounced its bourgeois origins and elaborated and painfully rationalized its institutional, legal, metaphysical, and religious defenses\xc2\xa0... When the crisis came it chose to fight. It proved to be the death struggle of a society, which went down in ruins.[73]'b"Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin (1852)[74] and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.[75]"b'While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion\xc2\xa0... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence."[76] The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.[77]'b'The election of Abraham Lincoln in November 1860 was the final trigger for secession.[78] Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.'b'According to Lincoln, the people of the United States had shown that they can be successful in establishing and administering a republic, but a third challenge faced the nation, maintaining the republic, based on the people\'s vote. The people must now show: "successful maintenance [of the Republic] against a formidable internal attempt to overthrow it. It is now for them to demonstrate to the world that those who can fairly carry an election can also suppress a rebellion; that ballots are the rightful and peaceful successors of bullets; and that when ballots have fairly and constitutionally decided, there can be no successful appeal back to bullets; that there can be no successful appeal, except to ballots themselves, at succeeding elections. Such will be a great lesson of peace; teaching men that what they cannot take by an election, neither can they take it by a war".[79]'b'The election of Lincoln caused the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws and, even, secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860, and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states\' rights for slave owners in the South, but contained a complaint about states\' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.'b'Among the ordinances of secession passed by the individual states, those of three\xe2\x80\x94Texas, Alabama, and Virginia\xe2\x80\x94specifically mentioned the plight of the "slaveholding states" at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures.[80] However, at least four states\xe2\x80\x94South Carolina,[81] Mississippi,[82] Georgia,[83] and Texas[84]\xe2\x80\x94also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement\'s influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive slave clause of the Constitution.'b'These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861.[85] They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress".[86] One quarter of the U.S. Army\xe2\x80\x94the entire garrison in Texas\xe2\x80\x94was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.'b'As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts),[87] the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.'b"On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it.[88] It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would have voted in favor of it.[89] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[90]"b'On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[91] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[91]'b'The South sent delegations to Washington and offered to pay for the federal properties[which?] and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[92] Secretary of State William Seward, who at the time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed.[92] President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina\'s Fort Sumter.'b"Fort Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. fort's garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan, who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12\xe2\x80\x9313, forcing its capitulation."b'The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins said:'b"However, much of the North's attitude was based on the false belief that only a minority of Southerners were actually in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners really did favor secession, they might have hesitated at attempting the enormous task of conquering a united South.[95]"b'Lincoln called on all the states to send forces to recapture the fort and other federal properties. With the scale of the rebellion apparently small so far, Lincoln called for only 75,000 volunteers for 90\xc2\xa0days.[96] The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal.[97] On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.[98]'b'Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.[99]'b'Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. West Virginia then joined them as an additional border state after it separated from Virginia and became a state of the Union in 1863.'b"Maryland's territory surrounded the United States' capital of Washington, DC and could cut it off from the North.[100] It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53\xe2\x80\x9313) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war.[101] Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North.[102] Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened.[101][103] All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.[104]"b'In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. (See also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[105]'b'Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.[106]'b"After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving).[107] The inclusion of 24 secessionist counties[108] in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war.[109][110] Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000\xe2\x80\x9322,000 soldiers to both the Confederacy and the Union.[111]"b'A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[112]'b'The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy\'s soldier.[113]'b'As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[114] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.[115]'b'In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law\xe2\x80\x94conscription\xe2\x80\x94as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt.[116] The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.[117]'b"When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[118] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.[119]"b'In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[120] At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units.[121] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[122]'b'From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan\'s assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.[123]'b'Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:'b"Some historians emphasize that Civil War soldiers were driven by political ideology, holding firm beliefs about the importance of liberty, Union, or state rights, or about the need to protect or to destroy slavery. Others point to less overtly political reasons to fight, such as the defense of one's home and family, or the honor and brotherhood to be preserved when fighting alongside other men. Most historians agree that no matter what a soldier thought about when he went into the war, the experience of combat affected him profoundly and sometimes altered his reasons for continuing the fight.[124]"b"At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their own army where they were paid but not allowed to perform any military duties.[125] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[126]"b'The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396.[127][128] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[129] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.'b"By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible.[130] Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.[131]"b'In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[132]'b'The Civil War occurred during the early stages of the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union\'s naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[133] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union\'s own ironclad warships, they were unsuccessful.[134]'b"The Confederacy experimented with a submarine, which did not work well,[135] and with building an ironclad ship, the CSS Virginia, which was based on rebuilding a sunken Union ship, the Merrimack. On its first foray on March 8, 1862, the Virginia inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, the USS Monitor, arrived to challenge it in the Chesapeake Bay. The resulting three hour battle between the Ironclads was a draw, but it marked the worldwide transition to ironclad warships.[136] Not long after the battle the Confederacy was forced to scuttle the Virginia to prevent its capture, while the Union built many copies of the Monitor. Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Britain.[137]"b'British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were designed for speed and were so small that only a small amount of cotton went out.[138] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released.[139] The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[140]'b"Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[141] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.[142]"b'To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested.[134] After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.[143]'b'The 1862 Union strategy called for simultaneous advances along four axes:[144]'b'Ulysses Grant used river transport and Andrew Foote\'s gunboats of the Western Flotilla to threaten the Confederacy\'s "Gibraltar of the West" at Columbus, Kentucky. Though rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.[145]'b"In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[146] They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry (February 6, 1862) and Fort Donelson (February 11 to 16, 1862), and supplied Grant's forces as he moved into Tennessee. At Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory\xe2\x80\x94the first battle with the high casualty rates that would repeat over and over.[147] Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. On April 24, 1862, U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[148]"b'Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at Vicksburg, Mississippi in July 1863, and in the Union fully controlling the Mississippi River soon after.[149]'b'In one of the first highly visible battles, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces near Washington was repulsed.'b"Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign,[150][151][152] Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[153] The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[154] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops."b"Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[153][155] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[156]"b"When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[157] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker."b"Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863.[158] Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[159] Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[160] This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[161] However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant."b"While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[162] Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization."b'The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans,[163] which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.'b"General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state.[164] Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.[165]"b"The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged."b"The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh;[166] and the Battle of Vicksburg,[167] which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[168] driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy."b'Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[171] Roving Confederate bands such as Quantrill\'s Raiders terrorized the countryside, striking both military installations and civilian settlements.[172] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.'b'By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.[169]'b'Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union.[173] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[174]'b'After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him.[175] Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.'b'At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[176] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[177] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[178]'b"Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[179]"b"Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[180]"b"Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president.[181] Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.[182]"b'Leaving Atlanta, and his base of supplies, Sherman\'s army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman\'s army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee\'s army.[183]'b'Lee\'s army, thinned by desertion and casualties, was now much smaller than Grant\'s. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler\'s Creek.[184]'b"Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House.[185] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller."b"On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[186] On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William T. Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day.[1] On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department.[187] On June 23, Cherokee leader Stand Watie became the last Confederate General to surrender his forces.[188]"b"Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators.[189][190] The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.[191]"b'Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860\xe2\x80\x9362 crop failures in Europe made the North\'s grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half.[191] When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.[192]'b'Lincoln\'s foreign policy was deficient in 1861 in terms of appealing to European public opinion. Diplomats had to explain that United States was not committed to the ending of slavery, but instead they repeated legalistic arguments about the unconstitutionality of secession. Confederate spokesmen, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. In addition, the European aristocracy (the dominant factor in every major country) was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[193]'b'U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (CSS Alabama, CSS Shenandoah, CSS Tennessee, CSS Tallahassee, CSS Florida, and some others). The most famous, the CSS Alabama, did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for politicians in Britain, where the antislavery movement was powerful.[194]'b"War loomed in late 1861 between the U.S. and Britain over the Trent affair, involving the U.S. Navy's boarding of the British ship Trent and seizure of two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation between North and South\xe2\x80\x93 though even such an offer would have risked war with the U.S. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on this.[195]"b"The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.[196]"b'The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty.[197]'b"Historians have debated whether the Confederacy could have won the war. Most scholars, including James McPherson, argue that Confederate victory was at least possible.[198] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.[199]"b'Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[199] Lincoln was not a military dictator, and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.[203]'b'Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat.[204][205] Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back\xc2\xa0... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don\'t think the South ever had a chance to win that War."[206]'b'A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win."[207][208] Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederate army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict.[209] However, most historians reject the argument.[210] James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864\xe2\x80\x9365, he says most Confederate soldiers were fighting hard.[211] Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up\xe2\x80\x94some few deserters\xe2\x80\x94plenty tired of war, but the masses determined to fight it out."[212]'b"Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[213] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[214]"b'Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[215] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:'b'The North\'s victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[216]'b'Scholars have debated what the effects of the war were on political and economic power in the South.[217] The prevailing view is that the southern planter elite retained its powerful position in the South.[217] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[217]'b'The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths\xe2\x80\x94two-thirds by disease, and 50,000 civilians.[11] Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000.[21][218] The war accounted for more American deaths than in all other U.S. wars combined.[219]'b'Based on 1860 census figures, 8 percent of all white males aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[220][221] About 56,000 soldiers died in prison camps during the War.[222] An estimated 60,000 men lost limbs in the war.[223]'b'Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:[6]'b'In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[7]'b'Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle.[6] Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War.[224]:16 Notably, their mortality rate was significantly higher than white soldiers:'b'[We] find, according to the revised official data, that of the slightly over two millions troops in the United States Volunteers, over 316,000 died (from all causes), or 15.2 percent. Of the 67,000 Regular Army (white) troops, 8.6 percent, or not quite 6,000, died. Of the approximately 180,000 United States Colored Troops, however, over 36,000 died, or 20.5 percent. In other words, the mortality "rate" amongst the United States Colored Troops in the Civil War was thirty-five percent greater than that among other troops, notwithstanding the fact that the former were not enrolled until some eighteen months after the fighting began.[224]:16'b"Confederate records compiled by historian William F. Fox list 74,524 killed and died of wounds and 59,292 died of disease. Including Confederate estimates of battle losses where no records exist would bring the Confederate death toll to 94,000 killed and died of wounds. Fox complained, however, that records were incomplete, especially during the last year of the war, and that battlefield reports likely under-counted deaths (many men counted as wounded in battlefield reports subsequently died of their wounds). Thomas L. Livermore, using Fox's data, put the number of Confederate non-combat deaths at 166,000, using the official estimate of Union deaths from disease and accidents and a comparison of Union and Confederate enlistment records, for a total of 260,000 deaths.[6] However, this excludes the 30,000 deaths of Confederate troops in prisons, which would raise the minimum number of deaths to 290,000."b'The United States National Park Service uses the following figures in its official tally of war losses:[2]'b'Union: 853,838'b'Confederate: 914,660'b"While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, Superintendent of the 1870 Census, used census and Surgeon General data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 Census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.[218]"b'Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[22] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.'b"Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in an area where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor James Downs states that tens to hundreds of thousands of slaves died during the war from disease, starvation, exposure, or execution at the hands of the Confederates, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.[225]"b'Losses were far higher than during the recent defeat of Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Mini\xc3\xa9 balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.[226]'b"The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.[227]"b'The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century.[228] The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.'b'While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee\'s army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[229] Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal.[230] Lincoln\'s decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[231] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans\' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.[232]'b'The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army.[e] About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.[f]'b'During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[238] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.[239]'b'At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Fr\xc3\xa9mont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected.[240] But only the District of Columbia accepted Lincoln\'s gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat".[241] Lincoln laid the groundwork for public support in an open letter published in abolitionist Horace Greeley\'s newspaper.[242]'b'In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors\' Conference added support for the proclamation.[243] Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong\xc2\xa0... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling\xc2\xa0... I claim not to have controlled events, but confess plainly that events have controlled me."[244]'b"Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.[245]"b"Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[246] The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France.[247] By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.[248]"b'In Texas v. White, 74 U.S. 700 (1869) the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.[249]'b'Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877.[250] It comprised multiple complex methods to resolve the outstanding issues of the war\'s aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery\xe2\x80\x94and prevent semi-slavery status.[251]'b'President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson\'s work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended.[252] With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.'b'The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war\'s aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[253] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[254]'b'Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academe, leading to a proliferation of solid studies by non-scholars who are thoroughly familiar with the primary sources, pay close attention to battles and campaigns, and write for the large public readership, rather than the small scholarly community. Bruce Catton and Shelby Foote are among the best-known writers.[255][256] Practically every major figure in the war, both North and South, has had a serious biographical study.[257] Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:'b"God had chastised them and given them a special mission\xe2\x80\x94to maintain orthodoxy, strict biblicism, personal piety, and traditional race relations. Slavery, they insisted, had not been sinful. Rather, emancipation was a historical tragedy and the end of Reconstruction was a clear sign of God's favor.[258]"b'In sharp contrast, Black preachers interpreted the Civil War as:'b"God's gift of freedom. They appreciated opportunities to exercise their independence, to worship in their own way, to affirm their worth and dignity, and to proclaim the fatherhood of God and the brotherhood of man. Most of all, they could form their own churches, associations, and conventions. These institutions offered self-help and racial uplift, and provided places where the gospel of liberation could be proclaimed. As a result, black preachers continued to insist that God would protect and help him; God would be their rock in a stormy land.[259]"b'Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations.[260] Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[261] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing African-American progress to a white man\'s reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[262]'b"The interpretation of the Civil War presented by Charles A. Beard and Mary R. Beard in The Rise of American Civilization (1927) was highly influential among historians and the general public until the civil rights movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:"b'[A] social cataclysm in which the capitalists, laborers, and farmers of the North and West drove from power in the national government the planting aristocracy of the South.[263]'b'The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.[264]'b"The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861, but the oldest surviving monument is the Hazen monument, erected at Stones River near Murfreesboro, Tennessee, in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead in the Battle of Stones River. In the 1890s, the United States government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park in Tennessee and the Antietam National Battlefield in Maryland in 1890. The Shiloh National Military Park was established in 1894, followed by the Gettysburg National Military Park in 1895 and Vicksburg National Military Park in 1899. In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service.[265]"b'The modern Civil War battlefield preservation movement began in 1987 with the founding of the Association for the Preservation of Civil War Sites (APCWS), a grassroots organization created by Civil War historians and others to preserve battlefield land by acquiring it. In 1991, the original Civil War Trust was created in the mold of the Statue of Liberty/Ellis Island Foundation, but failed to attract corporate donors and soon helped manage the disbursement of U.S. Mint Civil War commemorative coin revenues designated for battlefield preservation. Although the two organizations joined forces on a number of battlefield acquisitions, ongoing conflicts prompted the boards of both organizations to facilitate a merger, which happened in 1999 with the creation of the Civil War Preservation Trust. In 2011, the organization was renamed The Civil War Trust. From 1987 through late 2017, The Trust and its predecessor organizations saved more than 40,000 acres at 126 Civil War battlefields and sites in 21 states.[266]'b"The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary. [267] Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as Birth of a Nation (1915), Gone with the Wind (1939), and more recently Lincoln (2012). Ken Burns produced a notable PBS series on television titled The Civil War (1990). It was digitally remastered and re-released in 2015."b'There were numerous technological innovations during the Civil War that had a great impact on 19th century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[268] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[269][270] It was also in this war when countries first used aerial warfare, in the form of reconnaissance balloons, to a significant effect.[271] It saw the first action involving steam-powered ironclad warships in naval warfare history.[272] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare, as well as the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[273]'b'General reference'b'Union'b'Confederacy'b'Ethnic articles'b'Topical articles'b'National articles'b'State articles'b'Memorials'b''Singular-value decomposition
b'The singular-value decomposition can be computed using the following observations:'b'Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.'b'Suppose M is a m \xc3\x97 n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the form'b'where'b'The diagonal entries \xcf\x83i of \xce\xa3 are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, \xce\xa3, is uniquely determined by M (though not the matrices U and V, see below).'b'In the special, yet common case when M is an m \xc3\x97 m real square matrix with positive determinant, U, V\xe2\x88\x97, and \xce\xa3 are real m \xc3\x97 m matrices as well, \xce\xa3 can be regarded as a scaling matrix, and U, V\xe2\x88\x97 can be viewed as rotation matrices. Thus the expression U\xce\xa3V\xe2\x88\x97 can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.'b"Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = V\xce\xa3V\xe2\x88\x97) with eigenvalue scale factors \xcf\x83i along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV\xe2\x88\x97). If the rotation is done first, M = P'R, then R is the same and P' = U\xce\xa3U\xe2\x88\x97 has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate."b'As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n \xc3\x97 n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m \xc3\x97 n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.'b'Since U and V\xe2\x88\x97 are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector \xcf\x83i Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U\xe2\x88\x97 and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U\xe2\x88\x97, V, and V\xe2\x88\x97 are orthonormal bases.'b'Consider the 4 \xc3\x97 5 matrix'b'A singular-value decomposition of this matrix is given by U\xce\xa3V\xe2\x88\x97'b'Notice \xce\xa3 is zero outside of the diagonal and one diagonal element is zero. Furthermore, because the matrices U and V\xe2\x88\x97 are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V\xe2\x88\x97 are real valued, each is an orthogonal matrix.'b'is also a valid singular-value decomposition.'b'In any singular-value decomposition'b'the diagonal entries of \xce\xa3 are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:'b'As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank\xe2\x80\x93nullity theorem cannot be the same dimension if m \xe2\x89\xa0 n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m \xe2\x88\x92 n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n \xe2\x88\x92 m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.'b'Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ei\xcf\x86 (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.'b'The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = U\xce\xa3V\xe2\x88\x97 is'b'where \xce\xa3+ is the pseudoinverse of \xce\xa3, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.'b"A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x\xe2\x88\x97A = 0, with x\xe2\x88\x97 denoting the conjugate transpose of x, is called a left null vector of A."b'A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.'b'Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.'b'As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \xce\xa3. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.'b'Here Ui and Vi are the i-th columns of the corresponding SVD matrices, \xcf\x83i are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero \xcf\x83i is exactly the rank of the matrix.'b"Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,"b'which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]'b'It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O \xe2\x88\x92 A. The solution is the product UV\xe2\x88\x97.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV\xe2\x88\x97 where I is the identity matrix, so that if A = U\xce\xa3V\xe2\x88\x97 then the product A = UV\xe2\x88\x97 amounts to replacing the singular values with ones.'b'A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,'b'This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.'b"The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules."b'The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]'b'The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.'b'The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the \xce\xa3 matrix is larger than one.'b'One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.'b'SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]'b"Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]"b'Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.'b'Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]'b'The singular-value decomposition is very general in the sense that it can be applied to any m \xc3\x97 n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.'b'Given an SVD of M, as described above, the following two relations hold:'b'The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:'b'In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU\xe2\x88\x97 for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU\xe2\x88\x97 is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each \xcf\x83i to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=U\xce\xa3U* is positive semidefinite and normal, and R=UV* is unitary.'b'An eigenvalue \xce\xbb of a matrix M is characterized by the algebraic relation Mu = \xce\xbbu. When M is Hermitian, a variational characterization is also available. Let M be a real n \xc3\x97 n symmetric matrix. Define'b'By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| \xe2\x89\xa4 1}. By the Lagrange multipliers theorem, u necessarily satisfies'b'where the nabla symbol, \xe2\x88\x87, is the del operator.'b'A short calculation shows the above leads to Mu = \xce\xbbu (symmetry of M is needed here). Therefore, \xce\xbb is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.'b'Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.'b'This section gives these two arguments for existence of singular-value decomposition.'b'Let M be an m \xc3\x97 n complex matrix. Since M\xe2\x88\x97M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n \xc3\x97 n matrix V such that'b'where D is diagonal and positive definite. Partition V appropriately so we can write'b'Therefore:'b'The second equation implies MV2 = 0. Also, since V is unitary:'b'where the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. Define'b'Then'b'We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, since'b'the columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:'b'For V1 we already have V2 to make it unitary. Now, define'b'which is the desired result:'b'Notice the argument could begin with diagonalizing MM\xe2\x88\x97 rather than M\xe2\x88\x97M (This shows directly that MM\xe2\x88\x97 and M\xe2\x88\x97M have the same non-zero eigenvalues).'b'The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.'b'Let M denote an m \xc3\x97 n matrix with real entries. Let Sm\xe2\x88\x921 and Sn\xe2\x88\x921 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the function'b'Consider the function \xcf\x83 restricted to Sm\xe2\x88\x921 \xc3\x97 Sn\xe2\x88\x921. Since both Sm\xe2\x88\x921 and Sn\xe2\x88\x921 are compact sets, their product is also compact. Furthermore, since \xcf\x83 is continuous, it attains a largest value for at least one pair of vectors u \xe2\x88\x88 Sm\xe2\x88\x921 and v \xe2\x88\x88 Sn\xe2\x88\x921. This largest value is denoted \xcf\x831 and the corresponding vectors are denoted u1 and v1. Since \xcf\x831 is the largest value of \xcf\x83(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.'b'Proof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:'b'After some algebra, this becomes'b'Plugging this into the pair of equations above, we have'b'This proves the statement.'b'More singular vectors and singular values can be found by maximizing \xcf\x83(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.'b'The passage from real to complex is similar to the eigenvalue case.'b'Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).'b'The linear transformation'b'has a particularly simple description with respect to these orthonormal bases: we have'b'where \xcf\x83i is the i-th diagonal entry of \xce\xa3, and T(Vi) = 0 for i > min(m,n).'b'The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T\xc2\xa0: Kn \xe2\x86\x92 Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.'b'To get a more visual flavour of singular values and SVD factorization \xe2\x80\x94 at least when working on real vector spaces \xe2\x80\x94 consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V\xe2\x88\x97 sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D \xe2\x88\x98 V\xe2\x88\x97 then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U \xe2\x88\x98 D \xe2\x88\x98 V\xe2\x88\x97 coincides with T.'b'The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m \xe2\x89\xa5 n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).'b'The first step can be done using Householder reflections for a cost of 4mn2 \xe2\x88\x92 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).'b'The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.'b'The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 \xc3\x97 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 \xc3\x97 2 eigenvalue methods (Golub & Van Loan 1996, \xc2\xa78.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).'b'There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or'b'The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M \xe2\x87\x92 Q R and LQ decomposition of R gives R \xe2\x87\x92 L P*. Thus, at every iteration, we have M \xe2\x87\x92 Q L P*, update M \xe2\x87\x90 L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.'b'In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m\xc3\x97n matrix M of rank r:'b"Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n\xc2\xa0\xe2\x89\xaa\xc2\xa0m. The matrix U'n is thus m\xc3\x97n, \xce\xa3n is n\xc3\x97n diagonal, and V is n\xc3\x97n."b'The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if\xc2\xa0n\xc2\xa0\xe2\x89\xaa\xc2\xa0m.'b'Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values \xce\xa3r are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r\xc2\xa0\xe2\x89\xaa\xc2\xa0n. The matrix Ur is thus m\xc3\x97r, \xce\xa3r is r\xc3\x97r diagonal, and Vr* is r\xc3\x97n.'b'Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values \xce\xa3t are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t\xe2\x89\xaar. The matrix Ut is thus m\xc3\x97t, \xce\xa3t is t\xc3\x97t diagonal, and Vt* is t\xc3\x97n.'b'The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]'b'The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spaces'b'But, in the matrix case, (M* M)\xc2\xbd is a normal matrix, so ||M* M||\xc2\xbd is the largest eigenvalue of (M* M)\xc2\xbd, i.e. the largest singular value of M.'b"The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)\xc2\xbd] (the eigenvalues of M* M are the squares of the singular values)."b'The singular values are related to another norm on the space of operators. Consider the Hilbert\xe2\x80\x93Schmidt inner product on the n \xc3\x97 n matrices, defined by'b'So the induced norm is'b'Since the trace is invariant under unitary equivalence, this shows'b'where \xcf\x83i are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert\xe2\x80\x93Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:'b'In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.'b'Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.'b'TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:'b'The factorization M = U\xce\xa3V\xe2\x88\x97 can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X,\xc2\xa0\xce\xbc), and a non-negative measurable f such that'b'This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such that'b'is an unitary operator.'b'As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write'b'and notice that U V* is still a partial isometry while VTf V* is positive.'b'The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero \xce\xbb in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M\xe2\x88\x97M. Applying the diagonalization result, the unitary image of its positive square root Tf\xc2\xa0 has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {\xcf\x83i}. For any \xcf\x88 \xe2\x88\x88 H,'b'where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. \xcf\x83i are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.'b'Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:'b'The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.'b'Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.'Method of moments (statistics)
b'In statistics, the method of moments is a method of estimation of population parameters. One starts with deriving equations that relate the population moments (i.e., the expected values of powers of the random variable under consideration) to the parameters of interest. Then a sample is drawn and the population moments are estimated from the sample. The equations are then solved for the parameters of interest, using the sample moments in place of the (unknown) population moments. This results in estimates of those parameters. The method of moments was introduced by Pafnuty Chebyshev in 1887.'b''b''b'The method of moments is fairly simple and yields consistent estimators (under very weak assumptions), though these estimators are often biased.'b"In some respects, when estimating parameters of a known family of probability distributions, this method was superseded by Fisher's method of maximum likelihood, because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased."b'However, in some cases the likelihood equations may be intractable without computers, whereas the method-of-moments estimators can be quickly and easily calculated by hand.'b'Estimates by the method of moments may be used as the first approximation to the solutions of the likelihood equations, and successive improved approximations may then be found by the Newton\xe2\x80\x93Raphson method. In this way the method of moments can assist in finding maximum likelihood estimates.'b'In some cases, infrequent with large samples but not so infrequent with small samples, the estimates given by the method of moments are outside of the parameter space; it does not make sense to rely on them then. That problem never arises in the method of maximum likelihood. Also, estimates by the method of moments are not necessarily sufficient statistics, i.e., they sometimes fail to take into account all relevant information in the sample.'b'When estimating other structural parameters (e.g., parameters of a utility function, instead of parameters of a known probability distribution), appropriate probability distributions may not be known, and moment-based estimates may be preferred to maximum likelihood estimation.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Explicit semantic analysis
b'In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf\xe2\x80\x93idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.[1]'b'ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization[2] and has been used by this pair of researchers to compute what they refer to as "semantic relatedness" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name "explicit semantic analysis" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.[1][3]'b''b''b'To perform the basic variant of ESA, one starts with a collection of texts, say, all Wikipedia articles; let the number of documents in the collection be N. These are all turned into "bags of words", i.e., term frequency histograms, stored in an inverted index. Using this inverted index, one can find for any word the set of Wikipedia articles containing this word; in the vocabulary of Egozi, Markovitch and Gabrilovitch, "each word appearing in the Wikipedia corpus can be seen as triggering each of the concepts it points to in the inverted index."[1]'b'The output of the inverted index for a single word query is a list of indexed documents (Wikipedia articles), each given a score depending on how often the word in question occurred in them (weighted by the total number of words in the document). Mathematically, this list is an N-dimensional vector of word-document scores, where a document not containing the query word has score zero. To compute the relatedness of two words, one compares the vectors (say u and v) by computing the cosine similarity,'b'and this gives numeric estimate of the semantic relatedness of the words. The scheme is extended from single words to multi-word texts by simply summing the vectors of all words in the text.[3]'b'ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically orthogonal concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of information retrieval systems when it is based not on Wikipedia, but on the Reuters corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".[4] To explain this observation, links have been shown between ESA and the generalized vector space model.[5] Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".[1]'b'Cross-language explicit semantic analysis (CL-ESA) is a multilingual generalization of ESA.[6] CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.'Latent semantic analysis
b'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]'b'An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).[2]'b''b''b'LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency\xe2\x80\x93inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.'b'This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.'b'After the construction of the occurrence matrix, LSA finds a low-rank approximation[4] to the term-document matrix. There could be various reasons for these approximations:'b'The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:'b'This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.'b'Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:'b'Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:'b'The matrix products giving us the term and document correlations then become'b'You can now do the following:'b'To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:'b'The new low-dimensional space typically can be used to:'b'Synonymy and polysemy are fundamental problems in natural language processing:'b'LSA has been used to assist in performing prior art searches for patents.[8]'b'The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words. These findings are referred to as the Semantic Proximity Effect.[9]'b'When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list. These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.[10]'b'Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.[11]'b"The SVD is typically computed using large matrix methods (for example, Lanczos methods) but may also be computed incrementally and with greatly reduced resources via a neural network-like approach, which does not require the large, full-rank matrix to be held in memory.[12] A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.[13] MATLAB and Python implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution. In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.[14]"b"Some of LSA's drawbacks include:"b'In semantic hashing [17] documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.'b'Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.[18]'b'LSI is also an application of correspondence analysis, a multivariate statistical technique developed by Jean-Paul Benz\xc3\xa9cri[19] in the early 1970s, to a contingency table built from word counts in documents.'b'Called "latent semantic indexing" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s. The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\xe2\x80\x99t share a specific word or words with the search criteria.'b'LSI overcomes two of the most problematic constraints of Boolean keyword queries: multiple words that have similar meanings (synonymy) and words that have more than one meaning (polysemy)[clarification needed]. Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems.[20] As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.'b'LSI is also used to perform automated document categorization. In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.[21] Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.[22] LSI uses example documents to establish the conceptual basis for each category. During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.'b'Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI. Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection of unstructured text.'b'Because it uses a strictly mathematical approach, LSI is inherently independent of language. This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri. LSI can also perform cross-linguistic concept searching and example-based categorization. For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.[citation needed]'b'LSI is not restricted to working only with words. It can also process arbitrary character strings. Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.[23]'b'LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).[24] This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion. LSI also deals effectively with sparse, ambiguous, and contradictory data.'b'Text does not need to be in sentence form for LSI to be effective. It can work with lists, free-form notes, email, Web-based content, etc. As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.'b'LSI has proven to be a useful solution to a number of conceptual matching problems.[25][26] The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.[27]'b'LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.'b'Some common local weighting functions[29] are defined in the following table.'b'Some common global weighting functions are defined in the following table.'b'In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A\xe2\x80\x94a measure of its unique dimensions \xe2\x89\xa4 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.'b'The SVD is then truncated to reduce the rank by keeping only the largest k \xc2\xab\xc2\xa0r diagonal entries in the singular value matrix S, where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A. This reduced set of matrices is often denoted with a modified formula such as:'b'Efficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.'b'Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons.'b'The computed Tk and Dk matrices define the term and document vector spaces, which with the computed singular values, Sk, embody the conceptual information derived from the document collection. The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.'b'The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index. By a simple transformation of the A = T S DT equation into the equivalent D = AT T S\xe2\x88\x921 equation, a new vector, d, for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S\xe2\x88\x921. The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.'b'A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored. These terms will have no impact on the global weights and learned correlations derived from the original collection of text. However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.'b'The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in. Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added. When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in [13]) be used.'b'It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems. As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.'b'LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.[32] Below are some other ways in which LSI is being used:'b'LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential. Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.[47]'b'Early challenges to LSI focused on scalability and performance. LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.[48] However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome. Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package.[49]'b'Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD. As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).[50] However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.[51]'b'Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain. The variance contained in the data can be viewed by plotting the singular values (S) in a scree plot. Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain. Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain. Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.[52][53][54]'b'Due to its cross-domain applications in Information Retrieval, Natural Language Processing (NLP), Cognitive Science and Computational Linguistics, LSA has been implemented to support many different kinds of applications.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Hierarchical Dirichlet process
b'In statistics and machine learning, the hierarchical Dirichlet process (HDP) is a nonparametric Bayesian approach to clustering grouped data.[1][2] It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. This method allows groups to share statistical strength via sharing of clusters across groups. The base distribution being drawn from a Dirichlet process is important, because draws from a Dirichlet process are atomic probability measures, and the atoms will appear in all group-level Dirichlet processes. Since each atom corresponds to a cluster, clusters are shared across all groups. It was developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei and published in the Journal of the American Statistical Association in 2006,[1] as a formalization and generalization of the infinite hidden Markov model published in 2002.[3]'b''b''b'Thus the set of atoms is shared across all groups, with each group having its own group-specific atom masses. Relating this representation back to the observed data, we see that each data item is described by a mixture model:'b'The HDP mixture model is a natural nonparametric generalization of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data.[1] Here each group is a document consisting of a bag of words, each cluster is a topic, and each document is a mixture of topics. The HDP is also a core component of the infinite hidden Markov model,[3] which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.[1] [4]'b'The HDP can be generalized in a number of directions. The Dirichlet processes can be replaced by Pitman-Yor processes, resulting in the Hierarchical Pitman-Yor process. The hierarchy can be deeper, with multiple levels of groups arranged in a hierarchy. Such an arrangement has been exploited in the sequence memoizer, a Bayesian nonparametric model for sequences which has a multi-level hierarchy of Pitman-Yor processes.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Mallet (software project)
b'MALLET is a Java "Machine Learning for Language Toolkit".'b''b''b'MALLET is an integrated collection of Java code useful for statistical natural language processing, document classification, cluster analysis, information extraction, topic modeling and other machine learning applications to text.'b'MALLET was developed primarily by Andrew McCallum, of the University of Massachusetts Amherst, with assistance from graduate students and faculty from both UMASS and the University of Pennsylvania.'b''Gensim
b'Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.'b''b''b'Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms,[1] hierarchical Dirichlet processes (HDP), latent semantic analysis (LSA, LSI, SVD) and latent Dirichlet allocation (LDA), including distributed parallel versions.[2]'b'Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation Scalability of Semantic Analysis in Natural Language Processing of Radim \xc5\x98eh\xc5\xaf\xc5\x99ek, the creator of Gensim.[3]'b'Gensim has been used and cited in over 800 commercial and academic applications, in a diverse array of disciplines from medicine to insurance claim analysis to patent search[4][5] The software has been covered in several new articles, podcasts and interviews since 2009.[6][7][8]'b'The open source code is developed and hosted on GitHub[9] and a public support forum is maintained on Google Groups[10] and Gitter.[11]'b'Gensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme.[12]'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Journal of Machine Learning Research
b'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Kevin Murphy (Google) and Bernhard Sch\xc3\xb6lkopf (Max Planck Institute for Intelligent Systems).'b''b''b'The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online.[2]'b'Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.[1]'b'In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007[3] and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems.'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'arXiv
b'arXiv (pronounced "archive")[2] is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million article milestone on October 3, 2008,[3][4] and hit a million by the end of 2014.[5][6] By October 2016 the submission rate had grown to more than 10,000 per month.[6][7]'b''b''b'The arXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side.[9] Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993.[6][10] The term e-print was quickly adopted to describe the articles.'b"It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org.[11] It is now hosted principally by Cornell, with eight mirrors around the world.[12]"b'Its existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access[13] and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.'b'The annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions.[14] This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Annual donations were envisaged to vary in size between $2,300 to $4,000, based on each institution\xe2\x80\x99s usage. As of 14\xc2\xa0January\xc2\xa02014[update], 174 institutions have pledged support for the period 2013\xe2\x80\x932017 on this basis, with a projected revenue from this source of approximately $340,000.[15]'b'In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv\'s operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence".[16] However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.'b'Although the arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic,[17] or reject submissions that are not scientific papers. The lists of moderators for many sections of the arXiv are publicly available,[18] but moderators for most of the physics sections remain unlisted.'b'Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines.[19] Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area.[17] New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.[20]'b'A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston\'s geometrization conjecture, including the Poincar\xc3\xa9 conjecture as a particular case, uploaded by Grigori Perelman in November 2002.[21] Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it\'s all there [on the arXiv]\xc2\xa0\xe2\x80\x93 let them go and read about it".[22] Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.[23]'b'While the arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat\'s last theorem using only high-school mathematics, they are "surprisingly rare".[24][better\xc2\xa0source\xc2\xa0needed] The arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them.[25]'b'Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.'b"The standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's front, a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front.[26] A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides TablearXiv,[27] a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv.[28] A full text and author search engine for arXiv is provided by Scientillion.[29] Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them."b'Files on arXiv can have a number of different copyright statuses:[30]'b'Some authors have voiced concern over the lack of transparency in the arXiv academic peer-review process.[31] Demetris Christopoulos from the National and Kapodistrian University of Athens likens arXiv to a non-declared Journal without a known editor in chief, without a specific written policy regarding submitted papers, and that applies hidden censorship to all papers that do not fall within established scientific dogma. [32]'International Standard Book Number
b'The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.'b'The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).'b'Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]'b'Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.'b''b''b'The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]'b'The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]'b'An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" \xe2\x80\x93 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN\xc2\xa00-340-01381-8; the check digit does not need to be re-calculated.'b'Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):'b'A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]'b'ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[15] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.'b'Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[16] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[17]'b'Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [18]'b'Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Biblioth\xc3\xa8que et Archives nationales du Qu\xc3\xa9bec.'b'Colombia: C\xc3\xa1mara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.'b'Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[19]'b'India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[20] There is no fee associated in getting ISBN in India.[21]'b'Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[22] The original national prefix 978-88 is reserved for publishing companies, starting at \xe2\x82\xac49 for a ten-codes block[23] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of \xe2\x82\xac25 for a single code.'b'Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]'b'Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[24][25][26]'b'Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.'b'New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[27]'b'Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.'b'Philippines: The National Library of the Philippines is responsible for ISBN registrations for Philippine publishers, authors, universities, institutions, and government departments that are responsible for publishing books. As of 2017[update], a fee of \xe2\x82\xb1120.00 per title was charged for the issuance of an ISBN.[28]'b'South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.'b'United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from \xc2\xa3120 (plus VAT) for the smallest block on a standard turnaround of ten days.[29]'b'United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[5] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125 for a single number. Access is immediate when requested via their website.[30]'b'Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.'b" The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[31] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0\xe2\x80\x935, 600\xe2\x80\x93621, 7, 80\xe2\x80\x9394, 950\xe2\x80\x93989, 9926\xe2\x80\x939989, and 99901\xe2\x80\x9399976.[32] Books published in rare languages typically have longer group identifiers.[33]"b'Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[34]'b'The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.'b'The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]'b'A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (\xe2\x82\xac1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[35] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.'b'Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.'b'By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[36] Here are some sample ISBN-10 codes, illustrating block length variations.'b'English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[37]'b'A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without \xe2\x80\x93 the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".'b'The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[38] \xe2\x80\x93 which is the last digit of the ten-digit ISBN \xe2\x80\x93 must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.'b'For example, for an ISBN-10 of 0-306-40615-2:'b'Formally, using modular arithmetic, we can say:'b"It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:"b'Formally, we can say:'b"The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN\xc2\xa0\xe2\x80\x93 the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[39]"b'In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).'b'Each of the first nine digits of the ten-digit ISBN\xe2\x80\x94excluding the check digit itself\xe2\x80\x94is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.'b'For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:'b'Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 \xe2\x80\x93 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)'b'For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:'b'Thus the check digit is 2.'b'It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:'b'The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.'b"The 2005 edition of the International ISBN Agency's official manual[40] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10."b'Formally, using modular arithmetic, we can say:'b'The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.'b'For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:'b'Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.'b'In general, the ISBN-13 check digit is calculated as follows.'b'Let'b'Then'b'This check system\xc2\xa0\xe2\x80\x93 similar to the UPC check digit formula\xc2\xa0\xe2\x80\x93 does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3\xc3\x976+1\xc3\x971 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3\xc3\x971+1\xc3\x976 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.'b'Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).'b'The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.'b'Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[41] For example, ISBN\xc2\xa00-590-76484-5 is shared by two books \xe2\x80\x93 Ninja gaiden\xc2\xae: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.'b'Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[42] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.'b'Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[43]'b'Currently the barcodes on a book\'s back cover (or inside a mass-market paperback book\'s front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[44] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).'b'Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[45] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.'b'Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[46]'b'Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.'Book sources
b'This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter. In Wikipedia, numbers preceded by "ISBN" link directly to this page.\n'b'This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). \n'b'Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).\n'b'Alabama\n'b'California\n'b'Colorado\n'b'Delaware\n'b'Florida\n'b'Georgia\n'b'Indiana\n'b'Iowa \n'b'Kansas\n'b'Kentucky\n'b'Massachusetts\n'b'Michigan\n'b'Minnesota\n'b'Missouri\n'b'Nebraska\n'b'New Jersey\n'b'New Mexico\n'b'New York\n'b'North Carolina\n'b'Ohio\n'b'Oklahoma\n'b'Oregon\n'b'Pennsylvania\n'b'Rhode Island\n'b'South Carolina\n'b'South Dakota\n'b'Tennessee\n'b'Texas\n'b'Utah\n'b'Washington state\n'b'Wisconsin\n'b'\n'b'Find your book on a site that compiles results from other online sites:\n'b'These sites allow you to search the catalogs of many individual booksellers:\n'b'\n'b'If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below \xe2\x80\x93 they are more likely to have sources appropriate for that language.\n'b'These links produce citations in various referencing styles.\n'b"You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.\n"b'You can also convert between 10 and 13 digit ISBN numbers with these tools:\n'b'\nEdit this page\n'b'\n'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'PubMed Central
b"PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. As one of the major research databases within the suite of resources that have been developed by the National Center for Biotechnology Information (NCBI), PubMed Central is much more than just a document repository. Submissions into PMC undergo an indexing and formatting procedure which results in enhanced metadata, medical ontology, and unique identifiers which all enrich the XML structured data for each article on deposit.[1] Content within PMC can easily be interlinked to many other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to freely discover, read and build upon this portfolio of biomedical knowledge.[2]"b'PubMed Central should not be confused with PubMed. These are two very different services at their core.[3] While PubMed is a searchable database of biomedical citations and abstracts, the full-text article referenced in the PubMed record will physically reside elsewhere. (Sometimes in print, sometimes online, sometimes free, sometimes behind a toll-wall accessible only to paying subscribers). PubMed Central is a free digital archive of articles, accessible to anyone from anywhere via a basic web browser. The full text of all PubMed Central articles is free to read, with varying provisions for reuse.'b'As of December\xc2\xa02016[update], the PMC archive contained over 4.1 million articles,[4] with contributions coming directly from publishers or authors depositing their own manuscripts into the repository per the NIH Public Access Policy. Older data shows that from Jan 2013 \xe2\x80\x93 Jan 2014 author-initiated deposits exceeded 103,000 papers during this 12-month period.[5] PMC also identifies about 4,000 journals which now participate in some capacity to automatically deposit their published content into the PMC repository.[6] Some participating publishers will delay the release of their articles on PubMed Central for a set time after publication, this is often referred to as an "embargo period", and can range from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common). However, PubMed Central is a key example of "systematic external distribution by a third party"[7] which is still prohibited by the contributor agreements of many publishers.'b''b''b'Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[8]'b'A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.'b'The National Library of Medicine "NLM Journal Publishing Tag Set" journal article markup language is freely available.[9] The Association of Learned and Professional Society Publishers comments that "it is likely to become the standard for preparing scholarly content for both books and journals".[10] A related DTD is available for books.[11] The Library of Congress and the British Library have announced support for the NLM DTD.[12] It has also been popular with journal service providers.[13]'b'With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[14] This includes NASA content, with the interface branded as "PubSpace".[15][16]'b'Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).'b'Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[17]'b'Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers\' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come "live" when the resources become available.'b'An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.'b'When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.'b'In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.'b'Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[18] to cautious concern by others.[19] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version-of-record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[20] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[21]'b'The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[22] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[23]'b'The change in procedure has received criticism.[24] The American Physiological Society has expressed reservations about the implementation of the policy.[25]'b'The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of "PMC" followed by a string of seven numbers. The format is:[26]'b'Authors applying for NIH awards must include the PMCID in their application.'PubMed
b'PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.'b'From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]'b'In addition to MEDLINE, PubMed provides access to:'b'Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]'b'Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]'b'As of 11\xc2\xa0July\xc2\xa02017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed\'s records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950s to 16% in 2016.[8] Other significant proportion of records correspond to \xe2\x80\x9cChemistry\xe2\x80\x9d (8.69%), \xe2\x80\x9cTherapy\xe2\x80\x9d (8.39%) and "Infection" (5%).'b'In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]'b"Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window."b"PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms."b'The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:'b'Likewise,'b"A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11] By default the results are sorted by Most Recent, but this changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.[12]"b'For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.[13][14]'b'When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).'b'Publication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.[15]'b'Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]'b'A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the \'Find related data\' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The \'related articles\' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.[18]'b'PubMed automatically links to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes).[19] This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.[19]'b'The PubMed optional facility "My NCBI" (with free registration) provides tools for'b'and a wide range of other options.[20] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[21]'b'LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[22] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March\xc2\xa02010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[23] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.'b'In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[24] In February 2018, PubMed Commons was discontinued due to the fact that "usage has remained minimal".[25][26]'b'PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[27] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[28]'b'askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[29]'b'A PMID (PubMed identifier or PubMed unique identifier)[30] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[31]'b'The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.'b'The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[32] As of October\xc2\xa02008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[33] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.'b'Lu[33] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:'b'As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[33] Without the need to store about 90\xc2\xa0GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[47]'b'Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950s to 16% in 2016.[48]'b'The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.[49]'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Topic model
b'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.'b'Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.[1]'b''b''b'An early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998.[2] Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999.[3] Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words.[4] Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.'b'Topic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.'b"Approaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the Pennsylvania Gazette during 1728\xe2\x80\x931800. Griffiths & Steyvers use topic modeling on abstract from the journal PNAS to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the Richmond Times-Dispatch to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829\xe2\x80\x932008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time."b'Yin et al.[6] introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.'b'Chang and Blei[7] included network information between linked documents in the relational topic model, which allows to model links between websites.'b'The author-topic model by Rosen-Zvi et al.[8] models the topics associated with authors of documents to improve the topic detection for documents with authorship information.'b'In practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.[9] Several groups of researchers starting with Papadimitriou et al.[2] have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.[10]'Machine learning
b'Machine learning is a field of computer science that gives computer systems the ability to "learn" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.[1]'b'The name Machine learning was coined in 1959 by Arthur Samuel.[2] Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] \xe2\x80\x93 such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.'b'Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies.'b'Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[12]'b'Effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, machine-learning programs often fail to deliver.[13][14]'b''b''b'Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\'s proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[16] In Turing\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.'b''b'Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning "signal" or "feedback" available to a learning system:'b'Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[5]:3'b'Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.'b'Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[17]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[18] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[19]:488'b'However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708\xe2\x80\x93710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[19]:25'b'Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[20] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.'b'Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.'b'Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21]'b'Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]'b'Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[23] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.'b'Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]'b'A core objective of a learner is to generalize from its experience.[25][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.'b'The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\xe2\x80\x93variance decomposition is one way to quantify generalization error.'b'For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27]'b'In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.'b"Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value."b'Association rule learning is a method for discovering interesting relations between variables in large databases.'b'An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.'b'Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[28]'b'Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.'b'Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.'b'Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.'b'A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.'b'Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.'b'Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.'b'Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[29] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[30]'b'In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.'b'Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[31] A popular heuristic method for sparse dictionary learning is K-SVD.'b"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[32]"b'A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[33][34] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[35]'b'Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules\xe2\x80\x99 to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[36] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.'b'Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[37]'b'Applications for machine learning include:'b'In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[43] Shortly after the prize was awarded, Netflix realized that viewers\' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[44]'b'In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. [45]'b'In 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[46]'b'In 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[47]'b'Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[48]'b'In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\xe2\x80\x99s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC\xe2\x80\x99s associated Area Under the Curve (AUC).'b'Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[49] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[50][51] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.'b'Because language contains biases, machines trained on language corpora will necessarily also learn bias.[52]'b'Software suites containing a variety of machine learning algorithms include the following\xc2\xa0:'Natural-language processing
b'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language\xc2\xa0data.'b'Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.'b''b''b'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.'b'The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.'b'Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".'b'During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.'b"Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks."b'Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.'b'Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.'b'In recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural-language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'b'Since the so-called "statistical revolution"[9][10] in the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.'b'Formerly, many language-processing tasks typically involved the direct hand coding of rules,[11][12] which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).'b'Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.'b'Systems based on machine-learning algorithms have many advantages over hand-produced rules:'b'The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.'b'Though NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.'b''Statistical model
b'A statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.'b'The assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.'b'A statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Ad\xc3\xa8r quoting Kenneth Bollen).[1]'b'All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.'b''b''b'Suppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: heighti\xc2\xa0= b0\xc2\xa0+ b1agei\xc2\xa0+ \xce\xb5i, where b0 is the intercept, b1 is a parameter that age is multiplied by in obtaining a prediction of height, \xce\xb5i is the error term, and i identifies the child. This implies that height is predicted by age, with some error.'b'An admissible model must be consistent with all the data points. Thus, a straight line (heighti\xc2\xa0= b0\xc2\xa0+ b1agei) cannot be the equation for a model of the data. The line cannot be the equation for a model, unless it exactly fits all the data points\xe2\x80\x94i.e. all the data points lie perfectly on the line. The error term, \xce\xb5i, must be included in the equation, so that the model is consistent with all the data points.'b'To do statistical inference, we would first need to assume some probability distributions for the \xce\xb5i. For instance, we might assume that the \xce\xb5i distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.'b'A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, \xce\xb5 is a stochastic variable; without that variable, the model would be deterministic.'b'Statistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).'b'There are three purposes for a statistical model, according to Konishi\xc2\xa0& Kitagawa.[4]'b'As an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that'b'In this example, the dimension, k, equals 2.'b'As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)'b'Parametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, "These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies".[5]'b'Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model'b'has, nested within it, the linear model'b'\xe2\x80\x94we constrain the parameter b2 to equal 0.'b'In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\xc2\xa01). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.'b'Models can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.'b'Konishi & Kitagawa state: "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models."[6] Relatedly, Sir David Cox has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".[7]'Bioinformatics
b'Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]'b''b''b'Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]'b'Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]'b'Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics."[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]'b'To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:'b'The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\xe2\x80\x93protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.'b'Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.'b'Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.'b'Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.'b'Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.'b'Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.'b'Since the Phage \xce\xa6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.'b'Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.'b'Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.'b'In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.'b'The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.'b'Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).'b'Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:'b'Future work endeavours to reconstruct the now more complex tree of life.'b'The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.'b'The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.'b'Many of these studies are based on the homology detection and protein families computation.[22]'b'Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]'b"With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]"b'In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.'b'Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]'b'With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]'b'Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.'b'The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.'b'Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]'b'Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.'b'For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.'b'Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.'b'Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.'b'Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.'b'The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]'b'Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]'b'Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \xe2\x80\x93 a.k.a. Mad Cow Disease \xe2\x80\x93 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]'b"One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably."b'One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]'b'Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.'b'Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\xe2\x80\x93protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.'b'Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.'b'Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\xe2\x80\x93protein interactions only based on these 3D shapes, without performing protein\xe2\x80\x93protein interaction experiments. A variety of methods have been developed to tackle the protein\xe2\x80\x93protein docking problem, though it seems that there is still much work to be done in this field.'b'Other interactions encountered in the field include Protein\xe2\x80\x93ligand (including drug) and protein\xe2\x80\x93peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.'b'The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:'b'The area of research draws from statistics and computational linguistics.'b"Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:"b'Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.'b'Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.'b'Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.'b'The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.'b'Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.'b'Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.'b'Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.'b'Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.'b'The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]'b'An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]'b'SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.'b'Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.'b'A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to'b'Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.'b'In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.'b'It was decided that the BioCompute paradigm would be in the form of digital \xe2\x80\x98lab notebooks\xe2\x80\x99 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]'b'In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \xe2\x80\x9cstandard trial use\xe2\x80\x9d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]'b'Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\xcf\x80 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\xcf\x80 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\xcf\x80 operating system.[51][52]'b"MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications."b'There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).'b''Probabilistic latent semantic analysis
b'Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.'b'Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.'b''b''b'Their parameters are learned using the EM algorithm.'b'PLSA may be used in a discriminative setting, via Fisher kernels.[1]'b'PLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.'b'It is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.[2]'b'This is an example of a latent class model (see references therein), and it is related[5][6] to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.[7]'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'David Blei
b'David M. Blei is a Professor in the Statistics and Computer Science departments at Columbia University. Prior to fall 2014 he was an Associate Professor in the Department of Computer Science at Princeton University. His work is primarily in machine learning.'b''b''b'His research interests include topic models and he was one of the original developers of latent Dirichlet allocation. As of October 25, 2017, his publications have been cited 50,850 times, giving him an h-index of 64.[1]'b'He was named Fellow of ACM "For contributions to the theory and practice of probabilistic topic modeling and Bayesian machine learning" in 2015.[2]'b''Andrew Ng
b"Andrew Yan-Tak Ng (Chinese: \xe5\x90\xb3\xe6\x81\xa9\xe9\x81\x94; born 1976) is a Chinese American computer scientist. He is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor (formerly associate professor) at Stanford University. Ng is also the co-founder and chairman of Coursera, an online education platform.[2]"b''b''b"Ng was born in the UK in 1976. His parents were both from Hong Kong. He spent time in Hong Kong and Singapore[1] and later graduated from Raffles Institution in Singapore in 1992. In 1997, he received his undergraduate degree in computer science from Carnegie Mellon University in Pittsburgh, Pennsylvania. Ng earned his master's degree from Massachusetts Institute of Technology in Cambridge, Massachusetts in 1998 and received his PhD from University of California, Berkeley in 2002. He started working at Stanford University during that year and currently lives in Palo Alto, California. He married Carol E. Reiley in 2014.[3]"b"Andrew was a professor at Stanford University Department of Computer Science and Department of Electrical Engineering. He became Director of the Stanford Artificial Intelligence Lab where he taught students and undertook research related to data mining and machine learning. From 2011 to 2012, he worked at Google, where he founded and led the Google Brain Deep Learning Project. In 2012, he co-founded Coursera to offer free online courses for everyone after over 100,000 students registered for Ng's popular course.[4] Today, several million people have taken the online course. In 2014, he joined[5] Baidu as Chief Scientist, and carried out research related to big data and A.I. In March 2017, he announced his resignation from Baidu.[6]"b'He soon afterwards launched Deeplearning.ai,[7] an online curriculum of classes. Then Ng launchedLanding.ai[8], bringing AI to manufacturing factories, announcing a partnership with FoxConn.[9]'b'In 2018, Ng unveiled the AI Fund,[10] raising $175 million to invest in new startups. He is also the chairman of Woebot and on the board of drive.ai.[11][12]'b'Ng researches primarily in machine learning and deep learning. His early work includes the Stanford Autonomous Helicopter project, which developed one of the most capable autonomous helicopters in the world,[13][14] and the STAIR (STanford Artificial Intelligence Robot) project,[15] which resulted in ROS, a widely used open-source robotics software platform.'b'In 2011, Ng founded the Google Brain project at Google, which developed very large scale artificial neural networks using Google\'s distributed computer infrastructure.[16] Among its notable results was a neural network trained using deep learning algorithms on 16,000 CPU cores, that learned to recognize higher-level concepts, such as cats, after watching only YouTube videos, and without ever having been told what a "cat" is.[17][18] The project\'s technology is currently also used in the Android Operating System\'s speech recognition system.[19]'b'He together with David M. Blei and Michael I. Jordan, coauthored the influential paper that introduced Latent Dirichlet allocation.[20]'b'Ng started the Stanford Engineering Everywhere (SEE) program, which in 2008 placed a number of Stanford courses online, for free. Ng taught one of these courses, Machine Learning, which consisted of video lectures by him, along with the student materials used in the Stanford CS229 class.'b'The "applied" version of the Stanford class (CS229a) was hosted on ml-class.org and started in October 2011, with over 100,000 students registered for its first iteration; the course featured quizzes and graded programming assignments and became one of the first successful MOOCs made by Stanford professors.[22] His work subsequently led to the founding of Coursera in 2012.'b"Ng is also the author or co-author of over 100 published papers in machine learning, robotics, and related fields. His work in computer vision and deep learning has been frequently featured in press releases and reviews.[23] In 2008, he was named to the MIT Technology Review TR35 as one of the top 35 innovators in the world under the age of 35.[24][25] Ng was awarded a Sloan Fellowship (2007). For his work in artificial intelligence, he is also a recipient of the Computers and Thought Award (2009). In 2013 at the age of 37, he was named one of Times 100 Most Influential People[26] and Fortune's 40 under 40.[27]"Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Dirichlet distribution
b'The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process.'b''b''b'The Dirichlet distribution of order K\xc2\xa0\xe2\x89\xa5\xc2\xa02 with parameters \xce\xb11, ..., \xce\xb1K >\xc2\xa00 has a probability density function with respect to Lebesgue measure on the Euclidean space RK\xe2\x88\x921 given by'b'The normalizing constant is the multivariate Beta function, which can be expressed in terms of the gamma function:'b'When \xce\xb1=1[2], the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard (K\xc2\xa0\xe2\x88\x92\xc2\xa01)-simplex, i.e. it is uniform over all points in its support. This particular distribution is known as the flat Dirichlet distribution. Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.'b'Let'b'Then[3][4]'b'Note that the matrix so defined is singular.'b'More generally, moments of Dirichlet-distributed random variables can be expressed as[5]'b'The mode of the distribution is[6] the vector (x1, ..., xK) with'b'The marginal distributions are beta distributions:[7]'b"The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations). This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet. Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one. This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties."b'Formally, this can be expressed as follows. Given a model'b'then the following holds:'b'This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector \xce\xb1 as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution.'b'In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models. See the section on applications below for more information.'b'In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out) is a Dirichlet-multinomial distribution. This distribution plays an important role in hierarchical Bayesian models, because when doing inference over such models using methods such as Gibbs sampling or variational Bayes, Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.'b'and'b'If'b'then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum,'b'The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series. It is given by Phillips[11] as'b'For K independently distributed Gamma distributions:'b'we have:[13]:402'b'Although the Xis are not independent from one another, they can be seen to be generated from a set of K independent gamma random variable.[13]:594 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.'b'Because the Dirichlet distribution is an exponential family distribution it has a conjugate prior. The conjugate prior is of the form:[14]'b'Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models. (Note that in many fields, such as in natural language processing, categorical variables are often imprecisely called "multinomial variables". Such a usage is liable to cause confusion, just as if Bernoulli distributions and binomial distributions were commonly conflated.)'b'Inference over hierarchical Bayesian models is often done using Gibbs sampling, and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable. This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution, conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters). One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.'b'and then set'b'The Jacobian now looks like'b'The determinant can be evaluated by noting that it remains unchanged if multiples of a row are added to another row, and adding each of the first K-1 rows to the bottom row to obtain'b'Substituting for x in the joint pdf and including the Jacobian, one obtains:'b'Which is equivalent to'b'Below is example Python code to draw the sample:'b'This formulation is correct regardless of how the Gamma distributions are parameterized (shape/scale vs. shape/rate) because they are equivalent when scale and rate equal 1.0.'b'and let'b'Finally, set'b'This iterative procedure corresponds closely to the "string cutting" intuition described below.'b'Below is example Python code to draw the sample:'b'Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value \xce\xb1 to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.'b'One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. The \xce\xb1/\xce\xb10 values specify the mean lengths of the cut pieces of string resulting from the distribution. The variance around this mean varies inversely with \xce\xb10.'b'Consider an urn containing balls of K different colors. Initially, the urn contains \xce\xb11 balls of color 1, \xce\xb12 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir(\xce\xb11,...,\xce\xb1K).[16]'b'For a formal proof, note that the proportions of the different colored balls form a bounded [0,1]K-valued martingale, hence by the martingale convergence theorem, these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.'b'Note that each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls.'Pachinko allocation
b'In machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents. [1] The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more flexibility and greater expressive power than latent Dirichlet allocation.[2] While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The model is named for pachinko machines\xe2\x80\x94a game popular in Japan, in which metal balls bounce down around a complex collection of pins until they land in various bins at the bottom.[3]'b''b''b"Pachinko allocation was first described by Wei Li and Andrew McCallum in 2006.[3] The idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007.[4] In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP).[2] The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst."b'\nPAM connects words in V and topics in T with an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the interior levels and the leaves are words.'b'The probability of generating a whole corpus is the product of the probability for every document:'b''File:Topic model scheme.webm
b'https://creativecommons.org/licenses/by-sa/4.0 CC BY-SA 4.0 Creative Commons Attribution-Share Alike 4.0 truetrue'b'Click on a date/time to view the file as it appeared at that time.\n'b'The following other wikis use this file:\n'Pennsylvania Gazette
b"The Pennsylvania Gazette was one of the United States' most prominent newspapers from 1728, before the time period of the American Revolution, until 1800."b''b''b"The newspaper was first published in 1728 by Samuel Keimer and was the second newspaper to be published in Pennsylvania under the name The Universal Instructor in all Arts and Sciences: and Pennsylvania Gazette, alluding to Keimer's intention to print out a page of Ephraim Chambers' Cyclopaedia, or Universal Dictionary of Arts and Sciences in each copy.[1] On October 2, 1729, Benjamin Franklin and Hugh Meredith bought the paper and shortened its name, as well as dropping Keimer's grandiose plan to print out the Cyclopaedia.[1] Franklin not only printed the paper but also often contributed pieces to the paper under aliases. His newspaper soon became the most successful in the colonies."b'On August 6, 1741 Franklin published an editorial about deceased Andrew Hamilton, a lawyer and public figure in Philadelphia who had been a friend. The editorial praised the man highly and showed Franklin had held the man in high esteem.[2]'b'In 1752, Franklin published a third-person account of his pioneering kite experiment in The Pennsylvania Gazette, without mentioning that he himself had performed it.[3]'b'Primarily a publication for classified ads, merchants and individuals listed notices of employment, lost and found goods and items for sale; the newspaper also reprinted foreign news. Most entries involved stories of travel.[4] In the July 31, 1776 edition the front page lists military movements announced by John Hancock alongside the sale of a plantation in Chester County, Pa., a three-dollar reward for a horse that strayed from home, three pounds reward for the return of a fleeing 23-year-old Irish servant named Jane Stepberd, three pounds reward for runaway Negroe man Moses Graves and the sale of an unnamed "hearty Scotch Servant GIRL".'b"This newspaper, among other firsts, would print the first political cartoon in America, Join, or Die, authored by Franklin himself.[5] It ceased publication in 1800, ten years after Franklin's death.[6]"b'It is claimed that the publication later reemerged as the Saturday Evening Post in 1821.[7]'b'There are three known copies of the original issue, which are held by the Historical Society of Pennsylvania, the Library Company of Philadelphia, and the Wisconsin State Historical Society.[1]'b'Today, The Pennsylvania Gazette moniker is used by an unrelated bi-monthly alumni magazine of the University of Pennsylvania, which Franklin founded and served as a trustee.'b'Archives are available online for a fee.[6]'b'President of Pennsylvania (1785\xe2\x80\x931788), Ambassador to France (1779\xe2\x80\x931785)'Proceedings of the National Academy of Sciences of the United States of America
b'Proceedings of the National Academy of Sciences of the United States of America (PNAS) is the official scientific journal of the National Academy of Sciences, published since 1915. With broad coverage, spanning the biological, physical, and social sciences, the journal publishes original research alongside scientific reviews, commentaries, and letters. In 1999\xe2\x80\x932009, the last period for which data are available, PNAS was the second most cited journal across all fields of science.[1] PNAS is published weekly in print, and daily online in PNAS Early Edition.'b''b''b'PNAS was established by the National Academy of Sciences (NAS) in 1914, with its first issue published in 1915. The NAS itself had been founded in 1863 as a private institution, but chartered by the United States Congress, with the goal to "investigate, examine, experiment, and report upon any subject of science or art". By 1914 the Academy had been well established.'b"Prior to the inception of PNAS, the National Academy of Sciences published three volumes of organizational transactions, consisting mostly of minutes of meetings and annual reports. In accordance with the guiding principles established by astronomer George Ellery Hale, the foreign secretary of NAS in 1914, PNAS publishes brief first announcements of Academy members' and foreign associates' more important contributions to research and of work that appears to a member to be of particular importance.[2]"b'The following people have been editors-in-chief of the journal:'b'The first managing editor of the journal was mathematician Edwin Bidwell Wilson.'b'All research papers published in PNAS are peer-reviewed.[2] The standard mode is for papers to be submitted directly to PNAS rather than going through an Academy member. Members may handle the peer review process for up to 4 of their own papers per year\xe2\x80\x94this is an open review process because the member selects and communicates directly with the referees. These submissions and reviews, like all for PNAS, are evaluated for publication by the PNAS Editorial Board. Until July 1, 2010, members were allowed to communicate up to 2 papers from non-members to PNAS every year. The review process for these papers was anonymous in that the identities of the referees were not revealed to the authors. Referees were selected by the NAS member.[2][4][5] PNAS eliminated communicated submissions through NAS members as of July 1, 2010, while continuing to make the final decision on all PNAS papers.[6]'b'In 2003, PNAS issued an editorial stating its policy on publication of sensitive material in the life sciences.[7] PNAS stated that it would "continue to monitor submitted papers for material that may be deemed inappropriate and that could, if published, compromise the public welfare." This statement was in keeping with the efforts of several other journals.[8][9] In 2005 PNAS published an article titled "Analyzing a bioterror attack on the food supply: The case of botulinum toxin in milk"[10] despite objections raised by the U.S. Department of Health and Human Services.[11] The paper was published with a commentary by the president of the Academy at the time, Bruce Alberts, titled "Modeling attacks on the food supply".[12]'b'PNAS is widely read by researchers, particularly those involved in basic sciences, around the world. PNAS Online receives over 21 million hits per month.[13] The journal is notable for its policy of making research articles freely available online to everyone six months after publication (delayed open access), or immediately if authors have chosen the "open access" option (hybrid open access). Immediately free online access (without the six-month delay) is available to more than 100 developing countries[14] and for some categories of papers such as colloquia. Abstracts, tables of contents, and online supporting information are free. Anyone can sign up to receive free tables of contents by email.[15]'b'Because PNAS is self-sustaining and receives no direct funding from the U.S.\xc2\xa0government or the National Academy of Sciences, the journal charges authors publication fees and subscription fees to offset the cost of the editorial and publication process.'b'According to the Journal Citation Reports, the journal has a 2015 impact factor of 9.423.[16] PNAS is the second most cited scientific journal, with nearly 1.4\xc2\xa0million citations from 1999 to 2009 (the Journal of Biological Chemistry is the most cited journal over this period).[17]'b'PNAS has received occasional criticism for releasing papers to science journalists as much as a week before making them available to the general public; this practice is known as a news embargo.[18] According to critics, this allows mainstream news outlets to misrepresent or exaggerate the implications of experimental findings before the scientific community is able to respond.[19][20] Science writer Ed Yong, on the other hand, has argued that the real problem is not embargoes themselves, but the press releases issued by research institutes and universities.[18]'b'In January 2011, PNAS started considering manuscripts for exclusive online publication, "PNAS Plus" papers.[21] These have a larger maximum page limit (10 rather than 6 pages). Accompanying these papers both online and in print was a one- to two-page summary description written by the authors for a broad readership. Since mid-October 2012, PNAS Plus authors no longer need to submit author summaries and are instead asked to submit a 120-word-maximum statement about the significance of their paper. The significance statement will appear both online and in print.[22] Since July 15, 2013, the significance statement is required for all research articles.'b"In 2006 PNAS launched a new section of the journal dedicated to sustainability science, an emerging field of research dealing with the interactions between natural and social systems, and with how those interactions affect the challenge of sustainability: meeting the needs of present and future generations while substantially reducing poverty and conserving the planet's life support systems. See the Sustainability Science portal here."Richmond Times-Dispatch
b'The Richmond Times-Dispatch (RTD or TD for short) is the primary daily newspaper in Richmond, the capital of Virginia, United States. It is also the primary newspaper of record for the state of Virginia.[2][3][4]'b''b''b"The Times-Dispatch has the second-highest circulation of any Virginia newspaper, after Norfolk's The Virginian-Pilot.[5] In addition to the Richmond area (Petersburg, Chester, Hopewell, Colonial Heights and surrounding areas), the Times-Dispatch has substantial readership in Charlottesville, Lynchburg, and Waynesboro. As the primary paper of the state's capital, the Times-Dispatch serves as a newspaper of record for rural regions of the state that lack large local papers."b'Although the Richmond Compiler published in Virginia\'s capitol beginning in 1815, and merged with a later newspaper called The Times, the Times and Compiler failed in 1853, despite an attempt of former banker James A. Cowardin and William H. Davis to revive it several years before. In 1850, Cowardin and Davis established a rival newspaper called the Richmond Dispatch, and by 1852 the Dispatch bragged of having circulation three times as large as any other daily paper in the city, and advertising dominated even its front page. Cowardin began his only term in the Virginia House of Delegates (as a Whig) in 1853, but many thought the city\'s pre-eminent paper the Richmond Examiner.[6] John Hammersley bought half of the newspaper company in 1859, and continued as a joint publisher on the masthead until May 5, 1862, when no name appeared. By April 1861, the newspaper announced its circulation was \xe2\x80\x9cwithin a fraction of 13,000.\xe2\x80\x9d[7] The newspaper had been staunchly pro-slavery since 1852, and called Union soldiers "thieves and cut-throats".[8] Most of its wartime issues are now available online.[9] In 1864, Hammersley brought new presses from England, having run the Union blockade, although he sold half his interest to James W. Lewellen before his dangerous departure (presumably through Wilmington, North Carolina, the last Southern port open to Confederate vessels in 1864).'b'The Richmond Daily Dispatch published its last wartime issue on April 1, 1865; and its office was destroyed the next night during the fire set by Confederate soldiers as they left the city. However, it resumed publication on December 9, 1865, establishing a new office at 12th and Main Streets and accepting Henry K. Ellyson as part-owner as well as editor.[10] By 1866, the Dispatch was one of five papers "carrying prestige from ante bellum days" published in Richmond (of 7 newspapers). Although the newspaper initially opposed the Ku Klux Klan, the Richmond Dispatch accepted Klan advertising in 1868, as it fought Congressional Reconstruction and the Virginia Constitutional Convention of 1868. However, it later accepted the resulting state constitution (after anti-Confederate provisions were stripped) as well as allowing Negroes on juries and in the legislature. Ellyson briefly served as Richmond\'s mayor in 1870, selected by Richmond\'s city council appointed by Governor Gilbert C. Walker. After what some called the "Municipal War" because the prior appointed mayor George Chahoon refused to relinquish his office and mob violence and blockades, the Virginia Supreme Court declared Ellyson the mayor but awaited elections. After skullduggery concerning stolen ballots in the pro-Chahoon Jackson Ward and the election commission declared Ellyson the winner, he refused to serve under the resulting cloud, leading to yet another problematic election won by the Conservative Party candidate. The revived Dispatch later opposed former Confederate General William Mahone and his Readjuster Party.[11] After James Cowardin died in 1882, his son Charles took the helm (with Ellyson\'s assistance, and with Ellyson family members handling business operations), and the paper stopped supporting Negro rights, instead criticizing Del. John Mercer Langston with racial stereotypes.[12]'b"In 1886, Lewis Ginter founded the Richmond Daily Times. A year later, lawyer Joseph Bryan (1845-1908) bought the Daily Times from Ginter, beginning the paper's long association with the Bryan family. Bryan and Ginter had previously helped revitalize the Tanner & Delany Engine Company, transforming it into the Richmond Locomotive Works, which had 800 employees by 1893 and built 200 locomotives per year. In 1890, the Daily Times changed its name to the Richmond Times. In 1896, Bryan acquired the eight-year-old rival Manchester Leader and launched the Evening Leader. In 1899, the evening Richmond News was founded. John L. Williams, owner of the Dispatch, bought the News in 1900."b'By 1903, it was obvious Richmond was not big enough to support four papers. That year, Williams and Bryan agreed to merge Richmond\'s main newspapers. The morning papers merged to become the Richmond Times-Dispatch under Bryan\'s ownership, while the evening papers merged to become The Richmond News Leader under Williams\' ownership. Bryan bought the News Leader in 1908, but died later that year. (Joseph Bryan Park was donated by his widow, Isobel ("Belle") Stewart Bryan, and named for him).'b"His son John Stewart Bryan had given up his own legal career in 1900 to become a reporter working for the Dispatch and helped found the Associated Press and then became vice-president of the publishing company.[13] Upon his father's death, John Stewart Bryan became owner and publisher of the two papers, but in 1914 sold a controlling interest in the Times-Dispatch to three families. He hired Douglas Southall Freeman as editor of the News Leader in 1915, and remained in control until becoming President of the College of William and Mary in 1934 (and publishing a biography of his father the following year). John Stewart Bryan but reacquired the Times-Dispatch in 1940 when the two papers' business interests merged to form Richmond Newspapers, in which Bryan held a 54-percent interest. That conglomeration is now known as Media General. Other publishers in the Bryan family include D. Tennant Bryan and John Stewart Bryan III."b'On June 1, 1992, four days after its sponsored contestant Amanda Goad won the Scripps National Spelling Bee, the News Leader, which had been losing circulation for many years, ceased publication and was folded into the Times-Dispatch.'b"The Richmond Times-Dispatch drew national attention for its coverage of a December 21, 2004, attack by a suicide bomber on an American military base in Mosul, Iraq. The deadliest attack on an American military installation since the war began, the attack injured 69 people and killed 22, including two with the Virginia National Guard's Richmond-based 276th Engineer Battalion. Stories and photographs about the attack by a Times-Dispatch reporter embedded with the 276th were read, heard and seen across the nation."b'In 1990, The RTD borrowed an idea [14] from a local entrepreneur, Barry "Mad Dog" Gottlieb, to encourage a "Tacky Christmas Lights Tour," also known by locals as the "Tacky Light Tour". Every week, the RTD lists the addresses of houses where the most tacky Christmas lights can be found. This tradition has begun to spread to other cities, like Fairfax, Virginia (DC area) [15] as well as San Francisco and Los Angeles.'b"Diane Cantor, the wife of former House Majority Leader Republican Eric Cantor, sits on Media General's Board of Directors.[16] This drew some conflict-of-interest allegations because the RTD serves much of the congressman's 7th district, but no evidence surfaced that she was involved in the paper's content. Her association with the paper was noted at the end of Times-Dispatch stories about Rep. Cantor."b"On May 17, 2012, Media General [17] announced the sale of its newspaper division to BH Media, a subsidiary of Warren Buffett's Berkshire Hathaway company. The sale included all of Media General's newspapers except The Tampa Tribune and its associated publications. Berkshire Hathaway bought 63 newspapers for $142 million and, as part of the deal, offered Media General a $400 million term loan at 10.5 percent interest that will mature in 2020 and a $45 million revolving line of credit. Berkshire Hathaway received a seat on Media General's board of directors and an option to purchase a 19.9% stake in the company.[18] The deal closed on June 25, 2012."b"This also brought to a close Diane Cantor's relationship with the RTD."b'A prominent newspaper in the state, the Times-Dispatch frequently features commentary from important figures from around Virginia, such as officials and presidents from Virginia Commonwealth University, the College of William and Mary, and the University of Virginia. Former Richmond Mayor Douglas Wilder, who had articles published in the paper before he held that position, often outlined policies his administration was implementing. During the 2004 U.S. presidential campaign, its Commentary sections featured some pieces by Retired Admiral Roy Hoffmann, a founding member of the Swift Boat Veterans for Truth and resident of Richmond suburb Chesterfield, against Democratic candidate John Kerry.'b"Editorially, the Times-Dispatch has historically leaned conservative, leading the paper to frequently endorse candidates of the Republican Party. It supported many of former President George W. Bush's policies, including the 2003 invasion of Iraq and a flat income tax. However, the paper is not unilaterally conservative; for example, a 2005 editorial called for the then House Majority Leader Tom DeLay to relinquish his leadership position on ethical grounds. There are also some liberal syndicated columnists who appear frequently, especially Leonard Pitts."b'During the Civil Rights Movement, the Times-Dispatch, like nearly every major newspaper in Virginia, was an ardent supporter of segregation.[19]'b"In the 2016 presidential election, the Times-Dispatch endorsed Libertarian candidate Gary Johnson over major party candidates Donald Trump and Hillary Clinton. Clinton's running mate, Tim Kaine, is a Richmond resident who served as mayor of the city from 1998-2001. From at least 1980 until its Johnson endorsement in 2016, the Times-Dispatch had only endorsed Republican presidential candidates.[20]"b'Like most major papers, the sports section has MLB, NASCAR, MLS, NBA, NCAA, NFL, and NHL scores and results. The Times-Dispatch sports pages naturally focus on Richmond and Virginia professional and college teams. In addition to Richmond Flying Squirrels and Richmond Kickers coverage, readers can see in-depth coverage of the Washington Redskins in the fall and the Washington Nationals in the summer. "Virginians in the Pros" and similar features track all sorts of professional athletes who were born, lived in, or attended college in Virginia. Large automobile racing events like the Sprint Cup (at the Richmond International Raceway) are often given a separate preview guide.'b'Catering to the vast array of Virginia hunters, fishers, hikers, and outdoorsmen, somewhere between half a page to a whole page most days is dedicated to outdoors articles, written by Lee Graves, who succeeded Garvey Winegar in November 2003. The "Scoreboard," which features minor-league standings, Vegas betting, and other sports scores, also gives tide measurements, river levels, and skiing conditions, depending on the season.'b'Virginians have traditionally been highly supportive of high school athletics, and its flagship paper is a testament to that. Particular emphasis is given to American football and basketball; The Times-Dispatch ranks area teams in these sports, in the style of the NCAA polls, and generally updates them weekly. In the fall, Sunday editions have the scores of all high school football games played that weekend from across the state. Prep games are also receive above-average coverage in baseball, cross country, golf, lacrosse, soccer, softball, swimming, tennis, track and field, and volleyball. Stories are frequently done on notable prep athletes, such as those from foreign countries, those with disabilities, those who play a multitude of sports, or those who had little or no prior experience in a sport which they now excel in.'b'The business desk consists of six reporters; they cover technology, retail, energy, insurance, banking, economics, real estate, manufacturing, transportation and consumer issues. Unlike many newspapers, the Times-Dispatch produces a widely read Monday business section, Metro Business. It contains a center cover story on a regional business-related issue and is filled with events for the coming week, advice columnists and gadget reviews. In June 2006, the decision was made to remove the stock tables from the daily sections beginning July 15 and replace the numerous pages with a "Markets Review" section for subscribers who request it. The stock section was eliminated in 2009, as was the Sunday Real Estate section (both were cost-cutting moves). The Sunday Business section, which had been a showcase of general business-interest stories and features, has been rechristened Moneywise and now features primarily consumer-related coverage. Moneywise is also among select Sunday business sections nationwide that print Wall Street Journal Sunday pages.'b'On July 12, 2006, Richmond-based news magazine Style Weekly ran a cover story [21] titled "Truth and Consequences," a piece that took a look at the Times-Dispatch\'s operations as the paper settled into its first year with new management. The report described new editor Glenn Proctor, who took over Nov. 14, 2005, as an "inelegant, blunt and harsh critic \xe2\x80\x94 to the point of saying, repeatedly, that some reporters\' work \'sucks.\'" The piece described a newsroom teetering on the edge, preparing for promised changes \xe2\x80\x94 such as possible layoffs, fewer pages and combined sections \xe2\x80\x94 that eventually were realized. On April 2, 2009, the Times-Dispatch cut 90 jobs, laying off 59 workers, including 28 newsroom jobs. Proctor left the paper in 2011.'b'The front page of the Times-Dispatch\xe2\x80\x99s August 14, 2011 Sunday paper consisted entirely of a Wells Fargo advertisement, commemorating said bank\xe2\x80\x99s acquisition of Wachovia properties in Virginia.[22]'b'Notable columnists published include:'American Civil War
b'Union victory'b'2,200,000:[a]'b'750,000\xe2\x80\x931,000,000:[a][4]'b'110,000+ killed in action/died of wounds\n230,000+ accident/disease deaths[6][7]\n25,000\xe2\x80\x9330,000 died in Confederate prisons[2][6]'b'365,000+ total dead[8] 282,000+ wounded[7]\n181,193 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][9]'b'94,000+ killed in action/died of wounds[6]\n26,000\xe2\x80\x9331,000 died in Union prisons[7]'b'290,000+ total dead\n137,000+ wounded\n436,658 captured[2]\n[better\xc2\xa0source\xc2\xa0needed][10]'b"The American Civil War (known by other names) was a civil war that was fought in the United States from 1861 to 1865. As a result of the long-standing controversy over slavery, war broke out in April 1861, when Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated. The nationalists of the Union proclaimed loyalty to the U.S. Constitution. They faced secessionists of the Confederate States, who advocated for states' rights to expand slavery."b'Among the 34 U.S. states in February 1861, seven Southern slave states individually declared their secession from the U.S. to form the Confederate States of America, or the South. The Confederacy grew to include eleven slave states. The Confederacy was never diplomatically recognized by the United States government, nor was it recognized by any foreign country (although the United Kingdom and France granted it belligerent status). The states that remained loyal to the U.S. (including the border states where slavery was legal) were known as the Union or the North.'b"The Union and Confederacy quickly raised volunteer and conscription armies that fought mostly in the South over four years. The Union finally won the war when General Robert E. Lee surrendered to General Ulysses S. Grant at the Battle of Appomattox Court House, followed by a series of surrenders by Confederate generals throughout the southern states. Four years of intense combat left 620,000 to 750,000 people dead, more than the number of U.S. military deaths in all other wars combined (at least until approximately the Vietnam War).[15] Much of the South's infrastructure was destroyed, especially the transportation systems, railroads, mills, and houses. The Confederacy collapsed, slavery was abolished, and 4 million slaves were freed. The Reconstruction Era (1863\xe2\x80\x931877) overlapped and followed the war, with the process of restoring national unity, strengthening the national government, and granting civil rights to freed slaves throughout the country. The Civil War is the most studied and written about episode in U.S. history.[16]"b''b''b"In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories. The Southern states viewed this as a violation of their constitutional rights and as the first step in a grander Republican plan to eventually abolish slavery. The three pro-Union candidates together received an overwhelming 82% majority of the votes cast nationally: Republican Lincoln's votes centered in the north, Democrat Stephen A. Douglas' votes were distributed nationally and Constitutional Unionist John Bell's votes centered in Tennessee, Kentucky, and Virginia. The Republican Party, dominant in the North, secured a plurality of the popular votes and a majority of the electoral votes nationally, so Lincoln was constitutionally elected president. He was the first Republican Party candidate to win the presidency. However, before his inauguration, seven slave states with cotton-based economies declared secession and formed the Confederacy. The first six to declare secession had the highest proportions of slaves in their populations, a total of 49 percent.[17] The first seven with state legislatures to resolve for secession included split majorities for unionists Douglas and Bell in Georgia with 51% and Louisiana with 55%. Alabama had voted 46% for those unionists, Mississippi with 40%, Florida with 38%, Texas with 25%, and South Carolina cast Electoral College votes without a popular vote for president.[18] Of these, only Texas held a referendum on secession."b'Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln\'s March 4, 1861, inaugural address declared that his administration would not initiate a civil war. Speaking directly to the "Southern States", he attempted to calm their fears of any threats to slavery, reaffirming, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so."[19] After Confederate forces seized numerous federal forts within territory claimed by the Confederacy, efforts at compromise failed and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene, but none did, and none recognized the new Confederate States of America.'b"Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, the battle was inconclusive from 1861\xe2\x80\x931862. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal.[20] To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies, and seized New Orleans. The 1863 Union Siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William T. Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years, known as the Reconstruction Era."b'The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships and iron-clad ships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I, World War II and subsequent conflicts. It remains the deadliest war in American history. From 1861 to 1865, it is estimated that 620,000 to 750,000 soldiers died,[21] along with an undetermined number of civilians.[b] By one estimate, the war claimed the lives of 10 percent of all Northern males 20\xe2\x80\x9345 years old, and 30 percent of all Southern white males aged 18\xe2\x80\x9340.[23]'b'The causes of secession were complex and have been controversial since the war began, but most academic scholars\xc2\xa0identify\xc2\xa0slavery as a central cause of the war. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war.[24] Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to promote pro-slavery acts and policies.[25][26]'b'Slavery was a major cause of disunion.[27] Although there were opposing views even in the Union States,[28][29] most northern soldiers were largely indifferent on the subject of slavery,[30] while Confederates fought the war largely to protect a southern society of which slavery was an integral part.[31] From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with republicanism. The strategy of the anti-slavery forces was containment\xe2\x80\x94to stop the expansion and thus put slavery on a path to gradual extinction.[32] The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights.[33] Southern whites believed that the emancipation of slaves would destroy the South\'s economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population.[34] In particular, southerners feared a repeat of "the horrors of Santo Domingo", in which nearly all white people \xe2\x80\x93 including men, women, children, and even many sympathetic to abolition \xe2\x80\x93 were killed after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea, and proposes it contributed to the segregation in the Jim Crow era following emancipation.[35] These fears were exacerbated by the recent attempts of John Brown to instigate an armed slave rebellion in the South.'b'Slavery was illegal in much of the North, having been outlawed in the late 18th and early 19th centuries. It was also fading in the border states and in Southern cities, but it was expanding in the highly profitable cotton districts of the rural South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide.'b"Sectionalism refers to the different economies, social structure, customs and political values of the North and South.[36][37] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention which manifested Northern dissastisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern Presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for poor freedmen. In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations.[38]"b'Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[39][40]'b'Historically, southern slave-holding states, because of their low-cost manual labor, had little perceived need for mechanization and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.[41]'b'The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress.[42][43] The tariff issue was and is sometimes cited\xe2\x80\x93long after the war\xe2\x80\x93by Lost Cause historians and neo-Confederate apologists. In 1860\xe2\x80\x9361 none of the groups that proposed compromises to head off secession raised the tariff issue.[44] Pamphleteers North and South rarely mentioned the tariff,[45] and when some did, for instance, Matthew Fontaine Maury[46] and John Lothrop Motley,[47] they were generally writing for a foreign audience.'b'The South argued that each state had the right to secede\xe2\x80\x94leave the Union\xe2\x80\x94at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a perpetual union.[48] Historian James McPherson writes concerning states\' rights and other non-slavery explanations:'b"While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[49]"b'Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. It was over territories west of the Mississippi that the proslavery and antislavery forces collided.[50]'b'With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well.[51][52] Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the southern states the question of the territorial expansion of slavery westward again became explosive.[53] Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."[54][55]'b'By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[56] The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.[57]'b'The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance\xe2\x80\x94that slavery could be excluded in a territory as it was done in the Northwest Ordinance of 1787 at the discretion of Congress,[58] thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.[59]'b'Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty\xe2\x80\x94which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter.[60] The Kansas\xe2\x80\x93Nebraska Act of 1854 legislated this doctrine.[61] In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when southern senators began to leave.[62]'b'The fourth theory was advocated by Mississippi Senator Jefferson Davis,[63] one of state sovereignty ("states\' rights"),[64] also known as the "Calhoun doctrine",[65] named after the South Carolinian political theorist and statesman John C. Calhoun.[66] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[67] "States\' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[68] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power."[69][70] These four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the U.S. Constitution prior to the 1860 presidential election.[71]'b'Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in the sense that their country was a national republic based on the belief that all people had inalienable political liberty and personal rights which could serve as an important example to the rest of the world. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, the division and redivision of the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state.'b'In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure\xe2\x80\x94first in their political institutions, then in their civil life together.'b'Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy.[72] C. Vann Woodward said of the latter group,'b'A great slave society\xc2\xa0... had grown up and miraculously flourished in the heart of a thoroughly bourgeois and partly puritanical republic. It had renounced its bourgeois origins and elaborated and painfully rationalized its institutional, legal, metaphysical, and religious defenses\xc2\xa0... When the crisis came it chose to fight. It proved to be the death struggle of a society, which went down in ruins.[73]'b"Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin (1852)[74] and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.[75]"b'While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion\xc2\xa0... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence."[76] The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.[77]'b'The election of Abraham Lincoln in November 1860 was the final trigger for secession.[78] Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.'b'According to Lincoln, the people of the United States had shown that they can be successful in establishing and administering a republic, but a third challenge faced the nation, maintaining the republic, based on the people\'s vote. The people must now show: "successful maintenance [of the Republic] against a formidable internal attempt to overthrow it. It is now for them to demonstrate to the world that those who can fairly carry an election can also suppress a rebellion; that ballots are the rightful and peaceful successors of bullets; and that when ballots have fairly and constitutionally decided, there can be no successful appeal back to bullets; that there can be no successful appeal, except to ballots themselves, at succeeding elections. Such will be a great lesson of peace; teaching men that what they cannot take by an election, neither can they take it by a war".[79]'b'The election of Lincoln caused the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws and, even, secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860, and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states\' rights for slave owners in the South, but contained a complaint about states\' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.'b'Among the ordinances of secession passed by the individual states, those of three\xe2\x80\x94Texas, Alabama, and Virginia\xe2\x80\x94specifically mentioned the plight of the "slaveholding states" at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures.[80] However, at least four states\xe2\x80\x94South Carolina,[81] Mississippi,[82] Georgia,[83] and Texas[84]\xe2\x80\x94also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement\'s influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive slave clause of the Constitution.'b'These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861.[85] They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress".[86] One quarter of the U.S. Army\xe2\x80\x94the entire garrison in Texas\xe2\x80\x94was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.'b'As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts),[87] the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.'b"On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it.[88] It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would have voted in favor of it.[89] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[90]"b'On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[91] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[91]'b'The South sent delegations to Washington and offered to pay for the federal properties[which?] and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[92] Secretary of State William Seward, who at the time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed.[92] President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina\'s Fort Sumter.'b"Fort Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. fort's garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan, who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12\xe2\x80\x9313, forcing its capitulation."b'The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins said:'b"However, much of the North's attitude was based on the false belief that only a minority of Southerners were actually in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners really did favor secession, they might have hesitated at attempting the enormous task of conquering a united South.[95]"b'Lincoln called on all the states to send forces to recapture the fort and other federal properties. With the scale of the rebellion apparently small so far, Lincoln called for only 75,000 volunteers for 90\xc2\xa0days.[96] The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal.[97] On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.[98]'b'Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.[99]'b'Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. West Virginia then joined them as an additional border state after it separated from Virginia and became a state of the Union in 1863.'b"Maryland's territory surrounded the United States' capital of Washington, DC and could cut it off from the North.[100] It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53\xe2\x80\x9313) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war.[101] Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North.[102] Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened.[101][103] All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.[104]"b'In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. (See also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[105]'b'Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.[106]'b"After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving).[107] The inclusion of 24 secessionist counties[108] in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war.[109][110] Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000\xe2\x80\x9322,000 soldiers to both the Confederacy and the Union.[111]"b'A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[112]'b'The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy\'s soldier.[113]'b'As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[114] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.[115]'b'In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law\xe2\x80\x94conscription\xe2\x80\x94as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt.[116] The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.[117]'b"When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[118] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.[119]"b'In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[120] At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units.[121] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[122]'b'From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan\'s assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.[123]'b'Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:'b"Some historians emphasize that Civil War soldiers were driven by political ideology, holding firm beliefs about the importance of liberty, Union, or state rights, or about the need to protect or to destroy slavery. Others point to less overtly political reasons to fight, such as the defense of one's home and family, or the honor and brotherhood to be preserved when fighting alongside other men. Most historians agree that no matter what a soldier thought about when he went into the war, the experience of combat affected him profoundly and sometimes altered his reasons for continuing the fight.[124]"b"At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their own army where they were paid but not allowed to perform any military duties.[125] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[126]"b'The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396.[127][128] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[129] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.'b"By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible.[130] Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.[131]"b'In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[132]'b'The Civil War occurred during the early stages of the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union\'s naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[133] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union\'s own ironclad warships, they were unsuccessful.[134]'b"The Confederacy experimented with a submarine, which did not work well,[135] and with building an ironclad ship, the CSS Virginia, which was based on rebuilding a sunken Union ship, the Merrimack. On its first foray on March 8, 1862, the Virginia inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, the USS Monitor, arrived to challenge it in the Chesapeake Bay. The resulting three hour battle between the Ironclads was a draw, but it marked the worldwide transition to ironclad warships.[136] Not long after the battle the Confederacy was forced to scuttle the Virginia to prevent its capture, while the Union built many copies of the Monitor. Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Britain.[137]"b'British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were designed for speed and were so small that only a small amount of cotton went out.[138] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released.[139] The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[140]'b"Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[141] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.[142]"b'To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested.[134] After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.[143]'b'The 1862 Union strategy called for simultaneous advances along four axes:[144]'b'Ulysses Grant used river transport and Andrew Foote\'s gunboats of the Western Flotilla to threaten the Confederacy\'s "Gibraltar of the West" at Columbus, Kentucky. Though rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.[145]'b"In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[146] They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry (February 6, 1862) and Fort Donelson (February 11 to 16, 1862), and supplied Grant's forces as he moved into Tennessee. At Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory\xe2\x80\x94the first battle with the high casualty rates that would repeat over and over.[147] Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. On April 24, 1862, U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[148]"b'Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at Vicksburg, Mississippi in July 1863, and in the Union fully controlling the Mississippi River soon after.[149]'b'In one of the first highly visible battles, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces near Washington was repulsed.'b"Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign,[150][151][152] Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[153] The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[154] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops."b"Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[153][155] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[156]"b"When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[157] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker."b"Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863.[158] Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[159] Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[160] This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[161] However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant."b"While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[162] Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization."b'The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans,[163] which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.'b"General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state.[164] Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.[165]"b"The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged."b"The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh;[166] and the Battle of Vicksburg,[167] which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[168] driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy."b'Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[171] Roving Confederate bands such as Quantrill\'s Raiders terrorized the countryside, striking both military installations and civilian settlements.[172] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.'b'By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.[169]'b'Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union.[173] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[174]'b'After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him.[175] Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.'b'At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[176] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[177] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[178]'b"Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[179]"b"Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[180]"b"Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president.[181] Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.[182]"b'Leaving Atlanta, and his base of supplies, Sherman\'s army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman\'s army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee\'s army.[183]'b'Lee\'s army, thinned by desertion and casualties, was now much smaller than Grant\'s. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler\'s Creek.[184]'b"Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House.[185] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller."b"On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[186] On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William T. Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day.[1] On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department.[187] On June 23, Cherokee leader Stand Watie became the last Confederate General to surrender his forces.[188]"b"Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators.[189][190] The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.[191]"b'Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860\xe2\x80\x9362 crop failures in Europe made the North\'s grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half.[191] When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.[192]'b'Lincoln\'s foreign policy was deficient in 1861 in terms of appealing to European public opinion. Diplomats had to explain that United States was not committed to the ending of slavery, but instead they repeated legalistic arguments about the unconstitutionality of secession. Confederate spokesmen, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. In addition, the European aristocracy (the dominant factor in every major country) was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[193]'b'U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (CSS Alabama, CSS Shenandoah, CSS Tennessee, CSS Tallahassee, CSS Florida, and some others). The most famous, the CSS Alabama, did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for politicians in Britain, where the antislavery movement was powerful.[194]'b"War loomed in late 1861 between the U.S. and Britain over the Trent affair, involving the U.S. Navy's boarding of the British ship Trent and seizure of two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation between North and South\xe2\x80\x93 though even such an offer would have risked war with the U.S. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on this.[195]"b"The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.[196]"b'The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty.[197]'b"Historians have debated whether the Confederacy could have won the war. Most scholars, including James McPherson, argue that Confederate victory was at least possible.[198] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.[199]"b'Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[199] Lincoln was not a military dictator, and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.[203]'b'Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat.[204][205] Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back\xc2\xa0... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don\'t think the South ever had a chance to win that War."[206]'b'A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win."[207][208] Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederate army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict.[209] However, most historians reject the argument.[210] James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864\xe2\x80\x9365, he says most Confederate soldiers were fighting hard.[211] Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up\xe2\x80\x94some few deserters\xe2\x80\x94plenty tired of war, but the masses determined to fight it out."[212]'b"Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[213] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[214]"b'Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[215] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:'b'The North\'s victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[216]'b'Scholars have debated what the effects of the war were on political and economic power in the South.[217] The prevailing view is that the southern planter elite retained its powerful position in the South.[217] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[217]'b'The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths\xe2\x80\x94two-thirds by disease, and 50,000 civilians.[11] Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000.[21][218] The war accounted for more American deaths than in all other U.S. wars combined.[219]'b'Based on 1860 census figures, 8 percent of all white males aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[220][221] About 56,000 soldiers died in prison camps during the War.[222] An estimated 60,000 men lost limbs in the war.[223]'b'Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:[6]'b'In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[7]'b'Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle.[6] Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War.[224]:16 Notably, their mortality rate was significantly higher than white soldiers:'b'[We] find, according to the revised official data, that of the slightly over two millions troops in the United States Volunteers, over 316,000 died (from all causes), or 15.2 percent. Of the 67,000 Regular Army (white) troops, 8.6 percent, or not quite 6,000, died. Of the approximately 180,000 United States Colored Troops, however, over 36,000 died, or 20.5 percent. In other words, the mortality "rate" amongst the United States Colored Troops in the Civil War was thirty-five percent greater than that among other troops, notwithstanding the fact that the former were not enrolled until some eighteen months after the fighting began.[224]:16'b"Confederate records compiled by historian William F. Fox list 74,524 killed and died of wounds and 59,292 died of disease. Including Confederate estimates of battle losses where no records exist would bring the Confederate death toll to 94,000 killed and died of wounds. Fox complained, however, that records were incomplete, especially during the last year of the war, and that battlefield reports likely under-counted deaths (many men counted as wounded in battlefield reports subsequently died of their wounds). Thomas L. Livermore, using Fox's data, put the number of Confederate non-combat deaths at 166,000, using the official estimate of Union deaths from disease and accidents and a comparison of Union and Confederate enlistment records, for a total of 260,000 deaths.[6] However, this excludes the 30,000 deaths of Confederate troops in prisons, which would raise the minimum number of deaths to 290,000."b'The United States National Park Service uses the following figures in its official tally of war losses:[2]'b'Union: 853,838'b'Confederate: 914,660'b"While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, Superintendent of the 1870 Census, used census and Surgeon General data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 Census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.[218]"b'Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[22] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.'b"Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in an area where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor James Downs states that tens to hundreds of thousands of slaves died during the war from disease, starvation, exposure, or execution at the hands of the Confederates, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.[225]"b'Losses were far higher than during the recent defeat of Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Mini\xc3\xa9 balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.[226]'b"The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.[227]"b'The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century.[228] The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.'b'While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee\'s army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[229] Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal.[230] Lincoln\'s decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[231] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans\' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.[232]'b'The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army.[e] About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.[f]'b'During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[238] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.[239]'b'At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Fr\xc3\xa9mont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected.[240] But only the District of Columbia accepted Lincoln\'s gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat".[241] Lincoln laid the groundwork for public support in an open letter published in abolitionist Horace Greeley\'s newspaper.[242]'b'In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors\' Conference added support for the proclamation.[243] Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong\xc2\xa0... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling\xc2\xa0... I claim not to have controlled events, but confess plainly that events have controlled me."[244]'b"Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.[245]"b"Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[246] The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France.[247] By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.[248]"b'In Texas v. White, 74 U.S. 700 (1869) the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.[249]'b'Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877.[250] It comprised multiple complex methods to resolve the outstanding issues of the war\'s aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery\xe2\x80\x94and prevent semi-slavery status.[251]'b'President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson\'s work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended.[252] With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.'b'The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war\'s aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[253] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[254]'b'Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academe, leading to a proliferation of solid studies by non-scholars who are thoroughly familiar with the primary sources, pay close attention to battles and campaigns, and write for the large public readership, rather than the small scholarly community. Bruce Catton and Shelby Foote are among the best-known writers.[255][256] Practically every major figure in the war, both North and South, has had a serious biographical study.[257] Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:'b"God had chastised them and given them a special mission\xe2\x80\x94to maintain orthodoxy, strict biblicism, personal piety, and traditional race relations. Slavery, they insisted, had not been sinful. Rather, emancipation was a historical tragedy and the end of Reconstruction was a clear sign of God's favor.[258]"b'In sharp contrast, Black preachers interpreted the Civil War as:'b"God's gift of freedom. They appreciated opportunities to exercise their independence, to worship in their own way, to affirm their worth and dignity, and to proclaim the fatherhood of God and the brotherhood of man. Most of all, they could form their own churches, associations, and conventions. These institutions offered self-help and racial uplift, and provided places where the gospel of liberation could be proclaimed. As a result, black preachers continued to insist that God would protect and help him; God would be their rock in a stormy land.[259]"b'Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations.[260] Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[261] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing African-American progress to a white man\'s reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[262]'b"The interpretation of the Civil War presented by Charles A. Beard and Mary R. Beard in The Rise of American Civilization (1927) was highly influential among historians and the general public until the civil rights movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:"b'[A] social cataclysm in which the capitalists, laborers, and farmers of the North and West drove from power in the national government the planting aristocracy of the South.[263]'b'The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.[264]'b"The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861, but the oldest surviving monument is the Hazen monument, erected at Stones River near Murfreesboro, Tennessee, in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead in the Battle of Stones River. In the 1890s, the United States government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park in Tennessee and the Antietam National Battlefield in Maryland in 1890. The Shiloh National Military Park was established in 1894, followed by the Gettysburg National Military Park in 1895 and Vicksburg National Military Park in 1899. In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service.[265]"b'The modern Civil War battlefield preservation movement began in 1987 with the founding of the Association for the Preservation of Civil War Sites (APCWS), a grassroots organization created by Civil War historians and others to preserve battlefield land by acquiring it. In 1991, the original Civil War Trust was created in the mold of the Statue of Liberty/Ellis Island Foundation, but failed to attract corporate donors and soon helped manage the disbursement of U.S. Mint Civil War commemorative coin revenues designated for battlefield preservation. Although the two organizations joined forces on a number of battlefield acquisitions, ongoing conflicts prompted the boards of both organizations to facilitate a merger, which happened in 1999 with the creation of the Civil War Preservation Trust. In 2011, the organization was renamed The Civil War Trust. From 1987 through late 2017, The Trust and its predecessor organizations saved more than 40,000 acres at 126 Civil War battlefields and sites in 21 states.[266]'b"The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary. [267] Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as Birth of a Nation (1915), Gone with the Wind (1939), and more recently Lincoln (2012). Ken Burns produced a notable PBS series on television titled The Civil War (1990). It was digitally remastered and re-released in 2015."b'There were numerous technological innovations during the Civil War that had a great impact on 19th century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[268] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[269][270] It was also in this war when countries first used aerial warfare, in the form of reconnaissance balloons, to a significant effect.[271] It saw the first action involving steam-powered ironclad warships in naval warfare history.[272] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare, as well as the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[273]'b'General reference'b'Union'b'Confederacy'b'Ethnic articles'b'Topical articles'b'National articles'b'State articles'b'Memorials'b''Singular-value decomposition
b'The singular-value decomposition can be computed using the following observations:'b'Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.'b'Suppose M is a m \xc3\x97 n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the form'b'where'b'The diagonal entries \xcf\x83i of \xce\xa3 are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, \xce\xa3, is uniquely determined by M (though not the matrices U and V, see below).'b'In the special, yet common case when M is an m \xc3\x97 m real square matrix with positive determinant, U, V\xe2\x88\x97, and \xce\xa3 are real m \xc3\x97 m matrices as well, \xce\xa3 can be regarded as a scaling matrix, and U, V\xe2\x88\x97 can be viewed as rotation matrices. Thus the expression U\xce\xa3V\xe2\x88\x97 can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.'b"Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = V\xce\xa3V\xe2\x88\x97) with eigenvalue scale factors \xcf\x83i along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV\xe2\x88\x97). If the rotation is done first, M = P'R, then R is the same and P' = U\xce\xa3U\xe2\x88\x97 has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate."b'As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n \xc3\x97 n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m \xc3\x97 n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.'b'Since U and V\xe2\x88\x97 are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector \xcf\x83i Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U\xe2\x88\x97 and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U\xe2\x88\x97, V, and V\xe2\x88\x97 are orthonormal bases.'b'Consider the 4 \xc3\x97 5 matrix'b'A singular-value decomposition of this matrix is given by U\xce\xa3V\xe2\x88\x97'b'Notice \xce\xa3 is zero outside of the diagonal and one diagonal element is zero. Furthermore, because the matrices U and V\xe2\x88\x97 are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V\xe2\x88\x97 are real valued, each is an orthogonal matrix.'b'is also a valid singular-value decomposition.'b'In any singular-value decomposition'b'the diagonal entries of \xce\xa3 are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:'b'As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank\xe2\x80\x93nullity theorem cannot be the same dimension if m \xe2\x89\xa0 n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m \xe2\x88\x92 n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n \xe2\x88\x92 m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.'b'Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ei\xcf\x86 (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.'b'The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = U\xce\xa3V\xe2\x88\x97 is'b'where \xce\xa3+ is the pseudoinverse of \xce\xa3, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.'b"A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x\xe2\x88\x97A = 0, with x\xe2\x88\x97 denoting the conjugate transpose of x, is called a left null vector of A."b'A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.'b'Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.'b'As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \xce\xa3. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.'b'Here Ui and Vi are the i-th columns of the corresponding SVD matrices, \xcf\x83i are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero \xcf\x83i is exactly the rank of the matrix.'b"Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,"b'which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]'b'It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O \xe2\x88\x92 A. The solution is the product UV\xe2\x88\x97.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV\xe2\x88\x97 where I is the identity matrix, so that if A = U\xce\xa3V\xe2\x88\x97 then the product A = UV\xe2\x88\x97 amounts to replacing the singular values with ones.'b'A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,'b'This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.'b"The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules."b'The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]'b'The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.'b'The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the \xce\xa3 matrix is larger than one.'b'One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.'b'SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]'b"Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]"b'Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.'b'Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]'b'The singular-value decomposition is very general in the sense that it can be applied to any m \xc3\x97 n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.'b'Given an SVD of M, as described above, the following two relations hold:'b'The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:'b'In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU\xe2\x88\x97 for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU\xe2\x88\x97 is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each \xcf\x83i to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=U\xce\xa3U* is positive semidefinite and normal, and R=UV* is unitary.'b'An eigenvalue \xce\xbb of a matrix M is characterized by the algebraic relation Mu = \xce\xbbu. When M is Hermitian, a variational characterization is also available. Let M be a real n \xc3\x97 n symmetric matrix. Define'b'By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| \xe2\x89\xa4 1}. By the Lagrange multipliers theorem, u necessarily satisfies'b'where the nabla symbol, \xe2\x88\x87, is the del operator.'b'A short calculation shows the above leads to Mu = \xce\xbbu (symmetry of M is needed here). Therefore, \xce\xbb is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.'b'Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.'b'This section gives these two arguments for existence of singular-value decomposition.'b'Let M be an m \xc3\x97 n complex matrix. Since M\xe2\x88\x97M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n \xc3\x97 n matrix V such that'b'where D is diagonal and positive definite. Partition V appropriately so we can write'b'Therefore:'b'The second equation implies MV2 = 0. Also, since V is unitary:'b'where the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. Define'b'Then'b'We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, since'b'the columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:'b'For V1 we already have V2 to make it unitary. Now, define'b'which is the desired result:'b'Notice the argument could begin with diagonalizing MM\xe2\x88\x97 rather than M\xe2\x88\x97M (This shows directly that MM\xe2\x88\x97 and M\xe2\x88\x97M have the same non-zero eigenvalues).'b'The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.'b'Let M denote an m \xc3\x97 n matrix with real entries. Let Sm\xe2\x88\x921 and Sn\xe2\x88\x921 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the function'b'Consider the function \xcf\x83 restricted to Sm\xe2\x88\x921 \xc3\x97 Sn\xe2\x88\x921. Since both Sm\xe2\x88\x921 and Sn\xe2\x88\x921 are compact sets, their product is also compact. Furthermore, since \xcf\x83 is continuous, it attains a largest value for at least one pair of vectors u \xe2\x88\x88 Sm\xe2\x88\x921 and v \xe2\x88\x88 Sn\xe2\x88\x921. This largest value is denoted \xcf\x831 and the corresponding vectors are denoted u1 and v1. Since \xcf\x831 is the largest value of \xcf\x83(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.'b'Proof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:'b'After some algebra, this becomes'b'Plugging this into the pair of equations above, we have'b'This proves the statement.'b'More singular vectors and singular values can be found by maximizing \xcf\x83(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.'b'The passage from real to complex is similar to the eigenvalue case.'b'Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).'b'The linear transformation'b'has a particularly simple description with respect to these orthonormal bases: we have'b'where \xcf\x83i is the i-th diagonal entry of \xce\xa3, and T(Vi) = 0 for i > min(m,n).'b'The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T\xc2\xa0: Kn \xe2\x86\x92 Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.'b'To get a more visual flavour of singular values and SVD factorization \xe2\x80\x94 at least when working on real vector spaces \xe2\x80\x94 consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V\xe2\x88\x97 sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D \xe2\x88\x98 V\xe2\x88\x97 then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U \xe2\x88\x98 D \xe2\x88\x98 V\xe2\x88\x97 coincides with T.'b'The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m \xe2\x89\xa5 n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).'b'The first step can be done using Householder reflections for a cost of 4mn2 \xe2\x88\x92 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).'b'The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.'b'The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 \xc3\x97 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 \xc3\x97 2 eigenvalue methods (Golub & Van Loan 1996, \xc2\xa78.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).'b'There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or'b'The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M \xe2\x87\x92 Q R and LQ decomposition of R gives R \xe2\x87\x92 L P*. Thus, at every iteration, we have M \xe2\x87\x92 Q L P*, update M \xe2\x87\x90 L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.'b'In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m\xc3\x97n matrix M of rank r:'b"Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n\xc2\xa0\xe2\x89\xaa\xc2\xa0m. The matrix U'n is thus m\xc3\x97n, \xce\xa3n is n\xc3\x97n diagonal, and V is n\xc3\x97n."b'The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if\xc2\xa0n\xc2\xa0\xe2\x89\xaa\xc2\xa0m.'b'Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values \xce\xa3r are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r\xc2\xa0\xe2\x89\xaa\xc2\xa0n. The matrix Ur is thus m\xc3\x97r, \xce\xa3r is r\xc3\x97r diagonal, and Vr* is r\xc3\x97n.'b'Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values \xce\xa3t are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t\xe2\x89\xaar. The matrix Ut is thus m\xc3\x97t, \xce\xa3t is t\xc3\x97t diagonal, and Vt* is t\xc3\x97n.'b'The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]'b'The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spaces'b'But, in the matrix case, (M* M)\xc2\xbd is a normal matrix, so ||M* M||\xc2\xbd is the largest eigenvalue of (M* M)\xc2\xbd, i.e. the largest singular value of M.'b"The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)\xc2\xbd] (the eigenvalues of M* M are the squares of the singular values)."b'The singular values are related to another norm on the space of operators. Consider the Hilbert\xe2\x80\x93Schmidt inner product on the n \xc3\x97 n matrices, defined by'b'So the induced norm is'b'Since the trace is invariant under unitary equivalence, this shows'b'where \xcf\x83i are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert\xe2\x80\x93Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:'b'In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.'b'Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.'b'TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:'b'The factorization M = U\xce\xa3V\xe2\x88\x97 can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X,\xc2\xa0\xce\xbc), and a non-negative measurable f such that'b'This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such that'b'is an unitary operator.'b'As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write'b'and notice that U V* is still a partial isometry while VTf V* is positive.'b'The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero \xce\xbb in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M\xe2\x88\x97M. Applying the diagonalization result, the unitary image of its positive square root Tf\xc2\xa0 has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {\xcf\x83i}. For any \xcf\x88 \xe2\x88\x88 H,'b'where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. \xcf\x83i are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.'b'Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:'b'The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.'b'Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.'Method of moments (statistics)
b'In statistics, the method of moments is a method of estimation of population parameters. One starts with deriving equations that relate the population moments (i.e., the expected values of powers of the random variable under consideration) to the parameters of interest. Then a sample is drawn and the population moments are estimated from the sample. The equations are then solved for the parameters of interest, using the sample moments in place of the (unknown) population moments. This results in estimates of those parameters. The method of moments was introduced by Pafnuty Chebyshev in 1887.'b''b''b'The method of moments is fairly simple and yields consistent estimators (under very weak assumptions), though these estimators are often biased.'b"In some respects, when estimating parameters of a known family of probability distributions, this method was superseded by Fisher's method of maximum likelihood, because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased."b'However, in some cases the likelihood equations may be intractable without computers, whereas the method-of-moments estimators can be quickly and easily calculated by hand.'b'Estimates by the method of moments may be used as the first approximation to the solutions of the likelihood equations, and successive improved approximations may then be found by the Newton\xe2\x80\x93Raphson method. In this way the method of moments can assist in finding maximum likelihood estimates.'b'In some cases, infrequent with large samples but not so infrequent with small samples, the estimates given by the method of moments are outside of the parameter space; it does not make sense to rely on them then. That problem never arises in the method of maximum likelihood. Also, estimates by the method of moments are not necessarily sufficient statistics, i.e., they sometimes fail to take into account all relevant information in the sample.'b'When estimating other structural parameters (e.g., parameters of a utility function, instead of parameters of a known probability distribution), appropriate probability distributions may not be known, and moment-based estimates may be preferred to maximum likelihood estimation.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Explicit semantic analysis
b'In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf\xe2\x80\x93idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.[1]'b'ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization[2] and has been used by this pair of researchers to compute what they refer to as "semantic relatedness" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts. The name "explicit semantic analysis" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.[1][3]'b''b''b'To perform the basic variant of ESA, one starts with a collection of texts, say, all Wikipedia articles; let the number of documents in the collection be N. These are all turned into "bags of words", i.e., term frequency histograms, stored in an inverted index. Using this inverted index, one can find for any word the set of Wikipedia articles containing this word; in the vocabulary of Egozi, Markovitch and Gabrilovitch, "each word appearing in the Wikipedia corpus can be seen as triggering each of the concepts it points to in the inverted index."[1]'b'The output of the inverted index for a single word query is a list of indexed documents (Wikipedia articles), each given a score depending on how often the word in question occurred in them (weighted by the total number of words in the document). Mathematically, this list is an N-dimensional vector of word-document scores, where a document not containing the query word has score zero. To compute the relatedness of two words, one compares the vectors (say u and v) by computing the cosine similarity,'b'and this gives numeric estimate of the semantic relatedness of the words. The scheme is extended from single words to multi-word texts by simply summing the vectors of all words in the text.[3]'b'ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically orthogonal concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of information retrieval systems when it is based not on Wikipedia, but on the Reuters corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".[4] To explain this observation, links have been shown between ESA and the generalized vector space model.[5] Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".[1]'b'Cross-language explicit semantic analysis (CL-ESA) is a multilingual generalization of ESA.[6] CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.'Latent semantic analysis
b'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]'b'An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).[2]'b''b''b'LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency\xe2\x80\x93inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.'b'This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.'b'After the construction of the occurrence matrix, LSA finds a low-rank approximation[4] to the term-document matrix. There could be various reasons for these approximations:'b'The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:'b'This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.'b'Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:'b'Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:'b'The matrix products giving us the term and document correlations then become'b'You can now do the following:'b'To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:'b'The new low-dimensional space typically can be used to:'b'Synonymy and polysemy are fundamental problems in natural language processing:'b'LSA has been used to assist in performing prior art searches for patents.[8]'b'The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words. These findings are referred to as the Semantic Proximity Effect.[9]'b'When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list. These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.[10]'b'Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.[11]'b"The SVD is typically computed using large matrix methods (for example, Lanczos methods) but may also be computed incrementally and with greatly reduced resources via a neural network-like approach, which does not require the large, full-rank matrix to be held in memory.[12] A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.[13] MATLAB and Python implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution. In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.[14]"b"Some of LSA's drawbacks include:"b'In semantic hashing [17] documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.'b'Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.[18]'b'LSI is also an application of correspondence analysis, a multivariate statistical technique developed by Jean-Paul Benz\xc3\xa9cri[19] in the early 1970s, to a contingency table built from word counts in documents.'b'Called "latent semantic indexing" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s. The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\xe2\x80\x99t share a specific word or words with the search criteria.'b'LSI overcomes two of the most problematic constraints of Boolean keyword queries: multiple words that have similar meanings (synonymy) and words that have more than one meaning (polysemy)[clarification needed]. Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems.[20] As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.'b'LSI is also used to perform automated document categorization. In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.[21] Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.[22] LSI uses example documents to establish the conceptual basis for each category. During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.'b'Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI. Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection of unstructured text.'b'Because it uses a strictly mathematical approach, LSI is inherently independent of language. This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri. LSI can also perform cross-linguistic concept searching and example-based categorization. For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.[citation needed]'b'LSI is not restricted to working only with words. It can also process arbitrary character strings. Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.[23]'b'LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).[24] This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion. LSI also deals effectively with sparse, ambiguous, and contradictory data.'b'Text does not need to be in sentence form for LSI to be effective. It can work with lists, free-form notes, email, Web-based content, etc. As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.'b'LSI has proven to be a useful solution to a number of conceptual matching problems.[25][26] The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.[27]'b'LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.'b'Some common local weighting functions[29] are defined in the following table.'b'Some common global weighting functions are defined in the following table.'b'In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A\xe2\x80\x94a measure of its unique dimensions \xe2\x89\xa4 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.'b'The SVD is then truncated to reduce the rank by keeping only the largest k \xc2\xab\xc2\xa0r diagonal entries in the singular value matrix S, where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A. This reduced set of matrices is often denoted with a modified formula such as:'b'Efficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.'b'Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons.'b'The computed Tk and Dk matrices define the term and document vector spaces, which with the computed singular values, Sk, embody the conceptual information derived from the document collection. The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.'b'The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index. By a simple transformation of the A = T S DT equation into the equivalent D = AT T S\xe2\x88\x921 equation, a new vector, d, for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S\xe2\x88\x921. The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.'b'A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored. These terms will have no impact on the global weights and learned correlations derived from the original collection of text. However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.'b'The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in. Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added. When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in [13]) be used.'b'It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems. As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.'b'LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.[32] Below are some other ways in which LSI is being used:'b'LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential. Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.[47]'b'Early challenges to LSI focused on scalability and performance. LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.[48] However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome. Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package.[49]'b'Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD. As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).[50] However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.[51]'b'Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain. The variance contained in the data can be viewed by plotting the singular values (S) in a scree plot. Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain. Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain. Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.[52][53][54]'b'Due to its cross-domain applications in Information Retrieval, Natural Language Processing (NLP), Cognitive Science and Computational Linguistics, LSA has been implemented to support many different kinds of applications.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'Hierarchical Dirichlet process
b'In statistics and machine learning, the hierarchical Dirichlet process (HDP) is a nonparametric Bayesian approach to clustering grouped data.[1][2] It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. This method allows groups to share statistical strength via sharing of clusters across groups. The base distribution being drawn from a Dirichlet process is important, because draws from a Dirichlet process are atomic probability measures, and the atoms will appear in all group-level Dirichlet processes. Since each atom corresponds to a cluster, clusters are shared across all groups. It was developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei and published in the Journal of the American Statistical Association in 2006,[1] as a formalization and generalization of the infinite hidden Markov model published in 2002.[3]'b''b''b'Thus the set of atoms is shared across all groups, with each group having its own group-specific atom masses. Relating this representation back to the observed data, we see that each data item is described by a mixture model:'b'The HDP mixture model is a natural nonparametric generalization of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data.[1] Here each group is a document consisting of a bag of words, each cluster is a topic, and each document is a mixture of topics. The HDP is also a core component of the infinite hidden Markov model,[3] which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.[1] [4]'b'The HDP can be generalized in a number of directions. The Dirichlet processes can be replaced by Pitman-Yor processes, resulting in the Hierarchical Pitman-Yor process. The hierarchy can be deeper, with multiple levels of groups arranged in a hierarchy. Such an arrangement has been exploited in the sequence memoizer, a Bayesian nonparametric model for sequences which has a multi-level hierarchy of Pitman-Yor processes.'Non-negative matrix factorization
b'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.'b'NMF finds applications in such fields as astronomy[3] [4], computer vision, document clustering,[1] chemometrics, audio signal processing and recommender systems.[5][6]'b''b''b'In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[7] In this framework the vectors in the right matrix are continuous curves rather than discrete vectors. Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[8][9] It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations.[10][11]'b'Let matrix V be the product of the matrices W and H,'b'Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. That is, each column of V can be computed as follows:'b'where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.'b'When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m \xc3\x97 n matrix, W is an m \xc3\x97 p matrix, and H is a p \xc3\x97 n matrix then p can be significantly less than both m and n.'b'Here is an example based on a text-mining application:'b'This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.'b"It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H."b'When the error function to be used is Kullback\xe2\x80\x93Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[13]'b'Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.'b'When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.'b'In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[15][16][17] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[18]'b'There are different types of non-negative matrix factorizations. The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]'b'Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback\xe2\x80\x93Leibler divergence to positive matrices (the original Kullback\xe2\x80\x93Leibler divergence is defined on probability distributions). Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.'b'Another type of NMF for images is based on the total variation norm.[19]'b'When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[20][21] although it may also still be referred to as NMF.[22]'b'Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[23][24][25]'b"There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[11] has been a popular method due to the simplicity of implementation. This algorithm is:"b'Note that the updates are done on an element by element basis not matrix multiplication.'b'We note that W and H multiplicative factor is identity matrix when V = W H.'b'More recently other algorithms have been developed. Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[26] or different, as some NMF variants regularize one of W and H.[20] Specific approaches include the projected gradient descent methods,[26][27] the active set method,[5][28] the optimal gradient method,[29] and the block principal pivoting method[30] among several others.[31]'b'Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[32] However, as in many other data mining applications, a local minimum may still prove to be useful.'b'The contribution of the sequential NMF components can be compared with the Karhunen\xe2\x80\x93Lo\xc3\xa8ve theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting[34][35]. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA[4], which is the indication of less over-fitting of sequential NMF.'b'Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[36] Kalofolias and Gallopoulos (2012)[37] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[38]'b'In Learning the parts of objects by non-negative matrix factorization Lee and Seung[39] proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.'b'It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[40] When NMF is obtained by minimizing the Kullback\xe2\x80\x93Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[41] trained by maximum likelihood estimation. That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.'b'NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[12][42] This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[14]'b'NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[43]'b'NMF extends beyond matrices to tensors of arbitrary order.[44][45][46] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.'b'Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[47]'b'NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[48]'b'The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[49]'b'More control over the non-uniqueness of NMF is obtained with sparsity constraints.[50]'b'In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3] and the direct imaging observations [4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [33] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.'b'Ren et al. (2018) [4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.'b'In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10\xe2\x81\xb5 to 10\xc2\xb9\xe2\x81\xb0, various statistical methods have been adopted [51] [52] [34], however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux [53] [35]. Forward modeling is currently optimized for point sources[35], however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors[4], rather than a computationally intensive data re-reduction on generated models.'b'NMF can be used for text mining applications. In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents. This matrix is factored into a term-feature and a feature-document matrix. The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.'b'One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[54] Another research group clustered parts of the Enron email dataset[55] with 65,033 messages and 91,133 terms into 50 clusters.[56] NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[57]'b'Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[38]'b'NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[58]'b'Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[61] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.'b'The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.'b'NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[21][62][63][64] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[65]'b'NMF, also referred in this field as factor analysis, has been used since the 80s[66] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[67]'b'Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,'Mallet (software project)
b'MALLET is a Java "Machine Learning for Language Toolkit".'b''b''b'MALLET is an integrated collection of Java code useful for statistical natural language processing, document classification, cluster analysis, information extraction, topic modeling and other machine learning applications to text.'b'MALLET was developed primarily by Andrew McCallum, of the University of Massachusetts Amherst, with assistance from graduate students and faculty from both UMASS and the University of Pennsylvania.'b''Gensim
b'Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.'b''b''b'Gensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms,[1] hierarchical Dirichlet processes (HDP), latent semantic analysis (LSA, LSI, SVD) and latent Dirichlet allocation (LDA), including distributed parallel versions.[2]'b'Some of the online algorithms in Gensim were also published in the 2011 PhD dissertation Scalability of Semantic Analysis in Natural Language Processing of Radim \xc5\x98eh\xc5\xaf\xc5\x99ek, the creator of Gensim.[3]'b'Gensim has been used and cited in over 800 commercial and academic applications, in a diverse array of disciplines from medicine to insurance claim analysis to patent search[4][5] The software has been covered in several new articles, podcasts and interviews since 2009.[6][7][8]'b'The open source code is developed and hosted on GitHub[9] and a public support forum is maintained on Google Groups[10] and Gitter.[11]'b'Gensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme.[12]'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Michael I. Jordan
b'Michael Irwin Jordan is an American scientist, Professor at the University of California, Berkeley and a researcher in machine learning, statistics, and artificial intelligence.[3][4][5]'b''b''b'Jordan received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego.[6] At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s.'b'Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998.[6]'b'In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics.'b'He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference[1] and the popularisation of the expectation-maximization algorithm[7] in machine learning.'b'In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access and pledged support for a new open access journal, the Journal of Machine Learning Research (JMLR), which was created by Leslie Kaelbling to support the evolution of the field of machine learning.[8]'b'Jordan received numerous awards, including a best student paper award [9] (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning."[10]'b'Prof. Jordan is a member of the National Academy of Science, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences.'b'He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009.'b'In 2016, Jordan was identified as the "most influential computer scientist", based on an analysis of the published literature by the Semantic Scholar project.[11]'Journal of Machine Learning Research
b'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Kevin Murphy (Google) and Bernhard Sch\xc3\xb6lkopf (Max Planck Institute for Intelligent Systems).'b''b''b'The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online.[2]'b'Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.[1]'b'In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007[3] and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems.'b''Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'arXiv
b'arXiv (pronounced "archive")[2] is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million article milestone on October 3, 2008,[3][4] and hit a million by the end of 2014.[5][6] By October 2016 the submission rate had grown to more than 10,000 per month.[6][7]'b''b''b'The arXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side.[9] Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993.[6][10] The term e-print was quickly adopted to describe the articles.'b"It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org.[11] It is now hosted principally by Cornell, with eight mirrors around the world.[12]"b'Its existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access[13] and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.'b'The annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions.[14] This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Annual donations were envisaged to vary in size between $2,300 to $4,000, based on each institution\xe2\x80\x99s usage. As of 14\xc2\xa0January\xc2\xa02014[update], 174 institutions have pledged support for the period 2013\xe2\x80\x932017 on this basis, with a projected revenue from this source of approximately $340,000.[15]'b'In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv\'s operation and development. Ginsparg was quoted in the Chronicle of Higher Education as saying it "was supposed to be a three-hour tour, not a life sentence".[16] However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.'b'Although the arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic,[17] or reject submissions that are not scientific papers. The lists of moderators for many sections of the arXiv are publicly available,[18] but moderators for most of the physics sections remain unlisted.'b'Additionally, an "endorsement" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines.[19] Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area.[17] New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.[20]'b'A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston\'s geometrization conjecture, including the Poincar\xc3\xa9 conjecture as a particular case, uploaded by Grigori Perelman in November 2002.[21] Perelman appears content to forgo the traditional peer-reviewed journal process, stating: "If anybody is interested in my way of solving the problem, it\'s all there [on the arXiv]\xc2\xa0\xe2\x80\x93 let them go and read about it".[22] Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.[23]'b'While the arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat\'s last theorem using only high-school mathematics, they are "surprisingly rare".[24][better\xc2\xa0source\xc2\xa0needed] The arXiv generally re-classifies these works, e.g. in "General mathematics", rather than deleting them.[25]'b'Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.'b"The standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's front, a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front.[26] A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides TablearXiv,[27] a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv.[28] A full text and author search engine for arXiv is provided by Scientillion.[29] Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them."b'Files on arXiv can have a number of different copyright statuses:[30]'b'Some authors have voiced concern over the lack of transparency in the arXiv academic peer-review process.[31] Demetris Christopoulos from the National and Kapodistrian University of Athens likens arXiv to a non-declared Journal without a known editor in chief, without a specific written policy regarding submitted papers, and that applies hidden censorship to all papers that do not fall within established scientific dogma. [32]'International Standard Book Number
b'The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.'b'The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).'b'Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]'b'Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.'b''b''b'The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]'b'The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]'b'An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" \xe2\x80\x93 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN\xc2\xa00-340-01381-8; the check digit does not need to be re-calculated.'b'Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]'b'An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):'b'A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]'b'ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[15] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.'b'Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[16] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[17]'b'Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [18]'b'Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Biblioth\xc3\xa8que et Archives nationales du Qu\xc3\xa9bec.'b'Colombia: C\xc3\xa1mara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.'b'Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[19]'b'India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[20] There is no fee associated in getting ISBN in India.[21]'b'Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[22] The original national prefix 978-88 is reserved for publishing companies, starting at \xe2\x82\xac49 for a ten-codes block[23] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of \xe2\x82\xac25 for a single code.'b'Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]'b'Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[24][25][26]'b'Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.'b'New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[27]'b'Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.'b'Philippines: The National Library of the Philippines is responsible for ISBN registrations for Philippine publishers, authors, universities, institutions, and government departments that are responsible for publishing books. As of 2017[update], a fee of \xe2\x82\xb1120.00 per title was charged for the issuance of an ISBN.[28]'b'South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.'b'United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from \xc2\xa3120 (plus VAT) for the smallest block on a standard turnaround of ten days.[29]'b'United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[5] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125 for a single number. Access is immediate when requested via their website.[30]'b'Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.'b" The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[31] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0\xe2\x80\x935, 600\xe2\x80\x93621, 7, 80\xe2\x80\x9394, 950\xe2\x80\x93989, 9926\xe2\x80\x939989, and 99901\xe2\x80\x9399976.[32] Books published in rare languages typically have longer group identifiers.[33]"b'Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[34]'b'The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.'b'The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]'b'A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (\xe2\x82\xac1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[35] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.'b'Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.'b'By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[36] Here are some sample ISBN-10 codes, illustrating block length variations.'b'English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[37]'b'A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without \xe2\x80\x93 the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".'b'The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[38] \xe2\x80\x93 which is the last digit of the ten-digit ISBN \xe2\x80\x93 must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.'b'For example, for an ISBN-10 of 0-306-40615-2:'b'Formally, using modular arithmetic, we can say:'b"It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:"b'Formally, we can say:'b"The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN\xc2\xa0\xe2\x80\x93 the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[39]"b'In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).'b'Each of the first nine digits of the ten-digit ISBN\xe2\x80\x94excluding the check digit itself\xe2\x80\x94is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.'b'For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:'b'Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 \xe2\x80\x93 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)'b'For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:'b'Thus the check digit is 2.'b'It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:'b'The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.'b"The 2005 edition of the International ISBN Agency's official manual[40] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10."b'Formally, using modular arithmetic, we can say:'b'The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.'b'For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:'b'Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.'b'In general, the ISBN-13 check digit is calculated as follows.'b'Let'b'Then'b'This check system\xc2\xa0\xe2\x80\x93 similar to the UPC check digit formula\xc2\xa0\xe2\x80\x93 does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3\xc3\x976+1\xc3\x971 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3\xc3\x971+1\xc3\x976 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.'b'Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).'b'The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.'b'Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[41] For example, ISBN\xc2\xa00-590-76484-5 is shared by two books \xe2\x80\x93 Ninja gaiden\xc2\xae: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.'b'Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[42] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.'b'Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[43]'b'Currently the barcodes on a book\'s back cover (or inside a mass-market paperback book\'s front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[44] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).'b'Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[45] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.'b'Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[46]'b'Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.'Book sources
b'This page allows users to search for multiple sources for a book given the 10- or 13-digit ISBN number. Spaces and dashes in the ISBN number do not matter. In Wikipedia, numbers preceded by "ISBN" link directly to this page.\n'b'This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). \n'b'Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).\n'b'Alabama\n'b'California\n'b'Colorado\n'b'Delaware\n'b'Florida\n'b'Georgia\n'b'Indiana\n'b'Iowa \n'b'Kansas\n'b'Kentucky\n'b'Massachusetts\n'b'Michigan\n'b'Minnesota\n'b'Missouri\n'b'Nebraska\n'b'New Jersey\n'b'New Mexico\n'b'New York\n'b'North Carolina\n'b'Ohio\n'b'Oklahoma\n'b'Oregon\n'b'Pennsylvania\n'b'Rhode Island\n'b'South Carolina\n'b'South Dakota\n'b'Tennessee\n'b'Texas\n'b'Utah\n'b'Washington state\n'b'Wisconsin\n'b'\n'b'Find your book on a site that compiles results from other online sites:\n'b'These sites allow you to search the catalogs of many individual booksellers:\n'b'\n'b'If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below \xe2\x80\x93 they are more likely to have sources appropriate for that language.\n'b'These links produce citations in various referencing styles.\n'b"You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.\n"b'You can also convert between 10 and 13 digit ISBN numbers with these tools:\n'b'\nEdit this page\n'b'\n'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'PubMed Central
b"PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. As one of the major research databases within the suite of resources that have been developed by the National Center for Biotechnology Information (NCBI), PubMed Central is much more than just a document repository. Submissions into PMC undergo an indexing and formatting procedure which results in enhanced metadata, medical ontology, and unique identifiers which all enrich the XML structured data for each article on deposit.[1] Content within PMC can easily be interlinked to many other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to freely discover, read and build upon this portfolio of biomedical knowledge.[2]"b'PubMed Central should not be confused with PubMed. These are two very different services at their core.[3] While PubMed is a searchable database of biomedical citations and abstracts, the full-text article referenced in the PubMed record will physically reside elsewhere. (Sometimes in print, sometimes online, sometimes free, sometimes behind a toll-wall accessible only to paying subscribers). PubMed Central is a free digital archive of articles, accessible to anyone from anywhere via a basic web browser. The full text of all PubMed Central articles is free to read, with varying provisions for reuse.'b'As of December\xc2\xa02016[update], the PMC archive contained over 4.1 million articles,[4] with contributions coming directly from publishers or authors depositing their own manuscripts into the repository per the NIH Public Access Policy. Older data shows that from Jan 2013 \xe2\x80\x93 Jan 2014 author-initiated deposits exceeded 103,000 papers during this 12-month period.[5] PMC also identifies about 4,000 journals which now participate in some capacity to automatically deposit their published content into the PMC repository.[6] Some participating publishers will delay the release of their articles on PubMed Central for a set time after publication, this is often referred to as an "embargo period", and can range from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common). However, PubMed Central is a key example of "systematic external distribution by a third party"[7] which is still prohibited by the contributor agreements of many publishers.'b''b''b'Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[8]'b'A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.'b'The National Library of Medicine "NLM Journal Publishing Tag Set" journal article markup language is freely available.[9] The Association of Learned and Professional Society Publishers comments that "it is likely to become the standard for preparing scholarly content for both books and journals".[10] A related DTD is available for books.[11] The Library of Congress and the British Library have announced support for the NLM DTD.[12] It has also been popular with journal service providers.[13]'b'With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[14] This includes NASA content, with the interface branded as "PubSpace".[15][16]'b'Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).'b'Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[17]'b'Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers\' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come "live" when the resources become available.'b'An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.'b'When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.'b'In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.'b'Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[18] to cautious concern by others.[19] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version-of-record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[20] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[21]'b'The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[22] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[23]'b'The change in procedure has received criticism.[24] The American Physiological Society has expressed reservations about the implementation of the policy.[25]'b'The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of "PMC" followed by a string of seven numbers. The format is:[26]'b'Authors applying for NIH awards must include the PMCID in their application.'PubMed
b'PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.'b'From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]'b'In addition to MEDLINE, PubMed provides access to:'b'Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]'b'Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]'b'As of 11\xc2\xa0July\xc2\xa02017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed\'s records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950s to 16% in 2016.[8] Other significant proportion of records correspond to \xe2\x80\x9cChemistry\xe2\x80\x9d (8.69%), \xe2\x80\x9cTherapy\xe2\x80\x9d (8.39%) and "Infection" (5%).'b'In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]'b"Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window."b"PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms."b'The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:'b'Likewise,'b"A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11] By default the results are sorted by Most Recent, but this changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.[12]"b'For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.[13][14]'b'When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).'b'Publication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.[15]'b'Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]'b'A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the \'Find related data\' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The \'related articles\' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.[18]'b'PubMed automatically links to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes).[19] This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.[19]'b'The PubMed optional facility "My NCBI" (with free registration) provides tools for'b'and a wide range of other options.[20] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[21]'b'LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[22] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March\xc2\xa02010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[23] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.'b'In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[24] In February 2018, PubMed Commons was discontinued due to the fact that "usage has remained minimal".[25][26]'b'PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[27] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[28]'b'askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[29]'b'A PMID (PubMed identifier or PubMed unique identifier)[30] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[31]'b'The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.'b'The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[32] As of October\xc2\xa02008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[33] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.'b'Lu[33] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:'b'As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[33] Without the need to store about 90\xc2\xa0GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[47]'b'Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950s to 16% in 2016.[48]'b'The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.[49]'Digital object identifier
b'In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.'b'A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.'b"The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless."b'The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.'b''b''b'A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.'b'The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]'b'For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).'b'DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.'b'The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.'b'The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]'b'Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL \xe2\x80\x94 providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]'b'The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL \xe2\x80\x93 the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.'b'Major applications of the DOI system currently include:'b"In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]"b'Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc\xe2\x80\xa6.[21]'b'The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.'b'The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]'b'The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.'b'The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.'b'A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]'b'A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]'b'The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).'b"A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name."b'DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.'b'To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.'b'Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.'b"Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]"b'An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.'b'The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.'b'The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.'b'Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]'b'Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.'b'The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation \xe2\x80\x93 Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]'b'DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]'b'The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]'b'The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:'b'URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.'Latent Dirichlet allocation
b"In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2] Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.[3][4]"b''b''b'In LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.[5]'b'For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.'b'Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.'b'With plate notation, the dependencies among the many variables can be captured concisely. The boxes are "plates" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:'b'(Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution.)'b'A formal description of LDA is as follows:'b'We can then mathematically describe the random variables as follows:'b'Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution;[1] alternative inference techniques use Gibbs sampling[6] and expectation propagation.[7]'b'According to the model, the total probability of the model is:'b'Clearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,'b'Thus,'b'Note that the same formula is derived in the article on the Dirichlet-multinomial distribution, as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.'b'Topic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.'b'The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model[9] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA),[10] where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.[11] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.'b'Variations on LDA have been used to automatically put natural images into categories, such as "bedroom" or "forest", by treating an image as a document, and small patches of the image as words;[12] one of the variations is called Spatial Latent Dirichlet Allocation.[13]'