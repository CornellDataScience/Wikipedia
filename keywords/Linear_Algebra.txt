Elementary algebra
A system with a greater number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.When trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express all solutions numerically because there are an infinite number of them if there are any.Systems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system isAnd using this value in the first equation in the system:which has clearly no solution.Multiplying by 2 both sides of the second equation, and adding it to the first one results inAs 0≠2, the second equation in the system has no solution. Therefore, the system has no solution. However, not all inconsistent systems are recognized at first sight. As an example, let us consider the systemIn the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example isUsing this value in one of the equations, the same solution as in the previous method is obtained.which simplifies toAdding 2 on each side of the equation:and multiplying by −1:Another way of solving the same system of linear equations is by substitution.which simplifies toAdding the two equations together to get:Multiplying the terms in the second equation by 2:An example of solving a system of linear equations is by using the elimination method:There are different methods to solve a system of linear equations with two variables.thenFor example, if:from which we obtainwhencethen, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we getFor example, iforwhencethen, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtainhas solutionsAll quadratic equations have exactly two solutions in complex numbers (but they may be equal to each other), a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equationFor this equation, −1 is a root of multiplicity 2. This means −1 appears two times, since the equation can be rewritten in factored form ashas no real number solution since no real number squared equals −1. Sometimes a quadratic equation has a root of multiplicity 2, such as:which is the same thing asQuadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:are solutions of the quadratic equation.where the symbol "±" indicates that bothFor other ways to solve this kind of equations, see below, System of linear equations.In other words, the son is aged 12, and since the father 22 years older, he must be 34. In 10 years time, the son will be 22, and the father will be twice his age, 44. This problem is illustrated on the associated plot of the equations.Now there are two related linear equations, each with two unknowns, which enables the production of a linear equation with just one variable, by subtracting one from the other (called the elimination method):[34]To solve a linear equation with two variables (unknowns), requires two related equations. For example, if it was also revealed that:This cannot be worked out by itself. If the son's age was made known, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.A linear equation with two variables has many (i.e. an infinite number of) solutions.[33] For example:In words: the child is 4 years old.To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable.[32] This problem and its solution are as follows:Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:The following sections lay out examples of some of the types of algebraic equations that may be encountered.Consider if the original fact were stated as "ab=0 implies a=0 or b=0." Then when we say "suppose abc=0," we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if abc=0 then a=0 or b=0 or c=0 if instead of letting a=a and b=bc we substitute a for a and b for bc (and with bc=0, substituting b for a and c for b). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression a into the a term of the original equation, the a substituted does not refer to the a in the statement "ab=0 implies a=0 or b=0."If x and y are integers, rationals, or real numbers, then xy=0 implies x=0 or y=0. Suppose abc=0. Then, substituting a for x and bc for y, we learn a=0 or bc=0. Then we can substitute again, letting x=b and y=c, to show that if bc=0 then b=0 or c=0. Therefore, if abc=0, then a=0 or (b=0 or c=0), so abc=0 implies a=0 or b=0 or c=0.Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,Elementary algebra builds on and extends arithmetic[20] by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers,[1] algebra introduces quantities without fixed values, known as variables.[2] This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
Euclidean space
Another line of generalization is to consider other number fields than one of real numbers. Over complex numbers, a Hilbert space can be seen as a generalization of Euclidean dot product structure, although the definition of the inner product becomes a sesquilinear form for compatibility with metric structure.If one replaces the inner product of a Euclidean space with an indefinite quadratic form, the result is a pseudo-Euclidean space. Smooth manifolds built from such spaces are called pseudo-Riemannian manifolds. Perhaps their most famous application is the theory of relativity, where flat spacetime is a pseudo-Euclidean space called Minkowski space, where rotations correspond to motions of hyperbolic spaces mentioned above. Further generalization to curved spacetimes form pseudo-Riemannian manifolds, such as in general relativity.Also, the concept of a Riemannian manifold permits an expression of the Euclidean structure in any smooth coordinate system, via metric tensor. From this tensor one can compute the Riemann curvature tensor. Where the latter equals to zero, the metric structure is locally Euclidean (it means that at least some open set in the coordinate space is isometric to a piece of Euclidean space), no matter whether coordinates are affine or curvilinear.A smooth manifold is a Hausdorff topological space that is locally diffeomorphic to Euclidean space. Diffeomorphism does not respect distance and angle, but if one additionally prescribes a smoothly varying inner product on the manifold's tangent spaces, then the result is what is called a Riemannian manifold. Put differently, a Riemannian manifold is a space constructed by deforming and patching together Euclidean spaces. Such a space enjoys notions of distance and angle, but they behave in a curved, non-Euclidean manner. The simplest Riemannian manifold, consisting of Rn with a constant inner product, is essentially identical to Euclidean n-space itself. Less trivial examples are n-sphere and hyperbolic spaces. Discovery of the latter in the 19th century was branded as the non-Euclidean geometry.Although Euclidean spaces are no longer considered to be the only possible setting for a geometry, they act as prototypes for other geometric objects. Ideas and terminology from Euclidean geometry (both traditional and analytic) are pervasive in modern mathematics, where other geometric objects share many similarities with Euclidean spaces, share part of their structure, or embed Euclidean spaces.Topographical maps and technical drawings are planar Euclidean. An idea behind them is the scale invariance of Euclidean geometry, that permits to represent large objects in a small sheet of paper, or a screen.Aside from countless uses in fundamental mathematics, a Euclidean model of the physical space can be used to solve many practical problems with sufficient precision. Two usual approaches are a fixed, or stationary reference frame (i.e. the description of a motion of objects as their positions that change continuously with time), and the use of Galilean space-time symmetry (such as in Newtonian mechanics). To both of them the modern Euclidean geometry provides a convenient formalism; for example, the space of Galilean velocities is itself a Euclidean space (see relative velocity for details).Since Euclidean space is a metric space, it is also a topological space with the natural topology induced by the metric. The metric topology on En is called the Euclidean topology, and it is identical to the standard topology on Rn. A set is open if and only if it contains an open ball around each of its points; in other words, open balls form a base of the topology. The topological dimension of the Euclidean n-space equals n, which implies that spaces of different dimension are not homeomorphic. A finer result is the invariance of domain, which proves that any subset of n-space, that is (with its subspace topology) homeomorphic to an open subset of n-space, is itself open.Root systems are special sets of Euclidean vectors. A root system is often identical to the set of vertices of a regular polytope.The concept of a polytope belongs to affine geometry, which is more general than Euclidean. But Euclidean geometry distinguish regular polytopes. For example, affine geometry does not see the difference between an equilateral triangle and a right triangle, but in Euclidean space the former is regular and the latter is not.Polytope is a concept that generalizes polygons on a plane and polyhedra in 3-dimensional space (which are among the earliest studied geometrical objects). A simplex is a generalization of a line segment (1-simplex) and a triangle (2-simplex). A tetrahedron is a 3-simplex.A triangle can be thought of as a 3-gon on a plane, a special (and the first meaningful in Euclidean geometry) case of a polygon.A (non-degenerate) triangle is defined by three points not lying on the same line. Any triangle lies on one plane. The concept of triangle is not specific to Euclidean spaces, but Euclidean triangles have numerous special properties and define many derived objects.This is not only a line which a pair (A, B) of distinct points defines. Points on the line which lie between A and B, together with A and B themselves, constitute a line segment A B. Any line segment has the length, which equals to distance between A and B. If A = B, then the segment is degenerate and its length equals to 0, otherwise the length is positive.Any two distinct points lie on exactly one line. Any line and a point outside it lie on exactly one plane. More generally, the properties of flats and their incidence of Euclidean space are shared with affine geometry, whereas the affine geometry is devoid of distances and angles.The simplest (after points) objects in Euclidean space are flats, or Euclidean subspaces of lesser dimension. Points are 0-dimensional flats, 1-dimensional flats are called (straight) lines, and 2-dimensional flats are planes. (n − 1)-dimensional flats are called hyperplanes.See below about expression of the Euclidean structure in curvilinear coordinates.Another approach, which goes in line with ideas of differential geometry and conformal geometry, is orthogonal coordinates, where coordinate hypersurfaces of different coordinates are orthogonal, although curved. Examples include the polar coordinate system on Euclidean plane, the second important plane coordinate system.Cartesian coordinates are arguably the standard, but not the only possible option for a Euclidean space. Skew coordinates are compatible with the affine structure of En, but make formulae for angles and distances more complicated.The group structure determines which conditions a metric space needs to satisfy to be a Euclidean space:Along with translations, rotations, reflections, as well as the identity transformation, Euclidean motions comprise also glide reflections (for n ≥ 2), screw operations and rotoreflections (for n ≥ 3), and even more complex combinations of primitive transformations for n ≥ 4.The structure of Euclidean spaces – distances, lines, vectors, angles (up to sign), and so on – is invariant under the transformations of their associated Euclidean group. For instance, translations form a commutative subgroup that acts freely and transitively on En, while the stabilizer of any point there is the aforementioned O(n).As the group of all isometries, ISO(n), the Euclidean group is important because it makes Euclidean geometry a case of Klein geometry, a theoretical framework including many alternative geometries.The Euclidean group E(n), also referred to as the group of all isometries ISO(n), treats translations, rotations, and reflections in a uniform way, considering them as group actions in the context of group theory, and especially in Lie group theory. These group actions preserve the Euclidean structure.Among linear transforms in O(n) which reverse the orientation are hyperplane reflections. This is the only possible case for n ≤ 2, but starting from three dimensions, such isometry in the general position is a rotoreflection.Groups SO(n) are well-studied for n ≤ 4. There are no non-trivial rotations in 0- and 1-spaces. Rotations of a Euclidean plane (n = 2) are parametrized by the angle (modulo 1 turn). Rotations of a 3-space are parametrized with axis and angle, whereas a rotation of a 4-space is a superposition of two 2-dimensional rotations around perpendicular planes.But a Euclidean space is orientable.[footnote 2] Each of these transformations either preserves or reverses orientation depending on whether its determinant is +1 or −1 respectively. Only transformations which preserve orientation, which form the special orthogonal group SO(n), are considered (proper) rotations. This group has, as a Lie group, the same dimension n(n − 1) /2 and is the identity component of O(n).where QT is the transpose of Q and I is the identity matrix.Such transforms constitute a group called the orthogonal group O(n). Its elements Q are exactly solutions of a matrix equationSymmetries of a Euclidean space are transformations which preserve the Euclidean metric (called isometries). Although aforementioned translations are most obvious of them, they have the same structure for any affine space and do not show a distinctive character of Euclidean geometry. Another family of symmetries leave one point fixed, which may be seen as the origin without loss of generality. All transformations, which preserves the origin and the Euclidean metric, are linear maps. Such transformations Q must, for any x and y, satisfy:Unlike the aforementioned situation with distance, the scale of angles is the same in pure mathematics, physics, and computing. It does not depend on the scale of distances; all distances may be multiplied by some fixed factor, and all angles will be preserved. Usually, the angle is considered a dimensionless quantity, but there are different units of measurement, such as radian (preferred in pure mathematics and theoretical physics) and degree (°) (preferred in most applications).The angle does not change if vectors x and y are multiplied by positive numbers.where arccos is the arccosine function. It is useful only for n > 1,[footnote 1] and the case n = 2 is somewhat special. Namely, on an oriented Euclidean plane one can define an angle between two vectors as a number defined modulo 1 turn (usually denoted as either 2π or 360°), such that ∠y x = −∠x y. This oriented angle is equal either to the angle θ from the formula above or to −θ. If one non-zero vector is fixed (such as the first basis vector), then each non-zero vector is uniquely defined by its magnitude and angle.The (non-reflex) angle θ (0° ≤ θ ≤ 180°) between vectors x and y is then given byThe metric space structure is the main reason behind the use of real numbers R, not some other ordered field, as the mathematical foundation of Euclidean (and many other) spaces. Euclidean space is a complete metric space, a property which is impossible to achieve operating over rational numbers, for example.This distance function (which makes a metric space) is sufficient to define all Euclidean geometry, including the dot product. Thus, a real coordinate space together with this Euclidean structure is called Euclidean space. Its vectors form an inner product space (in fact a Hilbert space), and a normed vector space.This distance function is called the Euclidean metric. This formula expresses a special case of the Pythagorean theorem.Finally, one can use the norm to define a metric (or distance function) on Rn byThis length function satisfies the required properties of a norm and is called the Euclidean norm on Rn.The inner product of x with itself is always non-negative. This product allows us to define the "length" of a vector x through square root:where xi and yi are ith coordinates of vectors x and y respectively. The result is always a real number.These are distances between points and the angles between lines or vectors, which satisfy certain conditions (see below), which makes a set of points a Euclidean space. The natural way to obtain these quantities is by introducing and using the standard inner product (also known as the dot product) on Rn.[3] The inner product of any two real n-vectors x and y is defined byA Euclidean space is not technically a vector space but rather an affine space, on which a vector space acts by translations, or, conversely, a Euclidean vector is the difference (displacement) in an ordered pair of points, not a single point. Intuitively, the distinction says merely that there is no canonical choice of where the origin should go in the space, because it can be translated anywhere. When a certain point is chosen, it can be declared the origin and subsequent calculations may ignore the difference between a point and its coordinate vector, as said above. See point–vector distinction for details.Once the Euclidean plane has been described in this language, it is actually a simple matter to extend its concept to arbitrary dimensions. For the most part, the vocabulary, formulae, and calculations are not made any more difficult by the presence of more dimensions. (However, rotations are more subtle in high dimensions, and visualizing high-dimensional spaces remains difficult, even for experienced mathematicians.)In order to make all of this mathematically precise, the theory must clearly define the notions of distance, angle, translation, and rotation for a mathematically described space. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a "mathematical" space is a number, not something expressed in inches or metres. The standard way to define such space, as carried out in the remainder of this article, is to define the Euclidean plane as a two-dimensional real vector space equipped with an inner product.[3] The reason for working with arbitrary vector spaces instead of Rn is that it is often preferable to work in a coordinate-free manner (that is, without choosing a preferred basis). For then:One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation about a fixed point in the plane, in which every point in the plane turns about that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).From the modern viewpoint, there is essentially only one Euclidean space of each dimension. While Euclidean space is defined by a set of axioms, these axioms do not specify how the points are to be represented.[2] Euclidean space can, as one possible choice of representation, be modeled using Cartesian coordinates. In this case, the Euclidean space is then modeled by the real coordinate space (Rn) of the same dimension. In one dimension, this is the real line; in two dimensions, it is the Cartesian plane; and in higher dimensions it is a coordinate space with three or more real number coordinates. Mathematicians denote the n-dimensional Euclidean space by En if they wish to emphasize its Euclidean nature, but Rn is used as well since the latter is assumed to have the standard Euclidean structure, and these two structures are not always distinguished. Euclidean spaces have finite dimension.[3]Classical Greek geometry defined the Euclidean plane and Euclidean three-dimensional space using certain postulates, while the other properties of these spaces were deduced as theorems. Geometric constructions are also used to define rational numbers. When algebra and mathematical analysis became developed enough, this relation reversed and now it is more common to define Euclidean space using Cartesian coordinates and the ideas of analytic geometry. It means that points of the space are specified with collections of real numbers, and geometric shapes are defined as equations and inequalities. This approach brings the tools of algebra and calculus to bear on questions of geometry and has the advantage that it generalizes easily to Euclidean spaces of more than three dimensions.In geometry, Euclidean space encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces. It is named after the Ancient Greek mathematician Euclid of Alexandria.[1] The term "Euclidean" distinguishes these spaces from other types of spaces considered in modern geometry. Euclidean spaces also generalize to higher dimensions.
Mathematics
A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. A solution to each of these problems carries a $1 million reward, and only one (the Riemann hypothesis) is duplicated in Hilbert's problems.The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.Arguably the most prestigious award in mathematics is the Fields Medal,[58][59] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[56] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[57]Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments;[55] the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.[c]In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.The study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "P = NP?" problem, one of the Millennium Prize Problems.[54] Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science,[citation needed] as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.[53] Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.Axioms in traditional thought were "self-evident truths", but that conception is problematic.[51] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[52]Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject.[b] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.[50]Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".Most of the mathematical notation in use today was not invented until the 16th century.[45] Before that, mathematics was written out in words, limiting mathematical discovery.[46] Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.[47] Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.[48] Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.[49]For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[42] Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erdős.[43][44] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics".[40] As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages.[41] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[39]The opinions of mathematicians on this matter are varied. Many mathematicians[38] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others[who?] feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.[citation needed]An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper.[34] However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians[who?] that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."[35] Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.[36][37]The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences".[12] More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery".[33] In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to natural science follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[15]Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems".[32] A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.Intuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other."[29] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870).[30] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).[31]Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[29] All have severe problems, none has widespread acceptance, and no reconciliation seems possible.[29]Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century.[27] Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[28] Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals.[6] There is not even consensus on whether mathematics is an art or a science.[7] A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[6] Some just say, "Mathematics is what mathematicians do."[6]The apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural τα μαθηματικά (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.[25] In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in English-speaking North America, math.[26]In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.[24]Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "teachers" rather than "mathematicians" in the modern sense.The word mathematics comes from Ancient Greek μάθημα (máthēma), meaning "that which is learnt",[22] "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times.[23] Its adjective is μαθηματικός (mathēmatikós), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, μαθηματικὴ τέχνη (mathēmatikḗ tékhnē), Latin: ars mathematica, meant "the mathematical art".Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[21]During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics: most of them include the contributions from Persian mathematicians such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.Between 600 and 300 BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics.[20]In Babylonian mathematics, elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus.[citation needed]Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[19] The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time.As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.[18]The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals,[17] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.[16]Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth."[11] Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences".[12] Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions".[13] David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise."[14] Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[15]Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[10]Mathematicians seek out patterns[8][9] and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.Mathematics (from Greek μάθημα máthēma, "knowledge, study, learning") is the study of such topics as quantity,[1] structure,[2] space,[1] and change.[3][4][5] It has no generally accepted definition.[6][7]
Linear equation
If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n – 1)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).In other words, if ai ≠ 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 ≠ 0, and rewrite the equationIf all the coefficients are zero, then either b ≠ 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewrittenAn everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point–slope form or slope–intercept form.where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.andA linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.Ergo,Thus,One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.andThese are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:andSince this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.becomes:Further, this representation extends to systems of linear equations.one can rewrite the equation in matrix form:Using the order of the standard formwhere a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.Using a determinant, one gets a determinant form, easy to remember:Expanding the products and regrouping the terms leads to the general form:Multiplying both sides of this equation by (x2 − x1) yields a form of the line generally referred to as the symmetric form:where (x1, y1) and (x2, y2) are two points on the line with x2 ≠ x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2 − y1)/(x2 − x1).The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y − y1) is proportional to the difference in the x coordinate (that is, x − x1). The proportionality constant is m (the slope of the line).where m is the slope of the line and (x1,y1) is any point on the line.where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.where A and B are not both equal to zero. The equation is usually written so that A ≥ 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is −A/B. The general form is sometimes written as:In the general (or standard[1]) form the linear equation is written as:Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and θ are variables; other letters represent constants (fixed numbers).Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.A common form of a linear equation in the two variables x and y isIf a = 0, then, if b = 0, every number is a solution of the equation, and, if b ≠ 0, there are no solutions (and the equation is said to be inconsistent).If a ≠ 0, there is a unique solutionA linear equation in one unknown x may always be rewrittenThis article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a ≠ 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a ≠ 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).
Linear map
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank–nullity theorem:If f : V → W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n × n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol ∗ denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f : V → W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.
Matrix (mathematics)
Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toA number λ and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n×n-matrix A is called positive-definite if for all nonzero vectors x ∈ Rn the associated quadratic form given bywhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2×2 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generallywhere 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:
Vector space
The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.generalizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x ∈ W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v ∈ V. An important example is the space of solutions of a system of inhomogeneous linear equationsRoughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapModules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[103] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 × R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.such that for every x in X, the fiber π−1(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X × V → X makes the product X × V into a "trivial" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of π to π−1(U) is isomorphic[nb 16] to the trivial bundle U × V → U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X × V). For example, the Möbius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 × R, because the latter is orientable whereas the former is not.[99]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous mapRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).[92][93]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2π), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isWhen Ω = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function f its value at the p: δ(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).[77]A distribution (or generalized function) is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain Ω:Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]When a field, F is explicitly stated, a common term used is F-algebra.The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 ⊗ v2 and v2 ⊗ v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 ⊗ v2 = − v2 ⊗ v1 yields the exterior algebra.[73]The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsExamples include the vector space of n-by-n matrices, with [x, y] = xy − yx, the commutator of two matrices, and R3, endowed with the cross product.Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone–Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61] The Hilbert space L2(Ω), with inner product given byImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]there exists a function f(x) belonging to the vector space Lp(Ω) such thatThe space of integrable functions on a given domain Ω (for example an interval) satisfying |f|p < ∞, and equipped with this norm are called Lebesgue spaces, denoted Lp(Ω).[nb 10] These spaces are complete.[59] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < ∞, satisfying the conditionMore generally than sequences of real numbers, functions f: Ω → R are endowed with a norm that replaces the above sum by the Lebesgue integralis finite. The topologies on the infinite-dimensional space ℓ p are inequivalent for different p. E.g. the sequence of vectors xn = (2−n, 2−n, ..., 2−n, 0, 0, ...), i.e. the first 2n components are 2−n, the following ones are 0, converges to the zero vector for p = ∞, but does not for p = 1:Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] A first example is the vector space ℓ p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 ≤ p ≤ ∞) given byFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V → W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V∗ consists of continuous functionals V → R (or to C). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and ∞-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifdenotes the limit of the corresponding finite partial sums of the sequence (fi)i∈N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.In such topological vector spaces one can consider series of vectors. The infinite sumConvergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Coordinate space Fn can be equipped with the standard dot product:where f+ denotes the positive part of f and f− the negative part.[48]A vector space may be given a partial order ≤, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionsFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.These rules ensure that the map f from the V × W to V ⊗ W that maps a tuple (v, w) to v ⊗ w is bilinear. The universality states that given any vector space X and any bilinear map g : V × W → X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v ⊗ w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.subject to the rulesThe tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorsThe tensor product V ⊗F W, or simply V ⊗ W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g : V × W → X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v ↦ g(v, w) is linear in the sense above and likewise for fixed v.The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.the derivatives of the function f appear linearly (as opposed to f′′(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)′ = f′ + g ′ and (c·f)′ = c·f′ for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).In the corresponding mapAn important example is the kernel of a linear map x ↦ Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.The kernel ker(f) of a linear map f : V → W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v) : v ∈ V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W ⊂ V, the quotient space V/W ("V modulo W") is defined as follows: as a set, it consists of v + W = {v + w : w ∈ W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a · (v + W) = (a · v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space "forgets" information that is contained in the subspace W.A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n – 1.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in λ, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.Endomorphisms, linear maps f : V → V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying λv = f(v), where λ is a scalar, is called an eigenvector of f with eigenvalue λ.[nb 5][35] Equivalently, v is an element of the kernel of the difference f − λ · Id (where Id is the identity map V → V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue λ is equivalent toThe determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Moreover, after choosing bases of V and W, any linear map f : V → W is uniquely represented by a matrix via this assignment.[33]or, using the matrix multiplication of the matrix A with the coordinate vector x:Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingOnce a basis of V is chosen, linear maps f : V → W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no "canonical" or preferred isomorphism; actually an isomorphism φ : Fn → V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via φ. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.Linear maps V → W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V∗.[28] Via the injective natural map V → V∗∗, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.An isomorphism is a linear map f : V → W such that there exists an inverse map g : W → V, which is a map such that the two possible compositions f ∘ g : W → W and g ∘ f : V → V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(α) over Q depends on α. If α satisfies some polynomial equation The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e−x and xe−x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i ∈ I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:yields f(x) = a e−x + bx e−x, where a and b are arbitrary constants, and ex is the natural exponential function.are given by triples with arbitrary a, b = a/2, and c = −5a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelySystems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions ofand similarly for multiplication. Such function spaces occur in many geometric situations, when Ω is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Functions from any fixed set Ω to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byIn fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c ⋅ (x + iy) = (c ⋅ x) + i(c ⋅ y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such asAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by Möbius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse −v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.Subtraction of two vectors and division by a (non-zero) scalar can be defined asLikewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:To qualify as a vector space, the set V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.Elements of V are commonly called vectors. Elements of F are commonly called scalars.A vector space over a field F is a set V together with two operations that satisfy the eight axioms listed below.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.andA second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (−1)v = −v has the opposite direction and the same length as v (blue vector pointing down in the right image).The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The concept of vector space will first be explained by describing two particular examples:Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.
Geometry
Leonhard Euler, in studying problems like the Seven Bridges of Königsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry geometria situs (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots.Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.While the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).[53]An important area of application is number theory. In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem.Geometry has also had a large effect on other areas of mathematics. For instance, the introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.Modern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.Mathematics and architecture are related, since, as with other arts, architects use mathematics for several reasons. Apart from the mathematics needed when engineering buildings, architects use geometry: to define the spatial form of a building; from the Pythagoreans of the sixth century BC onwards, to create forms considered harmonious, and thus to lay out buildings and their surroundings according to mathematical, aesthetic and sometimes religious principles; to decorate buildings with mathematical objects such as tessellations; and to meet environmental goals, such as to minimise wind speeds around the bases of tall buildings.Mathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.Geometry has found applications in many fields, some of which are described below.The study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 ("algebraic threefolds"), has been far advanced. Gröbner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane theory.The field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.[citation needed][dubious – discuss]Differential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is intrinsic, meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not a priori parts of some ambient flat Euclidean space.Euclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.In the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, absolute, geometry, which is known to be true a priori by an inner faculty of mind: Euclidean geometry was synthetic a priori.[51] This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed by Riemann in his 1867 inauguration lecture Über die Hypothesen, welche der Geometrie zu Grunde liegen (On the hypotheses on which geometry is based),[52] published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry.A different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.The theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M. C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.The issue of dimension still matters to geometry, in the absence of complete answers to classic questions. Dimensions 3 of space and 4 of space-time are special cases in geometric topology. Dimension 10 or 11 is a key number in string theory. Research may bring a satisfactory geometric reason for the significance of 10 and 11 dimensions.Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. Dimension has gone through stages of being any natural number n, possibly infinite with the introduction of Hilbert space, and any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses definitions; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything a priori.Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.A topology is a mathematical structure on a set that tells how elements of the set relate spatially to each other.[37] The best-known examples of topologies come from metrics, which are ways of measuring distances between points.[49] For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.[50]Manifolds are used extensively in physics, including in general relativity and string theory[48]A manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space.[37] In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.[45]A surface is a two-dimensional object, such as a sphere or paraboloid.[47] In differential geometry[45] and topology,[37] surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.[46]In topology, a curve is defined by a function from an interval of the real numbers to another space.[37] In differential geometry, the same definition is used, but the defining function is required to be differentiable [45] Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.[46]A curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.[44]In differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.[42][43]In Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right.[31] The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.[41]Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[31] In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[40]A plane is a flat, two-dimensional surface that extends infinitely far.[31] Planes are used in every area of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles;[37] it can be studied as an affine space, where collinearity and ratios can be studied but not distances;[38] it can be studied as the complex plane using techniques of complex analysis;[39] and so on.Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself".[31] In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation,[34] but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[35] In differential geometry, a geodesic is a generalization of the notion of a line to curved spaces.[36]Points are considered fundamental objects in Euclidean geometry. They have been defined in a variety of ways, including Euclid's definition as 'that which has no part'[31] and through the use of algebra or nested sets.[32] In many areas of geometry, such as analytic geometry, differential geometry, and topology, all objects are considered to be built up from points. However, there has been some study of geometry without reference to points.[33]Euclid took an abstract approach to geometry in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860), Carl Friedrich Gauss (1777–1855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.The following are some of the most important concepts in geometry.[6][7]Two developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry.[26][27] Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[28] Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry.[4] Omar Khayyám (1048–1131) found geometric solutions to cubic equations.[29] The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c. 1230–c. 1314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.[30]Indian mathematicians also made many important contributions in geometry. The Satapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.[3] According to (Hayashi 2005, p. 363), the Śulba Sūtras contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples,[22] which are particular cases of Diophantine equations.[23] In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero."[24] Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes. Brahmagupta wrote his astronomical work Brāhma Sphuṭa Siddhānta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[25] In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).[25]In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.[1] Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,[14] though the statement of the theorem has a long history.[15][16] Eudoxus (408–c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[17] as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time,[18] introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[19] The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[20] Archimedes (c. 287–212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi.[21] He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC.[8][9] Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000–1800 BC) and Moscow Papyrus (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum.[10] Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries.[11] South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.[12][13]Contemporary geometry has many subfields:Geometry has applications to many fields, including art, architecture, physics, as well as to other branches of mathematics.While geometry has evolved significantly throughout the years, there are some general concepts that are more or less fundamental to geometry. These include the concepts of points, lines, planes, surfaces, angles, and curves, as well as the more advanced notions of manifolds and topology or metric.[6]Geometry arose independently in a number of early cultures as a practical way for dealing with lengths, areas, and volumes. Geometry began to see elements of formal mathematical science emerging in the West as early as the 6th century BC.[1] By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment, Euclid's Elements, set a standard for many centuries to follow.[2] Geometry arose independently in India, with texts providing rules for geometric constructions appearing as early as the 3rd century BC.[3] Islamic scientists preserved Greek ideas and expanded on them during the Middle Ages.[4] By the early 17th century, geometry had been put on a solid analytic footing by mathematicians such as René Descartes and Pierre de Fermat. Since then, and into modern times, geometry has expanded into non-Euclidean geometry and manifolds, describing spaces that lie beyond the normal range of human experience.[5]Geometry (from the Ancient Greek: γεωμετρία; geo- "earth", -metron "measurement") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer.
Line (geometry)
The "shortness" and "straightness" of a line, interpreted as the property that the distance along the line between any two of its points is minimized (see triangle inequality), can be generalized and leads to the concept of geodesics in metric spaces.A line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these, if they are coplanar and either do not intersect or are collinear.In topology, a ray in a space X is a continuous embedding R+ → X. It is used to define the important concept of end of the space.The definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.In Euclidean geometry two rays with a common endpoint form an angle.Thus, we would say that two different points, A and B, define a line and a decomposition of this line into the disjoint union of an open segment (A, B) and two rays, BC and AD (the point D is not drawn in the diagram, but is to the left of A on the line AB). These are not opposite rays since they have different initial points.Given distinct points A and B, they determine a unique ray with initial point A. As two points define a unique line, this ray consists of all the points between A and B (including A and B) and all the points C on the line through A and B such that B is between A and C.[15] This is, at times, also expressed as the set of all points C such that A is not between B and C.[16] A point D, on the line determined by A and B but not in the ray with initial point A determined by B, will determine another ray with initial point A. With respect to the AB ray, the AD ray is called the opposite ray.Given a line and any point A on it, we may consider A as decomposing this line into two parts. Each such part is called a ray (or half-line) and the point A is called its initial point. The point A is considered to be a member of the ray.[14] Intuitively, a ray consists of those points on a line passing through A and proceeding indefinitely, starting at A, in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.In many models of projective geometry, the representation of a line rarely conforms to the notion of the "straight curve" as it is visualised in Euclidean geometry. In elliptic geometry we see a typical example of this.[13] In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.In three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.Perpendicular lines are lines that intersect at right angles.Parallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other—every point that is on either one of them is also on the other.For a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.For a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.With respect to triangles we have:For more general algebraic curves, lines could also be:In the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.In a sense,[12] all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:In the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.However, there are other notions of distance (such as the Manhattan distance) for which this property is not true.In Euclidean geometry, the Euclidean distance d(a,b) between two points a and b may be used to express the collinearity between three points by:[10][11]Equivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, k points in a plane are collinear if and only if any (k–1) pairs of points have the same pairwise slopes.has a rank less than 3. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.In affine coordinates, in n-dimensional space the points X=(x1, x2, ..., xn), Y=(y1, y2, ..., yn), and Z=(z1, z2, ..., zn) are collinear if the matrixThree points are said to be collinear if they lie on the same line. Three points usually determine a plane, but in the case of three collinear points this does not happen.The direction of the line is from a (t = 0) to b (t = 1), or in other words, in the direction of the vector b − a. Different choices of a and b can yield the same line.In more general Euclidean space, Rn (and analogously in every other affine space), the line L passing through two different points a and b (considered as vectors) is the subsetIn three-dimensional space, a first degree equation in the variables x, y, and z defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in n-dimensional space n-1 first-degree equations in the n coordinate variables define a line under suitable conditions.A ray starting at point A is described by limiting λ. One ray is obtained if λ ≥ 0, and the opposite ray comes from λ ≤ 0.where m is the slope of the line.The equation of a line which passes through the pole is simply given as:Similarly, a horizontal line that doesn't pass through the pole is given by the equationIn polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,where m is the slope of the line and b is the y-intercept. When θ = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:In polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:Unlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, θ and p, to be specified. If p > 0, then θ is uniquely defined modulo 2π. On the other hand, if the line is through the origin (c = 0, p = 0), one drops the c/|c| term to compute sinθ and cosθ, and θ is only defined modulo π.The normal form (also called the Hesse normal form,[9] after the German mathematician Ludwig Otto Hesse), is based on the normal segment for a given line, which is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:They may also be described as the simultaneous solutions of two linear equationswhere:In three dimensions, lines can not be described by a single linear equation, so they are frequently described by parametric equations:orIf x0 ≠ x1, this equation may be rewritten asThere are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.with fixed real coefficients a, b and c such that a and b are not both zero. Using this form, vertical lines correspond to the equations with b = 0.where:
In two dimensions, the equation for non-vertical lines is often given in the slope-intercept form:Lines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations.Any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.In an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians),[7] a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point.[8] In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.When geometry was first formalised by Euclid in the Elements, he defined a general line (straight or curved) to be "breadthless length" with a straight line being a line "which lies evenly with the points on itself".[5] These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms,[6] but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.In a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a description or mental image of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The "definition" of line in Euclid's Elements falls into this category.[4] Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.All definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which cannot be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition.[2] In geometry, it is frequently the case that the concept of line is taken as a primitive.[3] In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.When a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself"; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the 19th century (such as non-Euclidean, projective and affine geometry).The notion of line or straight line was introduced by ancient mathematicians to represent straight objects (i.e., having no curvature) with negligible width and depth. Lines are an idealization of such objects. Until the 17th century, lines were defined in this manner: "The [straight or curved] line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which […] will leave from its imaginary moving some vestige in length, exempt of any width. […] The straight line is that which is equally extended between its points."[1]
Plane (geometry)
The plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.The one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the "north pole" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.Alternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)In addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere on the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point). This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature.In the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.In the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.Differential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.The plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.At one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.In addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.whereAnother vector form for the equation of a plane, known as the Hesse normal form relies on the parameter D. This form is:[5]and the point r0 can be taken to be any of the given points p1,p2 or p3[6] (or any other point in the plane).This plane can also be described by the "point and a normal vector" prescription above. A suitable normal vector is given by the cross productThese equations are parametric in d. Setting d equal to any non-zero number and substituting it into these equations will yield one solution set.If D is non-zero (so for planes not through the origin) the values for a, b and c can be calculated as follows:This system can be solved using Cramer's rule and basic matrix manipulations. LetThe plane passing through p1, p2, and p3 can be described as the set of all points (x,y,z) that satisfy the following determinant equations:Let p1=(x1, y1, z1), p2=(x2, y2, z2), and p3=(x3, y3, z3) be non-collinear points.where s and t range over all real numbers, v and w are given linearly independent vectors defining the plane, and r0 is the vector representing the position of an arbitrary (but fixed) point on the plane. The vectors v and w can be visualized as vectors starting at r0 and pointing in different directions along the plane. Note that v and w can be perpendicular, but cannot be parallel.Alternatively, a plane may be described parametrically as the set of all points of the formThus for example a regression equation of the form y = d + ax + cz (with b = −1) establishes a best-fit plane in three-dimensional space when there are two explanatory variables.is a plane having the vector n = (a, b, c) as a normal.[4] This familiar equation for a plane is called the general form of the equation of the plane.[5]Conversely, it is easily shown that if a, b, c and d are constants and a, b, and c are not all zero, then the graph of the equationwherewhich is the point-normal form of the equation of a plane.[3] This is just a linear equation(The dot here means a dot (scalar) product.) Expanded this becomesSpecifically, let r0 be the position vector of some point P0 = (x0, y0, z0), and let n = (a, b, c) be a nonzero vector. The plane determined by the point P0 and the vector n consists of those points P, with position vector r, such that the vector drawn from P0 to P is perpendicular to n. Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points r such thatIn a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its "inclination".The following statements hold in three-dimensional Euclidean space but not in higher dimensions, though they have higher-dimensional analogues:In a Euclidean space of any number of dimensions, a plane is uniquely determined by any of the following:This section is solely concerned with planes embedded in three dimensions: specifically, in R3.A plane is a ruled surface.Euclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry.[1] He selected a small core of undefined terms (called common notions) and postulates (or axioms) which he then used to prove various geometrical statements. Although the plane in its modern sense is not directly given a definition anywhere in the Elements, it may be thought of as part of the common notions.[2] Euclid never used numbers to measure length, angle, or area. In this way the Euclidean plane is not quite the same as the Cartesian plane.When working exclusively in two-dimensional Euclidean space, the definite article is used, so, the plane refers to the whole space. Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, or, in other words, in the plane.In mathematics, a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. Planes can arise as subspaces of some higher-dimensional space, as with a room's walls extended infinitely far, or they may enjoy an independent existence in their own right, as in the setting of Euclidean geometry.
Rotation (mathematics)
The complex-valued matrices analogous to real orthogonal matrices are the unitary matrices. The set of all unitary matrices in a given dimension n forms a unitary group U(n) of degree n; and its subgroup representing proper rotations[clarification needed] is the special unitary group SU(n) of degree n. These complex rotations are important in the context of spinors. The elements of SU(2) are used to parametrize three-dimensional Euclidean rotations (see above), as well as respective transformations of the spin (see representation theory of SU(2)).As was stated above, Euclidean rotations are applied to rigid body dynamics. Moreover, most of mathematical formalism in physics (such as the vector calculus) is rotation-invariant; see rotation for more physical aspects. Euclidean rotations and, more generally, Lorentz symmetry described above are thought to be symmetry laws of nature. In contrast, the reflectional symmetry is not a precise symmetry law of nature.Rotations define important classes of symmetry: rotational symmetry is an invariance with respect to a particular rotation. The circular symmetry is an invariance with respect to all rotation about the fixed axis.Whereas SO(3) rotations, in physics and astronomy, correspond to rotations of celestial sphere as a 2-sphere in the Euclidean 3-space, Lorentz transformations from SO(3;1)+ induce conformal transformations of the celestial sphere. It is a broader class of the sphere transformations known as Möbius transformations.If a rotation is only in the three space dimensions, i.e. in a plane that is entirely in space, then this rotation is the same as a spatial rotation in three dimensions. But a rotation in a plane spanned by a space dimension and a time dimension is a hyperbolic rotation, a transformation between two different reference frames, which is sometimes called a "Lorentz boost". These transformations demonstrate the pseudo-Euclidean nature of the Minkowski space. They are sometimes described as squeeze mappings and frequently appear on Minkowski diagrams which visualize (1 + 1)-dimensional pseudo-Euclidean geometry on planar drawings. The study of relativity is concerned with the Lorentz group generated by the space rotations and hyperbolic rotations.[2]One application of this is special relativity, as it can be considered to operate in a four-dimensional space, spacetime, spanned by three space dimensions and one of time. In special relativity this space is linear and the four-dimensional rotations, called Lorentz transformations, have practical physical interpretations. The Minkowski space is not a metric space, and the term isometry is inapplicable to Lorentz transformation.Affine geometry and projective geometry have not a distinct notion of rotation.In spherical geometry, a direct motion of the n-sphere (an example of the elliptic geometry) is the same as a rotation of (n + 1)-dimensional Euclidean space about the origin (SO(n + 1)). For odd n, most of these motions do not have fixed points on the n-sphere and, strictly speaking, are not rotations of the sphere; such motions are sometimes referred to as Clifford translations. Rotations about a fixed point in elliptic and hyperbolic geometries are not different from Euclidean ones.The doubly covering group of SO(n) is known as the Spin group, Spin(n). It can be conveniently described in terms of Clifford algebra. Unit quaternions present the group Spin(3).In general (and not necessarily for Euclidean vectors) the rotation of a vector space equipped with a quadratic form can be expressed as a bivector. This formalism is used in geometric algebra and, more generally, in the Clifford algebra representation of Lie groups.As was demonstrated above, there exist three multilinear algebra rotation formalisms: one of U(1), or complex numbers, for two dimensions, and yet two of versors, or quaternions, for three and four dimensions.The main disadvantage of matrices is that they are more expensive to calculate and do calculations with. Also in calculations where numerical instability is a concern matrices can be more prone to it, so calculations to restore orthonormality, which are expensive to do for matrices, need to be done more often.Matrices are often used for doing transformations, especially when a large number of points are being transformed, as they are a direct representation of the linear operator. Rotations represented in other ways are often converted to matrices before being used. They can be extended to represent rotations and transformations at the same time using homogeneous coordinates. Projective transformations are represented by 4×4 matrices. They are not rotation matrices, but a transformation that represents a Euclidean rotation has a 3×3 rotation matrix in the upper left corner.More generally, coordinate rotations in any dimension are represented by orthogonal matrices. The set of all orthogonal matrices in n dimensions which describe proper rotations (determinant = +1), together with the operation of matrix multiplication, forms the special orthogonal group SO(n).A single multiplication by a versor, either left or right, is itself a rotation, but in four dimensions. Any four-dimensional rotation about the origin can be represented with two quaternion multiplications: one left and one right, by two different unit quaternions.where v is the rotation vector treated as a quaternion.where q is the versor, q−1 is its inverse, and x is the vector treated as a quaternion with zero scalar part. The quaternion can be related to the rotation vector form of the axis angle rotation by the exponential map over the quaternions,A versor (also called a rotation quaternion) consists of four real numbers, constrained so the norm of the quaternion is 1. This constraint limits the degrees of freedom of the quaternion to three, as required. Unlike matrices and complex numbers two multiplications are needed:Unit quaternions, or versors, are in some ways the least intuitive representation of three-dimensional rotations. They are not the three-dimensional instance of a general approach. They are more compact than matrices and easier to work with than all other methods, so are often preferred in real-world applications.[citation needed]Another possibility to represent a rotation of three-dimensional Euclidean vectors are quaternions described below.Above-mentioned Euler angles and axis–angle representations can be easily converted to a rotation matrix.The set of all appropriate matrices together with the operation of matrix multiplication is the rotation group SO(3). The matrix A is a member of the three-dimensional special orthogonal group, SO(3), that is it is an orthogonal matrix with determinant 1. That it is an orthogonal matrix means that its rows are a set of orthogonal unit vectors (so they are an orthonormal basis) as are its columns, making it simple to spot and check if a matrix is a valid rotation matrix.This is multiplied by a vector representing the point to give the resultAs in two dimensions, a matrix can be used to rotate a point (x, y, z) to a point (x′, y′, z′). The matrix used is a 3×3 matrix,Since complex numbers form a commutative ring, vector rotations in two dimensions are commutative, unlike in higher dimensions. They have only one degree of freedom, as such rotations are entirely determined by the angle of rotation.[1]and equating real and imaginary parts gives the same result as a two-dimensional matrix:This can be rotated through an angle θ by multiplying it by eiθ, then expanding the product using Euler's formula as follows: Points on the R2 plane can be also presented as complex numbers: the point (x, y) in the plane is represented by the complex numberThe coordinates of the point after rotation are x′, y′, and the formulae for x′ and y′ areIn two dimensions, to carry out a rotation using a matrix, the point (x, y) to be rotated counterclockwise is written as a column vector, then multiplied by a rotation matrix calculated from the angle θ:As it was already stated, a (proper) rotation is different from an arbitrary fixed-point motion in its preservation of the orientation of the vector space. Thus, the determinant of a rotation orthogonal matrix must be 1. The only other possibility for the determinant of an orthogonal matrix is −1, and this result means the transformation is a hyperplane reflection, a point reflection (for odd n), or another kind of improper rotation. Matrices of all proper rotations form the special orthogonal group.A motion that preserves the origin is the same as a linear operator on vectors that preserves the same geometric structure but expressed in terms of vectors. For Euclidean vectors, this expression is their magnitude (Euclidean norm). In components, such operator is expressed with n × n orthogonal matrix that is multiplied to column vectors.When one considers motions of the Euclidean space that preserve the origin, the distinction between points and vectors, important in pure mathematics, can be erased because there is a canonical one-to-one correspondence between points and position vectors. The same is true for geometries other than Euclidean, but whose space is an affine space with a supplementary structure; see an example below. Alternatively, the vector description of rotations can be understood as a parametrization of geometric rotations up to their composition with translations. In other words, one vector rotation presents many equivalent rotations about all points in the space.A general rotation in four dimensions has only one fixed point, the centre of rotation, and no axis of rotation; see rotations in 4-dimensional Euclidean space for details. Instead the rotation has two mutually orthogonal planes of rotation, each of which is fixed in the sense that points in each plane stay within the planes. The rotation has two angles of rotation, one for each plane of rotation, through which points in the planes rotate. If these are ω1 and ω2 then all points not in the planes rotate through an angle between ω1 and ω2. Rotations in four dimensions about a fixed point have six degrees of freedom. A four-dimensional direct motion in general position is a rotation about certain point (as in all even Euclidean dimensions), but screw operations exist also.A three-dimensional rotation can be specified in a number of ways. The most usual methods are:Rotations in three-dimensional space differ from those in two dimensions in a number of important ways. Rotations in three dimensions are generally not commutative, so the order in which rotations are applied is important even about the same point. Also, unlike the two-dimensional case, a three-dimensional direct motion, in general position, is not a rotation but a screw operation. Rotations about the origin have three degrees of freedom (see rotation formalisms in three dimensions for details), the same as the number of dimensions.There are no non-trivial rotations in one dimension. In two dimensions, only a single angle is needed to specify a rotation about the origin – the angle of rotation that specifies an element of the circle group (also known as U(1)). The rotation is acting to rotate an object counterclockwise through an angle θ about the origin; see below for details. Composition of rotations sums their angles modulo 1 turn, which implies that all two-dimensional rotations about the same point commute. Rotations about different points, in general, do not commute. Any two-dimensional direct motion is either a translation or a rotation; see Euclidean plane isometry for details.A motion of a Euclidean space is the same as its isometry: it leaves the distance between any two points unchanged after the transformation. But a (proper) rotation also has to preserve the orientation structure. The "improper rotation" term refers to isometries that reverse (flip) the orientation. In the language of group theory the distinction is expressed as direct vs indirect isometries in the Euclidean group, where the former comprise the identity component. Any direct Euclidean motion can be represented as a composition of a rotation about the fixed point and a translation.Rotations of (affine) spaces of points and of respective vector spaces are not always clearly distinguished. The former are sometimes referred to as affine rotations (although the term is misleading), whereas the latter are vector rotations. See the article below for details.A representation of rotations is a particular formalism, either algebraic or geometric, used to parametrize a rotation map. This meaning is somehow inverse to the meaning in the group theory.For a particular rotation:The rotation group is a Lie group of rotations about a fixed point. This (common) fixed point is called the center of rotation and is usually identified with the origin. The rotation group is a point stabilizer in a broader group of (orientation-preserving) motions.Mathematically, a rotation is a map. All rotations about a fixed point form a group under composition called the rotation group (of a particular space). But in mechanics and, more generally, in physics, this concept is frequently understood as a coordinate transformation (importantly, a transformation of an orthonormal basis), because for any motion of a body there is an inverse transformation which if applied to the frame of reference results in the body being at the same coordinates. For example, in two dimensions rotating a body clockwise about a point keeping the axes fixed is equivalent to rotating the axes counterclockwise about the same point while the body is kept fixed. These two types of rotation are called active and passive transformations.Rotation in mathematics is a concept originating in geometry. Any rotation is a motion of a certain space that preserves at least one point. It can describe, for example, the motion of a rigid body around a fixed point. A rotation is different from other types of motions: translations, which have no fixed points, and (hyperplane) reflections, each of them having an entire (n − 1)-dimensional flat of fixed points in a n-dimensional space. A clockwise rotation is a negative magnitude so a counterclockwise turn has a positive magnitude.
Functional analysis
Functional analysis in its present form[update] includes the following tendencies:Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn–Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice.List of functional analysis topics.The closed graph theorem states the following: If X is a topological space and Y is a compact Hausdorff space, then the graph of a linear map T from X to Y is closed if and only if T is continuous.[3]The proof uses the Baire category theorem, and completeness of both X and Y is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if X and Y are taken to be Fréchet spaces.The open mapping theorem, also known as the Banach–Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:[2]then there exists a linear extension ψ : V → R of φ to the whole space V, i.e., there exists a linear functional ψ such thatHahn–Banach theorem:[2] If p : V → R is a sublinear function, and φ : U → R is a linear functional on a linear subspace U ⊆ V which is dominated by p on U, i.e.The Hahn–Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are "enough" continuous linear functionals defined on every normed vector space to make the study of the dual space "interesting".This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.where T is the multiplication operator:Theorem:[1] Let A be a bounded self-adjoint operator on a Hilbert space H. Then there is a measure space (X, Σ, μ) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H → L2μ(X) such thatThere are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let A be the operator of multiplication by t on L2[0, 1], that isThe theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.The uniform boundedness principle or Banach–Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn–Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.Important results of functional analysis include:Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fréchet derivative article.In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.More generally, functional analysis includes the study of Fréchet spaces and other topological vector spaces not endowed with a norm.The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra. The theory of nonlinear functionals was continued by students of Hadamard, in particular Fréchet and Lévy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.
Engineering
In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term.Business Engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or "Management engineering" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or Business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical & electronics, power distribution & generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[52][67]The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[64] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[65] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[61][66]There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).[61][62][63]Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[57]The heart for example functions much like a pump,[58] the skeleton is like a linked structure with levers,[59] the brain produces electrical signals etc.[60] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[57]Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[55][56]Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[53][54] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[citation needed]As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:An example of this is the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of Miner's rule to calculate fatigue damage. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[citation needed]There is a "real and important" difference between engineering and physics as similar to any science field has to do with technology.[42][43] Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology.[44][45][46] For technology, physics is an auxiliary and in a way technology is considered as applied physics.[47] Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training.[48] Physicists and engineers engage in different lines of work.[49] But PhD physicists who specialize in sectors of technology and applied science are titled as Technology officer, R&D Engineers and System Engineers.[50]In the book What Engineers Know and How They Know It,[41] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely "engineering scientists".[citation needed]There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]In Canada, many engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.[37]Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:Engineering companies in many established economies are facing significant challenges with regard to the number of professional engineers being trained, compared with the number retiring. This problem is very prominent in the UK where engineering has a poor image and low status.[33] There are many negative economic and political issues that this can cause, as well as ethical issues.[34] It is widely agreed that the engineering profession faces an "image crisis",[35] rather than it being fundamentally an unattractive career. Much work is needed to avoid huge problems in the UK and other western economies.All overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:Engineering is a key driver of innovation and human development. Sub-Saharan Africa, in particular, has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid.[citation needed] The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[31]Engineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.By its very nature engineering has interconnections with society, culture and human behavior. Every product or construction used by modern society is influenced by engineering. The results of engineering activity influence changes to the environment, society and economies, and its application brings with it a responsibility and public safety.The engineering profession engages in a wide range of activities, from large collaboration at the societal level, and also smaller individual projects. Almost all engineering projects are obligated to some sort of financing agency: a company, a set of investors, or a government. The few types of engineering that are minimally constrained by such issues are pro bono engineering and open-design engineering.In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[30]There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and AEC software for civil engineering.These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[29]One of the most widely used design tools in the profession is computer-aided design (CAD) software like CATIA, Autodesk Inventor, DSS SolidWorks or Pro Engineer which enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.The study of failed products is known as forensic engineering and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure.Engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. However, the greater the safety factor, the less efficient the design may be.[citation needed]Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.Usually, multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.According to Billy Vaughn Koen, the "engineering method is the use of heuristics to cause the best change in a poorly understood situation within the available resources." Koen argues that the definition of what makes one an engineer should not be based on what he produces, but rather how he goes about it.[28]A general methodology and epistemology of engineering can be inferred from the historical case studies and comments provided by Walter Vincenti.[27] Though Vincenti's case studies are from the domain of aeronautical engineering, his conclusions can be transferred into many other branches of engineering, too.Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The crucial and unique task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. More than ever, engineers are now required to have a proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their career.One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.New specialties sometimes combine with the traditional fields and form new branches – for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.Beyond these "Big Four", a number of other branches are recognized, though many can be thought of as sub-disciplines of the four major branches, or as cross-curricular disciplines among multiple. Historically, naval engineering and mining engineering were major branches. Other engineering fields sometimes included as major branches[citation needed] are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical,[24] geological, textile, industrial, materials,[25] and nuclear engineering.[26] These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.Mechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, and mechatronics.Electrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as Broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, controls, and electronics.Civil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings.[21][22] Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.[23]Chemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.Engineering is a broad discipline which is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches:[18][19][20] chemical engineering, civil engineering, electrical engineering, and mechanical engineering.In 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage.Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[17]Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[16]The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[4] Chemical engineering developed in the late nineteenth century.[4] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[4] The role of the chemical engineer was the design of these chemical plants and processes.[4]There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[15]The United States census of 1850 listed the occupation of "engineer" for the first time with a count of 2,000.[13] There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.[14]John Smeaton was the first self-proclaimed civil engineer and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbours, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. His lighthouse remained in use until 1877 and was dismantled and partially rebuilt at Plymouth Hoe where it is known as Smeaton's Tower. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement.The inventions of Thomas Newcomen and James Watt gave rise to modern mechanical engineering. The development of specialized machines and machine tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace Britain and abroad.[4]With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.The first steam engine was built in 1698 by Thomas Savery.[12] The development of this device gave rise to the Industrial Revolution in the coming decades, allowing for the beginnings of mass production.Ancient Chinese, Greek, Roman and Hungarian armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century B.C.,[11] the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.The earliest civil engineer known by name is Imhotep.[4] As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC.[7] Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, the first known mechanical computer,[8][9] and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.[10]The pyramids in Egypt, the Acropolis and the Parthenon in Greece, the Roman aqueducts, Via Appia and the Colosseum, Teotihuacán, the Great Wall of China, the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon, and the Pharos of Alexandria were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[4] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.The word "engine" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning "innate quality, especially mental power, hence a clever invention."[6]The term engineering is derived from the word engineer, which itself dates back to 1390 when an engine'er (literally, one who operates an engine) referred to "a constructor of military engines."[5] In this context, now obsolete, an "engine" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley.The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET)[2] has defined "engineering" as:The term engineering is derived from the Latin ingenium, meaning "cleverness" and ingeniare, meaning "to contrive, devise".[1]Engineering is the creative application of science, mathematical methods, and empirical evidence to the innovation, design, construction, operation and maintenance of structures, machines, materials, devices, systems, processes, and organizations. The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.
Mathematical model
that can be written also as:The language recognized by M is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where "*" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols "1".The state S1 represents that there has been an even number of 0s in the input so far, while S2 signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, M will finish in state S1, an accepting state, so the input string will be accepted.M = (Q, Σ, δ, q0, F) whereAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.[4]Many types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.As an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.The question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.Assessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a "typical" set of data.While it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.Defining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.A crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.Any model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by a neural network or other machine learning, the optimization of parameters is called training[citation needed][why?], while the optimization of model hyperparameters is called tuning and often uses cross-validation[citation needed]. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by curve fitting[citation needed].For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.In general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification[citation needed].An example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.Sometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification [3] can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.Usually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.Mathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.For example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.Objectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an index of performance, as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.Decision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants. The variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).In business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.A mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.Often when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.Since prehistorical times simple models such as maps and diagrams have been used.Different mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.It is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schrödinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.Throughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement[citation needed][example needed]. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.Mathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.Mathematical models are usually composed of relationships and variables. Relationships can be described by operators, such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:In the physical sciences, a traditional mathematical model contains most of the following elements:Mathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.A mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, artificial intelligence), as well as in the social sciences (such as economics, psychology, sociology, political science). Physicists, mathematicians, engineers, statisticians, operations research analysts, and economists use mathematical models most extensively[citation needed]. A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.
Nonlinear system
This corresponds to a free fall problem. A very useful qualitative picture of the pendulum's dynamics may be obtained by piecing together such linearizations, as seen in the figure at right. Other techniques may be used to find (exact) phase portraits and approximate periods.A classic, extensively studied nonlinear problem is the dynamics of a pendulum under the influence of gravity. Using Lagrangian mechanics, it may be shown[9] that the motion of a pendulum can be described by the dimensionless nonlinear equationOther methods include examining the characteristics and using the methods outlined above for ordinary differential equations.Another common (though less mathematic) tactic, often seen in fluid and heat mechanics, is to use scale analysis to simplify a general, natural equation in a certain specific boundary value problem. For example, the (very) nonlinear Navier-Stokes equations can be simplified into one linear partial differential equation in the case of transient, laminar, one dimensional flow in a circular pipe; the scale analysis provides conditions under which the flow is laminar and one dimensional and also yields the simplified equation.The most common basic approach to studying nonlinear partial differential equations is to change the variables (or otherwise transform the problem) so that the resulting problem is simpler (possibly even linear). Sometimes, the equation may be transformed into one or more ordinary differential equations, as seen in separation of variables, which is always useful whether or not the resulting ordinary differential equation(s) is solvable.Common methods for the qualitative analysis of nonlinear ordinary differential equations include:Second and higher order ordinary differential equations (more generally, systems of nonlinear equations) rarely yield closed form solutions, though implicit solutions and solutions involving nonelementary integrals are encountered.and the left-hand side of the equation is not a linear function of u and its derivatives. Note that if the u2 term were replaced with u, the problem would be linear (the exponential decay problem).First order ordinary differential equations are often exactly solvable by separation of variables, especially for autonomous equations. For example, the nonlinear equationOne of the greatest difficulties of nonlinear problems is that it is not generally possible to combine known solutions into new solutions. In linear problems, for example, a family of linearly independent solutions can be used to construct general solutions through the superposition principle. A good example of this is one-dimensional heat transport with Dirichlet boundary conditions, the solution of which can be written as a time-dependent linear combination of sinusoids of differing frequencies; this makes solutions very flexible. It is often possible to find several very specific solutions to nonlinear equations, however the lack of a superposition principle prevents the construction of new solutions.A system of differential equations is said to be nonlinear if it is not a linear system. Problems involving nonlinear differential equations are extremely diverse, and methods of solution or analysis are problem dependent. Examples of nonlinear differential equations are the Navier–Stokes equations in fluid dynamics and the Lotka–Volterra equations in biology.A nonlinear recurrence relation defines successive terms of a sequence as a nonlinear function of preceding terms. Examples of nonlinear recurrence relations are the logistic map and the relations that define the various Hofstadter sequences. Nonlinear discrete models that represent a wide class of nonlinear recurrence relationships include the NARMAX (Nonlinear Autoregressive Moving Average with eXogenous inputs) model and the related nonlinear system identification and analysis procedures.[8] These approaches can be used to study a wide class of complex nonlinear behaviors in the time, frequency, and spatio-temporal domains.For a single polynomial equation, root-finding algorithms can be used to find solutions to the equation (i.e., sets of values for the variables that satisfy the equation). However, systems of algebraic equations are more complicated; their study is one motivation for the field of algebraic geometry, a difficult branch of modern mathematics. It is even difficult to decide whether a given algebraic system has complex solutions (see Hilbert's Nullstellensatz). Nevertheless, in the case of the systems with a finite number of complex solutions, these systems of polynomial equations are now well understood and efficient methods exist for solving them.[7]Nonlinear algebraic equations, which are also called polynomial equations, are defined by equating polynomials to zero. For example,An equation written asAdditivity implies homogeneity for any rational α, and, for continuous functions, for any real α. For a complex α, homogeneity does not follow from additivity. For example, an antilinear map is additive but not homogeneous. The conditions of additivity and homogeneity are often combined in the superposition principleSome authors use the term nonlinear science for the study of nonlinear systems. This is disputed by others:As nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos[5] and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.Typically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one. In other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.In mathematics and physical sciences, a nonlinear system is a system in which the change of the output is not proportional to the change of the input.[1] Nonlinear problems are of interest to engineers, physicists,[2][3] mathematicians, and many other scientists because most systems are inherently nonlinear in nature.[4] Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems.
Determinant
where ωj is an nth root of 1.where ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]Third orderSecond orderwhere the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.In general, the nth-order Vandermonde determinant is[33]The third order Vandermonde determinant isThe Jacobian also occurs in the inverse function theorem.Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given bythe Jacobian matrix is the n × n matrix whose entries are given byFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. ForBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Another infinite-dimensional notion of determinant is the functional determinant.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,holds. In other words, the following diagram commutes:between the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityThe determinant defines a mappingThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.from the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isThe determinant can also be characterized as the unique functionThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn). As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple. We call this scalar the determinant of T.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.As ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThe determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant is therefore also called a similarity invariant. The determinant of a linear transformationThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThis identity is used in describing the tangent space of certain matrix Lie groups.Yet another equivalent formulation isExpressed in terms of the entries of A, these arewhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveBy definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]When D is a 1×1 matrix, B is a column vector, and C is a row vector thenWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]as can be seen by employing the decompositionWhen A is invertible, one hasThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenIt has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.the solution is given by Cramer's rule:For a matrix equationThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.Also,with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantis expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).where I is the identity matrix. More generally, ifAn important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way asThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l – 1)! tr(Al) aswhere the sum is taken over the set of all integers kl ≥ 0 satisfying the equationIn the general case, this may also be obtained from[9]cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromFor example, for n = 2, n = 3, and n = 4, respectively,the determinant of A is given byHere exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingor, for real matrices A,The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,being positive, for all k between 1 and n.A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatriceswhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationThe product of all non-zero eigenvalues is referred to as pseudo-determinant.From this general result several consequences follow.where Im and In are the m × m and n × n identity matrices, respectively.Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):In terms of the adjugate matrix, Laplace's expansion can be written as[7]The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,However, Laplace expansion is efficient for small matrices only.along the second column (j = 2 and the sum runs over i) is given by,Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i+jMi,j is known as a cofactor. The determinant of A is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant of a matrix product of square matrices equals the product of their determinants:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.can be computed using the following matrices:For example, the determinant ofProperty 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.Property 2 above implies that properties for columns have their counterparts in terms of rows:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.The determinant has many properties. Some basic properties of determinants arewhere now each ir and each jr should be summed over 1, …, n.or using two epsilon symbols asFor example, the determinant of a 3 × 3 matrix A (n = 3) isis notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:Here the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.The Leibniz formula for the determinant of an n × n matrix A isThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.which is the Leibniz formula for the determinant of a 3 × 3 matrix.this can be expanded out to giveThe Laplace formula for the determinant of a 3 × 3 matrix isThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (-b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The Leibniz formula for the determinant of a 2 × 2 matrix isThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.Assume A is a square matrix with n rows and n columns, so that it can be written asEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n × n matrix A in terms of its column vectorsThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 × 4 matrix:When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 × 4 matrix, the determinant of a 5 × 5 matrix, and so forth.Similarly, suppose we have a 3 × 3 matrix A, and we want the specific formula for its determinant |A|:In the case of a 2 × 2 matrix the specific formula for the determinant is:In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.
System of linear equations
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:where A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.A homogeneous system is equivalent to a matrix equation of the formA system of linear equations is homogeneous if all of the constant terms are zero:There is also a quantum algorithm for linear systems of equations.[3]A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.is given byCramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemThe last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsFor example, consider the following system:The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:Here x is the free variable, and y and z are dependent.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.The solution set to this system can be described by the following equations:For example, consider the following system:To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.There are several algorithms for solving a system of linear equations.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.For example, the equationsA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.For a more complicated example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For example, the equationsThe equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.The following pictures illustrate this trichotomy in the case of two variables:In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.A linear system may behave in any one of three possible ways:A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.The number of vectors in a basis for the span is now expressed as the rank of the matrix.where A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The vector equation is equivalent to a matrix equation of the formThis allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.A general system of m linear equations with n unknowns can be written asNow substitute this expression for x into the bottom equation:The simplest kind of linear system involves two equations and two variables:Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given byIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,
Gottfried Wilhelm Leibniz
Six important collections of English translations are Wiener (1951), Parkinson (1966), Loemker (1969), Ariew and Garber (1989), Woolhouse and Francks (1998), and Strickland (2006). The ongoing critical edition of all of Leibniz's writings is Sämtliche Schriften und Briefe.[159]The year given is usually that in which the work was completed, not of its eventual publication.The systematic cataloguing of all of Leibniz's Nachlass began in 1901. It was hampered by two world wars and decades of German division in two states with the cold war's "iron curtain" in between, separating scholars, and also scattering portions of his literary estates. The ambitious project has had to deal with seven languages contained in some 200,000 pages of written and printed paper. In 1985 it was reorganized and included in a joint program of German federal and state (Länder) academies. Since then the branches in Potsdam, Münster, Hanover and Berlin have jointly published 57 volumes of the critical edition, with an average of 870 pages, and prepared index and concordance works.The extant parts of the critical edition[159] of Leibniz's writings are organized as follows:Leibniz mainly wrote in three languages: scholastic Latin, French and German. During his lifetime, he published many pamphlets and scholarly articles, but only two "philosophical" books, the Combinatorial Art and the Théodicée. (He published numerous pamphlets, often anonymous, on behalf of the House of Brunswick-Lüneburg, most notably the "De jure suprematum" a major consideration of the nature of sovereignty.) One substantial book appeared posthumously, his Nouveaux essais sur l'entendement humain, which Leibniz had withheld from publication after the death of John Locke. Only in 1895, when Bodemann completed his catalogue of Leibniz's manuscripts and correspondence, did the enormous extent of Leibniz's Nachlass become clear: about 15,000 letters to more than 1000 recipients plus more than 40,000 other items. Moreover, quite a few of these letters are of essay length. Much of his vast correspondence, especially the letters dated after 1700, remains unpublished, and much of what is published has been so only in recent decades. The amount, variety, and disorder of Leibniz's writings are a predictable result of a situation he described in a letter as follows:The collection of manuscript papers of Leibniz at the Gottfried Wilhelm Leibniz Bibliothek – Niedersächische Landesbibliothek were inscribed on UNESCO's Memory of the World Register in 2007.[157]In 1985, the German government created the Leibniz Prize, offering an annual award of 1.55 million euros for experimental results and 770,000 euros for theoretical ones. It was the worlds largest prize for scientific achievement prior to the Fundamental Physics Prize.Nicholas Jolley has surmised that Leibniz's reputation as a philosopher is now perhaps higher than at any time since he was alive.[156] Analytic and contemporary philosophy continue to invoke his notions of identity, individuation, and possible worlds. Work in the history of 17th- and 18th-century ideas has revealed more clearly the 17th-century "Intellectual Revolution" that preceded the better-known Industrial and commercial revolutions of the 18th and 19th centuries.In 1900, Bertrand Russell published a critical study of Leibniz's metaphysics.[155] Shortly thereafter, Louis Couturat published an important study of Leibniz, and edited a volume of Leibniz's heretofore unpublished writings, mainly on logic. They made Leibniz somewhat respectable among 20th-century analytical and linguistic philosophers in the English-speaking world (Leibniz had already been of great influence to many Germans such as Bernhard Riemann). For example, Leibniz's phrase salva veritate, meaning interchangeability without loss of or compromising the truth, recurs in Willard Quine's writings. Nevertheless, the secondary literature on Leibniz did not really blossom until after World War II. This is especially true of English speaking countries; in Gregory Brown's bibliography fewer than 30 of the English language entries were published before 1946. American Leibniz studies owe much to Leroy Loemker (1904–1985) through his translations and his interpretive essays in LeClerc (1973).Leibniz's long march to his present glory began with the 1765 publication of the Nouveaux Essais, which Kant read closely. In 1768, Louis Dutens edited the first multi-volume edition of Leibniz's writings, followed in the 19th century by a number of editions, including those edited by Erdmann, Foucher de Careil, Gerhardt, Gerland, Klopp, and Mollat. Publication of Leibniz's correspondence with notables such as Antoine Arnauld, Samuel Clarke, Sophia of Hanover, and her daughter Sophia Charlotte of Hanover, began.Much of Europe came to doubt that Leibniz had discovered calculus independently of Newton, and hence his whole work in mathematics and physics was neglected. Voltaire, an admirer of Newton, also wrote Candide at least in part to discredit Leibniz's claim to having discovered calculus and Leibniz's charge that Newton's theory of universal gravitation was incorrect.[citation needed]When Leibniz died, his reputation was in decline. He was remembered for only one book, the Théodicée,[153] whose supposed central argument Voltaire lampooned in his popular book Candide, which concludes with the character Candide saying, "Non liquet" (it is not clear), a term that was applied during the Roman Republic to a legal verdict of "not proven". Voltaire's depiction of Leibniz's ideas was so influential that many believed it to be an accurate description. Thus Voltaire and his Candide bear some of the blame for the lingering failure to appreciate and understand Leibniz's ideas. Leibniz had an ardent disciple, Christian Wolff, whose dogmatic and facile outlook did Leibniz's reputation much harm. He also influenced David Hume who read his Théodicée and used some of his ideas.[154] In any event, philosophical fashion was moving away from the rationalism and system building of the 17th century, of which Leibniz had been such an ardent proponent. His work on law, diplomacy, and history was seen as of ephemeral interest. The vastness and richness of his correspondence went unrecognized.Leibniz also wrote a short paper, Primae veritates, first published by Louis Couturat in 1903 (pp. 518–523)[151] summarizing his views on metaphysics. The paper is undated; that he wrote it while in Vienna in 1689 was determined only in 1999, when the ongoing critical edition finally published Leibniz's philosophical writings for the period 1677–90.[152] Couturat's reading of this paper was the launching point for much 20th-century thinking about Leibniz, especially among analytic philosophers. But after a meticulous study of all of Leibniz's philosophical writings up to 1688—a study the 1999 additions to the critical edition made possible—Mercer (2001) begged to differ with Couturat's reading; the jury is still out.While making his grand tour of European archives to research the Brunswick family history that he never completed, Leibniz stopped in Vienna between May 1688 and February 1689, where he did much legal and diplomatic work for the Brunswicks. He visited mines, talked with mine engineers, and tried to negotiate export contracts for lead from the ducal mines in the Harz mountains. His proposal that the streets of Vienna be lit with lamps burning rapeseed oil was implemented. During a formal audience with the Austrian Emperor and in subsequent memoranda, he advocated reorganizing the Austrian economy, reforming the coinage of much of central Europe, negotiating a Concordat between the Habsburgs and the Vatican, and creating an imperial research library, official archive, and public insurance fund. He wrote and published an important paper on mechanics.Leibniz's attraction to Chinese philosophy originates from his perception that Chinese philosophy was similar to his own.[147] The historian E.R. Hughes suggests that Leibniz's ideas of "simple substance" and "pre-established harmony" were directly influenced by Confucianism, pointing to the fact that they were conceived during the period that he was reading Confucius Sinarum Philosophus.[147]Leibniz was perhaps the first major European intellectual to take a close interest in Chinese civilization, which he knew by corresponding with, and reading other works by, European Christian missionaries posted in China. Having read Confucius Sinarum Philosophus on the first year of its publication,[147] he concluded that Europeans could learn much from the Confucian ethical tradition. He mulled over the possibility that the Chinese characters were an unwitting form of his universal characteristic. He noted with fascination how the I Ching hexagrams correspond to the binary numbers from 000000 to 111111, and concluded that this mapping was evidence of major Chinese accomplishments in the sort of philosophical mathematics he admired.[148] Leibniz communicated his ideas of the binary system representing Christianity to the Emperor of China hoping it would convert him.[149] Leibniz may be the only major Western philosopher who attempted to accommodate Confucian ideas to prevailing European beliefs.[150]
He published the princeps editio (first modern edition) of the late medieval Chronicon Holtzatiae, a Latin chronicle of the County of Holstein.Leibniz the philologist was an avid student of languages, eagerly latching on to any information about vocabulary and grammar that came his way. He refuted the belief, widely held by Christian scholars in his day, that Hebrew was the primeval language of the human race. He also refuted the argument, advanced by Swedish scholars in his day, that a form of proto-Swedish was the ancestor of the Germanic languages. He puzzled over the origins of the Slavic languages and was fascinated by classical Chinese. Leibniz was also an expert in the Sanskrit language.[145]Leibniz devoted considerable intellectual and diplomatic effort to what would now be called ecumenical endeavor, seeking to reconcile first the Roman Catholic and Lutheran churches, and later the Lutheran and Reformed churches. In this respect, he followed the example of his early patrons, Baron von Boyneburg and the Duke John Frederick—both cradle Lutherans who converted to Catholicism as adults—who did what they could to encourage the reunion of the two faiths, and who warmly welcomed such endeavors by others. (The House of Brunswick remained Lutheran because the Duke's children did not follow their father.) These efforts included corresponding with the French bishop Jacques-Bénigne Bossuet, and involved Leibniz in some theological controversy. He evidently thought that the thoroughgoing application of reason would suffice to heal the breach caused by the Reformation.But at the same time, he arrived to propose an interreligious and multicultural project to create a universal system of justice, which required from him a broad interdisciplinary perspective. In order to propose it, he combined linguistics, especially sinology, moral and law philosophy, management, economics, and politics.[144]In 1677, Leibniz called for a European confederation, governed by a council or senate, whose members would represent entire nations and would be free to vote their consciences;[143] this is sometimes considered an anticipation of the European Union. He believed that Europe would adopt a uniform religion. He reiterated these proposals in 1715.While Leibniz was no apologist for absolute monarchy like Hobbes, or for tyranny in any form, neither did he echo the political and constitutional views of his contemporary John Locke, views invoked in support of democracy, in 18th-century America and later elsewhere. The following excerpt from a 1695 letter to Baron J. C. Boyneburg's son Philipp is very revealing of Leibniz's political sentiments:With the possible exception of Marcus Aurelius, no philosopher has ever had as much experience with practical affairs of state as Leibniz. Leibniz's writings on law, ethics, and politics[140] were long overlooked by English-speaking scholars, but this has changed of late.[141]Leibniz emphasized that research was a collaborative endeavor. Hence he warmly advocated the formation of national scientific societies along the lines of the British Royal Society and the French Academie Royale des Sciences. More specifically, in his correspondence and travels he urged the creation of such societies in Dresden, Saint Petersburg, Vienna, and Berlin. Only one such project came to fruition; in 1700, the Berlin Academy of Sciences was created. Leibniz drew up its first statutes, and served as its first President for the remainder of his life. That Academy evolved into the German Academy of Sciences, the publisher of the ongoing critical edition of his works.[139]He called for the creation of an empirical database as a way to further all sciences. His characteristica universalis, calculus ratiocinator, and a "community of minds"—intended, among other things, to bring political and religious unity to Europe—can be seen as distant unwitting anticipations of artificial languages (e.g., Esperanto and its rivals), symbolic logic, even the World Wide Web.Later in Leibniz’s career (after the death of von Boinburg), Leibniz moved to Paris and accepted a position as a librarian in the Hanoverian court of Johann Friedrich, Duke of Brunswick-Luneburg. Leibniz’s predecessor, Tobias Fleischer, had already created a cataloging system for the Duke’s library but it was a clumsy attempt. At this library, Leibniz focused more on advancing the library than on the cataloging. For instance, within a month of taking the new position, he developed a comprehensive plan to expand the library. He was one of the first to consider developing a core collection for a library and felt “that a library for display and ostentation is a luxury and indeed superfluous, but a well-stocked and organized library is important and useful for all areas of human endeavor and is to be regarded on the same level as schools and churches”.[138] Unfortunately, Leibniz lacked the funds to develop the library in this manner. After working at this library, by the end of 1690 Leibnez was appointed as privy-councilor and librarian of the Bibliotheca Augusta at Wolfenbuettel. It was an extensive library with at least 25,946 printed volumes[138]. At this library, Leibniz sought to improve the catalog. He was not allowed to make complete changes to the existing closed catalog, but was allowed to improve upon it so he started on that task immediately. He created an alphabetical author catalog and had also created other cataloging methods that were not implemented. While serving as librarian of the ducal libraries in Hanover and Wolfenbuettel, Leibniz effectively became one of the founders of library science. He also designed a book indexing system in ignorance of the only other such system then extant, that of the Bodleian Library at Oxford University. He also called on publishers to distribute abstracts of all new titles they produced each year, in a standard form that would facilitate indexing. He hoped that this abstracting project would eventually include everything printed from his day back to Gutenberg. Neither proposal met with success at the time, but something like them became standard practice among English language publishers during the 20th century, under the aegis of the Library of Congress and the British Library.Leibniz was groping towards hardware and software concepts worked out much later by Charles Babbage and Ada Lovelace. In 1679, while mulling over his binary arithmetic, Leibniz imagined a machine in which binary numbers were represented by marbles, governed by a rudimentary sort of punched cards.[136][137] Modern electronic digital computers replace Leibniz's marbles moving by gravity with shift registers, voltage gradients, and pulses of electrons, but otherwise they run roughly as Leibniz envisioned in 1679.In 1671, Leibniz began to invent a machine that could execute all four arithmetic operations, gradually improving it over a number of years. This "stepped reckoner" attracted fair attention and was the basis of his election to the Royal Society in 1673. A number of such machines were made during his years in Hanover by a craftsman working under his supervision. They were not an unambiguous success because they did not fully mechanize the carry operation. Couturat reported finding an unpublished note by Leibniz, dated 1674, describing a machine capable of performing some algebraic operations.[133] Leibniz also devised a (now reproduced) cipher machine, recovered by Nicholas Rescher in 2010.[134] In 1693, Leibniz described a design of a machine which could, in theory, integrate differential equations, which he called "integraph".[135]Leibniz may have been the first computer scientist and information theorist.[125] Early in life, he documented the binary numeral system (base 2), then revisited that system throughout his career.[126] While Leibniz was examining other cultures to compare his metaphysical views, he encountered an ancient Chinese book I Ching. Leibniz interpreted a diagram which showed yin and yang and corresponded it to a zero and one.[127] More information can be found in the Sinophile section. Leibniz may have plagiarized Juan Caramuel y Lobkowitz and Thomas Harriot, who independently developed the binary system, as he was familiar with their works on the binary system.[128] Juan Caramuel y Lobkowitz worked extensively on logarithms including logarithms with base 2.[129] Thomas Harriot's manuscripts contained a table of binary numbers and their notation, which he realized any number could be written on a base 2 system.[130] Regardless, Leibniz simplified the binary system and articulated logical properties such as conjunction, disjunction, negation, identity, inclusion, and the empty set.[131] He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.[132]In 1906, Garland published a volume of Leibniz's writings bearing on his many practical inventions and engineering work. To date, few of these writings have been translated into English. Nevertheless, it is well understood that Leibniz was a serious inventor, engineer, and applied scientist, with great respect for practical life. Following the motto theoria cum praxi, he urged that theory be combined with practical application, and thus has been claimed as the father of applied science. He designed wind-driven propellers and water pumps, mining machines to extract ore, hydraulic presses, lamps, submarines, clocks, etc. With Denis Papin, he invented a steam engine. He even proposed a method for desalinating water. From 1680 to 1685, he struggled to overcome the chronic flooding that afflicted the ducal silver mines in the Harz Mountains, but did not succeed.[124]In public health, he advocated establishing a medical administrative authority, with powers over epidemiology and veterinary medicine. He worked to set up a coherent medical training program, oriented towards public health and preventive measures. In economic policy, he proposed tax reforms and a national insurance program, and discussed the balance of trade. He even proposed something akin to what much later emerged as game theory. In sociology he laid the ground for communication theory.Leibniz found his most important interpreter in Wilhelm Wundt, founder of psychology as a discipline. Wundt used the "… nisi intellectu ipse" quotation 1862 on the title page of his Beiträge zur Theorie der Sinneswahrnehmung (Contributions on the Theory of Sensory Perception) and published a detailed and aspiring monograph on Leibniz[116] Wundt shaped the term apperception, introduced by Leibniz, into an experimental psychologically based apperception psychology that included neuropsychological modelling – an excellent example of how a concept created by a great philosopher could stimulate a psychological research program. One principle in the thinking of Leibniz played a fundamental role: “the principle of equality of separate but corresponding viewpoints.” Wundt characterized this style of thought (perspectivism) in a way that also applied for him – viewpoints that "supplement one another, while also being able to appear as opposites that only resolve themselves when considered more deeply."[117][118] Much of Leibniz's work went on to have a great impact on the field of psychology.[119] Leibniz thought that there are many petites perceptions, or small perceptions of which we perceive but of which we are unaware. He believed that by the principle that phenomena found in nature were continuous by default, it was likely that the transition between conscious and unconscious states had intermediary steps.[120] For this to be true, there must also be a portion of the mind of which we are unaware at any given time. His theory regarding consciousness in relation to the principle of continuity can be seen as an early theory regarding the stages of sleep. In this way, Leibniz's theory of perception can be viewed as one of many theories leading up to the idea of the unconscious. Leibniz was a direct influence on Ernst Platner, who is credited with originally coining the term Unbewußtseyn (unconscious).[121] Additionally, the idea of subliminal stimuli can be traced back to his theory of small perceptions.[122] Leibniz's ideas regarding music and tonal perception went on to influence the laboratory studies of Wilhelm Wundt.[123]Psychology had been a central interest of Leibniz.[109][110] He appears to be an "underappreciated pioneer of psychology" [111] He wrote on topics which are now regarded as fields of psychology: attention and consciousness, memory, learning (association), motivation (the act of "striving"), emergent individuality, the general dynamics of development (evolution). His discussions in the New Essays and Monadology often rely on everyday observations such as the behaviour of a dog or the noise of the sea, and he develops intuitive analogies (the synchronous running of clocks or the balance spring of a clock). He also devised postulates and principles that apply to psychology: the continuum of the unnoticed petite perceptions to the distinct, self-aware apperception, and psychophysical parallelism from the point of view of causality and of purpose: “Souls act according to the laws of final causes, through aspirations, ends and means. Bodies act according to the laws of efficient causes, i.e. the laws of motion. And these two realms, that of efficient causes and that of final causes, harmonize with one another.” [112] This idea refers to the mind-body problem, stating that the mind and brain do not act upon each other, but act alongside each other separately but in harmony.[113] Leibniz, however, did not use the term psychologia.[114] Leibniz’ epistemological position – against John Locke and English empiricism (sensualism) – was made clear: “Nihil est in intellectu quod non fuerit in sensu, nisi intellectu ipse.” – “Nothing is in the intellect that was not first in the senses, except the intellect itself.” [115] Principles that are not present in sensory impressions can be recognised in human perception and consciousness: logical inferences, categories of thought, the principle of causality and the principle of purpose (teleology).By proposing that the earth has a molten core, he anticipated modern geology. In embryology, he was a preformationist, but also proposed that organisms are the outcome of a combination of an infinite number of possible microstructures and of their powers. In the life sciences and paleontology, he revealed an amazing transformist intuition, fueled by his study of comparative anatomy and fossils. One of his principal works on this subject, Protogaea, unpublished in his lifetime, has recently been published in English for the first time. He worked out a primal organismic theory.[108] In medicine, he exhorted the physicians of his time—with some results—to ground their theories in detailed comparative observations and verified experiments, and to distinguish firmly scientific and metaphysical points of view.Leibniz's vis viva (Latin for "living force") is mv2, twice the modern kinetic energy. He realized that the total energy would be conserved in certain mechanical systems, so he considered it an innate motive characteristic of matter.[107] Here too his thinking gave rise to another regrettable nationalistic dispute. His vis viva was seen as rivaling the conservation of momentum championed by Newton in England and by Descartes in France; hence academics in those countries tended to neglect Leibniz's idea. In reality, both energy and momentum are conserved, so the two approaches are equally valid.The principle of sufficient reason has been invoked in recent cosmology, and his identity of indiscernibles in quantum mechanics, a field some even credit him with having anticipated in some sense. Those who advocate digital philosophy, a recent direction in cosmology, claim Leibniz as a precursor. In addition to his theories about the nature of reality, Leibniz's contributions to the development of calculus have also had a major impact on physics.One of Leibniz's projects was to recast Newton's theory as a vortex theory.[106] However, his project went beyond vortex theory, since at its heart there was an attempt to explain one of the most difficult problems in physics, that of the origin of the cohesion of matter.[106]Leibniz held a relationist notion of space and time, against Newton's substantivalist views.[103][104][105] According to Newton's substantivalism, space and time are entities in their own right, existing independently of things. Leibniz's relationism, on the other hand, describes space and time as systems of relations that exist between objects. The rise of general relativity and subsequent work in the history of physics has put Leibniz's stance in a more favorable light.Until the discovery of subatomic particles and the quantum mechanics governing them, many of Leibniz's speculative ideas about aspects of nature not reducible to statics and dynamics made little sense. For instance, he anticipated Albert Einstein by arguing, against Newton, that space, time and motion are relative, not absolute: "As for my own opinion, I have said more than once, that I hold space to be something merely relative, as time is, that I hold it to be an order of coexistences, as time is an order of successions."[102]Leibniz contributed a fair amount to the statics and dynamics emerging around him, often disagreeing with Descartes and Newton. He devised a new theory of motion (dynamics) based on kinetic energy and potential energy, which posited space as relative, whereas Newton was thoroughly convinced that space was absolute. An important example of Leibniz's mature physical thinking is his Specimen Dynamicum of 1695.[101]Leibniz's writings are currently discussed, not only for their anticipations and possible discoveries not yet recognized, but as ways of advancing present knowledge. Much of his writing on physics is included in Gerhardt's Mathematical Writings.Thus the fractal geometry promoted by Mandelbrot drew on Leibniz's notions of self-similarity and the principle of continuity: Natura non facit saltus.[56] We also see that when Leibniz wrote, in a metaphysical vein, that "the straight line is a curve, any part of which is similar to the whole", he was anticipating topology by more than two centuries. As for "packing", Leibniz told his friend and correspondent Des Bosses to imagine a circle, then to inscribe within it three congruent circles with maximum radius; the latter smaller circles could be filled with three even smaller circles by the same procedure. This process can be continued infinitely, from which arises a good idea of self-similarity. Leibniz's improvement of Euclid's axiom contains the same concept.But Hideaki Hirano argues differently, quoting Mandelbrot:[99]Leibniz was the first to use the term analysis situs,[97] later used in the 19th century to refer to what is now known as topology. There are two takes on this situation. On the one hand, Mates, citing a 1954 paper in German by Jacob Freudenthal, argues:The use of infinitesimals in mathematics was frowned upon by followers of Karl Weierstrass,[citation needed] but survived in science and engineering, and even in rigorous mathematics, via the fundamental computational device known as the differential. Beginning in 1960, Abraham Robinson worked out a rigorous foundation for Leibniz's infinitesimals, using model theory, in the context of a field of hyperreal numbers. The resulting non-standard analysis can be seen as a belated vindication of Leibniz's mathematical reasoning. Robinson's transfer principle is a mathematical implementation of Leibniz's heuristic law of continuity, while the standard part function implements the Leibnizian transcendental law of homogeneity.From 1711 until his death, Leibniz was engaged in a dispute with John Keill, Newton and others, over whether Leibniz had invented calculus independently of Newton. This subject is treated at length in the article Leibniz–Newton calculus controversy.Leibniz exploited infinitesimals in developing calculus, manipulating them in ways suggesting that they had paradoxical algebraic properties. George Berkeley, in a tract called The Analyst and also in De Motu, criticized these. A recent study argues that Leibnizian calculus was free of contradictions, and was better grounded than Berkeley's empiricist criticisms.[96]Leibniz is credited, along with Sir Isaac Newton, with the discovery of calculus (differential and integral calculus). According to Leibniz's notebooks, a critical breakthrough occurred on 11 November 1675, when he employed integral calculus for the first time to find the area under the graph of a function y = f(x).[91] He introduced several notations used to this day, for instance the integral sign ∫, representing an elongated S, from the Latin word summa, and the d used for differentials, from the Latin word differentia. This cleverly suggestive notation for calculus is probably his most enduring mathematical legacy. Leibniz did not publish anything about his calculus until 1684.[92] Leibniz expressed the inverse relation of integration and differentiation, later called the fundamental theorem of calculus, by means of a figure[93] in his 1693 paper Supplementum geometriae dimensoriae....[94] However, James Gregory is credited for the theorem's discovery in geometric form, Isaac Barrow proved a more generalized geometric version, and Newton developed supporting theory. The concept became more transparent as developed through Leibniz's formalism and new notation.[95] The product rule of differential calculus is still called "Leibniz's law". In addition, the theorem that tells how and when to differentiate under the integral sign is called the Leibniz integral rule.Leibniz wrote that circles "can most simply be expressed by this series, that is, the aggregate of fractions alternately added and subtracted."[88] However this formula is only accurate with a large number of terms, using 10,000,000 terms to obtain the correct value of π/4 to 8 decimal places.[89] Leibniz attempted to create a definition for a straight line while attempting to prove the parallel postulate. [90]The Leibniz formula for π states thatLeibniz's discoveries of Boolean algebra and of symbolic logic, also relevant to mathematics, are discussed in the preceding section. The best overview of Leibniz's writings on calculus may be found in Bos (1974).[80]Although the mathematical notion of function was implicit in trigonometric and logarithmic tables, which existed in his day, Leibniz was the first, in 1692 and 1694, to employ it explicitly, to denote any of several geometric concepts derived from a curve, such as abscissa, ordinate, tangent, chord, and the perpendicular.[77] In the 18th century, "function" lost these geometrical associations. Leibniz also believed that the sum of an infinite number of zeros would equal to one half using the analogy of the creation of the world from nothing. [78] Leibniz was also one the pioneers in actuarial science, calculating the purchase price of life annuities and the liquidation of a state's debt.[79]Russell's principal work on Leibniz found that many of Leibniz's most startling philosophical ideas and claims (e.g., that each of the fundamental monads mirrors the whole universe) follow logically from Leibniz's conscious choice to reject relations between things as unreal. He regarded such relations as (real) qualities of things (Leibniz admitted unary predicates only): For him "Mary is the mother of John" describes separate qualities of Mary and of John. This view contrasts with the relational logic of De Morgan, Peirce, Schröder and Russell himself, now standard in predicate logic. Notably, Leibniz also declared space and time to be inherently relational.[76]Leibniz published nothing on formal logic in his lifetime; most of what he wrote on the subject consists of working drafts. In his book History of Western Philosophy, Bertrand Russell went so far as to claim that Leibniz had developed logic in his unpublished writings to a level which was reached only 200 years later.The formal logic that emerged early in the 20th century also requires, at minimum, unary negation and quantified variables ranging over some universe of discourse.Leibniz is one of the most important logicians between Aristotle and 1847, when George Boole and Augustus De Morgan each published books that began modern formal logic. Leibniz enunciated the principal properties of what we now call conjunction, disjunction, negation, identity, set inclusion, and the empty set. The principles of Leibniz's logic and, arguably, of his whole philosophy, reduce to two:What Leibniz actually intended by his characteristica universalis and calculus ratiocinator, and the extent to which modern formal logic does justice to calculus, may never be established.[75]Because Leibniz was a mathematical novice when he first wrote about the characteristic, at first he did not conceive it as an algebra but rather as a universal language or script. Only in 1676 did he conceive of a kind of "algebra of thought", modeled on and including conventional algebra and its notation. The resulting characteristic included a logical calculus, some combinatorics, algebra, his analysis situs (geometry of situation), a universal concept language, and more.Complex thoughts would be represented by combining characters for simpler thoughts. Leibniz saw that the uniqueness of prime factorization suggests a central role for prime numbers in the universal characteristic, a striking anticipation of Gödel numbering. Granted, there is no intuitive or mnemonic way to number any set of elementary concepts using the prime numbers. Leibniz's idea of reasoning through a universal language of symbols and calculations, however, remarkably foreshadows great 20th century developments in formal systems, such as Turing completeness, where computation was used to define equivalent universal languages (see Turing degree).But Leibniz took his speculations much further. Defining a character as any written sign, he then defined a "real" character as one that represents an idea directly and not simply as the word embodying the idea. Some real characters, such as the notation of logic, serve only to facilitate reasoning. Many characters well known in his day, including Egyptian hieroglyphics, Chinese characters, and the symbols of astronomy and chemistry, he deemed not real.[73] Instead, he proposed the creation of a characteristica universalis or "universal characteristic", built on an alphabet of human thought in which each fundamental concept would be represented by a unique "real" character:Leibniz thought symbols were important for human understanding. He attached so much importance to the development of good notations that he attributed all his discoveries in mathematics to this. His notation for calculus is an example of his skill in this regard. Peirce, a 19th-century pioneer of semiotics, shared Leibniz's passion for symbols and notation, and his belief that these are essential to a well-running logic and mathematics.Leibniz's calculus ratiocinator, which resembles symbolic logic, can be viewed as a way of making such calculations feasible. Leibniz wrote memoranda[72] that can now be read as groping attempts to get symbolic logic—and thus his calculus—off the ground. These writings remained unpublished until the appearance of a selection edited by C.I. Gerhardt (1859). L. Couturat published a selection in 1901; by this time the main developments of modern logic had been created by Charles Sanders Peirce and by Gottlob Frege.Leibniz believed that much of human reasoning could be reduced to calculations of a sort, and that such calculations could resolve many differences of opinion:Leibniz wrote: "Why is there something rather than nothing? The sufficient reason ... is found in a substance which ... is a necessary being bearing the reason for its existence within itself."[68] Martin Heidegger called this question "the fundamental question of metaphysics".[69][70]Because God is "an absolutely perfect being" (I), Leibniz argues that God would be acting imperfectly if he acted with any less perfection than what he is able of (III). His syllogism then ends with the statement that God has made the world perfectly in all ways. This also effects how we should view God and his will. Leibniz states that, in lieu of God’s will, we have to understand that God "is the best of all masters" and he will know when his good succeeds, so we, therefore, must act in conformity to his good will – or as much of it as we understand (IV). In our view of God, Leibniz declares that we cannot admire the work solely because of the maker, lest we mar the glory and love God in doing so. Instead, we must admire the maker for the work he has done (II). Effectively, Leibniz states that if we say the earth is good because of the will of God, and not good according to some standards of goodness, then how can we praise God for what he has done if contrary actions are also praiseworthy by this definition (II). Leibniz then asserts that different principles and geometry cannot simply be from the will of God, but must follow from his understanding.[67]For Leibniz, "God is an absolutely perfect being." He describes this perfection later in section VI as the simplest form of something with the most substantial outcome (VI). Along these lines, he declares that every type of perfection "pertains to him (God) in the highest degree" (I). Even though his types of perfections are not specifically drawn out, Leibniz highlights the one thing that, to him, does certify imperfections and proves that God is perfect: "that one acts imperfectly if he acts with less perfection than he is capable of", and since God is a perfect being, he cannot act imperfectly (III). Because God cannot act imperfectly, the decisions he makes pertaining to the world must be perfect. Leibniz also comforts readers, stating that because he has done everything to the most perfect degree; those who love him cannot be injured. However, to love God is a subject of difficulty as Leibniz believes that we are "not disposed to wish for that which God desires" because we have the ability to alter our disposition (IV). In accordance with this, many act as rebels, but Leibniz says that the only way we can truly love God is by being content "with all that comes to us according to his will" (IV).Further, although human actions flow from prior causes that ultimately arise in God, and therefore are known as a metaphysical certainty to God, an individual's free will is exercised within natural laws, where choices are merely contingently necessary, to be decided in the event by a "wonderful spontaneity" that provides individuals an escape from rigorous predestination.Because reason and faith must be entirely reconciled, any tenet of faith which could not be defended by reason must be rejected. Leibniz then approached one of the central criticisms of Christian theism:[66] if God is all good, all wise and all powerful, how did evil come into the world? The answer (according to Leibniz) is that, while God is indeed unlimited in wisdom and power, his human creations, as creations, are limited both in their wisdom and in their will (power to act). This predisposes humans to false beliefs, wrong decisions and ineffective actions in the exercise of their free will. God does not arbitrarily inflict pain and suffering on humans; rather he permits both moral evil (sin) and physical evil (pain and suffering) as the necessary consequences of metaphysical evil (imperfection), as a means by which humans can identify and correct their erroneous decisions, and as a contrast to true good.Leibniz asserted that the truths of theology (religion) and philosophy cannot contradict each other, since reason and faith are both "gifts of God" so that their conflict would imply God contending against himself. The Theodicy is Leibniz's attempt to reconcile his personal philosophical system with his interpretation of the tenets of Christianity.[65] This project was motivated in part by Leibniz's belief, shared by many conservative philosophers and theologians during the Enlightenment, in the rational and enlightened nature of the Christian religion as compared to its purportedly less advanced non-Western counterparts. It was also shaped by Leibniz's belief in the perfectibility of human nature (if humanity relied on correct philosophy and religion as a guide), and by his belief that metaphysical necessity must have a rational or logical foundation, even if this metaphysical causality seemed inexplicable in terms of physical necessity (the natural laws identified by science).The Theodicy[64] tries to justify the apparent imperfections of the world by claiming that it is optimal among all possible worlds. It must be the best possible and most balanced world, because it was created by an all powerful and all knowing God, who would not choose to create an imperfect world if a better world could be known to him or possible to exist. In effect, apparent flaws that can be identified in this world must exist in every possible world, because otherwise God would have chosen to create the world that excluded those flaws.Monads are purported to have gotten rid of the problematic:The ontological essence of a monad is its irreducible simplicity. Unlike atoms, monads possess no material or spatial character. They also differ from atoms by their complete mutual independence, so that interactions among monads are only apparent. Instead, by virtue of the principle of pre-established harmony, each monad follows a preprogrammed set of "instructions" peculiar to itself, so that a monad "knows" what to do at each moment. By virtue of these intrinsic instructions, each monad is like a little mirror of the universe. Monads need not be "small"; e.g., each human being constitutes a monad, in which case free will is problematic.Leibniz's proof of God can be summarized in the Théodicée.[61] Reason is governed by the principle of contradiction and the principle of sufficient reason. Using the principle of reasoning, Leibniz concluded that the first reason of all things is God.[62]All that we see and experience are subject to change, and the fact that this world is contingent can be explained by the possibility of the world being arranged differently in space and time. The contingent world must have some necessary reason for its existence. Leibniz uses a geometry book as an example to explain his reasoning. If this book was copied from an infinite chain of copies, there must be a some reason for the content of the book.[63]Leibniz concluded that there must be the "monas monadum" or God.Leibniz's best known contribution to metaphysics is his theory of monads, as exposited in Monadologie. He proposes his theory that the universe is made of an infinite number of simple substances known as monads. Monads can also be compared to the corpuscles of the Mechanical Philosophy of René Descartes and others. These simple substances or monads are the "ultimate units of existence in nature". Monads have no parts but still exist by the qualities that they have. These qualities are continuously changing over time, and each monad is unique. They are also not affected by time and are subject to only creation and annihilation. [60] Monads are centers of force; substance is force, while space, matter, and motion are merely phenomenal.Leibniz would on occasion give a rational defense of a specific principle, but more often took them for granted.[59]Leibniz variously invoked one or another of seven fundamental philosophical Principles:[53]Unlike Descartes and Spinoza, Leibniz had a thorough university education in philosophy. He was influenced by his Leipzig professor Jakob Thomasius, who also supervised his BA thesis in philosophy.[4] Leibniz also eagerly read Francisco Suárez, a Spanish Jesuit respected even in Lutheran universities. Leibniz was deeply interested in the new methods and conclusions of Descartes, Huygens, Newton, and Boyle, but viewed their work through a lens heavily tinted by scholastic notions. Yet it remains the case that Leibniz's methods and concerns often anticipate the logic, and analytic and linguistic philosophy of the 20th century.Leibniz met Spinoza in 1676, read some of his unpublished writings, and has since been suspected of appropriating some of Spinoza's ideas. While Leibniz admired Spinoza's powerful intellect, he was also forthrightly dismayed by Spinoza's conclusions,[52] especially when these were inconsistent with Christian orthodoxy.Leibniz dated his beginning as a philosopher to his Discourse on Metaphysics, which he composed in 1686 as a commentary on a running dispute between Nicolas Malebranche and Antoine Arnauld. This led to an extensive and valuable correspondence with Arnauld;[50] it and the Discourse were not published until the 19th century. In 1695, Leibniz made his public entrée into European philosophy with a journal article titled "New System of the Nature and Communication of Substances".[51] Between 1695 and 1705, he composed his New Essays on Human Understanding, a lengthy commentary on John Locke's 1690 An Essay Concerning Human Understanding, but upon learning of Locke's 1704 death, lost the desire to publish it, so that the New Essays were not published until 1765. The Monadologie, composed in 1714 and published posthumously, consists of 90 aphorisms.Leibniz's philosophical thinking appears fragmented, because his philosophical writings consist mainly of a multitude of short pieces: journal articles, manuscripts published long after his death, and many letters to many correspondents. He wrote only two book-length philosophical treatises, of which only the Théodicée of 1710 was published in his lifetime.Leibniz never married. He complained on occasion about money, but the fair sum he left to his sole heir, his sister's stepson, proved that the Brunswicks had, by and large, paid him well. In his diplomatic endeavors, he at times verged on the unscrupulous, as was all too often the case with professional diplomats of his day. On several occasions, Leibniz backdated and altered personal manuscripts, actions which put him in a bad light during the calculus controversy. On the other hand, he was charming, well-mannered, and not without humor and imagination.[43] He had many friends and admirers all over Europe. On Leibniz's religious views, though he was a protestant, Leibniz learned to appreciate the good sides of Catholicism through his patrons and colleagues. He never admitted the Protestant view of Pope as an Antichrist.[44] Leibniz was claimed as a philosophical theist.[45][46][47][48] Leibniz remained committed to Trinitarian Christianity throughout his life. [49][page needed]Leibniz died in Hanover in 1716: at the time, he was so out of favor that neither George I (who happened to be near Hanover at that time) nor any fellow courtier other than his personal secretary attended the funeral. Even though Leibniz was a life member of the Royal Society and the Berlin Academy of Sciences, neither organization saw fit to honor his passing. His grave went unmarked for more than 50 years. Leibniz was eulogized by Fontenelle, before the French Academy of Sciences in Paris, which had admitted him as a foreign member in 1700. The eulogy was composed at the behest of the Duchess of Orleans, a niece of the Electress Sophia.In 1711, while traveling in northern Europe, the Russian Tsar Peter the Great stopped in Hanover and met Leibniz, who then took some interest in Russian matters for the rest of his life. In 1712, Leibniz began a two-year residence in Vienna, where he was appointed Imperial Court Councillor to the Habsburgs. On the death of Queen Anne in 1714, Elector George Louis became King George I of Great Britain, under the terms of the 1701 Act of Settlement. Even though Leibniz had done much to bring about this happy event, it was not to be his hour of glory. Despite the intercession of the Princess of Wales, Caroline of Ansbach, George I forbade Leibniz to join him in London until he completed at least one volume of the history of the Brunswick family his father had commissioned nearly 30 years earlier. Moreover, for George I to include Leibniz in his London court would have been deemed insulting to Newton, who was seen as having won the calculus priority dispute and whose standing in British official circles could not have been higher. Finally, his dear friend and defender, the Dowager Electress Sophia, died in 1714.In 1708, John Keill, writing in the journal of the Royal Society and with Newton's presumed blessing, accused Leibniz of having plagiarised Newton's calculus.[42] Thus began the calculus priority dispute which darkened the remainder of Leibniz's life. A formal investigation by the Royal Society (in which Newton was an unacknowledged participant), undertaken in response to Leibniz's demand for a retraction, upheld Keill's charge. Historians of mathematics writing since 1900 or so have tended to acquit Leibniz, pointing to important differences between Leibniz's and Newton's versions of calculus.Leibniz was appointed Librarian of the Herzog August Library in Wolfenbüttel, Lower Saxony, in 1691.The Elector Ernest Augustus commissioned Leibniz to write a history of the House of Brunswick, going back to the time of Charlemagne or earlier, hoping that the resulting book would advance his dynastic ambitions. From 1687 to 1690, Leibniz traveled extensively in Germany, Austria, and Italy, seeking and finding archival materials bearing on this project. Decades went by but no history appeared; the next Elector became quite annoyed at Leibniz's apparent dilatoriness. Leibniz never finished the project, in part because of his huge output on many other fronts, but also because he insisted on writing a meticulously researched and erudite book based on archival sources, when his patrons would have been quite happy with a short popular book, one perhaps little more than a genealogy with commentary, to be completed in three years or less. They never knew that he had in fact carried out a fair part of his assigned task: when the material Leibniz had written and collected for his history of the House of Brunswick was finally published in the 19th century, it filled three volumes.The Brunswicks tolerated the enormous effort Leibniz devoted to intellectual pursuits unrelated to his duties as a courtier, pursuits such as perfecting calculus, writing about other mathematics, logic, physics, and philosophy, and keeping up a vast correspondence. He began working on calculus in 1674; the earliest evidence of its use in his surviving notebooks is 1675. By 1677 he had a coherent system in hand, but did not publish it until 1684. Leibniz's most important mathematical papers were published between 1682 and 1692, usually in a journal which he and Otto Mencke founded in 1682, the Acta Eruditorum. That journal played a key role in advancing his mathematical and scientific reputation, which in turn enhanced his eminence in diplomacy, history, theology, and philosophy.The population of Hanover was only about 10,000, and its provinciality eventually grated on Leibniz. Nevertheless, to be a major courtier to the House of Brunswick was quite an honor, especially in light of the meteoric rise in the prestige of that House during Leibniz's association with it. In 1692, the Duke of Brunswick became a hereditary Elector of the Holy Roman Empire. The British Act of Settlement 1701 designated the Electress Sophia and her descent as the royal family of England, once both King William III and his sister-in-law and successor, Queen Anne, were dead. Leibniz played a role in the initiatives and negotiations leading up to that Act, but not always an effective one. For example, something he published anonymously in England, thinking to promote the Brunswick cause, was formally censured by the British Parliament.Among the few people in north Germany to accept Leibniz were the Electress Sophia of Hanover (1630–1714), her daughter Sophia Charlotte of Hanover (1668–1705), the Queen of Prussia and his avowed disciple, and Caroline of Ansbach, the consort of her grandson, the future George II. To each of these women he was correspondent, adviser, and friend. In turn, they all approved of Leibniz more than did their spouses and the future king George I of Great Britain.[41]Leibniz began promoting a project to use windmills to improve the mining operations in the Harz Mountains. This project did little to improve mining operations and was shut down by Duke Ernst August in 1685. [40]In 1677, he was promoted, at his request, to Privy Counselor of Justice, a post he held for the rest of his life. Leibniz served three consecutive rulers of the House of Brunswick as historian, political adviser, and most consequentially, as librarian of the ducal library. He thenceforth employed his pen on all the various political, historical, and theological matters involving the House of Brunswick; the resulting documents form a valuable part of the historical record for the period.Leibniz managed to delay his arrival in Hanover until the end of 1676 after making one more short journey to London, where Newton accused him of having seen Newton's unpublished work on calculus in advance.[38] This was alleged to be evidence supporting the accusation, made decades later, that he had stolen calculus from Newton. On the journey from London to Hanover, Leibniz stopped in The Hague where he met van Leeuwenhoek, the discoverer of microorganisms. He also spent several days in intense discussion with Spinoza, who had just completed his masterwork, the Ethics.[39]In 1675 he tried to get admitted to the French Academy of Sciences as a foreign honorary member, but it was considered that there were already enough foreigners there and so no invitation came. He left Paris in October 1676.In this regard, a 1669 invitation from the John Frederick of Brunswick to visit Hanover proved to have been fateful. Leibniz had declined the invitation, but had begun corresponding with the duke in 1671. In 1673, the duke offered Leibniz the post of counsellor. Leibniz very reluctantly accepted the position two years later, only after it became clear that no employment in Paris, whose intellectual stimulation he relished, or with the Habsburg imperial court, was forthcoming.[37]The mission ended abruptly when news of the Elector's death (12 February 1673) reached them. Leibniz promptly returned to Paris and not, as had been planned, to Mainz.[36] The sudden deaths of his two patrons in the same winter meant that Leibniz had to find a new basis for his career.When it became clear that France would not implement its part of Leibniz's Egyptian plan, the Elector sent his nephew, escorted by Leibniz, on a related mission to the English government in London, early in 1673.[35] There Leibniz came into acquaintance of Henry Oldenburg and John Collins. He met with the Royal Society where he demonstrated a calculating machine that he had designed and had been building since 1670. The machine was able to execute all four basic operations (adding, subtracting, multiplying, and dividing), and the society quickly made him an external member.Thus Leibniz went to Paris in 1672. Soon after arriving, he met Dutch physicist and mathematician Christiaan Huygens and realised that his own knowledge of mathematics and physics was patchy. With Huygens as his mentor, he began a program of self-study that soon pushed him to making major contributions to both subjects, including discovering his version of the differential and integral calculus. He met Nicolas Malebranche and Antoine Arnauld, the leading French philosophers of the day, and studied the writings of Descartes and Pascal, unpublished as well as published.[34] He befriended a German mathematician, Ehrenfried Walther von Tschirnhaus; they corresponded for the rest of their lives.Von Boyneburg did much to promote Leibniz's reputation, and the latter's memoranda and letters began to attract favorable notice. After Leibniz's service to the Elector there soon followed a diplomatic role. He published an essay, under the pseudonym of a fictitious Polish nobleman, arguing (unsuccessfully) for the German candidate for the Polish crown. The main force in European geopolitics during Leibniz's adult life was the ambition of Louis XIV of France, backed by French military and economic might. Meanwhile, the Thirty Years' War had left German-speaking Europe exhausted, fragmented, and economically backward. Leibniz proposed to protect German-speaking Europe by distracting Louis as follows. France would be invited to take Egypt as a stepping stone towards an eventual conquest of the Dutch East Indies. In return, France would agree to leave Germany and the Netherlands undisturbed. This plan obtained the Elector's cautious support. In 1672, the French government invited Leibniz to Paris for discussion,[33] but the plan was soon overtaken by the outbreak of the Franco-Dutch War and became irrelevant. Napoleon's failed invasion of Egypt in 1798 can be seen as an unwitting, late implementation of Leibniz's plan, after the Eastern hemisphere colonial supremacy in Europe had already passed from the Dutch to the British.[citation needed]Leibniz's first position was as a salaried secretary to an alchemical society in Nuremberg.[30] He knew fairly little about the subject at that time but presented himself as deeply learned. He soon met Johann Christian von Boyneburg (1622–1672), the dismissed chief minister of the Elector of Mainz, Johann Philipp von Schönborn.[31] Von Boyneburg hired Leibniz as an assistant, and shortly thereafter reconciled with the Elector and introduced Leibniz to him. Leibniz then dedicated an essay on law to the Elector in the hope of obtaining employment. The stratagem worked; the Elector asked Leibniz to assist with the redrafting of the legal code for the Electorate.[32] In 1669, Leibniz was appointed assessor in the Court of Appeal. Although von Boyneburg died late in 1672, Leibniz remained under the employment of his widow until she dismissed him in 1674.[citation needed]As an adult, Leibniz often introduced himself as "Gottfried von Leibniz". Many posthumously published editions of his writings presented his name on the title page as "Freiherr G. W. von Leibniz." However, no document has ever been found from any contemporary government that stated his appointment to any form of nobility.[29]Leibniz then enrolled in the University of Altdorf and quickly submitted a thesis, which he had probably been working on earlier in Leipzig.[27] The title of his thesis was Disputatio Inauguralis de Casibus Perplexis in Jure (Inaugural Disputation on Ambiguous Legal Cases).[21] Leibniz earned his license to practice law and his Doctorate in Law in November 1666. He next declined the offer of an academic appointment at Altdorf, saying that "my thoughts were turned in an entirely different direction".[28]In early 1666, at age 19, Leibniz wrote his first book, De Arte Combinatoria (On the Combinatorial Art), the first part of which was also his habilitation thesis in Philosophy, which he defended in March 1666.[21][23] His next goal was to earn his license and Doctorate in Law, which normally required three years of study. In 1666, the University of Leipzig turned down Leibniz's doctoral application and refused to grant him a Doctorate in Law, most likely due to his relative youth.[24][25] Leibniz subsequently left Leipzig.[26]In April 1661 he enrolled in his father's former university at age 15,[1][20] and completed his bachelor's degree in Philosophy in December 1662. He defended his Disputatio Metaphysica de Principio Individui (Metaphysical Disputation on the Principle of Individuation),[21] which addressed the principle of individuation, on 9 June 1663. Leibniz earned his master's degree in Philosophy on 7 February 1664. He published and defended a dissertation Specimen Quaestionum Philosophicarum ex Jure collectarum (An Essay of Collected Philosophical Problems of Right),[21] arguing for both a theoretical and a pedagogical relationship between philosophy and law, in December 1664. After one year of legal studies, he was awarded his bachelor's degree in Law on 28 September 1665.[22] His dissertation was titled De conditionibus (On Conditions).[21]Leibniz's father had been a Professor of Moral Philosophy at the University of Leipzig, and the boy later inherited his father's personal library. He was given free access to it from the age of seven. While Leibniz's schoolwork was largely confined to the study of a small canon of authorities, his father's library enabled him to study a wide variety of advanced philosophical and theological works—ones that he would not have otherwise been able to read until his college years.[18] Access to his father's library, largely written in Latin, also led to his proficiency in the Latin language, which he achieved by the age of 12. He also composed 300 hexameters of Latin verse, in a single morning, for a special event at school at the age of 13.[19]Leibniz was baptized on 3 July of that year at St. Nicholas Church, Leipzig; his godfather was the Lutheran theologian Martin Geier (de).[16] His father died when he was six years old, and from that point on he was raised by his mother.[17]In English:Gottfried Leibniz was born on 1 July 1646, toward the end of the Thirty Years' War, in Leipzig, Saxony, to Friedrich Leibniz and Catharina Schmuck. Friedrich noted in his family journal:Leibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in philosophy, probability theory, biology, medicine, geology, psychology, linguistics, and computer science. He wrote works on philosophy, politics, law, ethics, theology, history, and philology. Leibniz also contributed to the field of library science. While serving as overseer of the Wolfenbüttel library in Germany, he devised a cataloging system that would serve as a guide for many of Europe's largest libraries.[11] Leibniz's contributions to this vast array of subjects were scattered in various learned journals, in tens of thousands of letters, and in unpublished manuscripts. He wrote in several languages, but primarily in Latin, French, and German.[12] There is no complete gathering of the writings of Leibniz translated into English.[13]In philosophy, Leibniz is most noted for his optimism, i.e. his conclusion that our Universe is, in a restricted sense, the best possible one that God could have created, an idea that was often lampooned by others such as Voltaire. Leibniz, along with René Descartes and Baruch Spinoza, was one of the three great 17th-century advocates of rationalism. The work of Leibniz anticipated modern logic and analytic philosophy, but his philosophy also looks back to the scholastic tradition, in which conclusions are produced by applying reason to first principles or prior definitions rather than to empirical evidence.Gottfried Wilhelm (von) Leibniz (/ˈlaɪbnɪts/;[5] German: [ˈɡɔtfʁiːt ˈvɪlhɛlm fɔn ˈlaɪbnɪts][6] or [ˈlaɪpnɪts];[7] French: Godefroi Guillaume Leibnitz;[8] 1 July 1646 [O.S. 21 June] – 14 November 1716) was a German polymath and philosopher who occupies a prominent place in the history of mathematics and the history of philosophy, having developed differential and integral calculus independently of Isaac Newton.[9] Leibniz's notation has been widely used ever since it was published. It was only in the 20th century that his Law of Continuity and Transcendental Law of Homogeneity found mathematical implementation (by means of non-standard analysis). He became one of the most prolific inventors in the field of mechanical calculators. While working on adding automatic multiplication and division to Pascal's calculator, he was the first to describe a pinwheel calculator in 1685[10] and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, which is the foundation of virtually all digital computers.
Gabriel Cramer
He did extensive travel throughout Europe in the late 1730s, which greatly influenced his works in mathematics. He died in 1752 at Bagnols-sur-Cèze while traveling in southern France to restore his health.In 1750 he published Cramer's rule, giving a general formula for the solution for any unknown in a linear equation system having a unique solution, in terms of determinants implied by the system. This rule is still standard.He edited the works of the two elder Bernoullis, and wrote on the physical cause of the spheroidal shape of the planets and the motion of their apsides (1730), and on Newton's treatment of cubic curves (1746).He published his best-known work in his forties. This included his treatise on algebraic curves (1750). It contains the earliest demonstration that a curve of the n-th degree is determined by n(n + 3)/2 points on it, in general position. (See Cramer's theorem (algebraic curves).) This led to the misconception that is Cramer's paradox, concerning the number of intersections of two curves compared to the number of points that determine a curve.In 1728 he proposed a solution to the St. Petersburg Paradox that came very close to the concept of expected utility theory given ten years later by Daniel Bernoulli.Cramer showed promise in mathematics from an early age. At 18 he received his doctorate and at 20 he was co-chair[1] of mathematics at the University of Geneva.Gabriel Cramer (French: [kʁamɛʁ]; 31 July 1704 – 4 January 1752) was a Genevan mathematician. He was the son of physician Jean Cramer and Anne Mallet Cramer.
Cramer's rule

Carl Friedrich Gauss
Gauss's collective works are online at dz-srv1.sub.uni-goettingen.de Uni-goettingen.de includes German translations of Latin texts and commentaries by various authorities.In 1929 the Polish mathematician Marian Rejewski, who helped to solve the German Enigma cipher machine in December 1932, began studying actuarial statistics at Göttingen. At the request of his Poznań University professor, Zdzisław Krygowski, on arriving at Göttingen Rejewski laid flowers on Gauss's grave.[69]Things named in honor of Gauss include:In 2007 a bust of Gauss was placed in the Walhalla temple.[67]Daniel Kehlmann's 2005 novel Die Vermessung der Welt, translated into English as Measuring the World (2006), explores Gauss's life and work through a lens of historical fiction, contrasting them with those of the German explorer Alexander von Humboldt. A film version directed by Detlev Buck was released in 2012.[66]From 1989 through 2001, Gauss's portrait, a normal distribution curve and some prominent Göttingen buildings were featured on the German ten-mark banknote. The reverse featured the approach for Hanover. Germany has also issued three postage stamps honoring Gauss. One (no. 725) appeared in 1955 on the hundredth anniversary of his death; two others, nos. 1246 and 1811, in 1977, the 200th anniversary of his birth.He referred to mathematics as "the queen of sciences"[64] and supposedly once espoused a belief in the necessity of immediately understanding Euler's identity as a benchmark pursuant to becoming a first-class mathematician.[65]According to Isaac Asimov, Gauss was once interrupted in the middle of a problem and told that his wife was dying. He is purported to have said, "Tell her to wait a moment till I'm done."[63] This anecdote is briefly discussed in G. Waldo Dunnington's Gauss, Titan of Science where it is suggested that it is an apocryphal story.Gauss's presumed method was to realize that pairwise addition of terms from opposite ends of the list yielded identical intermediate sums: 1 + 100 = 101, 2 + 99 = 101, 3 + 98 = 101, and so on, for a total sum of 50 × 101 = 5050. However, the details of the story are at best uncertain (see[8] for discussion of the original Wolfgang Sartorius von Waltershausen source and the changes in other versions); some authors, such as Joseph Rotman in his book A first course in Abstract Algebra, question whether it ever happened.Another story has it that in primary school after the young Gauss misbehaved, his teacher, J.G. Büttner, gave him a task: add a list of integers in arithmetic progression; as the story is most often told, these were the numbers from 1 to 100. The young Gauss reputedly produced the correct answer within seconds, to the astonishment of his teacher and his assistant Martin Bartels.There are several stories of his early genius. According to one, his gifts became very apparent at the age of three when he corrected, mentally and without fault in his calculations, an error his father had made on paper while calculating finances.The British mathematician Henry John Stephen Smith (1826–1883) gave the following appraisal of Gauss:In 1821, he was made a foreign member of the Royal Swedish Academy of Sciences. Gauss was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1822.[61]That is, curvature does not depend on how the surface might be embedded in 3-dimensional space or 2-dimensional space.The geodetic survey of Hanover, which required Gauss to spend summers traveling on horseback for a decade,[60] fueled Gauss's interest in differential geometry and topology, fields of mathematics dealing with curves and surfaces. Among other things, he came up with the notion of Gaussian curvature. This led in 1828 to an important theorem, the Theorema Egregium (remarkable theorem), establishing an important property of the notion of curvature. Informally, the theorem says that the curvature of a surface can be determined entirely by measuring angles and distances on the surface.Letters from Gauss years before 1829 reveal him obscurely discussing the problem of parallel lines. Waldo Dunnington, a biographer of Gauss, argues in Gauss, Titan of Science that Gauss was in fact in full possession of non-Euclidean geometry long before it was published by Bolyai, but that he refused to publish any of it because of his fear of controversy.[58][59]This unproved statement put a strain on his relationship with Bolyai who thought that Gauss was "stealing" his idea.[57]Bolyai's son, János Bolyai, discovered non-Euclidean geometry in 1829; his work was published in 1832. After seeing it, Gauss wrote to Farkas Bolyai: "To praise it would amount to praising myself. For the entire content of the work ... coincides almost exactly with my own meditations which have occupied my mind for the past thirty or thirty-five years."Research on these geometries led to, among other things, Einstein's theory of general relativity, which describes the universe as non-Euclidean. His friend Farkas Wolfgang Bolyai with whom Gauss had sworn "brotherhood and the banner of truth" as a student, had tried in vain for many years to prove the parallel postulate from Euclid's other axioms of geometry.Gauss also claimed to have discovered the possibility of non-Euclidean geometries but never published it. This discovery was a major paradigm shift in mathematics, as it freed mathematicians from the mistaken belief that Euclid's axioms were the only way to make geometry consistent and non-contradictory.In 1818 Gauss, putting his calculation skills to practical use, carried out a geodetic survey of the Kingdom of Hanover, linking up with previous Danish surveys. To aid the survey, Gauss invented the heliotrope, an instrument that uses a mirror to reflect sunlight over great distances, to measure positions.Gauss proved the method under the assumption of normally distributed errors (see Gauss–Markov theorem; see also Gaussian). The method had been described earlier by Adrien-Marie Legendre in 1805, but Gauss claimed that he had been using it since 1794 or 1795.[55] In the history of statistics, this disagreement is called the "priority dispute over the discovery of the method of least squares."[56]The discovery of Ceres led Gauss to his work on a theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as Theoria motus corporum coelestium in sectionibus conicis solem ambientum (Theory of motion of the celestial bodies moving in conic sections around the Sun). In the process, he so streamlined the cumbersome mathematics of 18th-century orbital prediction that his work remains a cornerstone of astronomical computation.[citation needed] It introduced the Gaussian gravitational constant, and contained an influential treatment of the method of least squares, a procedure used in all sciences to this day to minimize the impact of measurement error.Zach noted that "without the intelligent work and calculations of Doctor Gauss we might not have found Ceres again". Though Gauss had up to that point been financially supported by his stipend from the Duke, he doubted the security of this arrangement, and also did not believe pure mathematics to be important enough to deserve support. Thus he sought a position in astronomy, and in 1807 was appointed Professor of Astronomy and Director of the astronomical observatory in Göttingen, a post he held for the remainder of his life.One such method was the fast Fourier transform. While this method is traditionally attributed to a 1965 paper by J. W. Cooley and J. W. Tukey, Gauss developed it as a trigonometric interpolation method. His paper, Theoria Interpolationis Methodo Nova Tractata,[53] was only published posthumously in Volume 3 of his collected works. This paper predates the first presentation by Joseph Fourier on the subject in 1807.[54]Gauss's method involved determining a conic section in space, given one focus (the Sun) and the conic's intersection with three given lines (lines of sight from the Earth, which is itself moving on an ellipse, to the planet) and given the time it takes the planet to traverse the arcs determined by these lines (from which the lengths of the arcs can be calculated by Kepler's Second Law). This problem leads to an equation of the eighth degree, of which one solution, the Earth's orbit, is known. The solution sought is then separated from the remaining six based on physical conditions. In this work, Gauss used comprehensive approximation methods which he created for that purpose.[52]Gauss, who was 24 at the time, heard about the problem and tackled it. After three months of intense work, he predicted a position for Ceres in December 1801—just about a year after its first sighting—and this turned out to be accurate within a half-degree when it was rediscovered by Franz Xaver von Zach on 31 December at Gotha, and one day later by Heinrich Olbers in Bremen.In the same year, Italian astronomer Giuseppe Piazzi discovered the dwarf planet Ceres. Piazzi could only track Ceres for somewhat more than a month, following it for three degrees across the night sky. Then it disappeared temporarily behind the glare of the Sun. Several months later, when Ceres should have reappeared, Piazzi could not locate it: the mathematical tools of the time were not able to extrapolate a position from such a scant amount of data—three degrees represent less than 1% of the total orbit.Gauss also made important contributions to number theory with his 1801 book Disquisitiones Arithmeticae (Latin, Arithmetical Investigations), which, among other things, introduced the symbol ≡ for congruence and used it in a clean presentation of modular arithmetic, contained the first two proofs of the law of quadratic reciprocity, developed the theories of binary and ternary quadratic forms, stated the class number problem for them, and showed that a regular heptadecagon (17-sided polygon) can be constructed with straightedge and compass.In his 1799 doctorate in absentia, A new proof of the theorem that every integral rational algebraic function of one variable can be resolved into real factors of the first or second degree, Gauss proved the fundamental theorem of algebra which states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. Mathematicians including Jean le Rond d'Alembert had produced false proofs before him, and Gauss's dissertation contains a critique of d'Alembert's work. Ironically, by today's standard, Gauss's own attempt is not acceptable, owing to the implicit use of the Jordan curve theorem. However, he subsequently produced three other proofs, the last one in 1849 being generally rigorous. His attempts clarified the concept of complex numbers considerably along the way.Gauss summarized his views on the pursuit of knowledge in a letter to Farkas Bolyai dated 2 September 1808 as follows:Gauss supported the monarchy and opposed Napoleon, whom he saw as an outgrowth of revolution.Gauss usually declined to present the intuition behind his often very elegant proofs—he preferred them to appear "out of thin air" and erased all traces of how he discovered them.[citation needed] This is justified, if unsatisfactorily, by Gauss in his Disquisitiones Arithmeticae, where he states that all analysis (i.e., the paths one traveled to reach the solution of a problem) must be suppressed for sake of brevity.Before she died, Sophie Germain was recommended by Gauss to receive her honorary degree; she never received it.[50]On Gauss's recommendation, Friedrich Bessel was awarded an honorary doctor degree from Göttingen in March 1811.[47] Around that time, the two men engaged in an epistolary correspondence.[48] However, when they met in person in 1825, they quarrelled; the details are unknown.[49]Though he did take in a few students, Gauss was known to dislike teaching. It is said that he attended only a single scientific conference, which was in Berlin in 1828. However, several of his students became influential mathematicians, among them Richard Dedekind and Bernhard Riemann.Carl Gauss was an ardent perfectionist and a hard worker. He was never a prolific writer, refusing to publish work which he did not consider complete and above criticism. This was in keeping with his personal motto pauca sed matura ("few, but ripe"). His personal diaries indicate that he had made several important mathematical discoveries years or decades before his contemporaries published them. Scottish-American mathematician and writer Eric Temple Bell said that if Gauss had published all of his discoveries in a timely manner, he would have advanced mathematics by fifty years.[46]Gauss eventually had conflicts with his sons. He did not want any of his sons to enter mathematics or science for "fear of lowering the family name", as he believed none of them would surpass his own achievements.[45] Gauss wanted Eugene to become a lawyer, but Eugene wanted to study languages. They had an argument over a party Eugene held, which Gauss refused to pay for. The son left in anger and, in about 1832, emigrated to the United States, where he was quite successful. While working for the American Fur Company in the Midwest, he learned the Sioux language. Later, he moved to Missouri and became a successful businessman. Wilhelm also moved to America in 1837 and settled in Missouri, starting as a farmer and later becoming wealthy in the shoe business in St. Louis. It took many years for Eugene's success to counteract his reputation among Gauss's friends and colleagues. See also the letter from Robert Gauss to Felix Klein on 3 September 1912.Gauss had six children. With Johanna (1780–1809), his children were Joseph (1806–1873), Wilhelmina (1808–1846) and Louis (1809–1810). With Minna Waldeck he also had three children: Eugene (1811–1896), Wilhelm (1813–1879) and Therese (1816–1864). Eugene shared a good measure of Gauss's talent in languages and computation.[45] Therese kept house for Gauss until his death, after which she married.Gauss's personal life was overshadowed by the early death of his first wife, Johanna Osthoff, in 1809, soon followed by the death of one child, Louis. Gauss plunged into a depression from which he never fully recovered. He married again, to Johanna's best friend, Friederica Wilhelmine Waldeck, commonly known as Minna. When his second wife died in 1831 after a long illness,[44] one of his daughters, Therese, took over the household and cared for Gauss for the rest of his life. His mother lived in his house from 1817 until her death in 1839.[2]Though he was not a church-goer,[42] Gauss strongly upheld religious tolerance, believing "that one is not justified in disturbing another's religious belief, in which they find consolation for earthly sorrows in time of trouble."[2] When his son Eugene announced that he wanted to become a Christian missionary, Gauss approved of this, saying that regardless of the problems within religious organizations, missionary work was "a highly honorable" task.[43]Gauss declared he firmly believed in the afterlife, and saw spirituality as something essentially important for human beings.[39] He was quoted stating: "The world would be nonsense, the whole creation an absurdity without immortality,"[40] and for this statement he was severely criticized by the atheist Eugen Dühring who judged him as a narrow superstitious man.[41]Dunnington further elaborates on Gauss's religious views by writing:In connection to this, there is a record of a conversation between Rudolf Wagner and Gauss, in which they discussed William Whewell's book Of the Plurality of Worlds. In this work, Whewell had discarded the possibility of existing life in other planets, on the basis of theological arguments, but this was a position with which both Wagner and Gauss disagreed. Later Wagner explained that he did not fully believe in the Bible, though he confessed that he "envied" those who were able to easily believe.[31][35] This later led them to discuss the topic of faith, and in some other religious remarks, Gauss said that he had been more influenced by theologians like Lutheran minister Paul Gerhardt than by Moses.[36] Other religious influences included Wilhelm Braubach, Johann Peter Süssmilch, and the New Testament.[37]Apart from his correspondence, there are not many known details about Gauss' personal creed. Many biographers of Gauss disagree about his religious stance, with Bühler and others considering him a deist with very unorthodox views,[31][32][33] while Dunnington (though admitting that Gauss did not believe literally in all Christian dogmas and that it is unknown what he believed on most doctrinal and confessional questions) points out that he was, at least, a nominal Lutheran.[34]Gauss was a Lutheran Protestant, a member of the St. Albans Evangelical Lutheran church in Göttingen.[28] Potential evidence that Gauss believed in God comes from his response after solving a problem that had previously defeated him: "Finally, two days ago, I succeeded— not on account of my hard efforts, but by the grace of the Lord."[29] One of his biographers, G. Waldo Dunnington, described Gauss's religious views as follows:On 23 February 1855, Gauss died of a heart attack in Göttingen (then Kingdom of Hanover and now Lower Saxony);[3][18] he is interred in the Albani Cemetery there. Two people gave eulogies at his funeral: Gauss's son-in-law Heinrich Ewald, and Wolfgang Sartorius von Waltershausen, who was Gauss's close friend and biographer. Gauss's brain was preserved and was studied by Rudolf Wagner, who found its mass to be slightly above average, at 1,492 grams, and the cerebral area equal to 219,588 square millimeters[26] (340.362 square inches). Highly developed convolutions were also found, which in the early 20th century were suggested as the explanation of his genius.[27]In 1854, Gauss selected the topic for Bernhard Riemann's Habilitationsvortrag, "Über die Hypothesen, welche der Geometrie zu Grunde liegen" (habilitation lecture About the hypotheses that underlie Geometry).[24] On the way home from Riemann's lecture, Weber reported that Gauss was full of praise and excitement.[25]In 1845, he became an associated member of the Royal Institute of the Netherlands; when that became the Royal Netherlands Academy of Arts and Sciences in 1851, he joined as a foreign member.[23]In 1840, Gauss published his influential Dioptrische Untersuchungen,[19] in which he gave the first systematic analysis on the formation of images under a paraxial approximation (Gaussian optics).[20] Among his results, Gauss showed that under a paraxial approximation an optical system can be characterized by its cardinal points[21] and he derived the Gaussian lens formula.[22]Gauss remained mentally active into his old age, even while suffering from gout and general unhappiness.[18] For example, at the age of 62, he taught himself Russian.[18]In 1831, Gauss developed a fruitful collaboration with the physics professor Wilhelm Weber, leading to new knowledge in magnetism (including finding a representation for the unit of magnetism in terms of mass, charge, and time) and the discovery of Kirchhoff's circuit laws in electricity.[18] It was during this time that he formulated his namesake law. They constructed the first electromechanical telegraph in 1833,[18] which connected the observatory with the institute for physics in Göttingen. Gauss ordered a magnetic observatory to be built in the garden of the observatory, and with Weber founded the "Magnetischer Verein" (magnetic club in German), which supported measurements of Earth's magnetic field in many regions of the world. He developed a method of measuring the horizontal intensity of the magnetic field which was in use well into the second half of the 20th century, and worked out the mathematical theory for separating the inner and outer (magnetospheric) sources of Earth's magnetic field.On 9 October 1805,[15] Gauss married Johanna Osthoff (1780-1809), and had a son and a daughter with her.[15][16] Johanna died on 11 October 1809,[15][16][17] and her most recent child, Louis, died the following year.[15] He then married Minna Waldeck (1788-1831)[15][16] on 4 August 1810,[15] and had three more children.[16] Gauss was never quite the same without his first wife, so he grew to dominate his children, just like his father.[16] Minna Waldeck died on 12 September 1831.[15][16]In 1801, Gauss announced that he had calculated the orbit of an asteroid by the name of Ceres.[10] He also allowed some of his genius to be made public with the publication of Disquisitiones Arithmeticae,[10] and consequently gained widespread fame.[10]Gauss also discovered that every positive integer is representable as a sum of at most three triangular numbers on 10 July and then jotted down in his diary the note: "ΕΥΡΗΚΑ! num = Δ + Δ' + Δ". On 1 October he published a result on the number of solutions of polynomials with coefficients in finite fields, which 150 years later led to the Weil conjectures.The year 1796 was most productive for both Gauss and number theory. He discovered a construction of the heptadecagon on 30 March.[10][14] He further advanced modular arithmetic, greatly simplifying manipulations in number theory. On 8 April he became the first to prove the quadratic reciprocity law. This remarkably general law allows mathematicians to determine the solvability of any quadratic equation in modular arithmetic. The prime number theorem, conjectured on 31 May, gives a good understanding of how the prime numbers are distributed among the integers.Gauss's intellectual abilities attracted the attention of the Duke of Brunswick,[6][2] who sent him to the Collegium Carolinum (now Braunschweig University of Technology),[6] which he attended from 1792 to 1795,[9] and to the University of Göttingen from 1795 to 1798.[10] While at university, Gauss independently rediscovered several important theorems.[11] His breakthrough occurred in 1796 when he showed that a regular polygon can be constructed by compass and straightedge if the number of its sides is the product of distinct Fermat primes and a power of 2.[12] This was a major discovery in an important field of mathematics; construction problems had occupied mathematicians since the days of the Ancient Greeks, and the discovery ultimately led Gauss to choose mathematics instead of philology as a career. Gauss was so pleased with this result that he requested that a regular heptadecagon be inscribed on his tombstone. The stonemason declined, stating that the difficult construction would essentially look like a circle.[13]Gauss was a child prodigy. A contested story relates that, when he was eight, he figured out how to add up all the numbers from 1 to 100.[6][7][8] There are many other anecdotes about his precocity while a toddler, and he made his first ground-breaking mathematical discoveries while still a teenager. He completed his magnum opus, Disquisitiones Arithmeticae, in 1798, at the age of 21—though it was not published until 1801. This work was fundamental in consolidating number theory as a discipline and has shaped the field to the present day.Johann Carl Friedrich Gauss was born on 30 April 1777 in Brunswick (Braunschweig), in the Duchy of Brunswick-Wolfenbüttel (now part of Lower Saxony, Germany), to poor, working-class parents.[3] His mother was illiterate and never recorded the date of his birth, remembering only that he had been born on a Wednesday, eight days before the Feast of the Ascension (which occurs 39 days after Easter). Gauss later solved this puzzle about his birthdate in the context of finding the date of Easter, deriving methods to compute the date in both past and future years.[4] He was christened and confirmed in a church near the school he attended as a child.[5]Sometimes referred to as the Princeps mathematicorum[1] (Latin for "the foremost of mathematicians") and "the greatest mathematician since antiquity", Gauss had an exceptional influence in many fields of mathematics and science, and is ranked among history's most influential mathematicians.[2]
Gaussian elimination
Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]The Gaussian elimination can be performed over any field, not just the real numbers.One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 − 5n)/6 multiplications, and (2n3 + 3n2 − 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).All of this applies also to the reduced row echelon form, which is a particular row echelon form.One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:For example, consider the following matrixA variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.Computationally, for a n×n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = ∏diag(B) / d.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Suppose the goal is to find and describe the set of solutions to the following system of linear equations:A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.There are three types of elementary row operations which may be performed on the rows of a matrix:Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
Geodesy
Techniques for studying geodynamic phenomena on the global scale include:The science of studying deformations and motions of the Earth's crust and the solid Earth as a whole is called geodynamics. Often, study of the Earth's irregular rotation is also included in its definition.In geodesy, temporal change can be studied by a variety of techniques. Points on the Earth's surface change their location due to a variety of mechanisms:A metre was originally defined as the 10-millionth part of the length from equator to North Pole along the meridian through Paris (the target was not quite reached in actual implementation, so that is off by 200 ppm in the current definitions). This means that one kilometre is roughly equal to (1/40,000) * 360 * 60 meridional minutes of arc, which equals 0.54 nautical mile, though this is not exact because the two units are defined on different bases (the international nautical mile is defined as exactly 1,852 m, corresponding to a rounding of 1,000/0.54 m to four digits).One geographical mile, defined as one minute of arc on the equator, equals 1,855.32571922 m. One nautical mile is one minute of astronomical latitude. The radius of curvature of the ellipsoid varies with latitude, being the longest at the pole and the shortest at the equator as is the nautical mile.Geographical latitude and longitude are stated in the units degree, minute of arc, and second of arc. They are angles, not metric measures, and describe the direction of the local normal to the reference ellipsoid of revolution. This is approximately the same as the direction of the plumbline, i.e., local gravity, which is also the normal to the geoid surface. For this reason, astronomical position determination – measuring the direction of the plumbline by astronomical means – works fairly well provided an ellipsoidal model of the figure of the Earth is used.In the future gravity, and altitude, will be measured by relativistic time dilation measured by strontium optical clocks.Gravity is measured using gravimeters, of which there are two kinds. First, "absolute gravimeters" are based on measuring the acceleration of free fall (e.g., of a reflecting prism in a vacuum tube). They are used to establish the vertical geospatial control and can be used in the field. Second, "relative gravimeters" are spring-based and are more common. They are used in gravity surveys over large areas for establishing the figure of the geoid over these areas. The most accurate relative gravimeters are called "superconducting" gravimeters, which are sensitive to one-thousandth of one-billionth of Earth surface gravity. Twenty-some superconducting gravimeters are used worldwide for studying Earth's tides, rotation, interior, and ocean and atmospheric loading, as well as for verifying the Newtonian constant of gravitation.GPS receivers have almost completely replaced terrestrial instruments for large-scale base network surveys. For planet-wide geodetic surveys, previously impossible, we can still mention satellite laser ranging (SLR) and lunar laser ranging (LLR) and very-long-baseline interferometry (VLBI) techniques. All these techniques also serve to monitor irregularities in the Earth's rotation as well as plate tectonic motions.Geodetic GPS receivers produce directly three-dimensional coordinates in a geocentric coordinate frame. Such a frame is, e.g., WGS84, or the frames that are regularly produced and published by the International Earth Rotation and Reference Systems Service (IERS).For local detail surveys, tacheometers are commonly employed although the old-fashioned rectangular technique using angle prism and steel tape is still an inexpensive alternative. Real-time kinematic (RTK) GPS techniques are used as well. Data collected are tagged and recorded digitally for entry into a Geographic Information System (GIS) database.The theodolite is used to measure horizontal and vertical angles to target points. These angles are referred to the local vertical. The tacheometer additionally determines, electronically or electro-optically, the distance to target, and is highly automated to even robotic in its operations. The method of free station position is widely used.The level is used for determining height differences and height reference systems, commonly referred to mean sea level. The traditional spirit level produces these practically most useful heights above sea level directly; the more economical use of GPS instruments for height determination requires precise knowledge of the figure of the geoid, as GPS only gives heights above the GRS80 reference ellipsoid. As geoid knowledge accumulates, one may expect use of GPS heighting to spread.Here we define some basic observational concepts, like angles and coordinates, defined in geodesy (and astronomy as well), mostly from the viewpoint of the local observer.On the ellipsoid of revolution, geodesics may be written in terms of elliptic integrals, which are usually evaluated in terms of a series expansion—see, for example, Vincenty's formulae. In the general case, the solution is called the geodesic for the surface considered. The differential equations for the geodesic can be solved numerically.In plane geometry (valid for small areas on the Earth's surface), the solutions to both problems reduce to simple trigonometry. On a sphere, however, the solution is significantly more complex, because in the inverse problem the azimuths will differ between the two end points of the connecting great circle, arc.In geometric geodesy, two standard problems exist—the first (direct or forward) and the second (inverse or reverse).One purpose of point positioning is the provision of known points for mapping measurements, also known as (horizontal and vertical) control. In every country, thousands of such known points exist and are normally documented by national mapping agencies. Surveyors involved in real estate and insurance will use these to tie their local measurements.For surveying mappings, frequently Real Time Kinematic GPS is employed, tying in the unknown points with known terrestrial points close by in real time.Nowadays all but special measurements (e.g., underground or high-precision engineering measurements) are performed with GPS. The higher-order networks are measured with static GPS, using differential measurement to determine vectors between terrestrial points. These vectors are then adjusted in traditional network fashion. A global polyhedron of permanently operating GPS stations under the auspices of the IERS is used to define a single global, geocentric reference frame which serves as the "zero order" global reference to which national measurements are attached.Traditionally, a hierarchy of networks has been built to allow point positioning within a country. Highest in the hierarchy were triangulation networks. These were densified into networks of traverses (polygons), into which local mapping surveying measurements, usually with measuring tape, corner prism, and the familiar[where?] red and white poles, are tied.Point positioning is the determination of the coordinates of a point on land, at sea, or in space with respect to a coordinate system. Point position is solved by computation from measurements linking the known positions of terrestrial or extraterrestrial points with the unknown terrestrial position. This may involve transformations between or among astronomical and terrestrial coordinate systems. The known points used for point positioning can be triangulation points of a higher-order network or GPS satellites.In the abstract, a coordinate system as used in mathematics and geodesy is called a "coordinate system" in ISO terminology, whereas the International Earth Rotation and Reference Systems Service (IERS) uses the term "reference system". When these coordinates are realized by choosing datum points and fixing a geodetic datum, ISO says "coordinate reference system", while IERS says "reference frame". The ISO term for a datum transformation again is a "coordinate transformation".[3]Changing the coordinates of a point set referring to one datum, so to make them refer to another datum, is called a datum transformation. In the case of vertical data, this consists of simply adding a constant shift to all height values. In the case of plane or spatial coordinates, datum transformation takes the form of a similarity or Helmert transformation, consisting of a rotation and scaling operation in addition to a simple translation. In the plane, a Helmert transformation has four parameters; in space, seven.In case of plane or spatial coordinates, we typically need several datum points. A regional, ellipsoidal datum like ED 50 can be fixed by prescribing the undulation of the geoid and the deflection of the vertical in one datum point, in this case the Helmert Tower in Potsdam. However, an overdetermined ensemble of datum points can also be used.In the case of height data, it suffices to choose one datum point: the reference benchmark, typically a tide gauge at the shore. Thus we have vertical data like the NAP (Normaal Amsterdams Peil), the North American Vertical Datum 1988 (NAVD 88), the Kronstadt datum, the Trieste datum, and so on.Because geodetic point coordinates (and heights) are always obtained in a system that has been constructed itself using real observations, geodesists introduce the concept of a "geodetic datum": a physical realization of a coordinate system used for describing point locations. The realization is the result of choosing conventional coordinate values for one or more datum points.None of these heights is in any way related to geodetic or ellipsoidial heights, which express the height of a point above the reference ellipsoid. Satellite positioning receivers typically provide ellipsoidal heights, unless they are fitted with special conversion software based on a model of the geoid.Each has its advantages and disadvantages. Both orthometric and normal heights are heights in metres above sea level, whereas geopotential numbers are measures of potential energy (unit: m2 s−2) and not metric. Orthometric and normal heights differ in the precise way in which mean sea level is conceptually continued under the continental masses. The reference surface for orthometric heights is the geoid, an equipotential surface approximating mean sea level.Heights come in the following variants:In geodesy, point or terrain heights are "above sea level", an irregular, physically defined surface. Therefore, a height should ideally not be referred to as a coordinate. It is more like a physical quantity, and though it can be tempting to treat height as the vertical coordinate z, in addition to the horizontal coordinates x and y, and though this actually is a good approximation of physical reality in small areas, it quickly becomes invalid for regional considerations.[specify]The reverse transformation is given by:It is easy enough to "translate" between polar and rectangular coordinates in the plane: let, as above, direction and distance be α and s respectively, then we haveAn example of such a projection is UTM (Universal Transverse Mercator). Within the map plane, we have rectangular coordinates x and y. In this case the north direction used for reference is the map north, not the local north. The difference between the two is called meridian convergence.Rectangular coordinates in the plane can be used intuitively with respect to one's current location, in which case the x-axis will point to the local north. More formally, such coordinates can be obtained from three-dimensional coordinates using the artifice of a map projection. It is not possible to map the curved surface of the Earth onto a flat map surface without deformation. The compromise most often chosen—called a conformal projection—preserves angles and length ratios, so that small circles are mapped as small circles and small squares as squares.In surveying and mapping, important fields of application of geodesy, two general types of coordinate systems are used in the plane:The coordinate transformation between these two systems is described to good approximation by (apparent) sidereal time, which takes into account variations in the Earth's axial rotation (length-of-day variations). A more accurate description also takes polar motion into account, a phenomenon closely monitored by geodesists.Geocentric coordinate systems used in geodesy can be divided naturally into two classes:It is only because GPS satellites orbit about the geocenter, that this point becomes naturally the origin of a coordinate system defined by satellite geodetic means, as the satellite positions in space are themselves computed in such a system.Prior to the era of satellite geodesy, the coordinate systems associated with a geodetic datum attempted to be geocentric, but their origins differed from the geocenter by hundreds of meters, due to regional deviations in the direction of the plumbline (vertical). These regional geodetic data, such as ED 50 (European Datum 1950) or NAD 27 (North American Datum 1927) have ellipsoids associated with them that are regional "best fits" to the geoids within their areas of validity, minimizing the deflections of the vertical over these areas.The locations of points in three-dimensional space are most conveniently described by three cartesian or rectangular coordinates, X, Y and Z. Since the advent of satellite positioning, such coordinate systems are typically geocentric: the Z-axis is aligned with the Earth's (conventional or instantaneous) rotation axis.The geoid is "realizable", meaning it can be consistently located on the Earth by suitable simple measurements from physical objects like a tide gauge. The geoid can therefore be considered a real surface. The reference ellipsoid, however, has many possible instantiations and is not readily realizable, therefore it is an abstract surface. The third primary surface of geodetic interest—the topographic surface of the Earth—is a realizable surface.The 1980 Geodetic Reference System (GRS 80) posited a 6,378,137 m semi-major axis and a 1:298.257 flattening. This system was adopted at the XVII General Assembly of the International Union of Geodesy and Geophysics (IUGG). It is essentially the basis for geodetic positioning by the Global Positioning System (GPS) and is thus also in widespread use outside the geodetic community. The numerous systems that countries have used to create maps and charts are becoming obsolete as countries increasingly move to global, geocentric reference systems using the GRS 80 reference ellipsoid.A reference ellipsoid, customarily chosen to be the same size (volume) as the geoid, is described by its semi-major axis (equatorial radius) a and flattening f. The quantity f = a − b/a, where b is the semi-minor axis (polar radius), is a purely geometrical one. The mechanical ellipticity of the Earth (dynamical flattening, symbol J2) can be determined to high precision by observation of satellite orbit perturbations. Its relationship with the geometrical flattening is indirect. The relationship depends on the internal density distribution, or, in simplest terms, the degree of central concentration of mass.The geoid is essentially the figure of the Earth abstracted from its topographical features. It is an idealized equilibrium surface of sea water, the mean sea level surface in the absence of currents and air pressure variations, and continued under the continental masses. The geoid, unlike the reference ellipsoid, is irregular and too complicated to serve as the computational surface on which to solve geometrical problems like point positioning. The geometrical separation between the geoid and the reference ellipsoid is called the geoidal undulation. It varies globally between ±110 m, when referred to the GRS 80 ellipsoid.To a large extent, the shape of the Earth is the result of its rotation, which causes its equatorial bulge, and the competition of geological processes such as the collision of plates and of volcanism, resisted by the Earth's gravity field. This applies to the solid surface, the liquid surface (dynamic sea surface topography) and the Earth's atmosphere. For this reason, the study of the Earth's gravity field is called physical geodesy.It is primarily concerned with positioning within the temporally varying gravity field. Geodesy in the German-speaking world is divided into "higher geodesy" ("Erdmessung" or "höhere Geodäsie"), which is concerned with measuring the Earth on the global scale, and "practical geodesy" or "engineering geodesy" ("Ingenieurgeodäsie"), which is concerned with measuring specific parts or regions of the Earth, and which includes surveying. Such geodetic operations are also applied to other astronomical bodies in the solar system. It is also the science of measuring and understanding the earth's geometric shape, orientation in space, and gravity field.The word "geodesy" comes from the Ancient Greek word γεωδαισία geodaisia (literally, "division of the Earth").Geodesy ( /dʒiːˈɒdɪsi/)[1], also known as geodetics, is the science of accurately measuring and understanding three of Earth's fundamental properties—its geometric shape, its orientation in space, and its gravity field—as well as how they change over time.[2] Geodynamical phenomena include crustal motion, tides, and polar motion, which can be studied by designing global and national control networks, applying space and terrestrial techniques, and relying on datums and coordinate systems.
Hermann Grassmann
These philological accomplishments were honored during his lifetime; he was elected to the American Oriental Society and in 1876, he received an honorary doctorate from the University of Tübingen.Grassmann also discovered a sound law of Indo-European languages, which was named Grassmann's Law in his honor.Grassmann's mathematical ideas began to spread only towards the end of his life. 30 years after the publication of A1 the publisher wrote to Grassmann: “Your book Die Ausdehnungslehre has been out of print for some time. Since your work hardly sold at all, roughly 600 copies were used in 1864 as waste paper and the remaining few odd copies have now been sold out, with the exception of the one copy in our library”.[12] Disappointed by the reception of his work in mathematical circles, Grassmann lost his contacts with mathematicians as well as his interest in geometry. The last years of his life he turned to historical linguistics and the study of Sanskrit. He wrote books on German grammar, collected folk songs, and learned Sanskrit. He wrote a 2,000-page dictionary and a translation of the Rigveda (more than 1,000 pages) which earned him a membership of the American Orientalists' Society. In modern Rigvedic studies Grassmann's work is often cited. In 1955 the third edition of his dictionary to Rigveda was issued.[7]Adhémar Jean Claude Barré de Saint-Venant developed a vector calculus similar to that of Grassmann which he published in 1845. He then entered into a dispute with Grassmann about which of the two had thought of the ideas first. Grassmann had published his results in 1844, but Saint-Venant claimed that he had first developed these ideas in 1832.For an introduction to the role of Grassmann's work in contemporary mathematical physics see The Road to Reality[11] by Roger Penrose.Comprehension of Grassmann awaited the concept of vector spaces which then could express the multilinear algebra of his extension theory. To establish the priority of Grassmann over Hamilton, Josiah Willard Gibbs urged Grassmann's heirs to have the 1840 essay on tides published.[10] A. N. Whitehead's first monograph, the Universal Algebra (1898), included the first systematic exposition in English of the theory of extension and the exterior algebra. With the rise of differential geometry the exterior algebra was applied to differential forms.In 1872 Victor Schlegel published the first part of his System der Raumlehre which used Grassmann's approach to derive ancient and modern results in plane geometry. Felix Klein wrote a negative review of Schlegel's book citing its incompleteness and lack of perspective on Grassmann. Schlegel followed in 1875 with a second part of his System according to Grassmann, this time developing higher geometry. Meanwhile, Klein was advancing his Erlangen Program which also expanded the scope of geometry.[9]One of the first mathematicians to appreciate Grassmann's ideas during his lifetime was Hermann Hankel, whose 1867 Theorie der complexen ZahlensystemeIn 1840s, mathematicians were generally unprepared to understand Grassmann's ideas.[7] In the 1860s and 1870s various mathematicians came to ideas similar to that of Grassmann's, but Grassmann himself was not interested in mathematics anymore.[7]In 1862, Grassmann published a thoroughly rewritten second edition of A1, hoping to earn belated recognition for his theory of extension, and containing the definitive exposition of his linear algebra. The result, Die Ausdehnungslehre: Vollständig und in strenger Form bearbeitet [The Theory of Extension, Thoroughly and Rigorously Treated], hereinafter denoted A2, fared no better than A1, even though A2's manner of exposition anticipates the textbooks of the 20th century.Grassmann (1861) set out the first axiomatic presentation of arithmetic, making free use of the principle of induction. Peano and his followers cited this work freely starting around 1890. Lloyd C. Kannenberg published an English translation of The Ausdehnungslehre and Other works in 1995 (ISBN 0-8126-9275-6. -- ISBN 0-8126-9276-4).In 1853, Grassmann published a theory of how colors mix; it and its three color laws are still taught, as Grassmann's law. Grassmann's work on this subject was inconsistent with that of Helmholtz. Grassmann also wrote on crystallography, electromagnetism, and mechanics.In 1846, Möbius invited Grassmann to enter a competition to solve a problem first proposed by Leibniz: to devise a geometric calculus devoid of coordinates and metric properties (what Leibniz termed analysis situs). Grassmann's Geometrische Analyse geknüpft an die von Leibniz erfundene geometrische Charakteristik,[6] was the winning entry (also the only entry). Möbius, as one of the judges, criticized the way Grassmann introduced abstract notions without giving the reader any intuition as to why those notions were of value.A1 was a revolutionary text, too far ahead of its time to be appreciated. When Grassmann submitted it to apply for a professorship in 1847, the ministry asked Ernst Kummer for a report. Kummer assured that there were good ideas in it, but found the exposition deficient and advised against giving Grassmann a university position. Over the next 10-odd years, Grassmann wrote a variety of work applying his theory of extension, including his 1845 Neue Theorie der Elektrodynamik[5] and several papers on algebraic curves and surfaces, in the hope that these applications would lead others to take his theory seriously.Following an idea of Grassmann's father, A1 also defined the exterior product, also called "combinatorial product" (in German: äußeres Produkt[3] or kombinatorisches Produkt[4]), the key operation of an algebra now called exterior algebra. (One should keep in mind that in Grassmann's day, the only axiomatic theory was Euclidean geometry, and the general notion of an abstract algebra had yet to be defined.) In 1878, William Kingdon Clifford joined this exterior algebra to William Rowan Hamilton's quaternions by replacing Grassmann's rule epep = 0 by the rule epep = 1. (For quaternions, we have the rule i2 = j2 = k2 = −1.) For more details, see exterior algebra.Fearnley-Sander (1979) describes Grassmann's foundation of linear algebra as follows:In 1844, Grassmann published his masterpiece, his Die Lineale Ausdehnungslehre, ein neuer Zweig der Mathematik[1] [The Theory of Linear Extension, a New Branch of Mathematics], hereinafter denoted A1 and commonly referred to as the Ausdehnungslehre,[2] which translates as "theory of extension" or "theory of extensive magnitudes." Since A1 proposed a new foundation for all of mathematics, the work began with quite general definitions of a philosophical nature. Grassmann then showed that once geometry is put into the algebraic form he advocated, the number three has no privileged role as the number of spatial dimensions; the number of possible dimensions is in fact unbounded.One of the many examinations for which Grassmann sat required that he submit an essay on the theory of the tides. In 1840, he did so, taking the basic theory from Laplace's Mécanique céleste and from Lagrange's Mécanique analytique, but expositing this theory making use of the vector methods he had been mulling over since 1832. This essay, first published in the Collected Works of 1894–1911, contains the first known appearance of what is now called linear algebra and the notion of a vector space. He went on to develop those methods in his A1 and A2 (see references).Grassmann had eleven children, seven of whom reached adulthood. A son, Hermann Ernst Grassmann, became a professor of mathematics at the University of Giessen.Starting during the political turmoil in Germany, 1848–49, Hermann and his brother Robert published a Stettin newspaper, Deutsche Wochenschrift für Staat, Kirche und Volksleben, calling for German unification under a constitutional monarchy. (This eventuated in 1871.) After writing a series of articles on constitutional law, Hermann parted company with the newspaper, finding himself increasingly at odds with its political direction.In 1847, he was made an "Oberlehrer" or head teacher. In 1852, he was appointed to his late father's position at the Stettin Gymnasium, thereby acquiring the title of Professor. In 1847, he asked the Prussian Ministry of Education to be considered for a university position, whereupon that Ministry asked Kummer for his opinion of Grassmann. Kummer wrote back saying that Grassmann's 1846 prize essay (see below) contained "... commendably good material expressed in a deficient form." Kummer's report ended any chance that Grassmann might obtain a university post. This episode proved the norm; time and again, leading figures of Grassmann's day failed to recognize the value of his mathematics.In 1834 Grassmann began teaching mathematics at the Gewerbeschule in Berlin. A year later, he returned to Stettin to teach mathematics, physics, German, Latin, and religious studies at a new school, the Otto Schule. Over the next four years, Grassmann passed examinations enabling him to teach mathematics, physics, chemistry, and mineralogy at all secondary school levels.Although lacking university training in mathematics, it was the field that most interested him when he returned to Stettin in 1830 after completing his studies in Berlin. After a year of preparation, he sat the examinations needed to teach mathematics in a gymnasium, but achieved a result good enough to allow him to teach only at the lower levels. Around this time, he made his first significant mathematical discoveries, ones that led him to the important ideas he set out in his 1844 paper referred to as A1 (see references).Grassmann was an undistinguished student until he obtained a high mark on the examinations for admission to Prussian universities. Beginning in 1827, he studied theology at the University of Berlin, also taking classes in classical languages, philosophy, and literature. He does not appear to have taken courses in mathematics or physics.Grassmann was the third of 12 children of Justus Günter Grassmann, an ordained minister who taught mathematics and physics at the Stettin Gymnasium, where Hermann was educated.Hermann Günther Grassmann (German: Graßmann; April 15, 1809 – September 26, 1877) was a German polymath, known in his day as a linguist and now also as a mathematician. He was also a physicist, neohumanist, general scholar, and publisher. His mathematical work was little noted until he was in his sixties.
James Joseph Sylvester
Sylvester House, a portion of an undergraduate dormitory at Johns Hopkins University, is named in his honor. Several professorships there are named in his honor also.Sylvester invented a great number of mathematical terms such as "matrix" (in 1850),[6] "graph" (combinatorics)[7] and "discriminant".[8] He coined the term "totient" for Euler's totient function φ(n).[9] His collected scientific work fills four volumes. In 1880, the Royal Society of London awarded Sylvester the Copley Medal, its highest award for scientific achievement; in 1901, it instituted the Sylvester Medal in his memory, to encourage mathematical research after his death in Oxford. In Discrete geometry he is remembered for Sylvester's Problem and a result on the orchard problem.In 1883, he returned to England to take up the Savilian Professor of Geometry at Oxford University. He held this chair until his death, although in 1892 the University appointed a deputy professor to the same chair.In 1876[4] Sylvester again crossed the Atlantic Ocean to become the inaugural professor of mathematics at the new Johns Hopkins University in Baltimore, Maryland. His salary was $5,000 (quite generous for the time), which he demanded be paid in gold. After negotiation, agreement was reached on a salary that was not paid in gold.[5] In 1878 he founded the American Journal of Mathematics. The only other mathematical journal in the US at that time was the Analyst, which eventually became the Annals of Mathematics.In 1872, he finally received his B.A. and M.A. from Cambridge, having been denied the degrees due to his being a Jew.[1]One of Sylvester's lifelong passions was for poetry; he read and translated works from the original French, German, Italian, Latin and Greek, and many of his mathematical papers contain illustrative quotes from classical poetry. Following his early retirement, Sylvester (1870) published a book entitled The Laws of Verse in which he attempted to codify a set of laws for prosody in poetry.On his return to England, he was hired in 1844 by the Equity and Law Life Assurance Society for which he developed successful actuarial models and served as de facto CEO, a position that required a law degree. As a result, he studied for the Bar, meeting a fellow British mathematician studying law, Arthur Cayley, with whom he made significant contributions to invariant theory and also matrix theory during a long collaboration.[3][incomplete short citation] He did not obtain a position teaching university mathematics until 1855, when he was appointed professor of mathematics at the Royal Military Academy, Woolwich, from which he retired in 1869, because the compulsory retirement age was 55. The Woolwich academy initially refused to pay Sylvester his full pension, and only relented after a prolonged public controversy, during which Sylvester took his case to the letters page of The Times.Sylvester began his study of mathematics at St John's College, Cambridge in 1831,[1] where his tutor was John Hymers. Although his studies were interrupted for almost two years due to a prolonged illness, he nevertheless ranked second in Cambridge's famous mathematical examination, the tripos, for which he sat in 1837. However, Sylvester was not issued a degree, because graduates at that time were required to state their acceptance of the Thirty-Nine Articles of the Church of England, and Sylvester could not do so because he was Jewish. For the same reason, he was unable to compete for a Fellowship or obtain a Smith's prize.[2] In 1838, Sylvester became professor of natural philosophy at University College London and in 1839 a Fellow of the Royal Society of London. In 1841, he was awarded a BA and an MA by Trinity College, Dublin. In the same year he moved to the United States to become a professor of mathematics at the University of Virginia, but left after less than four months following a violent encounter with two students he had disciplined. He moved to New York City and began friendships with the Harvard mathematician Benjamin Peirce (father of Charles Sanders Peirce) and the Princeton physicist Joseph Henry. However, he left in November 1843 after being denied appointment as Professor of Mathematics at Columbia College (now University), again for his Judaism, and returned to England.Sylvester was born James Joseph in London, England. His father, Abraham Joseph, was a merchant. James adopted the surname Sylvester when his older brother did so upon emigration to the United States—a country which at that time required all immigrants to have a given name, a middle name, and a surname. At the age of 14, Sylvester was a student of Augustus De Morgan at the University of London. His family withdrew him from the University after he was accused of stabbing a fellow student with a knife. Subsequently, he attended the Liverpool Royal Institution.James Joseph Sylvester FRS (3 September 1814 – 15 March 1897) was an English mathematician. He made fundamental contributions to matrix theory, invariant theory, number theory, partition theory, and combinatorics. He played a leadership role in American mathematics in the later half of the 19th century as a professor at the Johns Hopkins University and as founder of the American Journal of Mathematics. At his death, he was professor at Oxford.
Arthur Cayley
A number of mathematical terms are named after him:An 1874 portrait of Cayley by Lowes Cato Dickinson and an 1884 portrait by William Longmaid are in the collection of Trinity College, Cambridge.[8]Cayley is buried in the Mill Road cemetery, Cambridge.The remainder of his papers were edited by Andrew Forsyth, his successor in the Sadleirian Chair. The Collected Mathematical papers number thirteen quarto volumes, and contain 967 papers. Cayley retained to the last his fondness for novel-reading and for travelling. He also took special pleasure in paintings and architecture, and he practiced water-colour painting, which he found useful sometimes in making mathematical diagrams.In 1889 the Cambridge University Press requested him to prepare his mathematical papers for publication in a collected form—a request which he appreciated very much. They are printed in magnificent quarto volumes, of which seven appeared under his own editorship. While editing these volumes, he was suffering from a painful internal malady, to which he succumbed on 26 January 1895, in the 74th year of his age. When the funeral took place, a great assemblage met in Trinity Chapel, comprising members of the University, official representatives of Russia and America, and many of the most illustrious philosophers of Britain.In 1883 Cayley was President of the British Association for the Advancement of Science. The meeting was held at Southport, in the north of England. As the President's address is one of the great popular events of the meeting, and brings out an audience of general culture, it is usually made as little technical as possible. Cayley (1996) took for his subject the Progress of Pure Mathematics.In 1893 Cayley became a foreign member of the Royal Netherlands Academy of Arts and Sciences.[7]In 1881 he received from the Johns Hopkins University, Baltimore, where Sylvester was then professor of mathematics, an invitation to deliver a course of lectures. He accepted the invitation, and lectured at Baltimore during the first five months of 1882 on the subject of the Abelian and Theta Functions.In 1876 he published a Treatise on Elliptic Functions. He took great interest in the movement for the University education of women. At Cambridge the women's colleges are Girton and Newnham. In the early days of Girton College he gave direct help in teaching, and for some years he was chairman of the council of Newnham College, in the progress of which he took the keenest interest to the last.In addition to his work on algebra, Cayley made fundamental contributions to algebraic geometry. Cayley and Salmon discovered the 27 lines on a cubic surface. Cayley constructed the Chow variety of all curves in projective 3-space.[6] He founded the algebro-geometric theory of ruled surfaces.The other duty of the chair — the advancement of mathematical science — was discharged in a handsome manner by the long series of memoirs that he published, ranging over every department of pure mathematics. But it was also discharged in a much less obtrusive way; he became the standing referee on the merits of mathematical papers to many societies both at home and abroad.At first the teaching duty of the Sadleirian professorship was limited to a course of lectures extending over one of the terms of the academic year; but when the University was reformed about 1886, and part of the college funds applied to the better endowment of the University professors, the lectures were extended over two terms. For many years the attendance was small, and came almost entirely from those who had finished their career of preparation for competitive examinations; after the reform the attendance numbered about fifteen. The subject lectured on was generally that of the memoir on which the professor was for the time engaged.At Cambridge University the ancient professorship of pure mathematics is denominated by the Lucasian, and is the chair that had been occupied by Isaac Newton. Around 1860, certain funds bequeathed by Lady Sadleir to the University, having become useless for their original purpose, were employed to establish another professorship of pure mathematics, called the Sadleirian. The duties of the new professor were defined to be "to explain and teach the principles of pure mathematics and to apply himself to the advancement of that science." To this chair Cayley was elected when 42 years old. He gave up a lucrative practice for a modest salary; but he never regretted the exchange, for the chair at Cambridge enabled him to end the divided allegiance between law and mathematics, and to devote his energies to the pursuit that he liked best. He at once married and settled down in Cambridge. More fortunate than Hamilton in his choice, his home life was one of great happiness. His friend and fellow investigator, Sylvester, once remarked that Cayley had been much more fortunate than himself; that they both lived as bachelors in London, but that Cayley had married and settled down to a quiet and peaceful life at Cambridge; whereas he had never married, and had been fighting the world all his days.His friend J. J. Sylvester, his senior by five years at Cambridge, was then an actuary, resident in London; they used to walk together round the courts of Lincoln's Inn, discussing the theory of invariants and covariants. During this period of his life, extending over fourteen years, Cayley produced between two and three hundred papers.[4]Because of the limited tenure of his fellowship it was necessary to choose a profession; like De Morgan, Cayley chose law, and at age 25 entered at Lincoln's Inn, London. He made a specialty of conveyancing. It was while he was a pupil at the bar examination that he went to Dublin to hear Hamilton's lectures on quaternions.[4]Cayley's tutor at Cambridge was George Peacock and his private coach was William Hopkins. He finished his undergraduate course by winning the place of Senior Wrangler, and the first Smith's prize.[3] His next step was to take the M.A. degree, and win a Fellowship by competitive examination. He continued to reside at Cambridge University for four years; during which time he took some pupils, but his main work was the preparation of 28 memoirs to the Mathematical Journal.At the unusually early age of 17 Cayley began residence at Trinity College, Cambridge. The cause of the Analytical Society had now triumphed, and the Cambridge Mathematical Journal had been instituted by Gregory and Robert Leslie Ellis. To this journal, at the age of twenty, Cayley contributed three papers, on subjects that had been suggested by reading the Mécanique analytique of Lagrange and some of the works of Laplace.Arthur Cayley was born in Richmond, London, England, on 16 August 1821. His father, Henry Cayley, was a distant cousin of Sir George Cayley, the aeronautics engineer innovator, and descended from an ancient Yorkshire family. He settled in Saint Petersburg, Russia, as a merchant. His mother was Maria Antonia Doughty, daughter of William Doughty. According to some writers she was Russian, but her father's name indicates an English origin. His brother was the linguist Charles Bagot Cayley. Arthur spent his first eight years in Saint Petersburg. In 1829 his parents were settled permanently at Blackheath, near London. Arthur was sent to a private school. At age 14 he was sent to King's College School. The school's master observed indications of mathematical genius and advised the father to educate his son not for his own business, as he had intended, but to enter the University of Cambridge.He postulated the Cayley–Hamilton theorem—that every square matrix is a root of its own characteristic polynomial, and verified it for matrices of order 2 and 3.[1] He was the first to define the concept of a group in the modern way—as a set with a binary operation satisfying certain laws.[2] Formerly, when mathematicians spoke of "groups", they had meant permutation groups. Cayley's theorem is named in honour of Cayley.As a child, Cayley enjoyed solving complex maths problems for amusement. He entered Trinity College, Cambridge, where he excelled in Greek, French, German, and Italian, as well as mathematics. He worked as a lawyer for 14 years.Arthur Cayley F.R.S. (/ˈkeɪli/; 16 August 1821 – 26 January 1895) was a British mathematician. He helped found the modern British school of pure mathematics.
Hüseyin Tevfik Pasha

Giuseppe Peano
In 1925 Peano switched Chairs unofficially from Infinitesimal Calculus to Complementary Mathematics, a field which better suited his current style of mathematics. This move became official in 1931. Giuseppe Peano continued teaching at Turin University until the day before he died, when he suffered a fatal heart attack.During the years 1913–1918, Peano published several papers that dealt with the remainder term for various numerical quadrature formulas, and introduced the Peano kernel.[4]After his mother died in 1910, Peano divided his time between teaching, working on texts aimed for secondary schooling including a dictionary of mathematics, and developing and promoting his and other auxiliary languages, becoming a revered member of the international auxiliary language movement. He used his membership of the Accademia dei Lincei to present papers written by friends and colleagues who were not members (the Accademia recorded and published all presented papers given in sessions).Also in 1908, Peano took over the chair of higher analysis at Turin (this appointment was to last for only two years). He was elected the director of Academia pro Interlingua. Having previously created Idiom Neutral, the Academy effectively chose to abandon it in favor of Peano's Latino sine flexione.The year 1908 was important for Peano. The fifth and final edition of the Formulario project, titled Formulario Mathematico, was published. It contained 4200 formulae and theorems, all completely stated and most of them proved. The book received little attention since much of the content was dated by this time. However, it remains a significant contribution to mathematical literature. The comments and examples were written in Latino sine flexione.In 1903 Peano announced his work on an international auxiliary language called Latino sine flexione ("Latin without inflexion," later called Interlingua, and the precursor of the Interlingua of the IALA). This was an important project for him (along with finding contributors for 'Formulario'). The idea was to use Latin vocabulary, since this was widely known, but simplify the grammar as much as possible and remove all irregular and anomalous forms to make it easier to learn. In one speech, he started speaking in Latin and, as he described each simplification, introduced it into his speech so that by the end he was talking in his new language.By 1901, Peano was at the peak of his mathematical career. He had made advances in the areas of analysis, foundations and logic, made many contributions to the teaching of calculus and also contributed to the fields of differential equations and vector analysis. Peano played a key role in the axiomatization of mathematics and was a leading pioneer in the development of mathematical logic. Peano had by this stage become heavily involved with the Formulario project and his teaching began to suffer. In fact, he became so determined to teach his new mathematical symbols that the calculus in his course was neglected. As a result, he was dismissed from the Royal Military Academy but retained his post at Turin University.Peano's students Mario Pieri and Alessandro Padoa had papers presented at the philosophy congress also. For the mathematical congress, Peano did not speak, but Padoa's memorable presentation has been frequently recalled. A resolution calling for the formation of an "international auxiliary language" to facilitate the spread of mathematical (and commercial) ideas, was proposed; Peano fully supported it.Paris was the venue for the Second International Congress of Mathematicians in 1900. The conference was preceded by the First International Conference of Philosophy where Peano was a member of the patronage committee. He presented a paper which posed the question of correctly formed definitions in mathematics, i.e. "how do you define a definition?". This became one of Peano's main philosophical interests for the rest of his life. At the conference Peano met Bertrand Russell and gave him a copy of Formulario. Russell was so struck by Peano's innovative logical symbols that he left the conference and returned home to study Peano's text.In 1898 he presented a note to the Academy about binary numeration and its ability to be used to represent the sounds of languages. He also became so frustrated with publishing delays (due to his demand that formulae be printed on one line) that he purchased a printing press.In 1890 Peano founded the journal Rivista di Matematica, which published its first issue in January 1891.[3] In 1891 Peano started the Formulario Project. It was to be an "Encyclopedia of Mathematics", containing all known formulae and theorems of mathematical science using a standard notation invented by Peano. In 1897, the first International Congress of Mathematicians was held in Zürich. Peano was a key participant, presenting a paper on mathematical logic. He also started to become increasingly occupied with Formulario to the detriment of his other work.In 1887, Peano married Carola Crosio, the daughter of the Turin-based painter Luigi Crosio, known for painting the Refugium Peccatorum Madonna.[2] In 1886, he began teaching concurrently at the Royal Military Academy, and was promoted to Professor First Class in 1889. In that year he published the Peano axioms, a formal foundation for the collection of natural numbers. The next year, the University of Turin also granted him his full professorship. Peano's famous space-filling curve appeared in 1890 as a counterexample. He used it to show that a continuous curve cannot always be enclosed in an arbitrarily small region. This was an early example of what came to be known as a fractal.Peano was born and raised on a farm at Spinetta, a hamlet now belonging to Cuneo, Piedmont, Italy. He attended the Liceo classico Cavour in Turin, and enrolled at the University of Turin in 1876, graduating in 1880 with high honors, after which the University employed him to assist first Enrico D'Ovidio, and then Angelo Genocchi, the Chair of calculus. Due to Genocchi's poor health, Peano took over the teaching of calculus course within two years. His first major work, a textbook on calculus, was published in 1884 and was credited to Genocchi. A few years later, Peano published his first book dealing with mathematical logic. Here the modern symbols for the union and intersection of sets appeared for the first time.[1]Giuseppe Peano (Italian: [dʒuˈzɛppe peˈaːno]; 27 August 1858 – 20 April 1932) was an Italian mathematician and glottologist. The author of over 200 books and papers, he was a founder of mathematical logic and set theory, to which he contributed much notation. The standard axiomatization of the natural numbers is named the Peano axioms in his honor. As part of this effort, he made key contributions to the modern rigorous and systematic treatment of the method of mathematical induction. He spent most of his career teaching mathematics at the University of Turin. He also wrote an international auxiliary language, the 'Latin sine flexione' ("Latin without inflections"), which is a simplified version of the Classical Latin. Most of his books and papers are in Latin sine flexione, other ones are in Italian.
Abstract algebra
In physics, groups are used to represent symmetry operations, and the usage of group theory could simplify differential equations. In gauge theory, the requirement of local symmetry can be used to deduce the equations describing a system. The groups that describe those symmetries are Lie groups, and the study of Lie groups and Lie algebras reveals much about the physical system; for instance, the number of force carriers in a theory is equal to the dimension of the Lie algebra, and these bosons interact with the force they mediate if the Lie algebra is nonabelian.[2]Because of its generality, abstract algebra is used in many fields of mathematics and science. For instance, algebraic topology uses algebraic objects to study topologies. The Poincaré conjecture, proved in 2003, asserts that the fundamental group of a manifold, which encodes information about connectedness, can be used to determine whether a manifold is a sphere or not. Algebraic number theory studies various number rings that generalize the set of integers. Using tools of algebraic number theory, Andrew Wiles proved Fermat's Last Theorem.Examples involving several operations include:Examples of algebraic structures with a single binary operation are:By abstracting away various amounts of detail, mathematicians have defined various algebraic structures that are used in many areas of mathematics. For instance, almost all systems studied are sets, to which the theorems of set theory apply. Those sets that have a certain binary operation defined on them form magmas, to which the concepts concerning magmas, as well those concerning sets, apply. We can add additional constraints on the algebraic structure, such as associativity (to form semigroups); identity, and inverses (to form groups); and other more complex structures. With additional structure, more theorems could be proved, but the generality is reduced. The "hierarchy" of algebraic objects (in terms of generality) creates a hierarchy of the corresponding theories: for instance, the theorems of group theory may be used when studying rings (algebraic objects that have two binary operations with certain axioms) since a ring is a group over one of its operations. In general there is a balance between the amount of generality and the richness of the theory: more general structures have usually fewer nontrivial theorems and fewer applications.These processes were occurring throughout all of mathematics, but became especially pronounced in algebra. Formal definition through primitive operations and axioms were proposed for many basic algebraic structures, such as groups, rings, and fields. Hence such things as group theory and ring theory took their places in pure mathematics. The algebraic investigations of general fields by Ernst Steinitz and of commutative and then general rings by David Hilbert, Emil Artin and Emmy Noether, building up on the work of Ernst Kummer, Leopold Kronecker and Richard Dedekind, who had considered ideals in commutative rings, and of Georg Frobenius and Issai Schur, concerning representation theory of groups, came to define abstract algebra. These developments of the last quarter of the 19th century and the first quarter of 20th century were systematically exposed in Bartel van der Waerden's Moderne algebra, the two-volume monograph published in 1930–1931 that forever changed for the mathematical world the meaning of the word algebra from the theory of equations to the theory of algebraic structures.The end of the 19th and the beginning of the 20th century saw a tremendous shift in the methodology of mathematics. Abstract algebra emerged around the start of the 20th century, under the name modern algebra. Its study was part of the drive for more intellectual rigor in mathematics. Initially, the assumptions in classical algebra, on which the whole of mathematics (and major parts of the natural sciences) depend, took the form of axiomatic systems. No longer satisfied with establishing properties of concrete objects, mathematicians started to turn their attention to general theory. Formal definitions of certain algebraic structures began to emerge in the 19th century. For example, results about various groups of permutations came to be seen as instances of general theorems that concern a general notion of an abstract group. Questions of structure and classification of various mathematical objects came to forefront.The abstract notion of a group appeared for the first time in Arthur Cayley's papers in 1854. Cayley realized that a group need not be a permutation group (or even finite), and may instead consist of matrices, whose algebraic properties, such as multiplication and inverses, he systematically investigated in succeeding years. Much later Cayley would revisit the question whether abstract groups were more general than permutation groups, and establish that, in fact, any group is isomorphic to a group of permutations.The theory of permutation groups received further far-reaching development in the hands of Augustin Cauchy and Camille Jordan, both through introduction of new concepts and, primarily, a great wealth of results about special classes of permutation groups and even some general theorems. Among other things, Jordan defined a notion of isomorphism, still in the context of permutation groups and, incidentally, it was he who put the term group in wide use.Note, however, that he got by without formalizing the concept of a group, or even of a permutation group. The next step was taken by Évariste Galois in 1832, although his work remained unpublished until 1846, when he considered for the first time what is now called the closure property of a group of permutations, which he expressed asPaolo Ruffini was the first person to develop the theory of permutation groups, and like his predecessors, also in the context of solving algebraic equations. His goal was to establish the impossibility of an algebraic solution to a general algebraic equation of degree greater than four. En route to this goal he introduced the notion of the order of an element of a group, conjugacy, the cycle decomposition of elements of permutation groups and the notions of primitive and imprimitive and proved some important theorems relating these concepts, such asPermutations were studied by Joseph-Louis Lagrange in his 1770 paper Réflexions sur la résolution algébrique des équations (Thoughts on the algebraic solution of equations) devoted to solutions of algebraic equations, in which he introduced Lagrange resolvents. Lagrange's goal was to understand why equations of third and fourth degree admit formulae for solutions, and he identified as key objects permutations of the roots. An important novel step taken by Lagrange in this paper was the abstract view of the roots, i.e. as symbols and not as numbers. However, he did not consider composition of permutations. Serendipitously, the first edition of Edward Waring's Meditationes Algebraicae (Meditations on Algebra) appeared in the same year, with an expanded version published in 1782. Waring proved the main theorem on symmetric functions, and specially considered the relation between the roots of a quartic equation and its resolvent cubic. Mémoire sur la résolution des équations (Memoire on the Solving of Equations) of Alexandre Vandermonde (1771) developed the theory of symmetric functions from a slightly different angle, but like Lagrange, with the goal of understanding solvability of algebraic equations.Leonhard Euler considered algebraic operations on numbers modulo an integer, modular arithmetic, in his generalization of Fermat's little theorem. These investigations were taken much further by Carl Friedrich Gauss, who considered the structure of multiplicative groups of residues mod n and established many properties of cyclic and more general abelian groups that arise in this way. In his investigations of composition of binary quadratic forms, Gauss explicitly stated the associative law for the composition of forms, but like Euler before him, he seems to have been more interested in concrete results than in general theory. In 1870, Leopold Kronecker gave a definition of an abelian group in the context of ideal class groups of a number field, generalizing Gauss's work; but it appears he did not tie his definition with previous work on groups, particularly permutation groups. In 1882, considering the same question, Heinrich M. Weber realized the connection and gave a similar definition that involved the cancellation property but omitted the existence of the inverse element, which was sufficient in his context (finite groups).There were several threads in the early development of group theory, in modern language loosely corresponding to number theory, theory of equations, and geometry.Numerous textbooks in abstract algebra start with axiomatic definitions of various algebraic structures and then proceed to establish their properties. This creates a false impression that in algebra axioms had come first and then served as a motivation and as a basis of further study. The true order of historical development was almost exactly the opposite. For example, the hypercomplex numbers of the nineteenth century had kinematic and physical motivations but challenged comprehension. Most theories that are now recognized as parts of algebra started as collections of disparate facts from various branches of mathematics, acquired a common theme that served as a core around which various results were grouped, and finally became unified on a basis of a common set of concepts. An archetypical example of this progressive synthesis can be seen in the history of group theory.As in other parts of mathematics, concrete problems and examples have played important roles in the development of abstract algebra. Through the end of the nineteenth century, many – perhaps most – of these problems were in some way related to the theory of algebraic equations. Major themes include:Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called variety of groups.Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory is a formalism that allows a unified way for expressing properties and constructions that are similar for various structures.In algebra, which is a broad division of mathematics, abstract algebra (occasionally called modern algebra) is the study of algebraic structures. Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras. The term abstract algebra was coined in the early 20th century to distinguish this area of study from the other parts of algebra.
Quantum mechanics
Each term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.andwith coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy viaandThe solutions are superpositions of left- and right-moving waves:The potential in this case is given by:This is another example illustrating the quantification of energy for bound states.and the corresponding energy levels arewhere Hn are the Hermite polynomialsThis problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given byAs in the classical case, the potential for the quantum harmonic oscillator is given byThis is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.The quantization of energy levels follows from this constraint on k, sincein which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of π,and D = 0. At x = L,The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where ψ must be zero. Thus, at x = 0,or, from Euler's formula,The general solutions of the Schrödinger equation for the particle in a box arethe previous equation is evocative of the classic kinetic energy analogue,With the differential operator defined byFor example, consider a free particle. In quantum mechanics, a free matter is described by a wave function. The particle properties of the matter become apparent when we measure its position and velocity. The wave properties of the matter become apparent when we measure its wave properties like interference. The wave–particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics. Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position—or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.[86] In an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.[87]Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures.[84] Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms.[85] Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement.[83] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.A more distant goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.[82]An inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.[81]Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.Many electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum mechanics is necessary to understanding and designing such electronic devices.Many modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.In many aspects modern technology operates at a scale where quantum effects are significant.Quantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry. Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved.[80] Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.Quantum mechanics has had enormous[79] success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).The Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[77] This is not accomplished by introducing some "new axiom" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical - not just formally mathematical, as in other interpretations - quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these "parallel universes" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation.[78] Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.John Bell showed that this "EPR" paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables.[76] Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement.Albert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, "God does not play with dice".[75] He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein–Podolsky–Rosen paradox.The Copenhagen interpretation — due largely to Niels Bohr and Werner Heisenberg — remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of "causality." It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born's basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics."[73] According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[74]Another popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete. More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616×10−35 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.The quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or "quantum electromagnetism"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity,[70][71] has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field.[72] Beyond this "grand unification", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However — and while special relativity is parsimoniously incorporated into quantum electrodynamics — the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely "compactified" (or infinitely curved) and not readily amenable to measurement or probing.Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th and 21st century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering Gödel's Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture "Gödel and the End of Physics" (2002).[69]Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.[68]Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this.[67] For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed.For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.[65][66]In Niels Bohr's mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics.[55][56][57][58] The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or "state" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition.[59][60] In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous "state" in the classical sense of that word.[61][62] Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined.[59] Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.[63][64]A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.[54]Quantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein–Podolsky–Rosen (EPR) paradox — an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism.[49] Quantum interference involves adding together probability amplitudes, whereas classical "waves" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems.[50] Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically.[51] This is in accordance with the following observations:Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[46] According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles).[47] The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers.[48] However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.Classical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.[45]It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.[44]When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.The rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space—although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This "high energy" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.[42] In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or "observables". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom).[43] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics - matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger).[41]There exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the "semi-classical equation of motion" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.The Schrödinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the "wave-like" behavior of quantum states. As it turns out, analytic solutions of the Schrödinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom—which contains just one more electron than does the hydrogen atom—has defied all attempts at a fully analytic treatment.Some wave functions produce probability distributions that are constant, or independent of time—such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).[40]Wave functions change as time progresses. The Schrödinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schrödinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.[39]During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.[37][38]The time evolution of a quantum state is described by the Schrödinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that - given a wave function at an initial time - it makes a definite prediction of what the wave function will be at any later time.[36]In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs); rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates). Usually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or "generalized" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process[34] that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates. For example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result.[30] It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.[35]Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction.[31][32] Naturally, these probabilities will depend on the quantum state at the "instant" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable ("eigen" can be translated from German as meaning "inherent" or "characteristic").[33]The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of "wave function collapse" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[30]According to one interpretation, as the result of a measurement the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable—which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute.In the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space.[28] This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability, often referred to as "clouds", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.[29]In the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac,[23] David Hilbert,[24] John von Neumann,[25] and Hermann Weyl,[26] the possible states of a quantum mechanical system are symbolized[27] as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space—variously called the state space or the associated Hilbert space of the system—that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system—for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave–particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.[22]The word quantum derives from the Latin, meaning "how great" or "how much".[19] In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics.[20][better source needed] Some fundamental aspects of the theory are still actively studied.[21]While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,[17] and superfluids.[18]By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann[16] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.[citation needed]It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave–particle duality.[citation needed]In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). In 1926 Erwin Schrödinger suggested a partial differential equation for the wave functions of particles like electrons. And when effectively restricted to a finite region, this equation allowed only certain modes, corresponding to discrete quantum states—whose properties turned out to be exactly the same as implied by matrix mechanics.[15] From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[citation needed]The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schrödinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.[14]Planck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself.[12] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[13] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.where h is Planck's constant.According to Planck, each energy element (E) is proportional to its frequency (ν):Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld.[11] This phase is known as old quantum theory.Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887). Around 1900-1910, the atomic theory and the corpuscular theory of light[10] first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation,[9] known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck.[8] Planck's hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets) precisely matched the observed patterns of black-body radiation.Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[7] In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.Important applications of quantum theory[5] include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.[6]Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schrödinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.Classical physics (the physics existing before quantum mechanics) is a set of fundamental theories which describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that: energy, momentum and other quantities of a system may be restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to the precision with which quantities can be known (uncertainty principle).[note 1]Quantum mechanics (QM; also known as quantum physics or quantum theory), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.[2]
Special relativity
On the other hand, the existence of antiparticles leads to the conclusion that relativistic quantum mechanics is not enough for a more accurate and complete theory of particle interactions. Instead, a theory of particles interpreted as quantized fields, called quantum field theory, becomes necessary; in which particles can be created and destroyed throughout space and time.In 1928, Paul Dirac constructed an influential relativistic wave equation, now known as the Dirac equation in his honour,[59] that is fully compatible both with special relativity and with the final version of quantum theory existing after 1926. This equation explained not only the intrinsic angular momentum of the electrons called spin, it also led to the prediction of the antiparticle of the electron (the positron),[59][60] and fine structure could only be fully explained with special relativity. It was the first foundation of relativistic quantum mechanics. In non-relativistic quantum mechanics, spin is phenomenological and cannot be explained.The early Bohr–Sommerfeld atomic model explained the fine structure of alkali metal atoms using both special relativity and the preliminary knowledge on quantum mechanics of the time.[58]Special relativity can be combined with quantum mechanics to form relativistic quantum mechanics and Quantum electrodynamics. It is an unsolved problem in physics how general relativity and quantum mechanics can be unified; quantum gravity and a "theory of everything", which require a unification including general relativity too, are active and ongoing areas in theoretical research.Despite the success of the Theory of Special Relativity, there are still detractors who insist on the existence of the aether. The basis for this is the experiment performed by Georges Sagnac that produced the Sagnac effect. However, this effect has been proven to reconcile with Special Relativity.Particle accelerators routinely accelerate and measure the properties of particles moving at near the speed of light, where their behavior is completely consistent with relativity theory and inconsistent with the earlier Newtonian mechanics. These machines would simply not work if they were not engineered according to relativistic principles. In addition, a considerable number of modern experiments have been conducted to test special relativity. Some examples:Several experiments predating Einstein's 1905 paper are now interpreted as evidence for relativity. Of these it is known Einstein was aware of the Fizeau experiment before 1905,[57] and historians have concluded that Einstein was at least aware of the Michelson–Morley experiment as early as 1899 despite claims he made in his later years that it played no role in his development of the theory.[22]Newtonian mechanics mathematically follows from special relativity at small velocities (compared to the speed of light) – thus Newtonian mechanics can be considered as a special relativity of slow moving bodies. See classical mechanics for a more detailed discussion.Special relativity is mathematically self-consistent, and it is an organic part of all modern physical theories, most notably quantum field theory, string theory, and general relativity (in the limiting case of negligible gravitational fields).Special relativity in its Minkowski spacetime is accurate only when the absolute value of the gravitational potential is much less than c2 in the region of interest.[55] In a strong gravitational field, one must use general relativity. General relativity becomes special relativity at the limit of a weak field. At very small scales, such as at the Planck length and below, quantum effects must be taken into consideration resulting in quantum gravity. However, at macroscopic scales and in the absence of strong gravitational fields, special relativity is experimentally tested to extremely high degree of accuracy (10−20)[56] and thus accepted by the physics community. Experimental results which appear to contradict it are not reproducible and are thus widely believed to be due to experimental errors.Maxwell's equations in the 3D form are already consistent with the physical content of special relativity, although they are easier to manipulate in a manifestly covariant form, i.e. in the language of tensor calculus.[54]The Lorentz transformation of the electric field of a moving charge into a non-moving observer's reference frame results in the appearance of a mathematical term commonly called the magnetic field. Conversely, the magnetic field generated by a moving charge disappears and becomes a purely electrostatic field in a comoving frame of reference. Maxwell's equations are thus simply an empirical fit to special relativistic effects in a classical model of the Universe. As electric and magnetic fields are reference frame dependent and thus intertwined, one speaks of electromagnetic fields. Special relativity provides the transformation rules for how an electromagnetic field in one inertial frame appears in another inertial frame.Theoretical investigation in classical electromagnetism led to the discovery of wave propagation. Equations generalizing the electromagnetic effects found that finite propagation speed of the E and B fields required certain behaviors on charged particles. The general study of moving charges forms the Liénard–Wiechert potential, which is a step towards special relativity.In a continuous medium, the 3D density of force combines with the density of power to form a covariant 4-vector. The spatial part is the result of dividing the force on a small cell (in 3-space) by the volume of that cell. The time component is −1/c times the power transferred to that cell divided by the volume of the cell. This will be used below in the section on electromagnetism.In the rest frame of the object, the time component of the four force is zero unless the "invariant mass" of the object is changing (this requires a non-closed system in which energy/mass is being directly added or removed from the object) in which case it is the negative of that rate of change of mass, times c. In general, though, the components of the four force are not equal to the components of the three-force, because the three force is defined by the rate of change of momentum with respect to coordinate time, i.e. dp/dt while the four force is defined by the rate of change of momentum with respect to proper time, i.e. dp/dτ.If a particle is not traveling at c, one can transform the 3D force from the particle's co-moving reference frame into the observer's reference frame. This yields a 4-vector called the four-force. It is the rate of change of the above energy momentum four-vector with respect to proper time. The covariant version of the four-force is:To use Newton's third law of motion, both forces must be defined as the rate of change of momentum with respect to the same time coordinate. That is, it requires the 3D force defined above. Unfortunately, there is no tensor in 4D which contains the components of the 3D force vector among its components.Note that the mass of systems measured in their center of momentum frame (where total momentum is zero) is given by the total energy of the system in this frame. It may not be equal to the sum of individual system masses measured in other frames.The rest energy is related to the mass according to the celebrated equation discussed above:We see that the rest energy is an independent invariant. A rest energy can be calculated even for particles and systems in motion, by translating to a frame in which momentum is zero.We can work out what this invariant is by first arguing that, since it is a scalar, it doesn't matter in which reference frame we calculate it, and then by transforming to a frame where the total momentum is zero.The invariant magnitude of the momentum 4-vector generates the energy–momentum relation:So in special relativity, the acceleration four-vector and the velocity four-vector are orthogonal.which means all velocity four-vectors have a magnitude of c. This is an expression of the fact that there is no such thing as being at coordinate rest in relativity: at the least, you are always moving forward through time. Differentiating the above equation by τ produces:The 4-velocity Uμ has an invariant form:is an invariant. Notice that when the line element dX2 is negative that √−dX2 is the differential of proper time, while when dX2 is positive, √dX2 is differential of the proper distance.so the squared length of the differential of the position four-vector dXμ constructed usingThe coordinate differentials transform also contravariantly:similarly for higher order tensors. Invariant expressions, particularly inner products of 4-vectors with themselves, provide equations that are useful for calculations, because one doesn't need to perform Lorentz transformations to determine the invariants.One can extend this idea to tensors of higher order, for a second order tensor we can form the invariants:Invariant means that it takes the same value in all inertial frames, because it is a scalar (0 rank tensor), and so no Λ appears in its trivial transformation. The magnitude of the 4-vector T is the positive square root of the inner product with itself:The metric can be used for raising and lowering indices on vectors and tensors. Invariants can be constructed using the metric, the inner product of a 4-vector T with another 4-vector S is:and this is the physical symmetry underlying special relativity.The Poincaré group is the most general group of transformations which preserves the Minkowski metric:The metric tensor allows one to define the inner product of two vectors, which in turn allows one to assign a magnitude to the vector. Given the four-dimensional nature of spacetime the Minkowski metric η has components (valid in any inertial reference frame) which can be arranged in a 4 × 4 matrix:The electromagnetic field tensor is another second order antisymmetric tensor field, with six components: three for the electric field and another three for the magnetic field. There is also the stress–energy tensor for the electromagnetic field, namely the electromagnetic stress–energy tensor.An example of a four dimensional second order antisymmetric tensor is the relativistic angular momentum, which has six components: three are the classical angular momentum, and the other three are related to the boost of the center of mass of the system. The derivative of the relativistic angular momentum with respect to proper time is the relativistic torque, also second order antisymmetric tensor.More generally, most physical quantities are best described as (components of) tensors. So to transform from one frame to another, we use the well-known tensor transformation law[53]The postulates of special relativity constrain the exact form the Lorentz transformation matrices take.More generally, the covariant components of a 4-vector transform according to the inverse Lorentz transformation:only in Cartesian coordinates. It's the covariant derivative which transforms in manifest covariance, in Cartesian coordinates this happens to reduce to the partial derivatives, but not in other coordinates.that is:The four-gradient of a scalar field φ transforms covariantly rather than contravariantly:The transformation rules for three-dimensional velocities and accelerations are very awkward; even above in standard configuration the velocity equations are quite complicated owing to their non-linearity. On the other hand, the transformation of four-velocity and four-acceleration are simpler by means of the Lorentz transformation matrix.The four-acceleration is the proper time derivative of 4-velocity:where m is the invariant mass.where the Lorentz factor is:Examples of other 4-vectors include the four-velocity Uμ, defined as the derivative of the position 4-vector with respect to proper time:where we define X0 = ct so that the time coordinate has the same dimension of distance as the other spatial dimensions; so that space and time are treated equally.[50][51][52] Now the transformation of the contravariant components of the position 4-vector can be compactly written as:The simplest example of a four-vector is the position of an event in spacetime, which constitutes a timelike component ct and spacelike component x = (x, y, z), in a contravariant position four vector with components:In Newtonian mechanics, quantities which have magnitude and direction are mathematically described as 3d vectors in Euclidean space, and in general they are parametrized by time. In special relativity, this notion is extended by adding the appropriate timelike quantity to a spacelike vector quantity, and we have 4d vectors, or "four vectors", in Minkowski spacetime. The components of vectors are written using tensor index notation, as this has numerous advantages. The notation makes it clear the equations are manifestly covariant under the Poincaré group, thus bypassing the tedious calculations to check this fact. In constructing such equations, we often find that equations previously thought to be unrelated are, in fact, closely connected being part of the same tensor equation. Recognizing other physical quantities as tensors simplifies their transformation laws. Throughout, upper indices (superscripts) are contravariant indices rather than exponents except when they indicate a square (this should be clear from the context), and lower indices (subscripts) are covariant indices. For simplicity and consistency with the earlier equations, Cartesian coordinates will be used.The Lorentz transformation in standard configuration above, i.e. for a boost in the x direction, can be recast into matrix form as follows:Above, the Lorentz transformation for the time coordinate and three space coordinates illustrates that they are intertwined. This is true more generally: certain pairs of "timelike" and "spacelike" quantities naturally combine on equal footing under the same Lorentz transformation.Note that, in 4d spacetime, the concept of the center of mass becomes more complicated, see center of mass (relativistic).The geometry of Minkowski space can be depicted using Minkowski diagrams, which are useful also in understanding many of the thought-experiments in special relativity.The cone in the −t region is the information that the point is 'receiving', while the cone in the +t section is the information that the point is 'sending'.soIf we extend this to three spatial dimensions, the null geodesics are the 4-dimensional cone: which is the equation of a circle of radius c dt.or simplywe see that the null geodesics lie along a dual-cone (see image right) defined by the equation;If we reduce the spatial dimensions to 2, so that we can represent the physics in a 3D spaceSome authors use X0 = t, with factors of c elsewhere to compensate; for instance, spatial coordinates are divided by c or factors of c±2 are included in the metric tensor.[49] These numerous conventions can be superseded by using natural units where c = 1. Then space and time have equivalent units, and no factors of c appear anywhere.The actual form of ds above depends on the metric and on the choices for the X0 coordinate. To make the time coordinate look like the space coordinates, it can be treated as imaginary: X0 = ict (this is called a Wick rotation). According to Misner, Thorne and Wheeler (1971, §2.3), ultimately the deeper understanding of both special and general relativity will come from the study of the Minkowski metric (described below) and to take X0 = ct, rather than a "disguised" Euclidean metric using ict as the time coordinate.where dX = (dX0, dX1, dX2, dX3) are the differentials of the four spacetime dimensions. This suggests a deep theoretical insight: special relativity is simply a rotational symmetry of our spacetime, analogous to the rotational symmetry of Euclidean space (see image right).[48] Just as Euclidean space uses a Euclidean metric, so spacetime uses a Minkowski metric. Basically, special relativity can be stated as the invariance of any spacetime interval (that is the 4D distance between any two events) when viewed from any inertial reference frame. All equations and effects of special relativity can be derived from this rotational symmetry (the Poincaré group) of Minkowski spacetime.where dx = (dx1, dx2, dx3) are the differentials of the three spatial dimensions. In Minkowski geometry, there is an extra dimension with coordinate X0 derived from time, such that the distance differential fulfillsIn 3D space, the differential of distance (line element) ds is defined bySpecial relativity uses a 'flat' 4-dimensional Minkowski space – an example of a spacetime. Minkowski spacetime appears to be very similar to the standard 3-dimensional Euclidean space, but there is a crucial difference with respect to time.Therefore, if causality is to be preserved, one of the consequences of special relativity is that no information signal or material object can travel faster than light in vacuum. However, some "things" can still move faster than light. For example, the location where the beam of a search light hits the bottom of a cloud can move faster than light when the search light is turned rapidly.[45][46]The interval AC in the diagram is 'space-like'; i.e., there is a frame of reference in which events A and C occur simultaneously, separated only in space. There are also frames in which A precedes C (as shown) and frames in which C precedes A. If it were possible for a cause-and-effect relationship to exist between events A and C, then paradoxes of causality would result. For example, if A was the cause, and C the effect, then there would be frames of reference in which the effect preceded the cause. Although this in itself won't give rise to a paradox, one can show[43][44] that faster than light signals can be sent back into one's own past. A causal paradox can then be constructed by sending the signal if and only if no signal was received previously.In diagram 2 the interval AB is 'time-like'; i.e., there is a frame of reference in which events A and B occur at the same location in space, separated only by occurring at different times. If A precedes B in that frame, then A precedes B in all frames. It is hypothetically possible for matter (or information) to travel from A to B, so there can be a causal relationship (with A the cause and B the effect).where v(t) is the velocity at a time, t, a is the acceleration of 1g and t is the time as measured by people on Earth.[40] Therefore, after 1 year of accelerating at 9.81 m/s2, the spaceship will be travelling at v = 0.77c relative to Earth. Time dilation will increase the travellers life span as seen from the reference frame of the Earth to 2.7 years, but his lifespan measured by a clock travelling with him will not change. During his journey, people on Earth will experience more time than he does. A 5-year round trip for him will take 6½ Earth years and cover a distance of over 6 light-years. A 20-year round trip for him (5 years accelerating, 5 decelerating, twice each) will land him back on Earth having travelled for 335 Earth years and a distance of 331 light years.[41] A full 40-year trip at 1 g will appear on Earth to last 58,000 years and cover a distance of 55,000 light years. A 40-year trip at 1.1 g will take 148,000 Earth years and cover about 140,000 light years. A one-way 28 year (14 years accelerating, 14 decelerating as measured with the astronaut's clock) trip at 1 g acceleration could reach 2,000,000 light-years to the Andromeda Galaxy.[41] This same time dilation is why a muon travelling close to c is observed to travel much further than c times its half-life (when at rest).[42]Since one can not travel faster than light, one might conclude that a human can never travel farther from Earth than 40 light years if the traveler is active between the ages of 20 and 60. One would easily think that a traveler would never be able to reach more than the very few solar systems which exist within the limit of 20–40 light years from the earth. But that would be a mistaken conclusion. Because of time dilation, a hypothetical spaceship can travel thousands of light years during the pilot's 40 active years. If a spaceship could be built that accelerates at a constant 1 g, it will, after a little less than a year, be travelling at almost the speed of light as seen from Earth. This is described by:Einstein acknowledged the controversy over his derivation in his 1907 survey paper on special relativity. There he notes that it is problematic to rely on Maxwell's equations for the heuristic mass–energy argument. The argument in his 1905 paper can be carried out with the emission of any massless particles, but the Maxwell equations are implicitly used to make it obvious that the emission of light in particular can be achieved only by doing work. To emit electromagnetic waves, all you have to do is shake a charged particle, and this is clearly doing work, so that the emission is of energy.[38][39]The energy and momentum are properties of matter and radiation, and it is impossible to deduce that they form a four-vector just from the two basic postulates of special relativity by themselves, because these don't talk about matter or radiation, they only talk about space and time. The derivation therefore requires some additional physical reasoning. In his 1905 paper, Einstein used the additional principles that Newtonian mechanics should hold for slow velocities, so that there is one energy scalar and one three-vector momentum at slow velocities, and that the conservation law for energy and momentum is exactly true in relativity. Furthermore, he assumed that the energy of light is transformed by the same Doppler-shift factor as its frequency, which he had previously shown to be true based on Maxwell's equations.[1] The first of Einstein's papers on this subject was "Does the Inertia of a Body Depend upon its Energy Content?" in 1905.[35] Although Einstein's argument in this paper is nearly universally accepted by physicists as correct, even self-evident, many authors over the years have suggested that it is wrong.[36] Other authors suggest that the argument was merely inconclusive because it relied on some implicit assumptions.[37]Mass–energy equivalence is a consequence of special relativity. The energy and momentum, which are separate in Newtonian mechanics, form a four-vector in relativity, and this relates the time component (the energy) to the space components (the momentum) in a non-trivial way. For an object at rest, the energy–momentum four-vector is (E/c, 0, 0, 0): it has a time component which is the energy, and three space components which are zero. By changing frames with a Lorentz transformation in the x direction with a small value of the velocity v, the energy momentum four-vector becomes (E/c, Ev/c2, 0, 0). The momentum is equal to the energy multiplied by the velocity divided by c2. As such, the Newtonian mass of an object, which is the ratio of the momentum to the velocity for slow velocities, is equal to E/c2.In addition to the papers referenced above—which give derivations of the Lorentz transformation and describe the foundations of special relativity—Einstein also wrote at least four papers giving heuristic arguments for the equivalence (and transmutability) of mass and energy, for E = mc2.The energy content of an object at rest with mass m equals mc2. Conservation of energy implies that, in any reaction, a decrease of the sum of the masses of particles must be accompanied by an increase in kinetic energies of the particles after the reaction. Similarly, the mass of an object can be increased by taking in kinetic energies.As an object's speed approaches the speed of light from an observer's point of view, its relativistic mass increases thereby making it more and more difficult to accelerate it from within the observer's frame of reference.The orientation of an object (i.e. the alignment of its axes with the observer's axes) may be different for different observers. Unlike other relativistic effects, this effect becomes quite significant at fairly low velocities as can be seen in the spin of moving particles.The usual example given is that of a train (frame S′ above) traveling due east with a velocity v with respect to the tracks (frame S). A child inside the train throws a baseball due east with a velocity u′ with respect to the train. In nonrelativistic physics, an observer at rest on the tracks will measure the velocity of the baseball (due east) as u = u′ + v, while in special relativity this is no longer true; instead the velocity of the baseball (due east) is given by the second equation: u = (u′ + v)/(1 + u′v/c2). Again, there is nothing special about the x or east directions. This formalism applies to any direction by considering parallel and perpendicular components of motion to the direction of relative velocity v, see main article for details.Notice that if the object were moving at the speed of light in the S system (i.e. u = c), then it would also be moving at the speed of light in the S′ system. Also, if both u and v are small with respect to the speed of light, we will recover the intuitive Galilean transformation of velocitiesThe other frame S will measure:Velocities (speeds) do not simply add. If the observer in S measures an object moving along the x axis at velocity u, then the observer in the S′ system, a frame of reference moving at velocity v in the x direction with respect to S, will measure the object moving with velocity u′ where (from the Lorentz transformations above):This shows that the length (Δx′) of the rod as measured in the frame in which it is moving (S′), is shorter than its length (Δx) in its own rest frame (S).Similarly, suppose a measuring rod is at rest and aligned along the x-axis in the unprimed system S. In this system, the length of this rod is written as Δx. To measure the length of this rod in the system S′, in which the rod is moving, the distances x′ to the end points of the rod must be measured simultaneously in that system S′. In other words, the measurement is characterized by Δt′ = 0, which can be combined with the fourth equation to find the relation between the lengths Δx and Δx′:The dimensions (e.g., length) of an object as measured by one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage).This shows that the time (Δt′) between the two ticks as seen in the frame in which the clock is moving (S′), is longer than the time (Δt) between these ticks as measured in the rest frame of the clock (S). Time dilation explains a number of physical phenomena; for example, the lifetime of muons produced by cosmic rays impinging on the Earth's atmosphere is measured to be greater than the lifetimes of muons measured in the laboratory.[34]Suppose a clock is at rest in the unprimed system S. The location of the clock on two different ticks is then characterized by Δx = 0. To find the relation between the times between these ticks as measured in both systems, the first equation can be used to find:The time lapse between two events is not invariant from one observer to another, but is dependent on the relative speeds of the observers' reference frames (e.g., the twin paradox which concerns a twin who flies off in a spaceship traveling near the speed of light and returns to discover that his or her twin sibling has aged much more).it is clear that two events that are simultaneous in frame S (satisfying Δt = 0), are not necessarily simultaneous in another inertial frame S′ (satisfying Δt′ = 0). Only if these events are additionally co-local in frame S (satisfying Δx = 0), will they be simultaneous in another frame S′.From the first equation of the Lorentz transformation in terms of coordinate differencesTwo events happening in two different locations that occur simultaneously in the reference frame of one inertial observer, may occur non-simultaneously in the reference frame of another inertial observer (lack of absolute simultaneity).The consequences of special relativity can be derived from the Lorentz transformation equations.[33] These transformations, and hence special relativity, lead to different physical predictions than those of Newtonian mechanics when relative velocities become comparable to the speed of light. The speed of light is so much larger than anything humans encounter that some of the effects predicted by relativity are initially counterintuitive.Another example where visual appearance is at odds with measurement comes from the observation of apparent superluminal motion in various radio galaxies, BL Lac objects, quasars, and other astronomical objects that eject relativistic-speed jets of matter at narrow angles with respect to the viewer. An optical illusion results giving the appearance of faster than light travel.[29][30][31] In Fig. 1‑14, galaxy M87 streams out a high-speed jet of subatomic particles almost directly towards us, but Penrose–Terrell rotation causes the jet to appear to be moving laterally in the same manner that the appearance of the cube in Fig. 1‑13 has been stretched out.[32]Fig. 1‑13 illustrates a cube viewed from a distance of four times the length of its sides. At high speeds, the sides of the cube that are perpendicular to the direction of motion appear hyperbolic in shape. The cube is actually not rotated. Rather, light from the rear of the cube takes longer to reach one's eyes compared with light from the front, during which time the cube has moved to the right. This illusion has come to be known as Terrell rotation or the Terrell–Penrose effect.[note 1]For many years, the distinction between the two had not been generally appreciated. For example, it had generally been thought that a length contracted object passing by an observer would in fact actually be seen as length contracted. In 1959, James Terrell and Roger Penrose independently pointed out that differential time lag effects in signals reaching the observer from the different parts of a moving object result in a fast moving object's visual appearance being quite different from its measured shape. For example, a receding object would appear contracted, an approaching object would appear elongated, and a passing object would have a skew appearance that has been likened to a rotation.[24][25][26][27] A sphere in motion retains the appearance of a sphere, although images on the surface of the sphere will appear distorted.[28]That being said, scientists make a fundamental distinction between measurement or observation on the one hand, versus visual appearance, or what one sees.Time dilation and length contraction are not optical illusions, but genuine effects. Measurements of these effects are not an artifact of Doppler shift, nor are they the result of neglecting to take into account the time it takes light to travel from an event to an observer.These effects are explicitly related to our way of measuring time intervals between events which occur at the same place in a given coordinate system (called "co-local" events). These time intervals will be different in another coordinate system moving with respect to the first, unless the events are also simultaneous. Similarly, these effects also relate to our measured distances between separated but simultaneous events in a given coordinate system of choice. If these events are not co-local, but are separated by distance (space), they will not occur at the same spatial distance from each other when seen from another moving coordinate system. However, the spacetime interval will be the same for all observers.we getWriting the Lorentz transformation and its inverse in terms of coordinate differences, where for instance one event has coordinates (x1, t1) and (x′1, t′1), another event has coordinates (x2, t2) and (x′2, t′2), and the differences are defined asA quantity invariant under Lorentz transformations is known as a Lorentz scalar.There is nothing special about the x-axis, the transformation can apply to the y or z axes, or indeed in any direction, which can be done by directions parallel to the motion (which are warped by the γ factor) and perpendicular; see main article for details.is the Lorentz factor and c is the speed of light in vacuum, and the velocity v of S′ is parallel to the x-axis. The y and z coordinates are unaffected; only the x and t coordinates are transformed. These Lorentz transformations form a one-parameter group of linear mappings, that parameter being called rapidity.whereDefine the event to have spacetime coordinates (t,x,y,z) in system S and (t′,x′,y′,z′) in a reference frame moving at a velocity v with respect to that frame, S′. Then the Lorentz transformation specifies that these coordinates are related in the following way:Since there is no absolute reference frame in relativity theory, a concept of 'moving' doesn't strictly exist, as everything is always moving with respect to some other reference frame. Instead, any two frames that move at the same speed in the same direction are said to be comoving. Therefore, S and S′ are not comoving.Suppose we have a second reference frame S′, whose spatial axes and clock exactly coincide with that of S at time zero, but it is moving at a constant velocity v with respect to S along the x-axis.In relativity theory we often want to calculate the position of a point from a different reference point.For example, the explosion of a firecracker may be considered to be an "event". We can completely specify an event by its four spacetime coordinates: The time of occurrence and its 3-dimensional spatial location define a reference point. Let's call this reference frame S.An event is an occurrence that can be assigned a single unique time and location in space relative to a reference frame: it is a "point" in spacetime. Since the speed of light is constant in relativity in each and every reference frame, pulses of light can be used to unambiguously measure distances and refer back the times that events occurred to the clock, even though light takes time to reach the clock after the event has transpired.Reference frames play a crucial role in relativity theory. The term reference frame as used here is an observational perspective in space which is not undergoing any change in motion (acceleration), from which a position can be measured along 3 spatial axes. In addition, a reference frame has the ability to determine measurements of the time of events using a 'clock' (any reference device with uniform periodicity).The principle of relativity, which states that physical laws have the same form in each inertial reference frame, dates back to Galileo, and was incorporated into Newtonian physics. However, in the late 19th century, the existence of electromagnetic waves led physicists to suggest that the universe was filled with a substance that they called "aether", which would act as the medium through which these waves, or vibrations travelled. The aether was thought to constitute an absolute reference frame against which speeds could be measured, and could be considered fixed and motionless. Aether supposedly possessed some wonderful properties: it was sufficiently elastic to support electromagnetic waves, and those waves could interact with matter, yet it offered no resistance to bodies passing through it. The results of various experiments, including the Michelson–Morley experiment, led to the theory of special relativity, by showing that there was no aether.[23] Einstein's solution was to discard the notion of an aether and the absolute state of rest. In relativity, any reference frame moving with uniform motion will observe the same laws of physics. In particular, the speed of light in vacuum is always measured to be c, even when measured by multiple systems that are moving at different (but constant) velocities.The constancy of the speed of light was motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous ether. There is conflicting evidence on the extent to which Einstein was influenced by the null result of the Michelson–Morley experiment.[21][22] In any case, the null result of the Michelson–Morley experiment helped the notion of the constancy of the speed of light gain widespread and rapid acceptance.From the principle of relativity alone without assuming the constancy of the speed of light (i.e. using the isotropy of space and the symmetry implied by the principle of special relativity) one can show that the spacetime transformations between inertial frames are either Euclidean, Galilean, or Lorentzian. In the Lorentzian case, one can then obtain relativistic interval conservation and a certain finite limiting speed. Experiments suggest that this speed is the speed of light in vacuum.[19][20]Thus many modern treatments of special relativity base it on the single postulate of universal Lorentz covariance, or, equivalently, on the single postulate of Minkowski spacetime.[17][18]Einstein consistently based the derivation of Lorentz invariance (the essential core of special relativity) on just the two basic principles of relativity and light-speed invariance. He wrote:Many of Einstein's papers present derivations of the Lorentz transformation based upon these two principles.[16]Henri Poincaré provided the mathematical framework for relativity theory by proving that Lorentz transformations are a subset of his Poincaré group of symmetry transformations. Einstein later derived these transformations from his axioms.Following Einstein's original presentation of special relativity in 1905, many different sets of postulates have been proposed in various alternative derivations.[14] However, the most common set of postulates remains those employed by Einstein in his original paper. A more mathematical statement of the Principle of Relativity made later by Einstein, which introduces the concept of simplicity not mentioned above is:The derivation of special relativity depends not only on these two explicit postulates, but also on several tacit assumptions (made in almost all theories of physics), including the isotropy and homogeneity of space and the independence of measuring rods and clocks from their past history.[13]Einstein discerned two fundamental propositions that seemed to be the most assured, regardless of the exact validity of the (then) known laws of either mechanics or electrodynamics. These propositions were the constancy of the speed of light and the independence of physical laws (especially the constancy of the speed of light) from the choice of inertial system. In his initial presentation of special relativity in 1905 he expressed these postulates as:[1]Galileo Galilei had already postulated that there is no absolute and well-defined state of rest (no privileged reference frames), a principle now called Galileo's principle of relativity. Einstein extended this principle so that it accounted for the constant speed of light,[10] a phenomenon that had been recently observed in the Michelson–Morley experiment. He also postulated that it holds for all the laws of physics, including both the laws of mechanics and of electrodynamics.[11]As Galilean relativity is now considered an approximation of special relativity that is valid for low speeds, special relativity is considered an approximation of general relativity that is valid for weak gravitational fields, i.e. at a sufficiently small scale and in conditions of free fall. Whereas general relativity incorporates noneuclidean geometry in order to represent gravitational effects as the geometric curvature of spacetime, special relativity is restricted to the flat spacetime known as Minkowski space. A locally Lorentz-invariant frame that abides by special relativity can be defined at sufficiently small scales, even in curved spacetime.The theory is "special" in that it only applies in the special case where the curvature of spacetime due to gravity is negligible.[6][7] In order to include gravity, Einstein formulated general relativity in 1915. Special relativity, contrary to some outdated descriptions, is capable of handling accelerations as well as accelerated frames of reference.[8][9]A defining feature of special relativity is the replacement of the Galilean transformations of Newtonian mechanics with the Lorentz transformations. Time and space cannot be defined separately from each other. Rather, space and time are interwoven into a single continuum known as spacetime. Events that occur at the same time for one observer can occur at different times for another.Special relativity implies a wide range of consequences, which have been experimentally verified,[3] including length contraction, time dilation, relativistic mass, mass–energy equivalence, a universal speed limit and relativity of simultaneity. It has replaced the conventional notion of an absolute universal time with the notion of a time that is dependent on reference frame and spatial position. Rather than an invariant time interval between two events, there is an invariant spacetime interval. Combined with other laws of physics, the two postulates of special relativity predict the equivalence of mass and energy, as expressed in the mass–energy equivalence formula E = mc2, where c is the speed of light in a vacuum.[4][5]Not until Einstein developed general relativity, to incorporate general (i.e., including accelerated) frames of reference and gravity, was the phrase "special relativity" employed. A translation that has often been used is "restricted relativity"; "special" really means "special case".[2]It was originally proposed by Albert Einstein in a paper published 26 September 1905 titled "On the Electrodynamics of Moving Bodies".[1] The inconsistency of Newtonian mechanics with Maxwell's equations of electromagnetism and the lack of experimental confirmation for a hypothesized luminiferous aether led to the development of special relativity, which corrects mechanics to handle situations involving motions at a significant fraction of the speed of light (known as relativistic velocities). As of today, special relativity is the most accurate model of motion at any speed when gravitational effects are negligible. Even so, the Newtonian mechanics model is still useful (due to its simplicity and high accuracy) as an approximation at small velocities relative to the speed of light.In physics, special relativity (SR, also known as the special theory of relativity or STR) is the generally accepted and experimentally well-confirmed physical theory regarding the relationship between space and time. In Albert Einstein's original pedagogical treatment, it is based on two postulates:
Statistics
Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was "required learning" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on "experimental" and "empirical" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.There are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis."Applied statistics" comprises descriptive statistics and the application of inferential statistics.[58][59] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[57]The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of "Type II" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[56]The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[44][45][46][47] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[48] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle[49]). Nevertheless, A. W. F. Edwards has remarked that it is "probably the most celebrated argument in evolutionary biology".[49] (about the sex ratio), the Fisherian runaway,[50][51][52][53][54][55] a concept in sexual selection about a positive feedback runaway affect found in evolution.Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which "is never proved or established, but is possibly disproved, in the course of experimentation".[42][43]The modern field of statistics emerged in the late 19th and early 20th century in three stages.[37] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight, eyelash length among others.[38] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[39] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[40] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[41]Its mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[36] The method of least squares was first described by Adrien-Marie Legendre in 1805.Some scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[35] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.Statistical methods date back at least to the 5th century BC.[citation needed]The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[34]Ways to avoid misuse of statistics include using proper diagrams and avoiding bias.[30] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[31] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[30] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[31] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[32] According to Huff, "The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism."[33]There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[28] A mistrust and misunderstanding of statistics is associated with the quotation, "There are three kinds of lies: lies, damned lies, and statistics". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[28] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[29]Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.Misuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.Some well-known statistical tests and procedures are:Some problems are usually associated with this framework (See criticism of hypothesis testing):While in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.The standard approach[23] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by "probability", that is as a Bayesian probability.Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.[26]Many statistical methods seek to minimize the residual sum of squares, and these are called "methods of least squares" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.Working from a null hypothesis, two basic forms of error are recognized:What statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence "beyond a reasonable doubt". However, "failure to reject H0" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not "prove" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[24][25]This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[23] The population being examined is described by a probability distribution that may have unknown parameters.The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. "The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer" (Hand, 2004, p. 82).[22]Other categorizations have been proposed. For example, Mosteller and Tukey (1977)[18] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[19] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[20] van den Berg (1991).[21]Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group.[17] A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[16]The basic steps of a statistical experiment are:A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies[15]—for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models. The idea of making inferences based on sampled data began around the mid-1600's in connection with estimating populations and developing precursors of life insurance.[14]When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).In applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal".Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[12][13]Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[8] or as a branch of mathematics.[9] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[10][11]Some definitions are:Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. In more recent years statistics has relied more on statistical software to produce tests such as descriptive analysis[5]Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and synthetic data drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative").[4] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[3] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.Statistics is a branch of mathematics dealing with the collection, analysis, interpretation, presentation, and organization of data.[1][2] In applying statistics to, for example, a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as "all people living in a country" or "every atom composing a crystal". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1] See glossary of probability and statistics.
Algorithm
A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:Turing's reduction yields the following:Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[97]Alan Turing's work[95] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'".[96] Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we[who?] might conjecture that all were influences.His symbol space would beEmil Post (1936) described the actions of a "computer" (human being) as follows:Here is a remarkable coincidence[according to whom?] of two men not knowing each other but describing a process of men-as-computers working on computations—and they yield virtually identical definitions.Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus[85] a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene.[86] Church's proof[87] that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction.[88] Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"[89]—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".[90] S. C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I",[91] and a few years later Kleene's renaming his Thesis "Church's Thesis"[92] and proposing "Turing's Thesis".[93]The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox.[84] The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules".[83] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".[82]Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".[80]Jacquard loom, Hollerith punch cards, telegraphy and telephony—the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[79] By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.This machine he displayed in 1870 before the Fellows of the Royal Society.[77] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".[78]Logical machines 1870—Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] . . .". With this machine he could analyze a "syllogism or any other simple logical argument".[76]The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular the verge escapement[73] that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[74] led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century.[75] Lovelace is credited with the first creation of an algorithm intended for processing on a computer – Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator – and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.Algorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus,[70][8]:Ch 9.2 and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[8]:Ch 9.1 Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[71]Researcher, Andrew Tutt, argues that algorithms should be overseen by a specialist regulatory agency, similar to FDA. His academic work emphasizes that the rise of increasingly complex algorithms calls for the need to think about the effects of algorithms today. Due to the nature and complexity of algorithms, it will prove to be difficult to hold algorithms accountable under criminal law. Tutt recognizes that while some algorithms will be beneficial to help meet technological demand, others should not be used or sold if they fail to meet safety requirements. Thus, for Tutt, algorithms will require "closer forms of federal uniformity, expert judgment, political independence, and pre-market review to prevent the introduction of unacceptably dangerous algorithms into the market".[68] The issue of algorithmic accountability[69] (the responsibility of algorithm designers to provide evidence of potential or realised harms) is of particular relevance in the field of dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms (see Explainable AI).Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.The adjective "continuous" when applied to the word "algorithm" can mean:Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.Algorithms can be classified by the amount of time they need to complete compared to their input size:Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories include many different types of algorithms. Some common paradigms are:One way to classify algorithms is by implementation means.There are various ways to classify algorithms, each with its own merits.To illustrate the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[60] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[61] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization. Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[59]The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B and R, S would require a detailed analysis.The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm;[58] rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.Can the algorithms be improved?: Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[57] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[55] Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.[56]But exceptional cases must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).Does an algorithm do what its author wants it to do? A few test cases usually suffice to confirm core functionality. One source[54] uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.The following version can be used with Object Oriented languages:The following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more types of instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.DONE:OUTPUT:E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.E0: [Ensure r ≥ s.]INPUT:The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:Only a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest.[53] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements.[50] Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s.[51] In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the "modulus", the integer-fractional part left over after the division.[52](Quasi-)formal description: Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:High-level description:One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description English prose, as:Canonical flowchart symbols[49]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures, are shown in the diagram.Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[45] Tausworthe augments the three Böhm-Jacopini canonical structures:[46] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[47] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[48]But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters".[44] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[42] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[43]Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".[38] Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[39] operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1).[40] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.[41]Computers (and computors), models of computation: A computer (or human "computor"[32]) is a restricted type of machine, a "discrete deterministic mechanical device"[33] that blindly follows its instructions.[34] Melzak's and Lambek's primitive models[35] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[36] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[37]Unfortunately there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[31]Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'"—such a proof would solve the Halting problem (ibid)."Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.Representations of algorithms can be classed into three accepted levels of Turing machine description:[28]There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as y = m + n – two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word in the following quotation:A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flow chart above and as an example in a later section.An informal definition could be "a set of rules that precisely defines a sequence of operations."[18] which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[19]The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.which translates as:Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum. This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name.[15] Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra.[16] In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word ἀριθμός 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.[17]The word 'algorithm' probably has its roots in latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to algorismus.[11][12] Al-Khwārizmī (Persian: خوارزمی‎, c. 780–850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in Uzbekistan.[13][14]The concept of algorithm has existed for centuries and the use of the concept can be ascribed to Greek mathematicians, e.g. the sieve of Eratosthenes and Euclid's algorithm;[8] the term algorithm itself derives from the 9th Century mathematician Muḥammad ibn Mūsā al'Khwārizmī, latinized 'Algoritmi'. A partial formalization of what would become the modern notion of algorithm began with attempts to solve the Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability"[9] or "effective method";[10] those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's "Formulation 1" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939.An algorithm is an effective method that can be expressed within a finite amount of space and time[1] and in a well-defined formal language[2] for calculating a function.[3] Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well-defined successive states, eventually producing "output"[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[7]
Determinant
where ωj is an nth root of 1.where ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]Third orderSecond orderwhere the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.In general, the nth-order Vandermonde determinant is[33]The third order Vandermonde determinant isThe Jacobian also occurs in the inverse function theorem.Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given bythe Jacobian matrix is the n × n matrix whose entries are given byFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. ForBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Another infinite-dimensional notion of determinant is the functional determinant.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,holds. In other words, the following diagram commutes:between the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityThe determinant defines a mappingThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.from the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isThe determinant can also be characterized as the unique functionThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn). As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple. We call this scalar the determinant of T.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.As ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThe determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant is therefore also called a similarity invariant. The determinant of a linear transformationThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThis identity is used in describing the tangent space of certain matrix Lie groups.Yet another equivalent formulation isExpressed in terms of the entries of A, these arewhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveBy definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]When D is a 1×1 matrix, B is a column vector, and C is a row vector thenWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]as can be seen by employing the decompositionWhen A is invertible, one hasThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenIt has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.the solution is given by Cramer's rule:For a matrix equationThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.Also,with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantis expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).where I is the identity matrix. More generally, ifAn important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way asThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l – 1)! tr(Al) aswhere the sum is taken over the set of all integers kl ≥ 0 satisfying the equationIn the general case, this may also be obtained from[9]cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromFor example, for n = 2, n = 3, and n = 4, respectively,the determinant of A is given byHere exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingor, for real matrices A,The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,being positive, for all k between 1 and n.A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatriceswhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationThe product of all non-zero eigenvalues is referred to as pseudo-determinant.From this general result several consequences follow.where Im and In are the m × m and n × n identity matrices, respectively.Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):In terms of the adjugate matrix, Laplace's expansion can be written as[7]The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,However, Laplace expansion is efficient for small matrices only.along the second column (j = 2 and the sum runs over i) is given by,Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i+jMi,j is known as a cofactor. The determinant of A is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant of a matrix product of square matrices equals the product of their determinants:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.can be computed using the following matrices:For example, the determinant ofProperty 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.Property 2 above implies that properties for columns have their counterparts in terms of rows:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.The determinant has many properties. Some basic properties of determinants arewhere now each ir and each jr should be summed over 1, …, n.or using two epsilon symbols asFor example, the determinant of a 3 × 3 matrix A (n = 3) isis notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:Here the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.The Leibniz formula for the determinant of an n × n matrix A isThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.which is the Leibniz formula for the determinant of a 3 × 3 matrix.this can be expanded out to giveThe Laplace formula for the determinant of a 3 × 3 matrix isThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (-b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The Leibniz formula for the determinant of a 2 × 2 matrix isThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.Assume A is a square matrix with n rows and n columns, so that it can be written asEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n × n matrix A in terms of its column vectorsThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 × 4 matrix:When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 × 4 matrix, the determinant of a 5 × 5 matrix, and so forth.Similarly, suppose we have a 3 × 3 matrix A, and we want the specific formula for its determinant |A|:In the case of a 2 × 2 matrix the specific formula for the determinant is:In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.
Gaussian elimination
Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]The Gaussian elimination can be performed over any field, not just the real numbers.One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 − 5n)/6 multiplications, and (2n3 + 3n2 − 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).All of this applies also to the reduced row echelon form, which is a particular row echelon form.One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:For example, consider the following matrixA variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.Computationally, for a n×n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = ∏diag(B) / d.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Suppose the goal is to find and describe the set of solutions to the following system of linear equations:A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.There are three types of elementary row operations which may be performed on the rows of a matrix:Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
School Mathematics Study Group
Perhaps the most authoritative collection of materials from the School Mathematics Study Group is now housed in the Archives of American Mathematics in the University of Texas at Austin's Center for American History.The School Mathematics Study Group (SMSG) was an American academic think tank focused on the subject of reform in mathematics education. Directed by Edward G. Begle and financed by the National Science Foundation, the group was created in the wake of the Sputnik crisis in 1958 and tasked with creating and implementing mathematics curricula for primary and secondary education, which it did until its termination in 1977. The efforts of the SMSG yielded a reform in mathematics education known as New Math which was promulgated in a series of reports, which culminated in a series published by Random House called the New Mathematical Library. In the early years SMSG also rushed out a set of draft textbooks in typewritten paperback format for elementary, middle and high school students.
Secondary school
A secondary school, locally may be called high school or senior high school. In some countries there are two phases to secondary education (ISCED 2) and (ISCED 3), here the junior high school, intermediate school, lower secondary school, or middle school occurs between the primary school (ISCED 1) and high school.The UK government published this downwardly revised space formula in 2014. It said the floor area should be 1050m² (+ 350m² if there is a sixth form) + 6.3m²/pupil place for 11- to 16-year-olds + 7m²/pupil place for post-16s. The external finishes were to be downgraded to meet a build cost of £1113/m². [9]Government accountants having read the advice then publish minimum guidelines on schools. These enable environmental modelling and establishing building costs. Future design plans are audited to ensure that these standards are met but not exceeded. Government ministries continue to press for the 'minimum' space and cost standards to be reduced.The building providing the education has to fulfil the needs of: The students, the teachers, the non-teaching support staff, the administrators and the community. It has to meet general government building guidelines, health requirements, minimal functional requirements for classrooms, toilets and showers, electricity and services, preparation and storage of textbooks and basic teaching aids. [8] An optimum secondary school will meet the minimum conditions and will have :According to standards used in the United Kingdom, a general classroom for 30 students needs to be 55 m², or more generously 62 m². A general art room for 30 students needs to be 83 m², but 104 m² for 3D textile work. A drama studio or a specialist science laboratory for 30 needs to be 90 m². Examples are given on how this can be configured for a 1,200 place secondary (practical specialism).[6] and 1,850 place secondary school.[7]Each country will have a different education system and priorities. [5] Schools need to accommodate students, staff, storage, mechanical and electrical systems, storage, support staff, ancillary staff and administration. The number of rooms required can be determined from the predicted roll of the school and the area needed.School building design does not happen in isolation. The building (or school campus) needs to accommodate:Within the English speaking world, there are three widely used systems to describe the age of the child. The first is the 'equivalent ages', then countries that base their education systems on the 'English model' use one of two methods to identify the year group, while countries that base their systems on the 'American K-12 model' refer to their year groups as 'grades'. This terminology extends into research literature. Below is a convenient comparison [4][3]Secondary schools typically follow on from primary schools and lead into vocational and tertiary education. Attendance is compulsory in most countries for students between the ages of 11 and 16. The organisations, buildings, and terminology are more or less unique in each country.[1][2]A secondary school is both an organization that provides secondary education and the building where this takes place. Some secondary schools can provide both lower secondary education and upper secondary education (levels 2 and 3 of the ISCED scale), but these can also be provided in separate schools, as in the American middle school- high school system.
Singular-value decomposition
The controversy around 2018 on Cambridge Analityca's use of suspiciously obtained Facebook data in helping political campaigns in the last US presidential elections and later many elections in other countries involved techniques related to SVD.[26]Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. σi are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero λ in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M∗M. Applying the diagonalization result, the unitary image of its positive square root Tf  has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {σi}. For any ψ ∈ H,and notice that U V* is still a partial isometry while VTf V* is positive.As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply writeis a unitary operator.This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such thatThe factorization M = UΣV∗ can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X, μ), and a non-negative measurable f such thatTP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.where σi are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert–Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:Since the trace is invariant under unitary equivalence, this showsSo the induced norm isThe singular values are related to another norm on the space of operators. Consider the Hilbert–Schmidt inner product on the n × n matrices, defined byThe last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)½] (the eigenvalues of M* M are the squares of the singular values).But, in the matrix case, (M* M)½ is a normal matrix, so ||M* M||½ is the largest eigenvalue of (M* M)½, i.e. the largest singular value of M.The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spacesThe sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values Σt are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t≪r. The matrix Ut is thus m×t, Σt is t×t diagonal, and Vt* is t×n.Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values Σr are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r ≪ n. The matrix Ur is thus m×r, Σr is r×r diagonal, and Vr* is r×n.The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if n ≪ m.Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n ≪ m. The matrix U'n is thus m×n, Σn is n×n diagonal, and V is n×n.In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m×n matrix M of rank r:The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M ⇒ Q R and LQ decomposition of R gives R ⇒ L P*. Thus, at every iteration, we have M ⇒ Q L P*, update M ⇐ L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, orThe same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 × 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 × 2 eigenvalue methods (Golub & Van Loan 1996, §8.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.The first step can be done using Householder reflections for a cost of 4mn2 − 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m ≥ n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).To get a more visual flavour of singular values and SVD factorization — at least when working on real vector spaces — consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V∗ sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D ∘ V∗ then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U ∘ D ∘ V∗ coincides with T.The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T : Kn → Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.where σi is the i-th diagonal entry of Σ, and T(Vi) = 0 for i > min(m,n).has a particularly simple description with respect to these orthonormal bases: we haveThe linear transformationBecause U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).The passage from real to complex is similar to the eigenvalue case.More singular vectors and singular values can be found by maximizing σ(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.This proves the statement.Plugging this into the pair of equations above, we haveAfter some algebra, this becomesProof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:Consider the function σ restricted to Sm−1 × Sn−1. Since both Sm−1 and Sn−1 are compact sets, their product is also compact. Furthermore, since σ is continuous, it attains a largest value for at least one pair of vectors u ∈ Sm−1 and v ∈ Sn−1. This largest value is denoted σ1 and the corresponding vectors are denoted u1 and v1. Since σ1 is the largest value of σ(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.Let M denote an m × n matrix with real entries. Let Sm−1 and Sn−1 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the functionThe singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.Notice the argument could begin with diagonalizing MM∗ rather than M∗M (This shows directly that MM∗ and M∗M have the same non-zero eigenvalues).which is the desired result:For V1 we already have V2 to make it unitary. Now, definethe columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, sinceThenwhere the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. DefineThe second equation implies MV2 = 0. Also, since V is unitary:Therefore:where D is diagonal and positive definite. Partition V appropriately so we can writeLet M be an m × n complex matrix. Since M∗M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n × n matrix V such thatThis section gives these two arguments for existence of singular-value decomposition.Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.A short calculation shows the above leads to Mu = λu (symmetry of M is needed here). Therefore, λ is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.where the nabla symbol, ∇, is the del operator.By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| ≤ 1}. By the Lagrange multipliers theorem, u necessarily satisfiesAn eigenvalue λ of a matrix M is characterized by the algebraic relation Mu = λu. When M is Hermitian, a variational characterization is also available. Let M be a real n × n symmetric matrix. DefineIn the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU∗ for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU∗ is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each σi to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=UΣU* is positive semidefinite and normal, and R=UV* is unitary.The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:Given an SVD of M, as described above, the following two relations hold:The singular-value decomposition is very general in the sense that it can be applied to any m × n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the Σ matrix is larger than one.The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules.This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O − A. The solution is the product UV∗.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV∗ where I is the identity matrix, so that if A = UΣV∗ then the product A = UV∗ amounts to replacing the singular values with ones.which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,Here Ui and Vi are the i-th columns of the corresponding SVD matrices, σi are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero σi is exactly the rank of the matrix.As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in Σ. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x∗A = 0, with x∗ denoting the conjugate transpose of x, is called a left null vector of A.where Σ+ is the pseudoinverse of Σ, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = UΣV∗ isNon-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor eiφ (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank–nullity theorem cannot be the same dimension if m ≠ n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m − n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n − m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.the diagonal entries of Σ are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:In any singular-value decompositionis also a valid singular-value decomposition.Notice Σ is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices U and V∗ are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V∗ are real valued, each is an orthogonal matrix.A singular-value decomposition of this matrix is given by UΣV∗Consider the 4 × 5 matrixSince U and V∗ are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector σi Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U∗ and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U∗, V, and V∗ are orthonormal bases.As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n × n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m × n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = VΣV∗) with eigenvalue scale factors σi along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV∗). If the rotation is done first, M = P'R, then R is the same and P' = UΣU∗ has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate.In the special, yet common, case when M is an m × m real square matrix with positive determinant: U, V∗, and Σ are real m × m matrices as well. Σ can be regarded as a scaling matrix, and U, V∗ can be viewed as rotation matrices. Thus the expression UΣV∗ can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.The diagonal entries σi of Σ are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, Σ, is uniquely determined by M (though not the matrices U and V, see below).whereSuppose M is a m × n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the formApplications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.The singular-value decomposition can be computed using the following observations:
Vector space
The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.generalizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x ∈ W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v ∈ V. An important example is the space of solutions of a system of inhomogeneous linear equationsRoughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapModules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[103] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 × R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.such that for every x in X, the fiber π−1(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X × V → X makes the product X × V into a "trivial" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of π to π−1(U) is isomorphic[nb 16] to the trivial bundle U × V → U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X × V). For example, the Möbius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 × R, because the latter is orientable whereas the former is not.[99]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous mapRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).[92][93]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2π), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isWhen Ω = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function f its value at the p: δ(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).[77]A distribution (or generalized function) is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain Ω:Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]When a field, F is explicitly stated, a common term used is F-algebra.The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 ⊗ v2 and v2 ⊗ v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 ⊗ v2 = − v2 ⊗ v1 yields the exterior algebra.[73]The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsExamples include the vector space of n-by-n matrices, with [x, y] = xy − yx, the commutator of two matrices, and R3, endowed with the cross product.Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone–Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61] The Hilbert space L2(Ω), with inner product given byImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]there exists a function f(x) belonging to the vector space Lp(Ω) such thatThe space of integrable functions on a given domain Ω (for example an interval) satisfying |f|p < ∞, and equipped with this norm are called Lebesgue spaces, denoted Lp(Ω).[nb 10] These spaces are complete.[59] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < ∞, satisfying the conditionMore generally than sequences of real numbers, functions f: Ω → R are endowed with a norm that replaces the above sum by the Lebesgue integralis finite. The topologies on the infinite-dimensional space ℓ p are inequivalent for different p. E.g. the sequence of vectors xn = (2−n, 2−n, ..., 2−n, 0, 0, ...), i.e. the first 2n components are 2−n, the following ones are 0, converges to the zero vector for p = ∞, but does not for p = 1:Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] A first example is the vector space ℓ p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 ≤ p ≤ ∞) given byFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V → W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V∗ consists of continuous functionals V → R (or to C). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and ∞-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifdenotes the limit of the corresponding finite partial sums of the sequence (fi)i∈N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.In such topological vector spaces one can consider series of vectors. The infinite sumConvergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Coordinate space Fn can be equipped with the standard dot product:where f+ denotes the positive part of f and f− the negative part.[48]A vector space may be given a partial order ≤, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionsFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.These rules ensure that the map f from the V × W to V ⊗ W that maps a tuple (v, w) to v ⊗ w is bilinear. The universality states that given any vector space X and any bilinear map g : V × W → X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v ⊗ w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.subject to the rulesThe tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorsThe tensor product V ⊗F W, or simply V ⊗ W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g : V × W → X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v ↦ g(v, w) is linear in the sense above and likewise for fixed v.The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.the derivatives of the function f appear linearly (as opposed to f′′(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)′ = f′ + g ′ and (c·f)′ = c·f′ for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).In the corresponding mapAn important example is the kernel of a linear map x ↦ Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.The kernel ker(f) of a linear map f : V → W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v) : v ∈ V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W ⊂ V, the quotient space V/W ("V modulo W") is defined as follows: as a set, it consists of v + W = {v + w : w ∈ W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a · (v + W) = (a · v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space "forgets" information that is contained in the subspace W.A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n – 1.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in λ, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.Endomorphisms, linear maps f : V → V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying λv = f(v), where λ is a scalar, is called an eigenvector of f with eigenvalue λ.[nb 5][35] Equivalently, v is an element of the kernel of the difference f − λ · Id (where Id is the identity map V → V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue λ is equivalent toThe determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Moreover, after choosing bases of V and W, any linear map f : V → W is uniquely represented by a matrix via this assignment.[33]or, using the matrix multiplication of the matrix A with the coordinate vector x:Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingOnce a basis of V is chosen, linear maps f : V → W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no "canonical" or preferred isomorphism; actually an isomorphism φ : Fn → V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via φ. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.Linear maps V → W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V∗.[28] Via the injective natural map V → V∗∗, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.An isomorphism is a linear map f : V → W such that there exists an inverse map g : W → V, which is a map such that the two possible compositions f ∘ g : W → W and g ∘ f : V → V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(α) over Q depends on α. If α satisfies some polynomial equation The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e−x and xe−x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i ∈ I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:yields f(x) = a e−x + bx e−x, where a and b are arbitrary constants, and ex is the natural exponential function.are given by triples with arbitrary a, b = a/2, and c = −5a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelySystems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions ofand similarly for multiplication. Such function spaces occur in many geometric situations, when Ω is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Functions from any fixed set Ω to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byIn fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c ⋅ (x + iy) = (c ⋅ x) + i(c ⋅ y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such asAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by Möbius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse −v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.Subtraction of two vectors and division by a (non-zero) scalar can be defined asLikewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:To qualify as a vector space, the set V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.Elements of V are commonly called vectors. Elements of F are commonly called scalars.A vector space over a field F is a set V together with two operations that satisfy the eight axioms listed below.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.andA second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (−1)v = −v has the opposite direction and the same length as v (blue vector pointing down in the right image).The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The concept of vector space will first be explained by describing two particular examples:Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.
Field (mathematics)

Real number
The real numbers can be generalized and extended in several different directions:In mathematics, real is used as an adjective, meaning that the underlying field is the field of the real numbers (or the real field). For example, real matrix, real polynomial and real Lie algebra. The word is also used as a noun, meaning a real number (as in "the set of all reals").The notation Rn refers to the cartesian product of n copies of R, which is an n-dimensional vector space over the field of the real numbers; this vector space may be identified to the n-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3‑dimensional space.The sets of positive real numbers and negative real numbers are often noted R+ and R−,[14] respectively; R+ and R− are also used.[15] The non-negative real numbers can be noted R≥0 but one often sees this set noted R+ ∪ {0}.[14] In French mathematics, the positive real numbers and negative real numbers commonly include zero, and these sets are noted respectively ℝ+ and ℝ−.[15] In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are noted ℝ+* and ℝ−*.[15]Mathematicians use the symbol R, or, alternatively, ℝ, the letter "R" in blackboard bold (encoded in Unicode as U+211D ℝ DOUBLE-STRUCK CAPITAL R (HTML &#8477;)), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression field of real numbers is frequently used when its algebraic properties are under consideration.In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".A real number is called computable if there exists an algorithm that yields its digits. Because there are only countably many algorithms,[13] but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.Physicists have occasionally suggested that a more fundamental theory would replace the real numbers with quantities that do not form a continuum, but such proposals remain speculative.[11]In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.Edward Nelson's internal set theory enriches the Zermelo–Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson's theory).The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.The real numbers are most often formalized using the Zermelo–Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.[10]The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering ≤ of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.[9]The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo–Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set B of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of B is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the Löwenheim–Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.The real numbers form a metric space: the distance between x and y is defined as the absolute value |x − y|. By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology – in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension 1. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.The reals are uncountable; that is: there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory.But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the largest Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the only uniformly complete ordered field, but it is the only uniformly complete Archimedean field, and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element z, z + 1 is larger), so this is not the sense that is meant.The real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.converges to a real number for every x, because the sumsFor example, the standard series of the exponential functionThe completeness property of the reals is the basis on which calculus, and, more generally mathematical analysis are built. In particular, the test that a sequence is a Cauchy sequence allows proving that a sequence has a limit, without computing it, and even without knowing it.The set of rational numbers is not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421; ...), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number (in the real numbers, in contrast, it converges to the positive square root of 2).Every convergent sequence is a Cauchy sequence, and the converse is true for real numbers, and this means that the topological space of the real numbers is complete.A sequence (xn) converges to the limit x if its elements eventually come and remain arbitrarily close to x, that is, if for any ε > 0 there exists an integer N (possibly depending on ε) such that the distance |xn − x| is less than ε for n greater than N.A sequence (xn) of real numbers is called a Cauchy sequence if for any ε > 0 there exists an integer N (possibly depending on ε) such that the distance |xn − xm| is less than ε for all n and m that are both greater than N. This definition, originally provided by Cauchy, formalizes the fact that the xn eventually come and remain arbitrarily close to each other.A main reason for using real numbers is that the reals contain all limits. More precisely, a sequence of real numbers has a limit, which is a real number, if (and only if) its elements eventually come and remain arbitrarily close to each other. This is formally defined in the following, and means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section). :More formally, the real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.A real number may be either rational or irrational; either algebraic or transcendental; and either positive, negative, or zero. Real numbers are used to measure continuous quantities. They may be expressed by decimal representations that have an infinite sequence of digits to the right of the decimal point; these are often represented in the same form as 324.823122147... The ellipsis (three dots) indicates that there would still be more digits to come.The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; ...) converges to a unique real number, in this case π. For details and other constructions of real numbers, see construction of the real numbers.For another axiomatization of ℝ, see Tarski's axiomatization of the reals.The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields ℝ1 and ℝ2, there exists a unique field isomorphism from ℝ1 to ℝ2, allowing us to think of them as essentially the same mathematical object.These properties imply Archimedean property (which is not implied by other definitions of completeness). That is, the set of integers is not upper-bounded in the reals. In fact, if this were false, then the integers would have a least upper bound N; then, N – 1 would not be an upper bound, and there would be an integer n such that n > N – 1, and thus n + 1 > N, which is a contradiction with the upper-bound property of N.The last property is what differentiates the reals from the rationals. For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.Let ℝ denote the set of all real numbers. Then:The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was given by Georg Cantor in 1871. In 1874, he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof.Évariste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither e nor e2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers; Georg Cantor (1873) extended and greatly simplified this proof.[7] Charles Hermite (1873) first proved that e is transcendental, and Ferdinand von Lindemann (1882), showed that π is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz[citation needed] and Paul Gordan.[8]In the 18th and 19th centuries, there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that π cannot be rational; Adrien-Marie Legendre (1794) completed the proof,[5] and showed that π is not the square root of a rational number.[6] Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel–Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects,[2] which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers.[3] The Egyptian mathematician Abū Kāmil Shujā ibn Aslam (c. 850–930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.[4]Simple fractions were used by the Egyptians around 1000 BC; the Vedic "Sulba Sutras" ("The rules of chords") in, c. 600 BC, include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava (c. 750–690 BC), who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined.[1] Around 500 BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers – indeed, the realization that a better definition was needed – was one of the most important developments of 19th-century mathematics. The current standard axiomatic definition is that real numbers form the unique Dedekind-complete ordered field (ℝ ; + ; · ; <), up to an isomorphism,[a] whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or infinite decimal representations, together with precise interpretations for the arithmetic operations and the order relation. All these definitions satisfy the axiomatic definition and are thus equivalent.The real numbers include all the rational numbers, such as the integer −5 and the fraction 4/3, and all the irrational numbers, such as √2 (1.41421356..., the square root of 2, an irrational algebraic number). Included within the irrationals are the transcendental numbers, such as π (3.14159265...). Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation, such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.In mathematics, a real number is a value that represents a quantity along a line. The adjective real in this context was introduced in the 17th century by René Descartes, who distinguished between real and imaginary roots of polynomials.
Set (mathematics)
The complement of A intersected with B is equal to the complement of A union to the complement of B.The complement of A union B equals the complement of A intersected with the complement of B.If A and B are any two sets then,Augustus De Morgan stated two laws about sets.A more general form of the principle can be used to find the cardinality of any finite union of sets:The inclusion–exclusion principle is a counting technique that can be used to count the number of elements in a union of two sets, if the size of each set and the size of their intersection are known. It can be expressed symbolically asFor most purposes, however, naive set theory is still useful.The reason is that the phrase well-defined is not very well-defined. It was important to free set theory of these paradoxes because nearly all of mathematics was being redefined in terms of set theory. In an attempt to avoid these paradoxes, set theory was axiomatized based on first-order logic, and thus axiomatic set theory was born.Although initially naive set theory, which defines a set merely as any well-defined collection, was well accepted, it soon ran into several obstacles. It was found that this definition spawned several paradoxes, most notably:One of the main applications of naive set theory is constructing relations. A relation from a domain A to a codomain B is a subset of the Cartesian product A × B. Given this concept, we are quick to see that the set F of all ordered pairs (x, x2), where x is real, is quite familiar. It has a domain set R and a codomain set that is also R, because the set of all squares is subset of the set of all reals. If placed in functional notation, this relation becomes f(x) = x2. The reason these two are equivalent is for any given value, y that the function is defined for, its corresponding ordered pair, (y, y2) is a member of the set F.Set theory is seen as the foundation from which virtually all of mathematics can be derived. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.Let A and B be finite sets; then the cardinality of the Cartesian product is the product of the cardinalities:Some basic properties of Cartesian products:Examples:A new set can be constructed by associating every element of one set with every element of another set. The Cartesian product of two sets A and B, denoted by A × B is the set of all ordered pairs (a, b) such that a is a member of A and b is a member of B.For example, the symmetric difference of {7,8,9,10} and {9,10,11,12} is the set {7,8,11,12}. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring (with the empty set as neutral element) and intersection as the multiplication of the ring.An extension of the complement is the symmetric difference, defined for sets A, B asSome basic properties of complements:Examples:In certain settings all sets under discussion are considered to be subsets of a given universal set U. In such cases, U \ A is called the absolute complement or simply complement of A, and is denoted by A′.Two sets can also be "subtracted". The relative complement of B in A (also called the set-theoretic difference of A and B), denoted by A \ B (or A − B), is the set of all elements that are members of A but not members of B. Note that it is valid to "subtract" members of a set that are not in the set, such as removing the element green from the set {1, 2, 3}; doing so has no effect.Some basic properties of intersections:Examples:A new set can also be constructed by determining which members two sets have "in common". The intersection of A and B, denoted by A ∩ B, is the set of all things that are members of both A and B. If A ∩ B = ∅, then A and B are said to be disjoint.Some basic properties of unions:Examples:Two sets can be "added" together. The union of A and B, denoted by A ∪ B, is the set of all things that are members of either A or B.There are several fundamental operations for constructing new sets from given sets.Each of the above sets of numbers has an infinite number of elements, and each can be considered to be a proper subset of the sets listed below it. The primes are used less frequently than the others outside of number theory and related fields.Positive and negative sets are denoted by a superscript - or +. For example, ℚ+ represents the set of positive rational numbers.Many of these sets are represented using blackboard bold or bold typeface. Special sets of numbers includeThere are some sets or kinds of sets that hold great mathematical importance and are referred to with such regularity that they have acquired special names and notational conventions to identify them. One of these is the empty set, denoted {} or ∅. A set with exactly one element, x, is a unit set, or singleton, {x}.[2]Some sets have infinite cardinality. The set N of natural numbers, for instance, is infinite. Some infinite cardinalities are greater than others. For instance, the set of real numbers has greater cardinality than the set of natural numbers. However, it can be shown that the cardinality of (which is to say, the number of points on) a straight line is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.There is a unique set with no members, called the empty set (or the null set), which is denoted by the symbol ∅ (other notations are used; see empty set). The cardinality of the empty set is zero. For example, the set of all three-sided squares has zero members and thus is the empty set. Though it may seem trivial, the empty set, like the number zero, is important in mathematics. Indeed, the existence of this set is one of the fundamental concepts of axiomatic set theory.The cardinality | S | of a set S is "the number of members of S." For example, if B = {blue, white, red}, then | B | = 3.Every partition of a set S is a subset of the powerset of S.The power set of an infinite (either countable or uncountable) set is always uncountable. Moreover, the power set of a set is always strictly "bigger" than the original set in the sense that there is no way to pair every element of S with exactly one element of P(S). (There is never an onto map or surjection from S onto P(S).)The power set of a finite set with n elements has 2n elements. For example, the set {1, 2, 3} contains three elements, and the power set shown above contains 23 = 8 elements.The power set of a set S is the set of all subsets of S. The power set contains S itself and the empty set because these are both subsets of S. For example, the power set of the set {1, 2, 3} is {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, ∅}. The power set of a set S is usually written as P(S).A partition of a set S is a set of nonempty subsets of S such that every element x in S is in exactly one of these subsets.An obvious but useful identity, which can often be used to show that two seemingly different sets are equal:Every set is a subset of the universal set:The empty set is a subset of every set and every set is a subset of itself:Examples:The expressions A ⊂ B and B ⊃ A are used differently by different authors; some authors use them to mean the same as A ⊆ B (respectively B ⊇ A), whereas others use them to mean the same as A ⊊ B (respectively B ⊋ A).If A is a subset of, but not equal to, B, then A is called a proper subset of B, written A ⊊ B (A is a proper subset of B) or B ⊋ A (B is a proper superset of A).If every member of set A is also a member of set B, then A is said to be a subset of B, written A ⊆ B (also pronounced A is contained in B). Equivalently, we can write B ⊇ A, read as B is a superset of A, B includes A, or B contains A. The relationship between sets established by ⊆ is called inclusion or containment.For example, with respect to the sets A = {1,2,3,4}, B = {blue, white, red}, and F = {n2 − 4 : n is an integer; and 0 ≤ n ≤ 19} defined above,If B is a set and x is one of the objects of B, this is denoted x ∈ B, and is read as "x belongs to B", or "x is an element of B". If y is not a member of B then this is written as y ∉ B, and is read as "y does not belong to B".In this notation, the colon (":") means "such that", and the description can be interpreted as "F is the set of all numbers of the form n2 − 4, such that n is a whole number in the range from 0 to 19 inclusive." Sometimes the vertical bar ("|") is used instead of the colon.The notation with braces may also be used in an intensional specification of a set. In this usage, the braces have the meaning "the set of all ...". So, E = {playing card suits} is the set whose four members are ♠, ♦, ♥, and ♣. A more general form of this is set-builder notation, through which, for instance, the set F of the twenty smallest integers that are four less than perfect square can be denotedwhere the ellipsis ("...") indicates that the list continues in the obvious way. Ellipses may also be used where sets have infinitely many members. Thus the set of positive even numbers can be written as {2, 4, 6, 8, ... }.For sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally asIn an extensional definition, a set member can be listed two or more times, for example, {11, 6, 6}. However, per extensionality, two definitions of sets which differ only in that one of the definitions lists set members multiple times, define, in fact, the same set. Hence, the set {11, 6, 6} is exactly identical to the set {11, 6}. Moreover, the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple). We can illustrate these two important points with an example:One often has the choice of specifying a set either intensionally or extensionally. In the examples above, for instance, A = C and B = D.The second way is by extension – that is, listing each member of the set. An extensional definition is denoted by enclosing the list of members in curly brackets:There are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:For technical reasons, Cantor's definition turned out to be inadequate; today, in contexts where more rigor is required, one can use axiomatic set theory, in which the notion of a "set" is taken as a primitive notion and the properties of sets are defined by a collection of axioms. The most basic properties are that a set can have elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other; this property is called the extensionality of sets.Sets are conventionally denoted with capital letters. Sets A and B are equal if and only if they have precisely the same elements.[2]A set is a well-defined collection of distinct objects. The objects that make up a set (also known as the set's elements or members) can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, one of the founders of set theory, gave the following definition of a set at the beginning of his Beiträge zur Begründung der transfiniten Mengenlehre:[1]The German word Menge, rendered as "set" in English, was coined by Bernard Bolzano in his work The Paradoxes of the Infinite.In mathematics, a set is a collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree.
Binary operation
Note that the dot product of two vectors is not a binary operation, external or otherwise, as it maps from S× S to K, where K is a field and S is a vector space over K.An external binary operation may alternatively be viewed as an action; K is acting on S.An example of an external binary operation is scalar multiplication in linear algebra. Here K is a field and S is a vector space over that field.An external binary operation is a binary function from K × S to S. This differs from a binary operation in the strict sense in that K need not be S; its elements come from outside.A binary operation f on a set S may be viewed as a ternary relation on S, that is, the set of triples (a, b, f(a,b)) in S × S × S for all a and b in S.However:A binary operation, ab, depends on the ordered pair (a, b) and so (ab)c (where the parentheses here mean first operate on the ordered pair (a, b) and then operate on the result of that using the ordered pair ((ab), c)) depends in general on the ordered pair ((a, b), c). Thus, for the general, non-associative case, binary operations can be represented with binary trees.Binary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses. They are also called, respectively, Polish notation and reverse Polish notation.Binary operations are often written using infix notation such as a ∗ b, a + b, a · b or (by juxtaposition with no symbol) ab rather than by functional notation of the form f(a, b). Powers are usually also written without operator, but with the second argument as superscript.Division (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative. Tetration (↑↑), as a binary operation on the natural numbers, is not commutative or associative and has no identity element.On the set of natural numbers N, the binary operation exponentiation, f(a,b) = ab, is not commutative since, in general, ab ≠ ba and is also not associative since f(f(a, b), c) ≠ f(a, f(b, c)). For instance, with a = 2, b = 3 and c = 2, f(23,2) = f(8,2) = 82 = 64, but f(2,32) = f(2,9) = 29 = 512. By changing the set N to the set of integers Z, this binary operation becomes a partial binary operation since it is now undefined when a = 0 and b is any negative integer. For either set, this operation has a right identity (which is 1) since f(a, 1) = a for all a in the set, which is not an identity (two sided identity) since f(1, b) ≠ b in general.On the set of real numbers R, subtraction, that is, f(a, b) = a − b, is a binary operation which is not commutative since, in general, a − b ≠ b − a. It is also not associative, since, in general, a − (b − c) ≠ (a − b) − c; for instance, 1 − (2 − 3) = 2 but (1 − 2) − 3 = −4.The first three examples above are commutative and all of the above examples are associative.Many binary operations of interest in both algebra and formal logic are commutative, satisfying f(a, b) = f(b, a) for all elements a and b in S, or associative, satisfying f(f(a, b), c) = f(a, f(b, c)) for all a, b and c in S. Many also have identity elements and inverse elements.Typical examples of binary operations are the addition (+) and multiplication (×) of numbers and matrices as well as composition of functions on a single set. For instance,Binary operations are the keystone of algebraic structures studied in abstract algebra: they are essential in the definitions of groups, monoids, semigroups, rings, and more. Most generally, a magma is a set together with some binary operation defined on it.Sometimes, especially in computer science, the term is used for any binary function.Because the result of performing the operation on a pair of elements of S is again an element of S, the operation is called a closed binary operation on S (or sometimes expressed as having the property of closure).[4] If f is not a function, but is instead a partial function, it is called a partial binary operation. For instance, division of real numbers is a partial binary operation, because one can't divide by zero: a/0 is not defined for any real a. Note however that both in algebra and model theory the binary operations considered are defined on all of S × S.More precisely, a binary operation on a set S is a map which sends elements of the Cartesian product S × S to S:[1][2][3]In mathematics, a binary operation on a set is a calculation that combines two elements of the set (called operands) to produce another element of the set. More formally, a binary operation is an operation of arity of two whose two domains and one codomain are the same set. Examples include the familiar elementary arithmetic operations of addition, subtraction, multiplication and division. Other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication and conjugation in groups.
Element (mathematics)

Euclidean vector
This distinction between vectors and pseudovectors is often ignored, but it becomes important in studying symmetry properties. See parity (physics).One example of a pseudovector is angular velocity. Driving in a car, and looking forward, each of the wheels has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the reflection of this angular velocity vector points to the right, but the actual angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include magnetic field, torque, or more generally any cross product of two (true) vectors.Some vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip and gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the orientation of space. A vector which gains a minus sign when the orientation of space changes is called a pseudovector or an axial vector. Ordinary vectors are sometimes called true vectors or polar vectors to distinguish them from pseudovectors. Pseudovectors occur most frequently as the cross product of two ordinary vectors.In the language of differential geometry, the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a contravariant vector to be a tensor of contravariant rank one. Alternatively, a contravariant vector is defined to be a tangent vector, and the rules for transforming a contravariant vector follow from the chain rule.Therefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely asWork is the dot product of force and displacementForce is a vector with dimensions of mass×length/time2 and Newton's second law is the scalar multiplicationAcceleration a of a point is vector which is the time derivative of velocity. Its dimensions are length/time2.where x0 is the position at time t=0. Velocity is the time derivative of position. Its dimensions are length/time.The velocity v of a point or particle is a vector, its length gives the speed. For constant velocity the position at time t will bewhich specifies the position of y relative to x. The length of this vector gives the straight-line distance from x to y. Displacement has the dimensions of length.Given two points x = (x1, x2, x3), y = (y1, y2, y3) their displacement is a vectorThe position vector has dimensions of length.The position of a point x = (x1, x2, x3) in three-dimensional space can be represented as a position vector whose base point is the originOften in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter t. For instance, if r represents the position vector of a particle, then r(t) gives a parametric representation of the trajectory of the particle. Vector-valued functions can be differentiated and integrated by differentiating or integrating the components of the vector, and many of the familiar rules from calculus continue to hold for the derivative and integral of vector-valued functions.In abstract vector spaces, the length of the arrow depends on a dimensionless scale. If it represents, for example, a force, the "scale" is of physical dimension length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if "1 newton" and "5 m" are both represented with an arrow of 2 cm, the scales are 1:250 and 1 m:50 N respectively. Equal length of vectors of different dimension has no particular significance unless there is some proportionality constant inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.Vectors have many uses in physics and other sciences.By applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.[13]The advantage of this method is that a direction cosine matrix can usually be obtained independently by using Euler angles or a quaternion to relate the two vector bases, so the basis conversions can be performed directly, without having to work out all the dot products described above.The properties of a direction cosine matrix, C are [[14]]:By referring collectively to e1, e2, e3 as the e basis and to n1, n2, n3 as the n basis, the matrix containing all the cjk is known as the "transformation matrix from e to n", or the "rotation matrix from e to n" (because it can be imagined as the "rotation" of a vector from one basis to another), or the "direction cosine matrix from e to n"[13] (because it contains direction cosines). The properties of a rotation matrix are such that its inverse is equal to its transpose. This means that the "rotation matrix from e to n" is the transpose of "rotation matrix from n to e".This matrix equation relates the scalar components of a in the n basis (u,v, and w) with those in the e basis (p, q, and r). Each matrix element cjk is the direction cosine relating nj to ek.[13] The term direction cosine refers to the cosine of the angle between two unit vectors, which is also equal to their dot product.[13] Therefore,and these equations can be expressed as the single matrix equationReplacing each dot product with a unique scalar givesDistributing the dot-multiplication givesThe values of p, q, r, and u, v, w relate to the unit vectors in such a way that the resulting vector sum is exactly the same physical vector a in both cases. It is common to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle). In such a case it is necessary to develop a method to convert between bases so the basic vector operations such as addition and subtraction can be performed. One way to express u, v, w in terms of p, q, r is to use column matrices along with a direction cosine matrix containing the information that relates the two bases. Such an expression can be formed by substitution of the above equations to formand the scalar components in the n basis are, by definition,In another orthnormal basis n = {n1, n2, n3} that is not necessarily aligned with e, the vector a is expressed asThe scalar components in the e basis are, by definition,All examples thus far have dealt with vectors expressed in terms of the same basis, namely, the e basis {e1, e2, e3}. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector. In the e basis, a vector a is expressed, by definition, asThe scalar triple product is linear in all three entries and anti-symmetric in the following sense:In components (with respect to a right-handed orthonormal basis), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the determinant of the 3-by-3 matrix having the three vectors as rowsIt has three primary uses. First, the absolute value of the box product is the volume of the parallelepiped which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are linearly dependent, which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors a, b and c are right-handed.The scalar triple product (also called the box product or mixed triple product) is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by (a b c) and defined as:For arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector (see below).The cross product can be written asThe length of a × b can be interpreted as the area of the parallelogram having a and b as sides.The cross product a × b is defined so that a, b, and a × b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.where θ is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist two unit vectors that are perpendicular to both a and b, namely, n and (–n).The cross product (also called the vector product or outer product) is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a × b, is a vector perpendicular to both a and b and is defined asThe dot product can also be defined as the sum of the products of the components of each vector aswhere θ is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of the component of b that points in the same direction as a.The dot product of two vectors a and b (sometimes called the inner product, or, since its result is a scalar, the scalar product) is denoted by a ∙ b and is defined as:If r is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples (r = −1 and r = 2) are given below:Intuitively, multiplying by a scalar r stretches a vector out by a factor of r. Geometrically, this can be visualized (at least in the case when r is an integer) as placing r copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.A vector may also be multiplied, or re-scaled, by a real number r. In the context of conventional vector algebra, these real numbers are often called scalars (from scale) to distinguish them from vectors. The operation of multiplying a vector by a scalar is called scalar multiplication. The resulting vector isSubtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a − b, as illustrated below:The difference of a and b isThe addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b isTo normalize a vector a = [a1, a2, a3], scale the vector by the reciprocal of its length ‖a‖. That is:A unit vector is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as normalizing a vector. A unit vector is often indicated with a hat as in â.This happens to be equal to the square root of the dot product, discussed below, of the vector with itself:which is a consequence of the Pythagorean theorem since the basis vectors e1, e2, e3 are orthogonal unit vectors.The length of the vector a can be computed with the Euclidean normThe length or magnitude or norm of the vector a is denoted by ‖a‖ or, less commonly, |a|, which is not to be confused with the absolute value (a scalar "norm").Two vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.are opposite ifandTwo vectors are opposite if they have the same magnitude but opposite direction. So two vectorsare equal ifandTwo vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectorsand assumes that all vectors have the origin as a common base point. A vector a will be written asThe following section uses the Cartesian coordinate system with basis vectorsIn these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a global coordinate system, or inertial reference frame).A vector can also be broken up with respect to "non-fixed" basis vectors that change their orientation as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively normal, and tangent to a surface (see figure). Moreover, the radial and tangential components of a vector relate to the radius of rotation of an object. The former is parallel to the radius and the latter is orthogonal to it.[12]The choice of a basis doesn't affect the properties of a vector or its behaviour under transformations.The decomposition or resolution[11] of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.As explained above a vector is often described by a set of vector components that add up to form the given vector. Typically, these components are the projections of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be decomposed or resolved with respect to that set.The notation ei is compatible with the index notation and the summation convention commonly used in higher level mathematics, physics, and engineering.where a1, a2, a3 are called the vector components (or vector projections) of a on the basis vectors or, equivalently, on the corresponding Cartesian axes x, y, and z (see figure), while a1, a2, a3 are the respective scalar components (or scalar projections).orThese have the intuitive interpretation as vectors of unit length pointing up the x, y, and z axis of a Cartesian coordinate system, respectively. In terms of these, any vector a in R3 can be expressed in the form:Another way to represent a vector in n-dimensions is to introduce the standard basis vectors. For instance, in three dimensions, there are three of them:These numbers are often arranged into a column vector or row vector, particularly when dealing with matrices, as follows:This can be generalised to n-dimensional Euclidean space (or Rn).In three dimensional Euclidean space (or R3), vectors are identified with triples of scalar components:As an example in two dimensions (see figure), the vector from the origin O = (0,0) to the point A = (2,3) is simply written asIn order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an n-dimensional Euclidean space can be represented as coordinate vectors in a Cartesian coordinate system. The endpoint of a vector can be identified with an ordered list of n real numbers (n-tuple). These numbers are the coordinates of the endpoint of the vector, with respect to a given Cartesian coordinate system, and are typically called the scalar components (or scalar projections) of the vector on the axes of the coordinate system.On a two-dimensional diagram, sometimes a vector perpendicular to the plane of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 ⊙) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 ⊗) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an arrow head on and viewing the flights of an arrow from the back.Vectors are usually shown in graphs or other diagrams as arrows (directed line segments), as illustrated in the figure. Here the point A is called the origin, tail, base, or initial point; point B is called the head, tip, endpoint, terminal point or final point. The length of the arrow is proportional to the vector's magnitude, while the direction in which the arrow points indicates the vector's direction.In pure mathematics, a vector is any element of a vector space over some field and is often represented as a coordinate vector. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has "magnitude and direction".In physics, as well as mathematics, a vector is often identified with a tuple of components, or list of numbers, that act as scalar coefficients for a set of basis vectors. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called covariant or contravariant depending on how the transformation of the vector's components is related to the transformation of the basis. In general, contravariant vectors are "regular vectors" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as gradient. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1 m becomes 1000 mm–a contravariant change in numerical value. In contrast, a gradient of 1 K/m becomes 0.001 K/mm–a covariant change in value. See covariance and contravariance of vectors. Tensors are another type of quantity that behave in this way; a vector is one type of tensor.However, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of vector spaces (for free vectors) and affine spaces (for bound vectors, as each represented by an ordered pair of "points"). An important example is Minkowski space that is important to our understanding of special relativity, where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from thermodynamics, where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.[10]In the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a length or magnitude and a direction to vectors. In addition, the notion of direction is strictly associated with the notion of an angle between two vectors. If the dot product of two vectors is defined — a scalar-valued product of two vectors —, then it's also possible to define a length; the dot product gives a convenient algebraic characterization of both angle (a function of the dot product between any two non-zero vectors) and length (the square root of the dot product of a vector by itself). In three dimensions, it is further possible to define the cross product, which supplies an algebraic characterization of the area and orientation in space of the parallelogram defined by two vectors (used as sides of the parallelogram). In any dimension (and, in particular, higher dimensions), it's possible to define the exterior product, which (among other things) supplies an algebraic characterization of the area and orientation in space of the n-dimensional parallelotope defined by n vectors.This coordinate representation of free vectors allows their algebraic features to be expressed in a convenient numerical fashion. For example, the sum of the two (free) vectors (1,2,3) and (−2,0,4) is the (free) vectorIn Cartesian coordinates a free vector may be thought of in terms of a corresponding bound vector, in this sense, whose initial point has the coordinates of the origin O = (0,0,0). It is then determined by the coordinates of that bound vector's terminal point. Thus the free vector represented by (1,0,0) is a vector of unit length pointing along the direction of the positive x-axis.Vectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is velocity, the magnitude of which is speed. For example, the velocity 5 meters per second upward could be represented by the vector (0,5) (in 2 dimensions with the positive y axis as 'up'). Another quantity represented by a vector is force, since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, displacement, linear acceleration, angular acceleration, linear momentum, and angular momentum. Other physical vectors, such as the electric and magnetic field, are represented as a system of vectors at each point of a physical space; that is, a vector field. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition: Angular displacement and electric current. Consequently, these are not vectors.Since the physicist's concept of force has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force F of 15 newtons. If the positive axis is also directed rightward, then F is represented by the vector 15 N, and if positive points leftward, then the vector for F is −15 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement Δs of 4 meters would be 4 m or −4 m, depending on its direction, and its magnitude would be 4 m regardless.The term vector also has generalizations to higher dimensions and to more formal approaches with much wider applications.This article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as geometric, spatial, or Euclidean vectors.In physics and engineering, a vector is typically regarded as a geometric entity characterized by a magnitude and a direction. It is formally defined as a directed line segment, or arrow, in a Euclidean space.[8] In pure mathematics, a vector is defined more generally as any element of a vector space. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called Euclidean space.Josiah Willard Gibbs, who was exposed to quaternions through James Clerk Maxwell's Treatise on Electricity and Magnetism, separated off their vector part for independent treatment. The first half of Gibbs's Elements of Vector Analysis, published in 1881, presents what is essentially the modern system of vector analysis.[6] In 1901 Edwin Bidwell Wilson published Vector Analysis, adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus.In 1878 Elements of Dynamic was published by William Kingdon Clifford. Clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.Peter Guthrie Tait carried the quaternion standard after Hamilton. His 1867 Elementary Treatise of Quaternions included extensive treatment of the nabla or del operator ∇.Several other mathematicians developed vector-like systems in the middle of the nineteenth century, including Augustin Cauchy, Hermann Grassmann, August Möbius, Comte de Saint-Venant, and Matthew O'Brien. Grassmann's 1840 work Theorie der Ebbe und Flut (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.[6]The term vector was introduced by William Rowan Hamilton as part of a quaternion, which is a sum q = s + v of a Real number s (also called scalar) and a 3-dimensional vector. Like Bellavitis, Hamilton viewed vectors as representative of classes of equipollent directed segments. As complex numbers use an imaginary unit to complement the real line, Hamilton considered the vector v to be the imaginary part of a quaternion:Giusto Bellavitis abstracted the basic idea in 1835 when he established the concept of equipollence. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an equivalence relation on the pairs of points (bipoints) in the plane and thus erected the first space of vectors in the plane.[6]:52–4The concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.[6]Vectors play an important role in physics: the velocity and acceleration of a moving object and the forces acting on it can all be described with vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can still be represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.A vector is what is needed to "carry" the point A to the point B; the Latin word vector means "carrier".[4] It was first used by 18th century astronomers investigating planet rotation around the Sun.[5] The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from A to B. Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.
Scalar multiplication
where i, j, k are the quaternion units. The non-commutativity of quaternion multiplication prevents the transition of changing ij = +k to ji = −k.For quaternion scalars and matrices:For a real scalar and matrix:When the underlying ring is commutative, for example, the real or complex number field, these two multiplications are the same, and are simply called scalar multiplication. However, for matrices over a more general ring that are not commutative, such as the quaternions, they may not be equal.explicitly:Similarly, the right scalar multiplication of a matrix A with a scalar λ is defined to beexplicitly:The left scalar multiplication of a matrix A with a scalar λ gives another matrix λA of the same size as A. The entries of λA are defined byThe same idea applies if K is a commutative ring and V is a module over K. K can even be a rig, but then there is no additive inverse. If K is not commutative, the distinct operations left scalar multiplication cv and right scalar multiplication vc may be defined.When V is Kn, scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such.As a special case, V may be taken to be K itself and scalar multiplication may then be taken to be simply the multiplication in the field.Scalar multiplication may be viewed as an external binary operation or as an action of the field on the vector space. A geometric interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor.Here + is addition either in the field or in the vector space, as appropriate; and 0 is the additive identity in either. Juxtaposition indicates either scalar multiplication or the multiplication operation in the field.Scalar multiplication obeys the following rules (vector in boldface):In general, if K is a field and V is a vector space over K, then scalar multiplication is a function from K × V to V. The result of applying this function to c in K and v in V is denoted cv.In mathematics, scalar multiplication is one of the basic operations defining a vector space in linear algebra[1][2][3] (or more generally, a module in abstract algebra[4][5]). In common geometrical contexts, scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction. The term "scalar" itself derives from this usage: a scalar is that which scales vectors. Scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and must be distinguished from inner product of two vectors (where the product is a scalar).
Axiom
Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.There is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms. The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.The objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.The Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.[12]This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.[citation needed]This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.Non-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought[citation needed] that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not tautologies. Another name for a non-logical axiom is postulate.[11]Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.[10]Other axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed.[9]These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms at least some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.In the field of mathematical logic, a clear distinction is made between two notions of axioms: logical and non-logical (somewhat similar to the ancient distinction between "axioms" and "postulates" respectively).Regardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither "proves" nor "disproves" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.As a consequence, it is not necessary to explicitly cite Einstein's axioms, the more so since they concern subtle points on the "reality" and "locality" of experiments.Another paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of "theorems" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics "hidden variables" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein's axioms compared to using Bohr's axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr's axioms, not Einstein's. (Bohr's axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)In 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity.Axioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment.It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.The formalist project suffered a decisive setback, when in 1931 Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of naïve set theory raised the possibility that any such system could turn out to be inconsistent.It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms.In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development.It is not correct to say that the axioms of field theory are "propositions that are regarded as true without proof." Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without any particular application in mind. The distinction between an "axiom" and a "postulate" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like "line" and "parallel" with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience.A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.The classical approach is well-illustrated by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a postulate. While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.[8]An "axiom", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion thatThe ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that, "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property."[7] Boethius translated 'postulate' as petitio and called the axioms notiones communes but in later manuscripts this usage was not always strictly kept.The root meaning of the word postulate is to "demand"; for instance, Euclid demands that one agree that some things can be done, e.g. any two points can be joined by a straight line, etc.[6]The word axiom comes from the Greek word ἀξίωμα (axíōma), a verbal noun from the verb ἀξιόειν (axioein), meaning "to deem worthy", but also "to require", which in turn comes from ἄξιος (áxios), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof.In both senses, an axiom is any mathematical statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom, or any mathematical statement, to be "true" is an open question[citation needed] in the philosophy of mathematics.[5]As used in mathematics, the term axiom is used in two related but distinguishable senses: "logical axioms" and "non-logical axioms". Logical axioms are usually statements that are taken to be true within the system of logic they define (e.g., (A and B) implies A), often shown in symbolic form, while non-logical axioms (e.g., a + b = b + a) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, "axiom", "postulate", and "assumption" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain.The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] As used in modern logic, an axiom is simply a premise or starting point for reasoning.[4]An axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek axíōma (ἀξίωμα) 'that which is thought worthy or fit' or 'that which commends itself as evident.'[1][2]
Associative property
Non-associative operations for which no conventional evaluation order is defined include the following.Right-associative operations include the following:Both left-associative and right-associative operations occur. Left-associative operations include the following:while a right-associative operation is conventionally evaluated from right to left:A left-associative operation is a non-associative operation that is conventionally evaluated from left to right, i.e.,In general, parentheses must be used to indicate the order of evaluation if a non-associative operation appears more than once in an expression. However, mathematicians agree on a particular order of evaluation for several common non-associative operations. This is simply a notational convention to avoid parentheses.Even though most computers compute with a 24 or 53 bits of mantissa,[8] this is an important source of rounding error, and approaches such as the Kahan summation algorithm are ways to minimise the errors. It can be especially problematic in parallel computing.[9][10]To illustrate this, consider a floating point representation with a 4-bit mantissa:
(1.0002×20 + 1.0002×20) + 1.0002×24 = 1.0002×21 + 1.0002×24 = 1.0012×24
1.0002×20 + (1.0002×20 + 1.0002×24) = 1.0002×20 + 1.0002×24 = 1.0002×24In mathematics, addition and multiplication of real numbers is associative. By contrast, in computer science, the addition and multiplication of floating point numbers is not associative, as rounding errors are introduced when dissimilar-sized values are joined together.[7]There are other specific types of non-associative structures that have been studied in depth; these tend to come from some specific applications or areas such as combinatorial mathematics. Other examples are Quasigroup, Quasifield, Non-associative ring, Non-associative algebra and Commutative non-associative magmas.The study of non-associative structures arises from reasons somewhat different from the mainstream of classical algebra. One area within non-associative algebra that has grown very large is that of Lie algebras. There the associative law is replaced by the Jacobi identity. Lie algebras abstract the essential nature of infinitesimal transformations, and have become ubiquitous in mathematics.whereasAlso note that infinite sums are not generally associative, for example:For such an operation the order of evaluation does matter. For example:Associativity of equivalence:Associativity of conjunction:Associativity of disjunction:Associativity is a property of some logical connectives of truth-functional propositional logic. The following logical equivalences demonstrate that associativity is a property of particular connectives. The following are truth-functional tautologies.andIn standard truth-functional propositional logic, association,[4][5] or associativity[6] are two valid rules of replacement. The rules allow one to move parentheses in logical expressions in logical proofs. The rules (using logical connectives notation) are:Some examples of associative operations include the following.As the number of elements increases, the number of possible ways to insert parentheses grows quickly, but they remain unnecessary for disambiguation.If the product operation is associative, the generalized associative law says that all these formulas will yield the same result, making the parenthesis unnecessary. Thus "the" product can be written unambiguously asIf a binary operation is associative, repeated application of the operation produces the same result regardless how valid pairs of parenthesis are inserted in the expression.[2] This is called the generalized associative law. For instance, a product of four elements may be written in five possible ways:The associative law can also be expressed in functional notation thus: f(f(x, y), z) = f(x, f(y, z)).Here, ∗ is used to replace the symbol of the operation, which may be any symbol, and even the absence of symbol (juxtaposition) as for multiplication.Formally, a binary operation ∗ on a set S is called associative if it satisfies the associative law:However, many important and interesting operations are non-associative; some examples include subtraction, exponentiation, and the vector cross product. In contrast to the theoretical properties of real numbers, the addition of floating point numbers in computer science is not associative, and the choice of how to associate an expression can have a significant effect on rounding error.Associative operations are abundant in mathematics; in fact, many algebraic structures (such as semigroups and categories) explicitly require their binary operations to be associative.Associativity is not the same as commutativity, which addresses whether or not the order of two operands changes the result. For example, the order does not matter in the multiplication of real numbers, that is, a × b = b × a, so we say that the multiplication of real numbers is a commutative operation.Even though the parentheses were rearranged on each line, the values of the expressions were not altered. Since this holds true when performing addition and multiplication on any real numbers, it can be said that "addition and multiplication of real numbers are associative operations".Within an expression containing two or more occurrences in a row of the same associative operator, the order in which the operations are performed does not matter as long as the sequence of the operands is not changed. That is, rearranging the parentheses in such an expression will not change its value. Consider the following equations:In mathematics, the associative property[1] is a property of some binary operations. In propositional logic, associativity is a valid rule of replacement for expressions in logical proofs.
Commutative property
Some forms of symmetry can be directly linked to commutativity. When a commutative operator is written as a binary function then the resulting function is symmetric across the line y = x. As an example, if we let a function f represent addition (a commutative operation) so that f(x,y) = x + y then f is a symmetric function, which can be seen in the image on the right.Most commutative operations encountered in practice are also associative. However, commutativity does not imply associativity. A counterexample is the functionThe associative property is closely related to the commutative property. The associative property of an expression containing two or more occurrences of the same operator states that the order operations are performed in does not affect the final result, as long as the order of terms doesn't change. In contrast, the commutative property states that the order of the terms does not affect the final result.In group and set theory, many algebraic structures are called commutative when certain operands satisfy the commutative property. In higher branches of mathematics, such as analysis and linear algebra the commutativity of well-known operations (such as addition and multiplication on real and complex numbers) is often used (or implicitly assumed) in proofs.[16][17][18]Commutativity is a property of some logical connectives of truth functional propositional logic. The following logical equivalences demonstrate that commutativity is a property of particular connectives. The following are truth-functional tautologies.andIn truth-functional propositional logic, commutation,[13][14] or commutativity[15] refer to two valid rules of replacement. The rules allow one to transpose propositional variables within logical expressions in logical proofs. The rules are:The first recorded use of the term commutative was in a memoir by François Servois in 1814,[1][11] which used the word commutatives when describing functions that have what is now called the commutative property. The word is a combination of the French word commuter meaning "to substitute or switch" and the suffix -ative meaning "tending to" so the word literally means "tending to substitute or switch." The term then appeared in English in 1838[2] in Duncan Farquharson Gregory's article entitled "On the real nature of symbolical algebra" published in 1840 in the Transactions of the Royal Society of Edinburgh.[12]Records of the implicit use of the commutative property go back to ancient times. The Egyptians used the commutative property of multiplication to simplify computing products.[8][9] Euclid is known to have assumed the commutative property of multiplication in his book Elements.[10] Formal uses of the commutative property arose in the late 18th and early 19th centuries, when mathematicians began to work on a theory of functions. Today the commutative property is a well-known and basic property used in most branches of mathematics.The vector product (or cross product) of two vectors in three dimensions is anti-commutative; i.e., b × a = −(a × b).Matrix multiplication is almost always noncommutative, for example:For the eight noncommutative functions, Bqp = Cpq; Mqp = Lpq; Cqp = Bpq; Lqp = Mpq; Fqp = Gpq; Iqp = Hpq; Gqp = Fpq; Hqp = Ipq.[7]Some truth functions are noncommutative, since the truth tables for the functions are different when one changes the order of the operands. For example, the truth tables for f (A, B) = A Λ ¬B (A AND NOT B) and f (B, A) = B Λ ¬A areSome noncommutative binary operations:[6]Two well-known examples of commutative binary operations:[4]The term "commutative" is used in several related senses.[4][5]The commutative property (or commutative law) is a property generally associated with binary operations and functions. If the commutative property holds for a pair of elements under a certain binary operation then the two elements are said to commute under that operation.In mathematics, a binary operation is commutative if changing the order of the operands does not change the result. It is a fundamental property of many binary operations, and many mathematical proofs depend on it. Most familiar as the name of the property that says "3 + 4 = 4 + 3" or "2 × 5 = 5 × 2", the property can also be used in more advanced settings. The name is needed because there are operations, such as division and subtraction, that do not have it (for example, "3 − 5 ≠ 5 − 3"); such operations are not commutative, and so are referred to as noncommutative operations. The idea that simple operations such as the multiplication and addition of numbers are commutative, was for many years implicitly assumed. Thus, this property was not named until the 19th century, when mathematics started to become formalized.[1][2] A corresponding property exists for binary relations; a binary relation is said to be symmetric if the relation applies regardless of the order of its operands; for example, equality is symmetric as two equal mathematical objects are equal regardless of their order.[3]
Identity element
It is also quite possible for (S, ∗) to have no identity element. A common example of this is the cross product of vectors; in this case, the absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied – so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive semigroup of positive natural numbers.As the last example (a semigroup) shows, it is possible for (S, ∗) to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if l is a left identity and r is a right identity then l = l ∗ r = r. In particular, there can never be more than one two-sided identity. If there were two, e and f, then e ∗ f would have to be equal to both e and f.An identity with respect to addition is called an additive identity (often denoted as 0) and an identity with respect to multiplication is called a multiplicative identity (often denoted as 1). These need not be ordinary addition and multiplication, but rather arbitrary operations. The distinction is used most often for sets that support both binary operations, such as rings and fields. The multiplicative identity is often called unity in the latter context (a ring with unity). This should not be confused with a unit, i.e. any element with a multiplicative inverse. Unity itself is necessarily a unit.Let (S, ∗) be a set S with a binary operation ∗ on it. Then an element e of S is called a left identity if e ∗ a = a for all a in S, and a right identity if a ∗ e = a for all a in S. If e is both a left identity and a right identity, then it is called a two-sided identity, or simply an identity.In mathematics, an identity element or neutral element is a special type of element of a set with respect to a binary operation on that set, which leaves other elements unchanged when combined with them. This concept is used in algebraic structures such as groups and rings. The term identity element is often shortened to identity (as will be done in this article) when there is no possibility of confusion, however, the identity implicitly depends on the binary operation it is coupled with.
Zero element
Taking a tensor product of any tensor with any zero tensor results in another zero tensor. Adding the zero tensor is equivalent to the identity operation.In mathematics, the zero tensor is a tensor, of any order, all of whose components are zero. The zero tensor of order 1 is sometimes known as the zero vector.The zero matrix represents the linear transformation sending all vectors to the zero vector.There is exactly one zero matrix of any given size m × n having entries in a given ring, so when the context is clear one often refers to the zero matrix. In general the zero element of a ring is unique and typically denoted as 0 without any subscript to indicate the parent ring. Hence the examples above represent zero matrices over any ring.In mathematics, particularly linear algebra, a zero matrix is a matrix with all its entries being zero. Some examples of zero matrices areIn mathematics, the zero module is the module consisting of only the additive identity for the module's addition function. In the integers, this identity is zero, which gives the name zero module. That the zero module is in fact a module is simple to show; it is closed under addition and multiplication trivially.A least element in a partially ordered set or lattice may sometimes be called a zero element, and written either as 0 or ⊥.If a category has a zero object 0, then there are canonical morphisms X → 0 and 0 → Y, and composing them gives a zero morphism 0XY : X → Y. In the category of groups, for example, zero morphisms are morphisms which always return group identities, thus generalising the function z(x) = 0.A zero morphism in a category is a generalised absorbing element under function composition: any morphism composed with a zero morphism gives a zero morphism. Specifically, if 0XY : X → Y is the zero morphism among morphisms from X to Y, and f : A → X and g : Y → B are arbitrary morphisms, then g ∘ 0XY = 0XB and 0XY ∘ f = 0AY.A zero object in a category is both an initial and terminal object (and so an identity under both coproducts and products). For example, the trivial structure (containing only the identity) is a zero object in categories where morphisms must map identities to identities. Specific examples include:Many absorbing elements are also additive identities, including the empty set and the zero function. Another important example is the distinguished element 0 in a field or ring, which is both the additive identity and the multiplicative absorbing element, and whose principal ideal is the smallest ideal.An absorbing element in a multiplicative semigroup or semiring generalises the property 0 ⋅ x = 0. Examples include:An additive identity is the identity element in an additive group. It generalises the property 0 + x = x. Examples include:In mathematics, a zero element is one of several generalizations of the number zero to other algebraic structures. These alternate meanings may or may not reduce to the same thing, depending on the context.
Inverse element
which is a singular matrix, and cannot be inverted.The left inverse doesn't exist, becauseAs an example of matrix inverses, consider:No rank deficient matrix has any (even one-sided) inverse. However, the Moore–Penrose inverse exists for all matrices, and coincides with the left or right (or true) inverse when it exists.Non-square matrices of full rank have several one-sided inverses:[3]The lower and upper adjoints in a (monotone) Galois connection, L and G are quasi-inverses of each other, i.e. LGL = L and GLG = G and one uniquely determines the other. They are not left or right inverses of each other however.All examples in this section involve associative operators, thus we shall use the terms left/right inverse for the unital magma-based definition, and quasi-inverse for its more general version.Clearly a group is both an I-semigroup and a *-semigroup. A class of semigroups important in semigroup theory are completely regular semigroups; these are I-semigroups in which one additionally has aa° = a°a; in other words every element has commuting pseudoinverse a°. There are few concrete examples of such semigroups however; most are completely simple semigroups. In contrast, a subclass of *-semigroups, the *-regular semigroups (in the sense of Drazin), yield one of best known examples of a (unique) pseudoinverse, the Moore–Penrose inverse. In this case however the involution a* is not the pseudoinverse. Rather, the pseudoinverse of x is the unique element y such that xyx = x, yxy = y, (xy)* = xy, (yx)* = yx. Since *-regular semigroups generalize inverse semigroups, the unique element defined this way in a *-regular semigroup is called the generalized inverse or Penrose–Moore inverse.A natural generalization of the inverse semigroup is to define an (arbitrary) unary operation ° such that (a°)° = a for all a in S; this endows S with a type ⟨2,1⟩ algebra. A semigroup endowed with such an operation is called a U-semigroup. Although it may seem that a° will be the inverse of a, this is not necessarily the case. In order to obtain interesting notion(s), the unary operation must somehow interact with the semigroup operation. Two classes of U-semigroups have been studied:[2]Outside semigroup theory, a unique inverse as defined in this section is sometimes called a quasi-inverse. This is generally justified because in most applications (e.g., all examples in this article) associativity holds, which makes this notion a generalization of the left/right inverse relative to an identity.In a monoid, the notion of inverse as defined in the previous section is strictly narrower than the definition given in this section. Only elements in the Green class H1 have an inverse from the unital magma perspective, whereas for any idempotent e, the elements of He have an inverse as defined in this section. Under this more general definition, inverses need not be unique (or exist) in an arbitrary semigroup or monoid. If all elements are regular, then the semigroup (or monoid) is called regular, and every element has at least one inverse. If every element has exactly one inverse as defined in this section, then the semigroup is called an inverse semigroup. Finally, an inverse semigroup with only one idempotent is a group. An inverse semigroup may have an absorbing element 0 because 000 = 0, whereas a group may not.In a semigroup S an element x is called (von Neumann) regular if there exists some element z in S such that xzx = x; z is sometimes called a pseudoinverse. An element y is called (simply) an inverse of x if xyx = x and y = yxy. Every regular element has at least one inverse: if x = xzx then it is easy to verify that y = zxz is an inverse of x as defined in this section. Another easy to prove fact: if y is an inverse of x then e = xy and f = yx are idempotents, that is ee = e and ff = f. Thus, every pair of (mutually) inverse elements gives rise to two idempotents, and ex = xf = x, ye = fy = y, and e acts as a left identity on x, while f acts a right identity, and the left/right roles are reversed for y. This simple observation can be generalized using Green's relations: every idempotent e in an arbitrary semigroup is a left identity for Re and right identity for Le.[1] An intuitive description of this fact is that every pair of mutually inverse elements produces a local left identity, and respectively, a local right identity.The definition in the previous section generalizes the notion of inverse in group relative to the notion of identity. It's also possible, albeit less obvious, to generalize the notion of an inverse by dropping the identity element but keeping associativity, i.e., in a semigroup.A left-invertible element is left-cancellative, and analogously for right and two-sided.The word 'inverse' is derived from Latin: inversus that means 'turned upside down', 'overturned'.In abstract algebra, the idea of an inverse element generalises concepts of a negation (sign reversal) in relation to addition, and a reciprocal in relation to multiplication. The intuition is of an element that can 'undo' the effect of combination with another given element. While the precise definition of an inverse element varies depending on the algebraic structure involved, these definitions coincide in a group.
Additive inverse
Natural numbers, cardinal numbers, and ordinal numbers, do not have additive inverses within their respective sets. Thus, for example, we can say that natural numbers do have additive inverses, but because these additive inverses are not themselves natural numbers, the set of natural numbers is not closed under taking additive inverses.All the following examples are in fact abelian groups:For example, since addition of real numbers is associative, each real number has a unique additive inverse.If + is associative (( x + y ) + z = x + ( y + z ) for all x, y, z), then an additive inverse is unique. To see this, let x′ and x″ each be additive inverses of x; thenThe notation + is usually reserved for commutative binary operations; i.e., such that x + y = y + x, for all x, y. If such an operation admits an identity element o (such that x + o ( = o + x ) = x for all x), then this element is unique ( o′ = o′ + o = o ). For a given x , if there exists x′ such that x + x′ ( = x′ + x ) = o , then x′ is called an additive inverse of x.In addition to the identities listed above, negation has the following algebraic properties:Hence, unary minus sign notation can be seen as a shorthand for subtraction with "0" symbol omitted, although in a correct typography there should be no space after unary "−".Conversely, additive inverse can be thought of as subtraction from zero:Additive inverse is closely related to subtraction, which can be viewed as an addition of the opposite:For a number and, generally, in any ring, the additive inverse can be calculated using multiplication by −1; that is, −n = −1 × n . Examples of rings of numbers are integers, rational numbers, real numbers, and complex numbers.The additive inverse is defined as its inverse element under the binary operation of addition (see the discussion below), which allows a broad generalization to mathematical objects other than numbers. As for any inverse operation, double additive inverse has no net effect: −(−x) = x.The additive inverse of a is denoted by unary minus: −a (see the discussion below). For example, the additive inverse of 7 is −7, because 7 + (−7) = 0, and the additive inverse of −0.3 is 0.3, because −0.3 + 0.3 = 0 .In mathematics, the additive inverse of a number a is the number that, when added to a, yields zero. This number is also known as the opposite (number),[1] sign change, and negation.[2] For a real number, it reverses its sign: the opposite to a positive number is negative, and the opposite to a negative number is positive. Zero is the additive inverse of itself.
Distributive property

1
In the philosophy of Plotinus and a number of other neoplatonists, The One is the ultimate reality and source of all existence. Philo of Alexandria (20 BC – AD 50) regarded the number one as God's number, and the basis for all numbers ("De Allegoriis Legum," ii.12 [i.66]).In number theory, 1 is the value of Legendre's constant, which was introduced in 1808 by Adrien-Marie Legendre in expressing the asymptotic behavior of the prime-counting function. Legendre's constant was originally conjectured to be approximately 1.08366, but was proven to equal exactly 1 in 1899.In category theory, 1 is sometimes used to denote the terminal object of a category.The generating function that has all coefficients 1 is given by1 is the only known Tamagawa number for a simply connected algebraic group over a number field.1 is the most common leading digit in many sets of data, a consequence of Benford's law.By definition, 1 is the probability of an event that is almost certain to occur.By definition, 1 is the magnitude, absolute value, or norm of a unit complex number, unit vector, and a unit matrix (more usually called an identity matrix). Note that the term unit matrix is sometimes used to mean something quite different.1 is the only positive integer divisible by exactly one positive integer (whereas prime numbers are divisible by exactly two positive integers, composite numbers are divisible by more than two positive integers, and zero is divisible by all positive integers). 1 was formerly considered prime by some mathematicians, using the definition that a prime is divisible only by 1 and itself. However, this complicates the fundamental theorem of arithmetic, so modern definitions exclude units.The definition of a field requires that 1 must not be equal to 0. Thus, there are no fields of characteristic 1. Nevertheless, abstract algebra can consider the field with one element, which is not a singleton and is not a set at all.1 is neither a prime number nor a composite number, but a unit (meaning of ring theory), like −1 and, in the Gaussian integers, i and −i. The fundamental theorem of arithmetic guarantees unique factorization over the integers only up to units. (For example, 4 = 22, but if units are included, is also equal to, say, (−1)6 × 123 × 22, among infinitely many similar "factorizations".)It is also the first and second number in the Fibonacci sequence (0 is the zeroth) and is the first number in many other mathematical sequences.Because of the multiplicative identity, if f(x) is a multiplicative function, then f(1) must equal 1.In many mathematical and engineering problems, numeric values are typically normalized to fall within the unit interval from 0 to 1, where 1 usually represents the maximum possible value in the range of parameters. Likewise, vectors are often normalized to give unit vectors, that is vectors of magnitude one, because these often have more desirable properties. Functions, too, are often normalized by the condition that they have integral one, maximum value one, or square integral one, depending on the application.1 is the first figurate number of every kind, such as triangular number, pentagonal number and centered hexagonal number, to name just a few.In a multiplicative group or monoid, the identity element is sometimes denoted 1, but e (from the German Einheit, "unity") is also traditional. However, 1 is especially common for the multiplicative identity of a ring, i.e., when an addition and 0 are also present. When such a ring has characteristic n not equal to 0, the element called 1 has the property that n1 = 1n = 0 (where this 0 is the additive identity of the ring). Important examples are finite fields.Formalizations of the natural numbers have their own representations of 1:There are two ways to write the real number 1 as a recurring decimal: as 1.000..., and as 0.999....Since the base 1 exponential function (1x) always equals 1, its inverse does not exist (which would be called the logarithm base 1 if it did exist).Tallying is often referred to as "base 1", since only one mark – the tally itself – is needed. This is more formally referred to as a unary numeral system. Unlike base 2 or base 10, this is not a positional notation.Mathematically, 1 is:Many older typewriters do not have a separate symbol for 1 and use the lowercase letter l instead. It is possible to find cases when the uppercase J is used, while it may be for decorative purposes.The glyph used today in the Western world to represent the number 1, a vertical line, often with a serif at the top and sometimes a short horizontal line at the bottom, traces its roots back to the Indians, who wrote 1 as a horizontal line, much like the Chinese character 一. The Gupta wrote it as a curved line, and the Nagari sometimes added a small circle on the left (rotated a quarter turn to the right, this 9-look-alike became the present day numeral 1 in the Gujarati and Punjabi scripts). The Nepali also rotated it to the right but kept the circle small.[3] This eventually became the top serif in the modern numeral, but the occasional short horizontal line at the bottom probably originates from similarity with the Roman numeral I. In some countries, the little serif at the top is sometimes extended into a long upstroke, sometimes as long as the vertical line, which can lead to confusion with the glyph for seven in other countries. Where the 1 is written with a long upstroke, the number 7 has a horizontal stroke through the vertical line.Any number multiplied by one remains that number, as one is the identity for multiplication. As a result, 1 is its own factorial, its own square, its own cube, and so on. One is also the result of the empty product, as any number multiplied by one is itself. It is also the only natural number that is neither composite nor prime with respect to division, but instead considered a unit (meaning of ring theory).One, sometimes referred to as unity,[2] is the first non-zero natural number. It is thus the integer before two and after zero, and the first positive odd number.Compare the Proto-Indo-European root *oi-no- (which means "one, single"[1]) to Greek oinos (which means "ace" on dice[1]), Latin unus (one[1]), Old Persian aivam, Old Church Slavonic -inu and ino-, Lithuanian vienas, Old Irish oin and Breton un (one[1]).Compare the Proto-Germanic root *ainaz to Old Frisian an, Gothic ains, Danish een, Dutch een, German eins and Old Norse einn.It comes from the English word an,[1] which comes from the Proto-Germanic root *ainaz.[1] The Proto-Germanic root *ainaz comes from the Proto-Indo-European root *oi-no-.[1]The word one can be used as a noun, an adjective and a pronoun.[1]1 (one, also called unit, unity, and (multiplicative) identity) is a number, numeral, and glyph. It represents a single entity, the unit of counting or measurement. For example, a line segment of unit length is a line segment of length 1. It is also the first of the infinite sequence of natural numbers, followed by 2.
Abelian group
Among mathematical adjectives derived from the proper name of a mathematician, the word "abelian" is rare in that it is often spelled with a lowercase a, rather than an uppercase A, indicating how ubiquitous the concept is in modern mathematics.[7]Moreover, abelian groups of infinite order lead, quite surprisingly, to deep questions about the set theory commonly assumed to underlie all of mathematics. Take the Whitehead problem: are all Whitehead groups of infinite order also free abelian groups? In the 1970s, Saharon Shelah proved that the Whitehead problem is:Nearly all well-known algebraic structures other than Boolean algebras are undecidable. Hence it is surprising that Tarski's student Wanda Szmielew (1955) proved that the first order theory of abelian groups, unlike its nonabelian counterpart, is decidable. This decidability, plus the fundamental theorem of finite abelian groups described above, highlight some of the successes in abelian group theory, but there are still many areas of current research:The collection of all abelian groups, together with the homomorphisms between them, forms the category Ab, the prototype of an abelian category.Many large abelian groups possess a natural topology, which turns them into topological groups.The additive group of a ring is an abelian group, but not all abelian groups are additive groups of rings (with nontrivial multiplication). Some important topics in this area of study are:The classification theorems for finitely generated, divisible, countable periodic, and rank 1 torsion-free abelian groups explained above were all obtained before 1950 and form a foundation of the classification of more general infinite abelian groups. Important technical tools used in classification of infinite abelian groups are pure and basic subgroups. Introduction of various invariants of torsion-free abelian groups has been one avenue of further progress. See the books by Irving Kaplansky, László Fuchs, Phillip Griffith, and David Arnold, as well as the proceedings of the conferences on Abelian Group Theory published in Lecture Notes in Mathematics for more recent findings.One of the most basic invariants of an infinite abelian group A is its rank: the cardinality of the maximal linearly independent subset of A. Abelian groups of rank 0 are precisely the periodic groups, while torsion-free abelian groups of rank 1 are necessarily subgroups of Q and can be completely described. More generally, a torsion-free abelian group of finite rank r is a subgroup of Qr. On the other hand, the group of p-adic integers Zp is a torsion-free abelian group of infinite Z-rank and the groups Zn
p with different n are non-isomorphic, so this invariant does not even fully capture properties of some familiar groups.An abelian group that is neither periodic nor torsion-free is called mixed. If A is an abelian group and T(A) is its torsion subgroup then the factor group A/T(A) is torsion-free. However, in general the torsion subgroup is not a direct summand of A, so A is not isomorphic to T(A) ⊕ A/T(A). Thus the theory of mixed groups involves more than simply combining the results about periodic and torsion-free groups.An abelian group is called torsion-free if every non-zero element has infinite order. Several classes of torsion-free abelian groups have been studied extensively:An abelian group is called periodic or torsion, if every element has finite order. A direct sum of finite cyclic groups is periodic. Although the converse statement is not true in general, some special cases are known. The first and second Prüfer theorems state that if A is a periodic group, and it either has a bounded exponent, i.e., nA = 0 for some natural number n, or is countable and the p-heights of the elements of A are finite for each p, then A is isomorphic to a direct sum of finite cyclic groups.[6] The cardinality of the set of direct summands isomorphic to Z/pmZ in such a decomposition is an invariant of A. These theorems were later subsumed in the Kulikov criterion. In a different direction, Helmut Ulm found an extension of the second Prüfer theorem to countable abelian p-groups with elements of infinite height: those groups are completely classified by means of their Ulm invariants.Two important special classes of infinite abelian groups with diametrically opposite properties are torsion groups and torsion-free groups, exemplified by the groups Q/Z (periodic) and Q (torsion-free).By contrast, classification of general infinitely generated abelian groups is far from complete. Divisible groups, i.e. abelian groups A in which the equation nx = a admits a solution x ∈ A for any natural number n and element a of A, constitute one important class of infinite abelian groups that can be completely characterized. Every divisible group is isomorphic to a direct sum, with summands isomorphic to Q and Prüfer groups Qp/Zp for various prime numbers p, and the cardinality of the set of summands of each type is uniquely determined.[5] Moreover, if a divisible group A is a subgroup of an abelian group G then A admits a direct complement: a subgroup C of G such that G = A ⊕ C. Thus divisible groups are injective modules in the category of abelian groups, and conversely, every injective abelian group is divisible (Baer's criterion). An abelian group without non-zero divisible subgroups is called reduced.The simplest infinite abelian group is the infinite cyclic group Z. Any finitely generated abelian group A is isomorphic to the direct sum of r copies of Z and a finite abelian group, which in turn is decomposable into a direct sum of finitely many cyclic groups of primary orders. Even though the decomposition is not unique, the number r, called the rank of A, and the prime powers giving the orders of finite cyclic summands are uniquely determined.The existence of algorithms for Smith normal form shows that the fundamental theorem of finitely generated abelian groups is not only a theorem of abstract existence, but provides a way for computing expression of finitely generated abelian groups as direct sums.where r is the number of zero rows at the bottom of r (and also the rank of the group). This is the fundamental theorem of finitely generated abelian groups.The Smith normal form of M is a matrixIt follows that the study of finitely generated abelian groups is totally equivalent with the study of integer matrices. In particular, changing the generating set of A is equivalent with multiplying M on the left by a unimodular matrix (that is an invertible integer matrix whose inverse is also an integer matrix). Changing the generating set of the kernel of M is equivalent with multiplying M on the right by an unimodular matrix.This homomorphism is surjective, and its kernel is finitely generated (since integers form a Noetherian ring). Let us consider the matrix M with integer entries, such that the entries of its jth column are the coefficients of the jth generator of the kernel. Then, the abelian group is isomorphic to the cokernel of linear map defined by M. Conversely every integer matrix defines a finitely generated abelian group.One can check that this yields the orders in the previous examples as special cases (see Hillar, C., & Rhea, D.).then one has in particular dk ≥ k, ck ≤ k, andandIn the most general case, where the ei and n are arbitrary, the automorphism group is more difficult to determine. It is known, however, that if one defineswhere GL is the appropriate general linear group. This is easily shown to have orderso elements of this subgroup can be viewed as comprising a vector space of dimension n over the finite field of p elements Fp. The automorphisms of this subgroup are therefore given by the invertible linear transformations, soOne special case is when n = 1, so that there is only one cyclic prime-power factor in the Sylow p-subgroup P. In this case the theory of automorphisms of a finite cyclic group can be used. Another special case is when n is arbitrary but ei = 1 for 1 ≤ i ≤ n. Here, one is considering P to be of the formfor some n > 0. One needs to find the automorphisms ofGiven this, the fundamental theorem shows that to compute the automorphism group of G it suffices to compute the automorphism groups of the Sylow p-subgroups separately (that is, all direct sums of cyclic subgroups, each with order a power of p). Fix a prime p and suppose the exponents ei of the cyclic factors of the Sylow p-subgroup are arranged in increasing order:One can apply the fundamental theorem to count (and sometimes determine) the automorphisms of a given finite abelian group G. To do this, one uses the fact that if G splits as a direct sum H ⊕ K of subgroups of coprime order, then Aut(H ⊕ K) ≅ Aut(H) ⊕ Aut(K).See also list of small groups for finite abelian groups of order 30 or less.For another example, every abelian group of order 8 is isomorphic to either Z8 (the integers 0 to 7 under addition modulo 8), Z4 ⊕ Z2 (the odd integers 1 to 15 under multiplication modulo 16), or Z2 ⊕ Z2 ⊕ Z2.For example, Z15 can be expressed as the direct sum of two cyclic subgroups of order 3 and 5: Z15 ≅ {0, 5, 10} ⊕ {0, 3, 6, 9, 12}. The same can be said for any abelian group of order 15, leading to the remarkable conclusion that all abelian groups of order 15 are isomorphic.in either of the following canonical ways:The cyclic group Zmn of order mn is isomorphic to the direct sum of Zm and Zn if and only if m and n are coprime. It follows that any finite abelian group G is isomorphic to a direct sum of the formThe classification was proven by Leopold Kronecker in 1870, though it was not stated in modern group-theoretic terms until later, and was preceded by a similar classification of quadratic forms by Gauss in 1801; see history for details.The fundamental theorem of finite abelian groups states that every finite abelian group G can be expressed as the direct sum of cyclic subgroups of prime-power order; it is also known as the basis theorem for finite abelian groups. This is generalized by the fundamental theorem of finitely generated abelian groups, with finite groups being the special case when G has zero rank; this in turn admits numerous further generalizations.Any group of prime order is isomorphic to a cyclic group and therefore abelian. Any group whose order is a square of a prime number is abelian.[4] In fact, for every prime number p there are (up to isomorphism) exactly two groups of order p2, namely Zp2 and Zp×Zp.Cyclic groups of integers modulo n, Z/nZ, were among the first examples of groups. It turns out that an arbitrary finite abelian group is isomorphic to a direct sum of finite cyclic groups of prime power order, and these orders are uniquely determined, forming a complete system of invariants. The automorphism group of a finite abelian group can be described directly in terms of these invariants. The theory had been first developed in the 1879 paper of Georg Frobenius and Ludwig Stickelberger and later was both simplified and generalized to finitely generated modules over a principal ideal domain, forming an important chapter of linear algebra.The center Z(G) of a group G is the set of elements that commute with every element of G. A group G is abelian if and only if it is equal to its center Z(G). The center of a group G is always a characteristic abelian subgroup of G. If the quotient group G/Z(G) of a group by its center is cyclic then G is abelian.[3]Somewhat akin to the dimension of vector spaces, every abelian group has a rank. It is defined as the maximal cardinality of a set of linearly independent elements of the group. The integers and the rational numbers have rank one, as well as every subgroup of the rationals.If f, g : G → H are two group homomorphisms between abelian groups, then their sum f + g, defined by (f + g) (x) = f(x) + g(x), is again a homomorphism. (This is not true if H is a non-abelian group.) The set Hom(G, H) of all group homomorphisms from G to H thus turns into an abelian group in its own right.Theorems about abelian groups (i.e. modules over the principal ideal domain Z) can often be generalized to theorems about modules over an arbitrary principal ideal domain. A typical example is the classification of finitely generated abelian groups which is a specialization of the structure theorem for finitely generated modules over a principal ideal domain. In the case of finitely generated abelian groups, this theorem guarantees that an abelian group splits as a direct sum of a torsion group and a free abelian group. The former may be written as a direct sum of finitely many groups of the form Z/pkZ for p prime, and the latter is a direct sum of finitely many copies of Z.If n is a natural number and x is an element of an abelian group G written additively, then nx can be defined as x + x + ... + x (n summands) and (−n)x = −(nx). In this way, G becomes a module over the ring Z of integers. In fact, the modules over Z can be identified with the abelian groups.Camille Jordan named abelian groups after Norwegian mathematician Niels Henrik Abel, because Abel found that the commutativity of the group of a polynomial implies that the roots of the polynomial can be calculated by using radicals. See Section 6.5 of Cox (2004) for more information on the historical background.In general, matrices, even invertible matrices, do not form an abelian group under multiplication because matrix multiplication is generally not commutative. However, some groups of matrices are abelian groups under matrix multiplication – one example is the group of 2×2 rotation matrices.This is true since if the group is abelian, then gi ⋅ gj = gj ⋅ gi. This implies that the (i, j)th entry of the table equals the (j, i)th entry, thus the table is symmetric about the main diagonal.To verify that a finite group is abelian, a table (matrix) – known as a Cayley table – can be constructed in a similar fashion to a multiplication table. If the group is G = {g1 = e, g2, ..., gn} under the operation ⋅, the (i, j)th entry of this table contains the product gi ⋅ gj. The group is abelian if and only if this table is symmetric about the main diagonal.Generally, the multiplicative notation is the usual notation for groups, while the additive notation is the usual notation for modules and rings. The additive notation may also be used to emphasize that a particular group is abelian, whenever both abelian and non-abelian groups are considered, some notable exceptions being near-rings and partially ordered groups, where an operation is written additively even when non-abelian.There are two main notational conventions for abelian groups – additive and multiplicative.A group in which the group operation is not commutative is called a "non-abelian group" or "non-commutative group".An abelian group is a set, A, together with an operation • that combines any two elements a and b to form another element denoted a • b. The symbol • is a general placeholder for a concretely given operation. To qualify as an abelian group, the set and operation, (A, •), must satisfy five requirements known as the abelian group axioms:The concept of an abelian group is one of the first concepts encountered in undergraduate abstract algebra, from which many other basic concepts, such as modules and vector spaces, are developed. The theory of abelian groups is generally simpler than that of their non-abelian counterparts, and finite abelian groups are very well understood. On the other hand, the theory of infinite abelian groups is an area of current research.In abstract algebra, an abelian group, also called a commutative group, is a group in which the result of applying the group operation to two group elements does not depend on the order in which they are written. That is, these are the groups that obey the axiom of commutativity. Abelian groups generalize the arithmetic of addition of integers. They are named after early 19th century mathematician Niels Henrik Abel.[1]
Sequence
An infinite binary sequence can represent a formal language (a set of strings) by setting the n th bit of the sequence to 1 if and only if the n th string (in shortlex order) is in the language. This representation is useful in the diagonalization method for proofs.[12]Infinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science. They are often referred to simply as sequences or streams, as opposed to finite strings. Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}). The set C = {0, 1}∞ of all infinite binary sequences is sometimes called the Cantor space.Automata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, Σ. Most familiar types of automata transition from state to state by reading input letters from Σ, following edges with matching labels; the ordered input for such an automaton forms a sequence called a word (or input word). The sequence of states encountered by the automaton when processing a word is called a run. A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter. This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter.An ordinal-indexed sequence is a generalization of a sequence. If α is a limit ordinal and X is a set, an α-indexed sequence of elements of X is a function from α to X. In this terminology an ω-indexed sequence is an ordinary sequence.In homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations. Spectral sequences are a generalization of exact sequences, and since their introduction by Jean Leray (1946), they have become an important research tool, particularly in homotopy theory.A similar definition can be made for certain other algebraic structures. For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.Note that the sequence of groups and homomorphisms may be either finite or infinite.of groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:In the context of group theory, a sequenceIf A is a set, the free monoid over A (denoted A*, also called Kleene star of A) is a monoid containing all the finite sequences (or strings) of zero or more elements of A, with the binary operation of concatenation. The free semigroup A+ is the subsemigroup of A* containing all elements except the empty sequence.Abstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.Sequences over a field may also be viewed as vectors in a vector space. Specifically, the set of F-valued sequences (where F is a field) is a function space (in fact, a product space) of F-valued functions over the set of natural numbers.The most important sequences spaces in analysis are the ℓp spaces, consisting of the p-power summable sequences, with the p-norm. These are special cases of Lp spaces for the counting measure on the set of natural numbers. Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted c and c0, with the sup norm. Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called an FK-space.A sequence space is a vector space whose elements are infinite sequences of real or complex numbers. Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers. The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication. All sequence spaces are linear subspaces of this space. Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.The most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers. This type can be generalized to sequences of elements of some vector space. In analysis, the vector spaces considered are often function spaces. Even more generally, one can study sequences with elements in some topological space.It may be convenient to have the sequence start with an index different from 1 or 0. For example, the sequence defined by xn = 1/log(n) would be defined only for n ≥ 2. When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given N.which is to say, infinite sequences of elements indexed by natural numbers.In analysis, when talking about sequences, one will generally consider sequences of the formThe topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.Sequences can be generalized to nets or filters. These generalizations allow one to extend some of the above theorems to spaces without metrics.Sequences play an important role in topology, especially in the study of metric spaces. For instance:and say that the sequence diverges or converges to negative infinity.In this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is an = n.Metric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.In contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined by x1 = 1 and xn+1 = xn + 2/xn/2 is Cauchy, but has no rational limit, cf. here. More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.A Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is Cauchy characterization of convergence for sequences:Moreover:An important property of a sequence is convergence. If a sequence converges, it converges to a particular value known as the limit. If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.Some other types of sequences that are easy to define include:A subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements. For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...). The positions of some elements change when other elements are deleted. However, the relative positions are preserved.If the sequence of real numbers (an) is such that all the terms are less than some real number M, then the sequence is said to be bounded from above. In less words, this means that there exists M such that for all n, an ≤ M. Any such M is called an upper bound. Likewise, if, for some real m, an ≥ m for all n greater than some N, then the sequence is bounded from below and any such m is called a lower bound. If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.The terms nondecreasing and nonincreasing are often used in place of increasing and decreasing in order to avoid any possible confusion with strictly increasing and strictly decreasing, respectively.A sequence of a finite length n is also called an n-tuple. Finite sequences include the empty sequence ( ) that has no elements.The length of a sequence is defined as the number of terms in the sequence.Sequences and their limits (see below) are important concepts for studying topological spaces. An important generalization of sequences is the concept of nets. A net is a function from a (possibly uncountable) directed set to a topological space. The notational conventions for sequences normally apply to nets as well.For the purposes of this article, we define a sequence to be a function whose domain is a convex subset of the set of integers. This definition covers several different uses of the word "sequence", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions). However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers. The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice. Many authors also impose a requirement on the codomain of a function before calling it a sequence, by requiring it to be the set R of real numbers,[2] the set C of complex numbers,[3] or a topological space.[4]There are many different notions of sequences in mathematics, some of which (e.g., exact sequence) are not covered by the definitions and notations introduced below.Not all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated. For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).The first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34. A more complicated example of a sequence that is defined recursively is Recaman's sequence.[1] We can define Recaman's sequence byThe Fibonacci sequence can be defined using a recursive rule along with two initial elements. The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.To define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it. In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule. The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions. Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.Sequences whose elements are related to the previous elements in a straightforward way are often defined using recursion. This is in contrast to the definition of sequence elements as a function of their position.In some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred. In these cases the index set may be implied by a listing of the first few abstract elements. For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.Other examples of sequences include ones made up of rational numbers, real numbers, and complex numbers. The sequence (.9, .99, .999, .9999, ...) approaches the number 1. In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion. For instance, π is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...). A related sequence is the sequence of decimal digits of π, i.e. (3, 1, 4, 1, 5, 9, ...). This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.For a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.The Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements. The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).The prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves. Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...). The prime numbers are widely used in mathematics and specifically in number theory.There are a number of ways to denote a sequence, some of which are more useful for specific types of sequences. One way to specify a sequence is to list the elements. For example, the first four odd numbers form the sequence (1, 3, 5, 7). This notation can be used for infinite sequences as well. For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...). Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements. Other ways to denote a sequence are discussed after the examples.A sequence can be thought of as a list of elements with a particular order. Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences. In particular, sequences are the basis for series, which are important in differential equations and analysis. Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...). In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed. Like a set, it contains members (also called elements, or terms). The number of elements (possibly infinite) is called the length of the sequence. Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n). The position of an element in a sequence is its rank or index; it is the integer from which the element is the image. It depends on the context or of a specific convention, if the first element has index 0 or 1. When a symbol has been chosen for denoting a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence is generally denoted Fn.
Function (mathematics)
The concept of categorification is an attempt to replace set-theoretic notions by category-theoretic ones. In particular, according to this idea, sets are replaced by categories, while functions between sets are replaced by functors.[13]The idea of structure-preserving functions, or homomorphisms, led to the abstract notion of morphism, the key concept of category theory. In fact, functions f: X → Y are the morphisms in the category of sets, including the empty set: if the domain X is the empty set, then the subset of X × Y describing the function is necessarily empty, too. However, this is still a well-defined function. Such a function is called an empty function. In particular, the identity function of the empty set is defined, a requirement for sets to form a category.In other parts of mathematics, non-single-valued relations are similarly conflated with functions: these are called multivalued functions, with the corresponding term single-valued function for ordinary functions.In some parts of mathematics, including recursion theory and functional analysis, it is convenient to study partial functions in which some values of the domain have no association in the graph; i.e., single-valued relations. For example, the function f such that f(x) = 1/x does not define a value for x = 0, since division by zero is not defined. Hence f is only a partial function from the real line to the real line. The term total function can be used to stress the fact that every element of the domain does appear as the first element of an ordered pair in the graph.Many operations in set theory, such as the power set, have the class of all sets as their domain, and therefore, although they are informally described as functions, they do not fit the set-theoretical definition outlined above, because a class is not necessarily a set. However some definitions of relations and functions define them as classes of pairs rather than sets of pairs and therefore do include the power set as a function.[12]Functions are commonly defined as a type of relation. A relation from X to Y is a set of ordered pairs (x, y) with x ∈ X and y ∈ Y. A function from X to Y can be described as a relation from X to Y that is left-total and right-unique. However, when X and Y are not specified there is a disagreement about the definition of a relation that parallels that for functions. Normally a relation is just defined as a set of ordered pairs and a correspondence is defined as a triple (X, Y, F), however the distinction between the two is often blurred or a relation is never referred to without specifying the two sets. The definition of a function as a triple defines a function as a type of correspondence, whereas the definition of a function as a set of ordered pairs defines a function as a type of relation.An alternative definition of the composite function g(f(x)) defines it for the set of all x in the domain of f such that f(x) is in the domain of g.[11] Thus the real square root of −x2 is a function only defined at 0 where it has the value 0.If a function is defined as a set of ordered pairs with no specific codomain, then f: X → Y indicates that f is a function whose domain is X and whose image is a subset of Y. This is the case in the ISO standard.[5] Y may be referred to as the codomain but then any set including the image of f is a valid codomain of f. This is also referred to by saying that "f maps X into Y"[5] In some usages X and Y may subset the ordered pairs, e.g. the function f on the real numbers such that y=x2 when used as in f: [0,4] → [0,4] means the function defined only on the interval [0,2].[10] With the definition of a function as an ordered triple this would always be considered a partial function.In the other definition a function is defined as a set of ordered pairs where each first element only occurs once. The domain is the set of all the first elements of a pair and there is no explicit codomain separate from the image.[8][9] Concepts like surjective have to be refined for such functions, more specifically by saying that a (given) function is surjective on a (given) set if its image equals that set. For example, we might say a function f is surjective on the set of real numbers.The above definition of "a function from X to Y" is generally agreed on,[citation needed] however there are two different ways a "function" is normally defined where the domain X and codomain Y are not explicitly or implicitly specified. Usually this is not a problem as the domain and codomain normally will be known. With one definition saying the function defined by f(x) = x2 on the reals does not completely specify a function as the codomain is not specified, and in the other it is a valid definition.The second definition can be used to define a set or class of functions without specifying a codomain. For example, the class of functions on the ordinal α. Also, an infinite sequence can be defined as a function on ω, the set of finite ordinals.The first definition allows the use of functions without specifying their domain or codomain. For example, this definition is used to state the replacement axiom of von Neumann–Bernays–Gödel set theory: For all classes F and for all sets X, if F is a function, then F[X] is a set. Since this definition uses set theory's definition of image, X does not have to be a subset or subclass of the domain of F.The definitions of function and image in set theory are more general than the ones given above—namely, the definition of function does not mention a domain or a codomain, and the definition of the image F[A] does not require that A be a subset of the domain.[6] Set theory also specializes its function definition to the cases where the domain or both the domain and codomain are specified. This produces the following three definitions.[7]The set of all functions from a set X to a set Y is denoted by X → Y, by [X → Y], or by YX. The latter notation is motivated by the fact that, when X and Y are finite and of size |X| and |Y|, then the number of functions X → Y is |YX| = |Y||X|. This is an example of the convention from enumerative combinatorics that provides notations for sets based on their cardinalities. If X is infinite and there is more than one element in Y then there are uncountably many functions from X to Y, though only countably many of them can be expressed with a formula or algorithm.There are many other special classes of functions that are important to particular branches of mathematics, or particular applications. Here is a partial list:The following table contains a few particularly important types of real-valued functions:where X is an arbitrary set, their (pointwise) sum f + g and product f ⋅ g are functions with the same domain and codomain. They are defined by the formulas:Real-valued functions enjoy so-called pointwise operations. That is, given two functionsA real-valued function f is one whose codomain is the set of real numbers or a subset thereof. If, in addition, the domain is also a subset of the reals, f is a real valued function of a real variable. The study of such functions is called real analysis.When the codomain is a direct product, it is frequently a vector space. In this case, one talks of vector-valued function or, mostly in physics, of vector field.It is common to consider also functions whose codomain is a product of sets. For example, Euclidean division maps very pair (a, b) of integers such b ≠ 0 to a pair of integers called the quotient and the remainder. Such a function may be considered as a tuple of functions. For example, Euclidean division is a pair of functions (quotient, remainder) such thatwhere the domain U has the formMore formally, a function of n variables is a function whose domain is a set of n-tuples. For example, multiplication of integers is a function of two variables, or bivariate function, whose domain it the set of all pairs (2-uples) of integers and the codomain is the set of integers. The same is true for every binary operation. More generally, every mathematical operation is defined as a multivariate function.A multivariate function, or function of several variables is a function that depends on several arguments. Such functions are commonly encountered. For example, the position of a car on a road is a function of the time and its speed.An extension of a function f is a function g such that f is a restriction of g. A typical use of this concept is the process of analytic continuation, that allows extending functions whose domain is a small part of the complex plane to functions whose domain is almost the whole complex plane.This often used for define partial inverse functions: if there is a subset S of a function f such that f|S is injective, then the canonical surjection of f|S on its image f|S(S) = f(S) is a bijection, which has an inverse function from f(S) to S. This is in this way that inverse trigonometric functions are defined. The cosine function, for example, is injective, when restricted to the interval (–0, π); the image of this restriction is the interval (–1, 1); this defines thus an inverse function from (–1, 1) to (–0, π), which is called arccosine and denoted arccos."One-to-one" and "onto" are terms that were more common in the older English language literature; "injective", "surjective", and "bijective" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English. As a word of caution, "a one-to-one function" is one that is injective, while a "one-to-one correspondence" refers to a bijective function. Also, the statement "f maps X onto Y" differs from "f maps X into B" in that the former implies that f is surjective), while the latter makes no assertion about the nature of f the mapping. In a complicated reasoning, the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms, which have also the advantage to be more symmetrical.The preimage by f of an element y of the codomain is sometimes called, in some contexts, the fiber of y under f.For example, the preimage of {4, 9} under the square function is the set {−3,−2,2,3}.The image of f is the image of the whole domain, that is f(X). It is also called the range of f, although the term may also refer to the codomain.[5]Some functions are uniquely defined by their domain and codomain, and are sometimes called canonical: In this section, we describe general properties of functions, that are independent of specific properties of the domain and the codomain.There are many variants of this method, see Histogram for details.In statistic, histogram are often used for representing very irregular functions. For example, for representing the function that associates his weight to each member of some population, one draws the histogram of the function that associates to each weight interval the number of people, whose weights belong to this interval.Histograms are often used for representing functions whose domain is finite, or is the natural numbers or the integers. In this case, an element x of the domain is represented by an interval of the x-axis, and a point (x, y) of the graph is represented by a rectangle with basis the interval corresponding to x and height y.It is possible to draw effectively the graph of a function only if the function is sufficiently regular, that is, either if the function is differentiable (or piecewise differentiable) or if its domain may be identified with the integers or a subset of the integers.As functions may be complicated objects, it is often useful to draw the graph of a function for getting a global view of its properties. Some functions may also represented histogramsWhen the domain of a function is the nonnegative integers or, more generally, when the domain may be well ordered, a function may be defined by induction or recursion, which means (roughly) that the definition of the function for a given input depends on previously defined values of the function. For example, the Fibonacci sequence is a function from the natural numbers into themselves that is defined by (see above for the use of indices for the argument of a function)Above ways of defining functions define them "pointwise", that is, each value is defined independently of the other values. This is not always the case.More generally, the computability theory is the study of the computable functions, that is the functions that can be computed by an algorithm.In general, a specific function is defined by associating to every element of the domain a property of this element (as in the above example of colored shapes) or the result of a computation taking as input. This computation may be described by a formula. (This is the starting point of algebra of replacing many similar computations on numbers by a single formula that describes the computation on variables representing unspecified numbers). This definition of a function uses frequently previously defined auxiliary functions.When the domain of a function is the set of natural numbers, one talks often of a sequence (mathematics), and one uses the index notation, that is, one denotesThe definition of a function by a formula may use case distinction as for the definition of the absolute value of real numbers:Similarly, the identity function on a set X is the functionor, more commonly,In many cases, a function is defined by a formula that describes how the value of the function is computed from the value of the argument. For example, if a multiplication is defined on a set X, then one may define a square function with domain and codomain X byThe symbol used to denote a particular function consists generally of a single letter in italic font, most often the lower-case letters f, g, h. Some widely used functions are represented by a symbol consisting of several letters (generally an abbreviation of their name). In this case, it is set in roman type, such as "sin" for the sine function.means that (x, y) belongs to the set of pairs defining the function f. When it is needed to refer explicitly to this set of pairs, it is denotedThe notationorA function f with domain X and codomain Y is commonly denotedConsidering the "color-of-the-shape" function above, the set X is the domain consisting of the four shapes, while Y is the codomain consisting of five colors. The "color-of-the-shape" function described above consists of the set of those ordered pairs, (shape, color) where the color is the actual color of the given shape.This formal definition is a precise rendition of the idea that to each x is associated an element y of Y, namely the uniquely specified element y with the property just mentioned.In this definition, X and Y are respectively called the domain and the codomain of the function f. If (x, y) belongs to the set defining f, then y is the value of f for the argument x, or the image of x under f. One says also that y is the value of f for the value x of the variable.A function f from X to Y is set of ordered pairs (x, y) such that x ∈ X and y ∈ Y, which is subject to the following condition: every element of X is the first component of one and only one ordered pair in the set.[4] In other words, for every x in X there is exactly one element y such that the ordered pair (x, y) belongs to the set of pairs defining the function f.When students first begin to study functions, intuitive ideas such as "rules" and "associates" are used. In more advanced mathematics, more precise definitions are needed. The precise mathematical definition of a function relies on the notion of an ordered pair.The term range is used in two different ways, sometimes used for the codomain, sometimes used for the image.A third example of a function has the set of polygons as domain and the set of natural numbers as codomain. The function associates a polygon with its number of vertices. For example, a triangle is associated with the number 3, a square with the number 4, and so on.A second example of a function is the following: the domain is chosen to be the set of natural numbers (1, 2, 3, 4, ...), and the codomain is the set of integers (..., −3, −2, −1, 0, 1, 2, 3, ...). The function associates to any natural number n the number 4−n. For example, to 1 it associates 3 and to 10 it associates −6.The input to a function is called the argument and the output is called the value. The set of all permitted inputs to a given function is called the domain of the function, while the set of permissible outputs is called the codomain. Thus, the domain of the "color-of-the-shape function" is the set of the four shapes, and the codomain consists of the five colors. The concept of a function does not require that every possible output is the value of some argument, e.g. the color blue is not the color of any of the four shapes in X.For an example of a function, let X be the set consisting of four shapes: a red triangle, a yellow rectangle, a green hexagon, and a red square; and let Y be the set consisting of five colors: red, blue, green, pink, and yellow. Linking each shape to its color is a function from X to Y: each shape is linked to a color (i.e., an element in Y), and each shape is "linked", or "mapped", to exactly one color. There is no shape that lacks a color and no shape that has more than one color. This function will be referred to as the "color-of-the-shape function".In analogy with arithmetic, it is possible to define addition, subtraction, multiplication, and division of functions, in those cases where the output is a number. Another important operation defined on functions is function composition, where the output from one function becomes the input to another function.In modern mathematics,[3] a function is defined by its set of inputs, called the domain; a set containing the set of outputs, and possibly additional elements, as members, called its codomain (or target); and the set of all input-output pairs, called its graph. Sometimes the codomain is called the function's "range", but more commonly the word "range" is used to mean, instead, specifically the set of outputs (this is also called the image of the function). For example, we could define a function using the rule f(x) = x2 by saying that the domain and codomain are the real numbers, and that the graph consists of all pairs of real numbers (x, x2). The image of this function is the set of non-negative real numbers. Collections of functions with the same domain and the same codomain are called function spaces, the properties of which are studied in such mathematical disciplines as real analysis, complex analysis, and functional analysis.Functions of various kinds are "the central objects of investigation"[2] in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. A picture, with some input, output pairs shown on Cartesian coordinates, is called the graph of the function, and may give an intuitive idea of some of the main features of the function, for example its maximums and minimums. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. In this case, if there is reason to believe the function is smooth, it is sometimes assumed that the values in between the points on the table also exist, but are not known with the accuracy of the measured points. For example, a table might give the temperature at a particular place at specified times, with the assumption that the temperature is a smooth function of time, even though the temperature at times other than those measured is not known. Another form in which functions are given is the implicit function, for example as an inverse to another function or as a solution of a differential equation.In mathematics, a function[1] is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number x to its square x2. The output of a function f corresponding to an input x is denoted by f(x) (read "f of x"). In this example, if the input is −3, then the output is 9, and we may write f(−3) = 9. Likewise, if the input is 3, then the output is also 9, and we may write f(3) = 9. (The same output may be produced by more than one input, but each input gives only one output.) The input variable(s) are sometimes referred to as the argument(s) of the function.
Polynomial ring
The skew-polynomial ring is defined similarly for a ring R and a ring endomorphism f of R, by extending the multiplication from the relation X·r = f(r)·X to produce an associative multiplication that distributes over the standard addition. More generally, given a homomorphism F from the monoid N of the positive integers into the endomorphism ring of R, the formula Xn·r = F(n)(r)·Xn allows constructing a skew-polynomial ring.(Lam 2001, §1,ex 1.11) Skew polynomial rings are closely related to crossed product algebras.This relation may be extended to define a skew multiplication between two polynomials in X with coefficients in R, which make them a non-commutative ring.A differential polynomial ring is a ring of differential operators formed from a ring R and a derivation δ of R into R. This derivation operates on R, and will be denoted X, when viewed as an operator. The elements of R also operate on R by multiplication. The composition of operators is denoted as the usual multiplication. It follows that the relation δ(ab) = aδ(b) + δ(a)b may be rewritten asOther generalizations of polynomials are differential and skew-polynomial rings.Just as the polynomial ring in n variables with coefficients in the commutative ring R is the free commutative R-algebra of rank n, the noncommutative polynomial ring in n variables with coefficients in the commutative ring R is the free associative, unital R-algebra on n generators, which is noncommutative when n > 1.For polynomial rings of more than one variable, the products X·Y and Y·X are simply defined to be equal. A more general notion of polynomial ring is obtained when the distinction between these two formal products is maintained. Formally, the polynomial ring in n noncommuting variables with coefficients in the ring R is the monoid ring R[N], where the monoid N is the free monoid on n letters, also known as the set of all strings over an alphabet of n symbols, with multiplication given by concatenation. Neither the coefficients nor the variables need commute amongst themselves, but the coefficients and variables commute with each other.Power series generalize the choice of exponent in a different direction by allowing infinitely many nonzero terms. This requires various hypotheses on the monoid N used for the exponents, to ensure that the sums in the Cauchy product are finite sums. Alternatively, a topology can be placed on the ring, and then one restricts to convergent infinite sums. For the standard choice of N, the non-negative integers, there is no trouble, and the ring of formal power series is defined as the set of functions from N to a ring R with addition component-wise, and multiplication given by the Cauchy product. The ring of power series can be seen as the completion of the polynomial ring.Several interesting examples of rings and groups are formed by taking N to be the additive monoid of non-negative rational numbers, (Osbourne 2000, §4.4). See also Puiseux series.Some authors such as (Lang 2002, II,§3) go so far as to take this monoid definition as the starting point, and regular single variable polynomials are the special case where N is the monoid of non-negative integers. Polynomials in several variables simply take N to be the direct product of several copies of the monoid of non-negative integers.where the latter sum is taken over all i, j in N that sum to n.andand then the formulas for addition and multiplication are the familiar:When N is commutative, it is convenient to denote the function a in R[N] as the formal sum:A simple generalization only changes the set from which the exponents on the variable are drawn. The formulas for addition and multiplication make sense as long as one can add exponents: Xi · Xj = Xi+j. A set for which addition makes sense (is closed and associative) is called a monoid. The set of functions from a monoid N to a ring R which are nonzero at only finitely many places can be given the structure of a ring known as R[N], the monoid ring of N with coefficients in R. The addition is defined component-wise, so that if c = a + b, then cn = an + bn for every n in N. The multiplication is defined as the Cauchy product, so that if c = a · b, then for each n in N, cn is the sum of all aibj where i, j range over all pairs of elements of N which sum to n.In the case of infinitely many indeterminates, one can consider a ring strictly larger than the polynomial ring but smaller than the power series ring, by taking the subring of the latter formed by power series whose monomials have a bounded degree. Its elements still have a finite degree and are therefore somewhat like polynomials, but it is possible for instance to take the sum of all indeterminates, which is not a polynomial. A ring of this kind plays a role in constructing the ring of symmetric functions.One slight generalization of polynomial rings is to allow for infinitely many indeterminates. Each monomial still involves only a finite number of indeterminates (so that its degree remains finite), and each polynomial is a still a (finite) linear combination of monomials. Thus, any individual polynomial involves only finitely many indeterminates, and any finite computation involving polynomials remains inside some subring of polynomials in finitely many indeterminates.Polynomial rings have been generalized in a great many ways, including polynomial rings with generalized exponents, power series rings, noncommutative polynomial rings, and skew-polynomial rings.In the following properties, R is a commutative ring and S = R[X1,…, Xn] is the ring of polynomials in n variables over R. The ring extension R ⊂ S can be built from R in n steps, by successively adjoining X1,…, Xn. Thus to establish each of the properties below, it is sufficient to consider the case n = 1.One of the basic techniques in commutative algebra is to relate properties of a ring with properties of its subrings. The notation R ⊂ S indicates that a ring R is a subring of a ring S. In this case S is called an overring of R and one speaks of a ring extension. This works particularly well for polynomial rings and allows one to establish many important properties of the ring of polynomials in several variables over a field, K[X1,…, Xn], by induction in n.A group of fundamental results concerning the relation between ideals of the polynomial ring K[X1,…, Xn] and algebraic subsets of Kn originating with David Hilbert is known under the name Nullstellensatz (literally: "zero-locus theorem").Polynomials in n variables with coefficients in K form a commutative ring denoted K[X1,…, Xn], or sometimes K[X], where X is a symbol representing the full set of variables, X = (X1,…, Xn), and called the polynomial ring in n variables. The polynomial ring in n variables can be obtained by repeated application of K[X] (the order by which is irrelevant). For example, K[X1, X2] is isomorphic to K[X1][X2]. This ring plays fundamental role in algebraic geometry. Many results in commutative and homological algebra originated in the study of its ideals and modules over this ring.and the degree of a polynomial p is the largest degree of a monomial occurring with non-zero coefficient in the expansion of p.The product Xα is called the monomial of multidegree α. A polynomial is a finite linear combination of monomials with coefficients in KA polynomial in n variables X1, …, Xn with coefficients in a field K is defined analogously to a polynomial in one variable, but the notation is more cumbersome. For any multi-index α = (α1, …, αn), where each αi is a non-negative integer, letexplaining why the sentences "Let P be a polynomial" and "Let P(X) be a polynomial" are equivalent.(in the first example R = K, and in the second one R = K[X]). Substituting X by itself results inwe haveLet K be a field or, more generally, a commutative ring, and R a ring containing K. For any polynomial P in K[X] and any element a in R, the substitution of X by a in P defines an element of R, which is denoted P(a). This element is obtained by, after the substitution, carrying on, in R, the operations indicated by the expression of the polynomial. This computation is called the evaluation of P at a. For example, if we haveThis homomorphism is given by the same formula as before, but it is not surjective in general. The existence and uniqueness of such a homomorphism φ expresses a certain universal property of the ring of polynomials in one variable and explains the ubiquity of polynomial rings in various questions and constructions of ring theory and commutative algebra.More generally, given a (not necessarily commutative) ring A containing K and an element a of A that commutes with all elements of K, there is a unique ring homomorphism from the polynomial ring K[X] to A that maps X to a:A particularly important application is to the case when the larger ring L is a field. Then the polynomial p must be irreducible. Conversely, the primitive element theorem states that any finite separable field extension L/K can be generated by a single element θ ∈ L and the preceding theory then gives a concrete description of the field L as the quotient of the polynomial ring K[X] by a principal ideal generated by an irreducible polynomial p. As an illustration, the field C of complex numbers is an extension of the field R of real numbers generated by a single element i such that i2 + 1 = 0. Accordingly, the polynomial X2 + 1 is irreducible over R andBy the assumption, any element of L appears as the right hand side of the last expression for suitable m and elements a0, ..., am of K. Therefore, φ is surjective and L is a homomorphic image of K[X]. More formally, let Ker φ be the kernel of φ. It is an ideal of K[X] and by the first isomorphism theorem for rings, L is isomorphic to the quotient of the polynomial ring K[X] by the ideal Ker φ. Since the polynomial ring is a principal ideal domain, this ideal is principal: there exists a polynomial p ∈ K[X] such thatSuppose that a commutative ring L contains K and there exists an element θ of L such that the ring L is generated by θ over K. Thus any element of L is a linear combination of powers of θ with coefficients in K. Then there is a unique ring homomorphism φ from K[X] into L which does not affect the elements of K itself (it is the identity map on K) and maps each power of X to the same power of θ. Its effect on the general polynomial amounts to "replacing X with θ":The ring K[X] of polynomials over K is obtained from K by adjoining one element, X. It turns out that any commutative ring L containing K and generated as a ring by a single element in addition to K can be described using K[X]. In particular, this applies to finite field extensions of K.Another corollary of the polynomial division with the remainder is the fact that every proper ideal I of K[X] is principal, i.e. I consists of the multiples of a single polynomial f. Thus the polynomial ring K[X] is a principal ideal domain, and for the same reason every Euclidean domain is a principal ideal domain. Also every principal ideal domain is a unique-factorization domain. These deductions make essential use of the fact that the polynomial coefficients lie in a field, namely in the polynomial division step, which requires the leading coefficient of q, which is only known to be non-zero, to have an inverse. If R is an integral domain that is not a field then R[X] is neither a Euclidean domain nor a principal ideal domain; however it could still be a unique factorization domain (and will be so if and only if R itself is a unique factorization domain, for instance if it is Z or another polynomial ring).where the quotient u and the remainder r are polynomials, the degree of r is less than the degree of q, and a decomposition with these properties is unique. The quotient and the remainder are found using the polynomial long division. The degree of the polynomial now plays a role similar to the absolute value of an integer: it is strictly less in the remainder r than it is in q, and when repeating this step such decrease cannot go on indefinitely. Therefore, eventually some division will be exact, at which point the last non-zero remainder is the greatest common divisor of the initial two polynomials. Using the existence of greatest common divisors, Gauss was able to simultaneously rigorously prove the fundamental theorem of arithmetic for integers and its generalization to polynomials. In fact there exist other commutative rings than Z and K[X] that similarly admit an analogue of the Euclidean algorithm; all such rings are called Euclidean rings. Rings for which there exists unique (in an appropriate sense) factorization of nonzero elements into irreducible factors are called unique factorization domains or factorial rings; the given construction shows that all Euclidean rings, and in particular Z and K[X], are unique factorization domains.The next property of the polynomial ring is much deeper. Already Euclid noted that every positive integer can be uniquely factored into a product of primes — this statement is now called the fundamental theorem of arithmetic. The proof is based on Euclid's algorithm for finding the greatest common divisor of natural numbers. At each step of this algorithm, a pair (a, b), a > b, of natural numbers is replaced by a new pair (b, r), where r is the remainder from the division of a by b, and the new numbers are smaller. Gauss remarked that the procedure of division with the remainder can also be defined for polynomials: given two polynomials p and q, where q ≠ 0, one can writeIt follows immediately that if K is an integral domain then so is K[X].[10]If K is a field, or more generally an integral domain, then from the definition of multiplication,[9]The degree of a polynomial p, written deg(p) is the largest k such that the coefficient of Xk is not zero.[4] In this case the coefficient pk is called the leading coefficient.[5] In the special case of zero polynomial, all of whose coefficients are zero, the degree has been variously left undefined,[6] defined to be −1,[7] or defined to be a special symbol −∞.[8]More generally, the field K can be replaced by any commutative ring R when taking the same construction as above, giving rise to the polynomial ring over R, which is denoted R[X].is considered an alternate notation for the sequence (p0, p1, p2, ..., pm, 0, 0, ...).Another equivalent definition is often preferred, although less intuitive, because it is easier to make it completely rigorous, which consists in defining a polynomial as an infinite sequence of elements of K, (p0, p1, p2, ... ) having the property that only a finite number of the elements are nonzero, or equivalently, a sequence for which there is some m so that pn = 0 for n > m. In this case, the expressionIt is easy to verify that these three operations satisfy the axioms of a commutative algebra. Therefore, polynomial rings are also called polynomial algebras.The scalar multiplication is the special case of the multiplication where p = p0 is reduced to its term which is independent of X, that isIf necessary, the polynomials p and q are extended by adding "dummy terms" with zero coefficients, so that the expressions for ri and si are always defined. Specifically, if m < n, then pi = 0 for m < i ≤ n.andwhere k = max(m, n), l = m + n,andthenandThe polynomial ring in X over K is equipped with an addition, a multiplication and a scalar multiplication that make it a commutative algebra. These operations are defined according to the ordinary rules for manipulating algebraic expressions. Specifically, ifThis terminology is suggested by real or complex polynomial functions. However, in general, X and its powers, Xk, are treated as formal symbols, not as elements of the field K or functions over it. One can think of the ring K[X] as arising from K by adding one new element X that is external to K and requiring that X commute with all elements of K.Two polynomials are defined to be equal when the corresponding coefficient of each Xk is equal.for any nonnegative integers k and l. The symbol X is called an indeterminate[2] or variable.[3]where p0, p1, ..., pm, the coefficients of p, are elements of K, and X, X2, are symbols, which are considered as "powers of X", and, by convention, follow the usual rules of exponentiation: X0 = 1, X1 = X, andThe polynomial ring, K[X], in X over a field K is defined[1] as the set of expressions, called polynomials in X, of the formA closely related notion is that of the ring of polynomial functions on a vector space.In mathematics, especially in the field of abstract algebra, a polynomial ring or polynomial algebra is a ring (which is also a commutative algebra) formed from the set of polynomials in one or more indeterminates (traditionally also called variables) with coefficients in another ring, often a field. Polynomial rings have influenced much of mathematics, from the Hilbert basis theorem, to the construction of splitting fields, and to the understanding of a linear operator. Many important conjectures involving polynomial rings, such as Serre's problem, have influenced the study of other rings, and have influenced even the definition of other rings, such as group rings and rings of formal power series.
Matrix (mathematics)
Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toA number λ and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n×n-matrix A is called positive-definite if for all nonzero vectors x ∈ Rn the associated quadratic form given bywhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2×2 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generallywhere 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:
Linear map
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank–nullity theorem:If f : V → W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n × n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol ∗ denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f : V → W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.
Linear map
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank–nullity theorem:If f : V → W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n × n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol ∗ denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f : V → W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.
Map (mathematics)
In the communities surrounding programming languages that treat functions as first-class citizens, a map often refers to the binary higher-order function that takes a function f and a list [v0, v1, ..., vn] as arguments and returns [f(v0), f(v1), ..., f(vn)], where n ≥ 0.In graph theory, a map is a drawing of a graph on a surface without overlapping edges (an embedding). If the surface is a plane then a map is a planar graph, similar to a political map.[3]In formal logic, the term map is sometimes used for a functional predicate, whereas a function is a model of such a predicate in set theory.In category theory, "map" is often used as a synonym for morphism or arrow, thus for something more general than a function.[2]A partial map is a partial function, and a total map is a total function. Related terms like domain, codomain, injective, continuous, etc. can be applied equally to maps and functions, with the same meaning. All these usages can be applied to "maps" as general functions or as functions with special properties.In the theory of dynamical systems, a map denotes an evolution function used to create discrete dynamical systems. See also Poincaré map.Sets of maps of special kinds are the subjects of many important theories: see for instance Lie group, mapping class group, permutation group.Some authors, such as Serge Lang,[1] use "function" only to refer to maps in which the codomain is a set of numbers (i.e. a subset of the fields R or C) and the term mapping for more general functions.In many branches of mathematics, the term map is used to mean a function, sometimes with a specific property of particular importance to that branch. For instance, a "map" is a continuous function in topology, a linear transformation in linear algebra, etc.In mathematics, the term mapping, sometimes shortened to map, refers to either a function, often with some sort of special structure, or a morphism in category theory, which generalizes the idea of a function. There are also a few, less common uses in logic and graph theory.
Bijection
When the partial bijection is on the same set, it is sometimes called a one-to-one partial transformation.[4] An example is the Möbius transformation simply defined on the complex plane, rather than its completion to the extended complex plane.[5]Another way of defining the same notion is to say that a partial bijection from A to B is any relation R (which turns out to be a partial function) with the property that R is the graph of a bijection f:A′→B′, where A′ is a subset of A and B′ is a subset of B.[3]The notion of one-to-one correspondence generalizes to partial functions, where they are called partial bijections, although partial bijections are only required to be injective. The reason for this relaxation is that a (proper) partial function is already undefined for a portion of its domain; thus there is no compelling reason to constrain its inverse to be a total function, i.e. defined everywhere on its domain. The set of all partial bijections on a given base set is called the symmetric inverse semigroup.[2]Bijections are precisely the isomorphisms in the category Set of sets and set functions. However, the bijections are not always the isomorphisms for more complex categories. For example, in the category Grp of groups, the morphisms must be homomorphisms since they must preserve the group structure, so the isomorphisms are group isomorphisms which are bijective homomorphisms.If X and Y are finite sets, then there exists a bijection between the two sets X and Y if and only if X and Y have the same number of elements. Indeed, in axiomatic set theory, this is taken as the definition of "same number of elements" (equinumerosity), and generalising this definition to infinite sets leads to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.Continuing with the baseball batting line-up example, the function that is being defined takes as input the name of one of the players and outputs the position of that player in the batting order. Since this function is a bijection, it has an inverse function which takes as input a position in the batting order and outputs the player who will be batting in that position.Stated in concise mathematical notation, a function f: X → Y is bijective if and only if it satisfies the conditionA bijection f with domain X (indicated by f: X → Y in functional notation) also defines a relation starting in Y and going to X (by turning the arrows around). The process of "turning the arrows around" for an arbitrary function does not, in general, yield a function, but properties (3) and (4) of a bijection say that this inverse relation is a function with domain Y. Moreover, properties (1) and (2) then say that this inverse function is a surjection and an injection, that is, the inverse function exists and is also a bijection. Functions that have inverse functions are said to be invertible. A function is invertible if and only if it is a bijection.The instructor was able to conclude that there were just as many seats as there were students, without having to count either set.In a classroom there are a certain number of seats. A bunch of students enter the room and the instructor asks them all to be seated. After a quick look around the room, the instructor declares that there is a bijection between the set of students and the set of seats, where each student is paired with the seat they are sitting in. What the instructor observed in order to reach this conclusion was that:Consider the batting line-up of a baseball or cricket team (or any list of all the players of any sports team where every player holds a specific spot in a line-up). The set X will be the players on the team (of size nine in the case of baseball) and the set Y will be the positions in the batting order (1st, 2nd, 3rd, etc.) The "pairing" is given by which player is in what position in this order. Property (1) is satisfied since each player is somewhere in the list. Property (2) is satisfied since no player bats in two (or more) positions in the order. Property (3) says that for each position in the order, there is some player batting in that position and property (4) states that two or more players are never batting in the same position in the list.Bijections are sometimes denoted by a two-headed rightwards arrow with tail (U+2916 ⤖ RIGHTWARDS TWO-HEADED ARROW WITH TAIL), as in f : X ⤖ Y. This symbol is a combination of the two-headed rightwards arrow (U+21A0 ↠ RIGHTWARDS TWO HEADED ARROW) sometimes used to denote surjections and the rightwards arrow with a barbed tail (U+21A3 ↣ RIGHTWARDS ARROW WITH TAIL) sometimes used to denote injections.Satisfying properties (1) and (2) means that a bijection is a function with domain X. It is more common to see properties (1) and (2) written as a single statement: Every element of X is paired with exactly one element of Y. Functions which satisfy property (3) are said to be "onto Y " and are called surjections (or surjective functions). Functions which satisfy property (4) are said to be "one-to-one functions" and are called injections (or injective functions).[1] With this terminology, a bijection is a function which is both a surjection and an injection, or using other words, a bijection is a function which is both "one-to-one" and "onto".For a pairing between X and Y (where Y need not be different from X) to be a bijection, four properties must hold:Bijective functions are essential to many areas of mathematics including the definitions of isomorphism, homeomorphism, diffeomorphism, permutation group, and projective map.A bijective function from a set to itself is also called a permutation.A bijection from the set X to the set Y has an inverse function from Y to X. If X and Y are finite sets, then the existence of a bijection means they have the same number of elements. For infinite sets the picture is more complicated, leading to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.In mathematics, a bijection, bijective function, or one-to-one correspondence is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. There are no unpaired elements. In mathematical terms, a bijective function f: X → Y is a one-to-one (injective) and onto (surjective) mapping of a set X to a set Y.
Isomorphism
In the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects X and Y, however, one asks if they are equal or not (they are both elements of the set Hom(X, Y), hence equality is the proper relationship), particularly in commutative diagrams.are three different descriptions for a mathematical object, all of which are isomorphic, but not equal because they are not all subsets of a single space: the first is a subset of R3, the second is C ≅ R2[note 4] plus an additional point, and the third is a subquotient of C2which can be presented as the one-point compactification of the complex plane C ∪ {∞} or as the complex projective line (a quotient space)Generally, saying that two objects are equal is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional spaceIf one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write ≈ for an unnatural isomorphism and ≅ for a natural isomorphism, as in V ≈ V* and V ≅ V**. This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of "the set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.This corresponds to transforming a column vector (element of V) to a row vector (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis". More subtly, there is a map from a vector space V to its double dual V** = { x: V* → K} that does not depend on the choice of basis: For all v ∈ V and φ ∈ V*,Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word isomorphism (Greek iso-, "same," and -morph, "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.and no one isomorphism is intrinsically better than any other.[note 2][note 3] On this view and in this sense, these two sets are not equal because one cannot consider them identical: one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.are equal; they are merely different presentations—the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {A,B,C} and {1,2,3} are not equal—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism isIn certain areas of mathematics, notably category theory, it is valuable to distinguish between equality on the one hand and isomorphism on the other.[3] Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the setsIn cybernetics, the good regulator or Conant–Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's Introduction to Mathematical Philosophy.In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.In graph theory, an isomorphism between two graphs G and H is a bijective map f from the vertices of G to the vertices of H that preserves the "edge structure" in the sense that there is an edge from vertex u to vertex v in G if and only if there is an edge from ƒ(u) to ƒ(v) in H. See graph isomorphism.In category theory, let the category C consist of two classes, one of objects and the other of morphisms. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism ƒ: a → b that has an inverse, i.e. there exists a morphism g: b → a with ƒg = 1b and gƒ = 1a. For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.In abstract algebra, two basic isomorphisms are defined:In a concrete category (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets. In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets. However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).If X = Y, then this is a relation-preserving automorphism.Such an isomorphism is called an order isomorphism or (less commonly) an isotone isomorphism.S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.If one object consists of a set X with a binary relation R and the other object consists of a set Y with a binary relation S then an isomorphism from X to Y is a bijective function ƒ: X → Y such that:[2]For example, (1,1) + (1,0) = (0,1), which translates in the other system as 1 + 3 = 4.or in general (a,b) ↦ (3a + 4b) mod 6.These structures are isomorphic under addition, under the following scheme:Isomorphisms are formalized using category theory. A morphism f : X → Y in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism g : Y → X in that category such that gf = 1X and fg = 1Y, where 1X and 1Y are the identity morphisms of X and Y, respectively.[1]A canonical isomorphism is a canonical map that is an isomorphism. Two objects are said to be canonically isomorphic if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space V to its second dual space is a canonical isomorphism; on the other hand, V is isomorphic to its dual space but not canonically in general.In topology, where the morphisms are continuous functions, isomorphisms are also called homeomorphisms or bicontinuous functions. In mathematical analysis, where the morphisms are differentiable functions, isomorphisms are also called diffeomorphisms.For most algebraic structures, including groups and rings, a homomorphism is an isomorphism if and only if it is bijective.In mathematics, an isomorphism (from the Ancient Greek: ἴσος isos "equal", and μορφή morphe "form" or "shape") is a homomorphism or morphism (i.e. a mathematical mapping) that admits an inverse.[note 1] Two mathematical objects are isomorphic if an isomorphism exists between them. An automorphism is an isomorphism whose source and target coincide. The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.
Determinant
where ωj is an nth root of 1.where ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]Third orderSecond orderwhere the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.In general, the nth-order Vandermonde determinant is[33]The third order Vandermonde determinant isThe Jacobian also occurs in the inverse function theorem.Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given bythe Jacobian matrix is the n × n matrix whose entries are given byFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. ForBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Another infinite-dimensional notion of determinant is the functional determinant.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,holds. In other words, the following diagram commutes:between the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityThe determinant defines a mappingThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.from the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isThe determinant can also be characterized as the unique functionThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn). As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple. We call this scalar the determinant of T.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.As ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThe determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant is therefore also called a similarity invariant. The determinant of a linear transformationThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThis identity is used in describing the tangent space of certain matrix Lie groups.Yet another equivalent formulation isExpressed in terms of the entries of A, these arewhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveBy definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]When D is a 1×1 matrix, B is a column vector, and C is a row vector thenWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]as can be seen by employing the decompositionWhen A is invertible, one hasThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenIt has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.the solution is given by Cramer's rule:For a matrix equationThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.Also,with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantis expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).where I is the identity matrix. More generally, ifAn important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way asThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l – 1)! tr(Al) aswhere the sum is taken over the set of all integers kl ≥ 0 satisfying the equationIn the general case, this may also be obtained from[9]cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromFor example, for n = 2, n = 3, and n = 4, respectively,the determinant of A is given byHere exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingor, for real matrices A,The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,being positive, for all k between 1 and n.A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatriceswhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationThe product of all non-zero eigenvalues is referred to as pseudo-determinant.From this general result several consequences follow.where Im and In are the m × m and n × n identity matrices, respectively.Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):In terms of the adjugate matrix, Laplace's expansion can be written as[7]The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,However, Laplace expansion is efficient for small matrices only.along the second column (j = 2 and the sum runs over i) is given by,Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i+jMi,j is known as a cofactor. The determinant of A is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant of a matrix product of square matrices equals the product of their determinants:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.can be computed using the following matrices:For example, the determinant ofProperty 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.Property 2 above implies that properties for columns have their counterparts in terms of rows:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.The determinant has many properties. Some basic properties of determinants arewhere now each ir and each jr should be summed over 1, …, n.or using two epsilon symbols asFor example, the determinant of a 3 × 3 matrix A (n = 3) isis notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:Here the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.The Leibniz formula for the determinant of an n × n matrix A isThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.which is the Leibniz formula for the determinant of a 3 × 3 matrix.this can be expanded out to giveThe Laplace formula for the determinant of a 3 × 3 matrix isThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (-b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The Leibniz formula for the determinant of a 2 × 2 matrix isThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.Assume A is a square matrix with n rows and n columns, so that it can be written asEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n × n matrix A in terms of its column vectorsThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 × 4 matrix:When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 × 4 matrix, the determinant of a 5 × 5 matrix, and so forth.Similarly, suppose we have a 3 × 3 matrix A, and we want the specific formula for its determinant |A|:In the case of a 2 × 2 matrix the specific formula for the determinant is:In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.
Range (mathematics)
In both cases, image f ⊆ range f ⊆ codomain f, with at least one of the containments being equality.When "range" is used to mean "image", the range of a function f is by definition {y | there exists an x in the domain of f such that y = f(x)}. In this case, the codomain of f must not be specified, because any codomain which contains this image as a (maybe trivial) subset will work.When "range" is used to mean "codomain", the image of a function f is already implicitly defined. It is (by definition of image) the (maybe trivial) subset of the "range" which equals {y | there exists an x in the domain of f such that y = f(x)}.Older books, when they use the word "range", tend to use it to mean what is now called the codomain.[1][2] More modern books, if they use the word "range" at all, generally use it to mean what is now called the image.[3] To avoid any confusion, a number of modern books don't use the word "range" at all.[4]As the term "range" can have different meanings, it is considered a good practice to define it the first time it is used in a textbook or article.The image of a function is the set of all outputs of the function. The image is always a subset of the codomain.The codomain of a function is some arbitrary super-set of image. In real analysis, it is the real numbers. In complex analysis, it is the complex numbers.In mathematics, and more specifically in naive set theory, the range of a function refers to either the codomain or the image of the function, depending upon usage. Modern usage almost always uses range to mean image.
Kernel (linear algebra)
Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]The problem of computing the kernel on a computer depends on the nature of the coefficients.are a basis of the kernel of A.The last three columns of B are zero columns. Therefore, the three last vectors of C,Putting the upper part in column echelon form by column operations on the whole matrix givesThenFor example, suppose thatIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.A basis of the kernel of a matrix may be computed by Gaussian elimination.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.Note also that the following dot products are zero:The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.Since c is a free variable, this can be expressed equally well as,for c a scalar.Now we can express an element of the kernel:Rewriting yields:Gauss–Jordan elimination reduces this to:which can be written in matrix form as:which can be expressed as a homogeneous system of linear equations involving x, y, and z:The kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichConsider the matrixWe give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Geometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax = b isThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.If u and v are two possible solutions to the above equation, thenThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The product Ax can be written in terms of the dot product of vectors as follows:The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The matrix equation is equivalent to a homogeneous system of linear equations:Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.This implies the rank–nullity theorem:It follows that the image of L is isomorphic to the quotient of V by the kernel:The kernel of L is a linear subspace of the domain V.[1] In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,
2 × 2 real matrices

Origin (mathematics)
The origin of the complex plane can be referred as the point where real axis and imaginary axis intersect each other. In other words, it is the complex number zero.[5]In Euclidean geometry, the origin may be chosen freely as any convenient point of reference.[4]In a polar coordinate system, the origin may also be called the pole. It does not itself have well-defined polar coordinates, because the polar coordinates of a point include the angle made by the positive x-axis and the ray from the origin to the point, and this ray is not well-defined for the origin itself.[3]In a Cartesian coordinate system, the origin is the point where the axes of the system intersect.[1] The origin divides each of these axes into two halves, a positive and a negative semiaxis.[2] Points can then be located with reference to the origin by giving their numerical coordinates—that is, the positions of their projections along each axis, either in the positive or negative direction. The coordinates of the origin are always all zero, for example (0,0) in two dimensions and (0,0,0) in three.[1]In physical problems, the choice of origin is often arbitrary, meaning any choice of origin will ultimately give the same answer. This allows one to pick an origin point that makes the mathematics as simple as possible, often by taking advantage of some kind of geometric symmetry.In mathematics, the origin of a Euclidean space is a special point, usually denoted by the letter O, used as a fixed point of reference for the geometry of the surrounding space.
Linear subspace
See the article on null space for an example.If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.See the article on column space for an example.If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.See the article on row space for an example.Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N ∩ N⊥ = N ≠ {0}. The same case presents the ⊥ operation in symplectic vector spaces.If V is an inner product space, then the orthogonal complement ⊥ of any subspace of V is again a subspace. This operation, understood as negation (¬), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.Here the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:For example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequalityIf U and W are subspaces, their sum is the subspaceFor every vector space V, the set {0} and V itself are subspaces of V.[7]Proof:Given subspaces U and W of a vector space V, then their intersection U ∩ W := {v ∈ V : v is an element of both U and W} is also a subspace of V.[6]A subspace cannot lie in any subspace of lesser dimension. If dim U = k, a finite number, and U ⊂ W, then dim W = k if and only if U = W.The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).for (t1, t2, ... , tk) ≠ (u1, u2, ... , uk).[5] If v1, ..., vk are linearly independent, then the coordinates t1, ..., tk for a vector in the span are uniquely determined.In general, vectors v1, ... , vk are called linearly independent ifIn general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1, 0, 0), (0, 0, 1), and (2, 0, 3) is just the xz-plane, with each point on the plane described by infinitely many different values of t1, t2, t3.The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:If the vectors v1, ... , vk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1, ... , vk.The set of all possible linear combinations is called the span:In general, a linear combination of vectors v1, v2, ... , vk is any vector of the formThe expression on the right is called a linear combination of the vectors (2, 5, −1) and (3, −4, 2). These two vectors are said to span the resulting subspace.In linear algebra, the system of parametric equations can be written as a single vector equation:is a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]For example, the set of all vectors (x, y, z) parameterized by the equationsThe subset of Kn described by a system of homogeneous linear parametric equations is a subspace:Every subspace of Kn can be described as the null space of some matrix (see algorithms, below).The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrixIn a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:is a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.For example (over real or rational numbers), the set of all vectors (x, y, z) satisfying the equationsThe solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F = 0 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.A way to characterize subspaces is that they are closed under linear combinations. That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W. Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.Examples that extend these themes are common in functional analysis.Example IV: Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions. The same sort of argument as before shows that this is a subspace too.Proof:Example III: Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R. Let C(R) be the subset consisting of continuous functions. Then C(R) is a subspace of RR.In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace. (The equation in example I was z = 0, and the equation in example II was x = y.) Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.Proof:Example II: Let the field be R again, but now let the vector space be the Cartesian plane R2. Take W to be the set of points (x, y) of R2 such that x = y. Then W is a subspace of R2.Proof:Example I: Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3. Take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V. Then W is a subspace if:In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspaces[disambiguation needed].
Linear span
(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)Let X be a normed space and let E be any non-empty subset of X. ThenClosed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma).The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.The closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.One mathematical formulation of this isIn functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,…,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,…,an of A, then the sum of cyclic modules,Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].This also indicates that a basis is a minimal spanning set when V is finite-dimensional.Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.This theorem is so well known that at times it is referred to as the definition of span of a set.Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.The set of functions xn where n is a non-negative integer spans the space of polynomials.The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.Another spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all subspaces containing that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.
Basis (linear algebra)
This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent.Hence Lmax is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.If Lmax would not span V, there would exist some vector w of V that cannot be expressed as a linear combination of elements of Lmax (with coefficients in the field F). In particular, w cannot be an element of Lmax. Let Lw = Lmax ∪ {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of Lmax, and Lmax is independent). As Lmax ⊆ Lw, and Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax), this contradicts the maximality of Lmax. Thus this shows that Lmax spans V.It remains to prove that Lmax is a basis of V. Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.As X is nonempty, and every totally ordered subset of (X, ⊆) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ⊆ L for some element L of X, then L = Lmax.Since (Y, ⊆) is totally ordered, every finite subset of LY is a subset of an element of Y, which is a linearly independent subset of V, and hence every finite subset of LY is linearly independent. Thus LY is linearly independent, so LY is an element of X. Therefore, LY is an upper bound for Y in (X, ⊆): it is an element of X, that contains every element Y.Let Y be a subset of X that is totally ordered by ⊆, and let LY be the union of all the elements of Y (which are themselves certain subsets of V).The set X is nonempty since the empty set is an independent subset of V, and it is partially ordered by inclusion, which is denoted, as usual, by ⊆.Let V be any vector space over some field F. Let X be the set of all linearly independent subsets of V.The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the n-dimensional cube [−1,1]n as a function of dimension, n. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within π/2 ±0.037π/2 then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within π/2 ±0.037π/2 then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each n, 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in n-dimensional ball. Choose N independent random vectors from a ball (they are independent and identically distributed). Let θ be a small positive number. Then forFor a probability distribution in Rn with a probability density function, such as the equidistribution in a n-dimensional ball with respect to Lebesgue measure, it can be shown that randomly and independently chosen n-vectors will form a basis with probability one, which is due to the fact that n linearly dependent vectors x1,..., xn in Rn should satisfy the equation det[x1,..., xn]=0 (zero determinant of the matrix with columns xi), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.[5][6]for suitable (real or complex) coefficients ak, bk. But many[2] square-integrable functions cannot be represented as finite linear combinations of these basis functions, which therefore do not comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.The functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are linearly independent, and every function f that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense thatIn the study of Fourier series, one learns that the functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions f satisfyingThe common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fréchet spaces.The maps sending a vector v to the components aj(v) are linear maps from V to F, because of φ−1 is linear. Hence they are linear functionals. They form a basis for the dual space of V, called the dual basis.The inverse of the linear isomorphism φ determined by an ordered basis {vi} equips V with coordinates: if, for a vector v ∈ V, φ−1(v) = (a1, a2,...,an) ∈ Fn, then the components aj = aj(v) are the coordinates of v in the sense that v = a1(v) v1 + a2(v) v2 + ... + an(v) vn.These two constructions are clearly inverse to each other. Thus ordered bases for V are in 1-1 correspondence with linear isomorphisms Fn → V.where x = x1e1 + x2e2 + ... + xnen is an element of Fn. It is not hard to check that φ is a linear isomorphism.Conversely, given an ordered basis, consider the map defined bywhere {ei} is the standard basis for Fn.is a linear isomorphism. Define an ordered basis {vi} for V bySuppose first thatProof. The proof makes use of the fact that the standard basis of Fn is an ordered basis.Suppose V is an n-dimensional vector space over a field F. A choice of an ordered basis for V is equivalent to a choice of a linear isomorphism φ from the coordinate space Fn to V.A basis is just a linearly independent set of vectors with or without a given ordering. For many purposes it is convenient to work with an ordered basis. For example, when working with a coordinate representation of a vector it is customary to speak of the "first" or "second" coordinate, which makes sense only if an ordering is specified for the basis. For finite-dimensional vector spaces one typically indexes a basis {vi} by the first n integers. An ordered basis is also called a frame.Since the above matrix has a nonzero determinant, its columns form a basis of R2. See: invertible matrix.Simply compute the determinantSince (−1,2) is clearly not a multiple of (1,1) and since (1,1) is not the zero vector, these two vectors are linearly independent. Since the dimension of R2 is 2, the two vectors already form a basis of R2 without needing any extension.Subtracting the first equation from the second, we get:Then we have to solve the equations:Part II: To prove that these two vectors generate R2, we have to let (a, b) be an arbitrary element of R2, and show that there exist numbers r, s ∈ R such that:Hence we have linear independence.Adding this equation to the first equation then:Subtracting the first equation from the second, we obtain:(i.e., they are linearly dependent). Then:To prove that they are linearly independent, suppose that there are numbers a, b such that:We have to prove that these two vectors are linearly independent and that they generate R2.Often, a mathematical result can be proven in more than one way. Here, using three different proofs, we show that the vectors (1,1) and (−1,2) form a basis for R2.A similar question is when does a subset S contain a basis. This occurs if and only if S spans V. In this case, S will usually contain several different bases.Let S be a subset of a vector space V. To extend S to a basis means to find a basis B that contains S as a subset. This can be done if and only if S is linearly independent. Almost always, there is more than one such B, except in rather special circumstances (i.e. S is already a basis, or S is empty and V has two elements).A set of vectors can be represented by a matrix of which each column consists of the components of the corresponding vector of the set. As a basis is a set of vectors, a basis can be given by a matrix of this kind. The change of basis of any object of the space is related to this matrix. For example, coordinate tuples change with its inverse.Given a vector space V over a field F and suppose that {v1, ..., vn} and {α1, ..., αn} are two bases for V. By definition, if ξ is a vector in V then ξ = x1α1 + ... + xnαn for a unique choice of scalars x1, ..., xn in F called the coordinates of ξ relative to the ordered basis {α1, ..., αn}. The vector x = (x1, ..., xn) in Fn is called the coordinate tuple of ξ (relative to this basis). The unique linear map φ : Fn → V with φ(vj) = αj for j = 1, ..., n is called the coordinate isomorphism for V and the basis {α1, ..., αn}. Thus φ(x) = ξ if and only if ξ = x1α1 + ... + xnαn.In M22, {M1,1, M1,2, M2,1, M2,2}, where M22 is the set of all 2×2 matrices. and Mm,n is the 2×2 matrix with a 1 in the m,n position and zeros everywhere else.In P2, where P2 is the set of all polynomials of degree at most 2, {1, x, x2} is the standard basis.In Rn, {e1, ..., en}, where ei is the ith column of the identity matrix.Standard bases for example:Also many vector sets can be attributed a standard basis which comprises both spanning and linearly independent vectors.Every vector space has a basis. The proof of this requires the axiom of choice. All bases of a vector space have the same cardinality (number of elements), called the dimension of the vector space. This result is known as the dimension theorem, and requires the ultrafilter lemma, a strictly weaker form of the axiom of choice.Again, B denotes a subset of a vector space V. Then, B is a basis if and only if any of the following equivalent conditions are met:It is often convenient to list the basis vectors in a specific order, for example, when considering the transformation matrix of a linear map with respect to a basis. We then speak of an ordered basis, which we define to be a sequence (rather than a set) of linearly independent vectors that span V: see Ordered bases and coordinates below.The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors. Settings that permit infinite linear combinations allow alternative definitions of the basis concept: see Related notions below.A vector space that has a finite basis is called finite-dimensional. To deal with infinite-dimensional spaces, we must generalize the above definition to include infinite basis sets. We therefore say that a set (finite or infinite) B ⊂ V is a basis, ifThe numbers ai are called the coordinates of the vector x with respect to the basis B, and by the first property they are uniquely determined.In more detail, suppose that B = { v1, …, vn } is a finite subset of a vector space V over a field F (such as the real or complex numbers R or C). Then B is a basis if it satisfies the following conditions:A basis B of a vector space V over a field F is a linearly independent subset of V that spans V.Given a basis of a vector space V, every element of V can be expressed uniquely as a linear combination of basis vectors, whose coefficients are referred to as vector coordinates or components. A vector space can have several distinct sets of basis vectors; however each such set has the same number of elements, with this number being the dimension of the vector space.In mathematics, a set of elements (vectors) in a vector space V is called a basis, or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set.[1] In more general terms, a basis is a linearly independent spanning set.
Linear subspace
See the article on null space for an example.If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.See the article on column space for an example.If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.See the article on row space for an example.Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N ∩ N⊥ = N ≠ {0}. The same case presents the ⊥ operation in symplectic vector spaces.If V is an inner product space, then the orthogonal complement ⊥ of any subspace of V is again a subspace. This operation, understood as negation (¬), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.Here the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:For example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequalityIf U and W are subspaces, their sum is the subspaceFor every vector space V, the set {0} and V itself are subspaces of V.[7]Proof:Given subspaces U and W of a vector space V, then their intersection U ∩ W := {v ∈ V : v is an element of both U and W} is also a subspace of V.[6]A subspace cannot lie in any subspace of lesser dimension. If dim U = k, a finite number, and U ⊂ W, then dim W = k if and only if U = W.The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).for (t1, t2, ... , tk) ≠ (u1, u2, ... , uk).[5] If v1, ..., vk are linearly independent, then the coordinates t1, ..., tk for a vector in the span are uniquely determined.In general, vectors v1, ... , vk are called linearly independent ifIn general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1, 0, 0), (0, 0, 1), and (2, 0, 3) is just the xz-plane, with each point on the plane described by infinitely many different values of t1, t2, t3.The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:If the vectors v1, ... , vk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1, ... , vk.The set of all possible linear combinations is called the span:In general, a linear combination of vectors v1, v2, ... , vk is any vector of the formThe expression on the right is called a linear combination of the vectors (2, 5, −1) and (3, −4, 2). These two vectors are said to span the resulting subspace.In linear algebra, the system of parametric equations can be written as a single vector equation:is a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]For example, the set of all vectors (x, y, z) parameterized by the equationsThe subset of Kn described by a system of homogeneous linear parametric equations is a subspace:Every subspace of Kn can be described as the null space of some matrix (see algorithms, below).The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrixIn a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:is a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.For example (over real or rational numbers), the set of all vectors (x, y, z) satisfying the equationsThe solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F = 0 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.A way to characterize subspaces is that they are closed under linear combinations. That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W. Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.Examples that extend these themes are common in functional analysis.Example IV: Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions. The same sort of argument as before shows that this is a subspace too.Proof:Example III: Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R. Let C(R) be the subset consisting of continuous functions. Then C(R) is a subspace of RR.In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace. (The equation in example I was z = 0, and the equation in example II was x = y.) Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.Proof:Example II: Let the field be R again, but now let the vector space be the Cartesian plane R2. Take W to be the set of points (x, y) of R2 such that x = y. Then W is a subspace of R2.Proof:Example I: Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3. Take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V. Then W is a subspace if:In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspaces[disambiguation needed].
Kernel (linear algebra)
Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]The problem of computing the kernel on a computer depends on the nature of the coefficients.are a basis of the kernel of A.The last three columns of B are zero columns. Therefore, the three last vectors of C,Putting the upper part in column echelon form by column operations on the whole matrix givesThenFor example, suppose thatIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.A basis of the kernel of a matrix may be computed by Gaussian elimination.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.Note also that the following dot products are zero:The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.Since c is a free variable, this can be expressed equally well as,for c a scalar.Now we can express an element of the kernel:Rewriting yields:Gauss–Jordan elimination reduces this to:which can be written in matrix form as:which can be expressed as a homogeneous system of linear equations involving x, y, and z:The kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichConsider the matrixWe give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Geometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax = b isThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.If u and v are two possible solutions to the above equation, thenThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The product Ax can be written in terms of the dot product of vectors as follows:The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The matrix equation is equivalent to a homogeneous system of linear equations:Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.This implies the rank–nullity theorem:It follows that the image of L is isomorphic to the quotient of V by the kernel:The kernel of L is a linear subspace of the domain V.[1] In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,
Linear combination

Linear span
(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)Let X be a normed space and let E be any non-empty subset of X. ThenClosed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma).The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.The closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.One mathematical formulation of this isIn functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,…,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,…,an of A, then the sum of cyclic modules,Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].This also indicates that a basis is a minimal spanning set when V is finite-dimensional.Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.This theorem is so well known that at times it is referred to as the definition of span of a set.Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.The set of functions xn where n is a non-negative integer spans the space of polynomials.The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.Another spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all subspaces containing that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.
Linear independence
If such a linear dependence exists, then the n vectors are linearly dependent. It makes sense to identify two linear dependencies if one arises as a non-zero multiple of the other, because in this case the two describe the same linear relationship among the vectors. Under this identification, the set of all linear dependencies among v1, ...., vn is a projective space.A linear dependence among vectors v1, ..., vn is a tuple (a1, ..., an) with n scalar components, not all zero, such thatTake the first derivative of the above equation such thatthen ai = 0 for all i in {1, ..., n}.SinceSuppose that a1, a2, ..., an are elements of R such thatThen e1, e2, ..., en are linearly independent.Let V = Rn and consider the following elements in V, known as the natural basis vectors:If there are more vectors than dimensions, the vectors are linearly dependent. This is illustrated in the example above of three vectors in R2.for all possible lists of m rows. (In case m = n, this requires only one determinant, as above. If m > n, then it is a theorem that the vectors must be linearly dependent.) This fact is valuable for theory; in practical calculations more efficient methods are available.Furthermore, the reverse is true. That is, we can test whether the m vectors are linearly dependent by testing whetherOtherwise, suppose we have m vectors of n coordinates, with m < n. Then A is an n×m matrix and Λ is a column vector with m entries, and we are again interested in AΛ = 0. As we saw previously, this is equivalent to a list of n equations. Consider the first m rows of A, the first m equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈i1,...,im〉 is any list of m rows, then the equation must be true for those rows.Since the determinant is non-zero, the vectors (1, 1) and (−3, 2) are linearly independent.We are interested in whether AΛ = 0 for some nonzero vector Λ. This depends on the determinant of A, which isWe may write a linear combination of the columns asIn this case, the matrix formed by the vectors iswhere a3 can be chosen arbitrarily. Thus, the vectors v1, v2 and v3 are linearly dependent.This equation is easily solved to define non-zero ai,Rearrange to solve for v3 and obtain,Row reduce this equation to obtain,are linearly dependent, form the matrix equation,In order to determine if the three vectors in R4,This shows that ai = 0, which means that the vectors v1 = (1, 1) and v2 = (−3, 2) are linearly independent.The same row reduction presented above yields,orTwo vectors: Now consider the linear dependence of the two vectors v1 = (1, 1), v2 = (−3, 2), and check,which shows that non-zero ai exist such that v3 = (2, 4) can be defined in terms of v1 = (1, 1), v2 = (−3, 2). Thus, the three vectors are linearly dependent.We can now rearrange this equation to obtainContinue the row reduction by (i) dividing the second row by 5, and then (ii) multiplying by 3 and adding to the first row, that isRow reduce this matrix equation by subtracting the first row from the second to obtain,orThree vectors: Consider the set of vectors v1 = (1, 1), v2 = (−3, 2) and v3 = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such thatAlso note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set. In general, n linearly independent vectors are required to describe any location in n-dimensional space.In this example the "3 miles north" vector and the "4 miles east" vector are linearly independent. That is to say, the north vector cannot be described in terms of the east vector, and vice versa. The third "5 miles northeast" vector is a linear combination of the other two vectors, and it makes the set of vectors linearly dependent, that is, one of the three vectors is unnecessary.A geographic example may help to clarify the concept of linear independence. A person describing the location of a certain place might say, "It is 3 miles north and 4 miles east of here." This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth's surface). The person might add, "The place is 5 miles northeast of here." Although this last statement is true, it is not necessary.A set of vectors which is linearly independent and spans some vector space, forms a basis for that vector space. For example, the vector space of all polynomials in x over the reals has the (infinite) subset {1, x, x2, ...} as a basis.A set X of elements of V is linearly independent if the corresponding family {x}x∈X is linearly independent. Equivalently, a family is dependent if a member is in the linear span of the rest of the family, i.e., a member is a linear combination of the rest of the family. The trivial case of the empty family must be regarded as linearly independent for theorems to apply.where the index set J is a nonempty, finite subset of I.In order to allow the number of linearly independent vectors in a vector space to be countably infinite, it is useful to define linear dependence as follows. More generally, let V be a vector space over a field K, and let {vi | i∈I} be a family of elements of V. The family is linearly dependent over K if there exists a family {aj | j∈J} of elements of K, not all zero, such thatA vector space can be of finite-dimension or infinite-dimension depending on the number of linearly independent basis vectors. The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a basis for a vector space.In the theory of vector spaces, a set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent. These concepts are central to the definition of dimension.[1]
Basis (linear algebra)
This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent.Hence Lmax is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.If Lmax would not span V, there would exist some vector w of V that cannot be expressed as a linear combination of elements of Lmax (with coefficients in the field F). In particular, w cannot be an element of Lmax. Let Lw = Lmax ∪ {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of Lmax, and Lmax is independent). As Lmax ⊆ Lw, and Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax), this contradicts the maximality of Lmax. Thus this shows that Lmax spans V.It remains to prove that Lmax is a basis of V. Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.As X is nonempty, and every totally ordered subset of (X, ⊆) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ⊆ L for some element L of X, then L = Lmax.Since (Y, ⊆) is totally ordered, every finite subset of LY is a subset of an element of Y, which is a linearly independent subset of V, and hence every finite subset of LY is linearly independent. Thus LY is linearly independent, so LY is an element of X. Therefore, LY is an upper bound for Y in (X, ⊆): it is an element of X, that contains every element Y.Let Y be a subset of X that is totally ordered by ⊆, and let LY be the union of all the elements of Y (which are themselves certain subsets of V).The set X is nonempty since the empty set is an independent subset of V, and it is partially ordered by inclusion, which is denoted, as usual, by ⊆.Let V be any vector space over some field F. Let X be the set of all linearly independent subsets of V.The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the n-dimensional cube [−1,1]n as a function of dimension, n. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within π/2 ±0.037π/2 then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within π/2 ±0.037π/2 then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each n, 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in n-dimensional ball. Choose N independent random vectors from a ball (they are independent and identically distributed). Let θ be a small positive number. Then forFor a probability distribution in Rn with a probability density function, such as the equidistribution in a n-dimensional ball with respect to Lebesgue measure, it can be shown that randomly and independently chosen n-vectors will form a basis with probability one, which is due to the fact that n linearly dependent vectors x1,..., xn in Rn should satisfy the equation det[x1,..., xn]=0 (zero determinant of the matrix with columns xi), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.[5][6]for suitable (real or complex) coefficients ak, bk. But many[2] square-integrable functions cannot be represented as finite linear combinations of these basis functions, which therefore do not comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.The functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are linearly independent, and every function f that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense thatIn the study of Fourier series, one learns that the functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions f satisfyingThe common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fréchet spaces.The maps sending a vector v to the components aj(v) are linear maps from V to F, because of φ−1 is linear. Hence they are linear functionals. They form a basis for the dual space of V, called the dual basis.The inverse of the linear isomorphism φ determined by an ordered basis {vi} equips V with coordinates: if, for a vector v ∈ V, φ−1(v) = (a1, a2,...,an) ∈ Fn, then the components aj = aj(v) are the coordinates of v in the sense that v = a1(v) v1 + a2(v) v2 + ... + an(v) vn.These two constructions are clearly inverse to each other. Thus ordered bases for V are in 1-1 correspondence with linear isomorphisms Fn → V.where x = x1e1 + x2e2 + ... + xnen is an element of Fn. It is not hard to check that φ is a linear isomorphism.Conversely, given an ordered basis, consider the map defined bywhere {ei} is the standard basis for Fn.is a linear isomorphism. Define an ordered basis {vi} for V bySuppose first thatProof. The proof makes use of the fact that the standard basis of Fn is an ordered basis.Suppose V is an n-dimensional vector space over a field F. A choice of an ordered basis for V is equivalent to a choice of a linear isomorphism φ from the coordinate space Fn to V.A basis is just a linearly independent set of vectors with or without a given ordering. For many purposes it is convenient to work with an ordered basis. For example, when working with a coordinate representation of a vector it is customary to speak of the "first" or "second" coordinate, which makes sense only if an ordering is specified for the basis. For finite-dimensional vector spaces one typically indexes a basis {vi} by the first n integers. An ordered basis is also called a frame.Since the above matrix has a nonzero determinant, its columns form a basis of R2. See: invertible matrix.Simply compute the determinantSince (−1,2) is clearly not a multiple of (1,1) and since (1,1) is not the zero vector, these two vectors are linearly independent. Since the dimension of R2 is 2, the two vectors already form a basis of R2 without needing any extension.Subtracting the first equation from the second, we get:Then we have to solve the equations:Part II: To prove that these two vectors generate R2, we have to let (a, b) be an arbitrary element of R2, and show that there exist numbers r, s ∈ R such that:Hence we have linear independence.Adding this equation to the first equation then:Subtracting the first equation from the second, we obtain:(i.e., they are linearly dependent). Then:To prove that they are linearly independent, suppose that there are numbers a, b such that:We have to prove that these two vectors are linearly independent and that they generate R2.Often, a mathematical result can be proven in more than one way. Here, using three different proofs, we show that the vectors (1,1) and (−1,2) form a basis for R2.A similar question is when does a subset S contain a basis. This occurs if and only if S spans V. In this case, S will usually contain several different bases.Let S be a subset of a vector space V. To extend S to a basis means to find a basis B that contains S as a subset. This can be done if and only if S is linearly independent. Almost always, there is more than one such B, except in rather special circumstances (i.e. S is already a basis, or S is empty and V has two elements).A set of vectors can be represented by a matrix of which each column consists of the components of the corresponding vector of the set. As a basis is a set of vectors, a basis can be given by a matrix of this kind. The change of basis of any object of the space is related to this matrix. For example, coordinate tuples change with its inverse.Given a vector space V over a field F and suppose that {v1, ..., vn} and {α1, ..., αn} are two bases for V. By definition, if ξ is a vector in V then ξ = x1α1 + ... + xnαn for a unique choice of scalars x1, ..., xn in F called the coordinates of ξ relative to the ordered basis {α1, ..., αn}. The vector x = (x1, ..., xn) in Fn is called the coordinate tuple of ξ (relative to this basis). The unique linear map φ : Fn → V with φ(vj) = αj for j = 1, ..., n is called the coordinate isomorphism for V and the basis {α1, ..., αn}. Thus φ(x) = ξ if and only if ξ = x1α1 + ... + xnαn.In M22, {M1,1, M1,2, M2,1, M2,2}, where M22 is the set of all 2×2 matrices. and Mm,n is the 2×2 matrix with a 1 in the m,n position and zeros everywhere else.In P2, where P2 is the set of all polynomials of degree at most 2, {1, x, x2} is the standard basis.In Rn, {e1, ..., en}, where ei is the ith column of the identity matrix.Standard bases for example:Also many vector sets can be attributed a standard basis which comprises both spanning and linearly independent vectors.Every vector space has a basis. The proof of this requires the axiom of choice. All bases of a vector space have the same cardinality (number of elements), called the dimension of the vector space. This result is known as the dimension theorem, and requires the ultrafilter lemma, a strictly weaker form of the axiom of choice.Again, B denotes a subset of a vector space V. Then, B is a basis if and only if any of the following equivalent conditions are met:It is often convenient to list the basis vectors in a specific order, for example, when considering the transformation matrix of a linear map with respect to a basis. We then speak of an ordered basis, which we define to be a sequence (rather than a set) of linearly independent vectors that span V: see Ordered bases and coordinates below.The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors. Settings that permit infinite linear combinations allow alternative definitions of the basis concept: see Related notions below.A vector space that has a finite basis is called finite-dimensional. To deal with infinite-dimensional spaces, we must generalize the above definition to include infinite basis sets. We therefore say that a set (finite or infinite) B ⊂ V is a basis, ifThe numbers ai are called the coordinates of the vector x with respect to the basis B, and by the first property they are uniquely determined.In more detail, suppose that B = { v1, …, vn } is a finite subset of a vector space V over a field F (such as the real or complex numbers R or C). Then B is a basis if it satisfies the following conditions:A basis B of a vector space V over a field F is a linearly independent subset of V that spans V.Given a basis of a vector space V, every element of V can be expressed uniquely as a linear combination of basis vectors, whose coefficients are referred to as vector coordinates or components. A vector space can have several distinct sets of basis vectors; however each such set has the same number of elements, with this number being the dimension of the vector space.In mathematics, a set of elements (vectors) in a vector space V is called a basis, or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set.[1] In more general terms, a basis is a linearly independent spanning set.
Axiom of choice

Rational number
The metric space (Q,dp) is not complete, and its completion is the p-adic number field Qp. Ostrowski's theorem states that any non-trivial absolute value on the rational numbers Q is equivalent to either the usual real absolute value or a p-adic absolute value.Then dp(x,y) = |x − y|p defines a metric on Q.In addition set |0|p = 0. For any rational number a/b, we set |a/b|p = |a|p / |b|p.Let p be a prime number and for any non-zero integer a, let |a|p = p−n, where pn is the highest power of p dividing a.In addition to the absolute value metric mentioned above, there are other metrics which turn Q into a topological field:By virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric d(x,y) = |x − y|, and this yields a third topology on Q. All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space; the real numbers are the completion of Q under the metric d(x,y) = |x − y|, above.The rationals are a dense subset of the real numbers: every real number has rational numbers arbitrarily close to it. A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.Any totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.The rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones. For example, for any two fractions such thatThe set of all rational numbers is countable. Since the set of all real numbers is uncountable, we say that almost all real numbers are irrational, in the sense of Lebesgue measure, i.e. the set of rational numbers is a null set.The algebraic closure of Q, i.e. the field of roots of rational polynomials, is the algebraic numbers.The rationals are the smallest field with characteristic zero: every other field of characteristic zero contains a copy of Q. The rational numbers are therefore the prime field for characteristic zero.The set Q, together with the addition and multiplication operations shown above, forms a field, the field of fractions of the integers Z.The integers may be considered to be rational numbers by the embedding that maps m to [(m,1)].We can also define a total order on Q. Let ∧ be the and-symbol and ∨ be the or-symbol. We say that [(m1,n1)] ≤ [(m2,n2)] if:The canonical choice for [(m,n)] is chosen so that n is positive and gcd(m,n) = 1, i.e. m and n share no common factors, i.e. m and n are coprime. For example, we would write [(1,2)] instead of [(2,4)] or [(−12,−24)], even though [(1,2)] = [(2,4)] = [(−12,−24)].The equivalence relation (m1,n1) ~ (m2,n2) if, and only if, m1n2 − m2n1 = 0 is a congruence relation, i.e. it is compatible with the addition and multiplication defined above, and we may define Q to be the quotient set (Z × (Z \ {0})) / ~, i.e. we identify two pairs (m1,n1) and (m2,n2) if they are equivalent in the above sense. (This construction can be carried out in any integral domain: see field of fractions.) We denote by [(m1,n1)] the equivalence class containing (m1,n1). If (m1,n1) ~ (m2,n2) then, by definition, (m1,n1) belongs to [(m2,n2)] and (m2,n2) belongs to [(m1,n1)]; in this case we can write [(m1,n1)] = [(m2,n2)]. Given any equivalence class [(m,n)] there are a countably infinite number of representation, sinceand, if m2 ≠ 0, division byMathematically we may construct the rational numbers as equivalence classes of ordered pairs of integers (m,n), with n ≠ 0. This space of equivalence classes is the quotient space (Z × (Z \ {0})) / ~, where (m1,n1) ~ (m2,n2) if, and only if, m1n2 − m2n1 = 0. We can define addition and multiplication of these pairs with the following rules:are different ways to represent the same rational value.where an are integers. Every rational number a/b can be represented as a finite continued fraction, whose coefficients an can be determined by applying the Euclidean algorithm to (a,b).A finite continued fraction is an expression such asIf a ≠ 0, thenThe result is in canonical form if the same is true for a/b. In particular,If n is a non-negative integer, thenThus, dividing a/b by c/d is equivalent to multiplying a/b by the reciprocal of c/d:If both b and c are nonzero, the division rule isA nonzero rational number a/b has a multiplicative inverse, also called its reciprocal,If a/b is in canonical form, the same is true for its opposite.Every rational number a/b has an additive inverse, often called its opposite,Even if both fractions are in canonical form, the result may be a reducible fraction.The rule for multiplication is:If both fractions are in canonical form, the result is in canonical form if and only if b and d are coprime integers.If both fractions are in canonical form, the result is in canonical form if and only if b and d are coprime integers.Two fractions are added as follows:If either denominator is negative, each fraction with a negative denominator must first be converted into an equivalent form with a positive denominator by changing the signs of both its numerator and denominator.If both denominators are positive, and, in particular, if both fractions are in canonical form,If both fractions are in canonical form thenAny integer n can be expressed as the rational number n/1, which is its canonical form as a rational number.Starting from a rational number a/b, its canonical form may be obtained by dividing a and b by their greatest common divisor, and, if b < 0, changing the sign of the resulting numerator and denominator.Every rational number may be expressed in a unique way as an irreducible fraction a/b, where a and b are coprime integers, and b > 0. This is often called the canonical form.The term rational in reference to the set Q refers to the fact that a rational number represents a ratio of two integers. In mathematics, "rational" is often used as a noun abbreviating "rational number". The adjective rational sometimes means that the coefficients are rational numbers. For example, a rational point is a point with rational coordinates (that is a point whose coordinates are rational numbers; a rational matrix is a matrix of rational numbers; a rational polynomial may be a polynomial with rational coefficients, although the term "polynomial over the rationals" is generally preferred, for avoiding confusion with "rational expression" and "rational function" (a polynomial is a rational expression and defines a rational function, even if its coefficients are not rational numbers). However, a rational curve is not a curve defined over the rationals, but a curve which can be parameterized by rational functions.In mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals.Rational numbers together with addition and multiplication form a field which contains the integers and is contained in any field containing the integers. In other words, the field of rational numbers is a prime field, and a field has characteristic zero if and only if it contains the rational numbers as a subfield. Finite extensions of Q are called algebraic number fields, and the algebraic closure of Q is the field of algebraic numbers.[3]Rational numbers can be formally defined as equivalence classes of pairs of integers (p, q) such that q ≠ 0, for the equivalence relation defined by (p1, q1) ~ (p2, q2) if, and only if p1q2 = p2q1. With this formal definition, the fraction p/q becomes the standard notation for the equivalence class of (p2, q2).A real number that is not rational is called irrational. Irrational numbers include √2, π, e, and φ. The decimal expansion of an irrational number continues without repeating. Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.[1]The decimal expansion of a rational number always either terminates after a finite number of digits or begins to repeat the same finite sequence of digits over and over. Moreover, any repeating or terminating decimal represents a rational number. These statements hold true not just for base 10, but also for any other integer base (e.g. binary, hexadecimal).
Cardinality
From this, one can show that in general the cardinalities of unions and intersections are related by[8]If A and B are disjoint sets, thenBoth have cardinalityThe second result was first demonstrated by Cantor in 1878, but it became more apparent in 1890, when Giuseppe Peano introduced the space-filling curves, curved lines that twist and turn enough to fill the whole of any square, or cube, or hypercube, or finite-dimensional space. These curves are not a direct proof that a line has the same number of points as a finite-dimensional space, but they can be used to obtain such a proof.The first of these results is apparent by considering, for instance, the tangent function, which provides a one-to-one correspondence between the interval (−½π, ½π) and R (see also Hilbert's paradox of the Grand Hotel).Cardinal arithmetic can be used to show not only that the number of points in a real number line is equal to the number of points in any segment of that line, but that this is equal to the number of points on a plane and, indeed, in any finite-dimensional space. These results are highly counterintuitive, because they imply that there exist proper subsets and proper supersets of an infinite set S that have the same size as S, although S contains elements that do not belong to its subsets, and the supersets of S contain elements that are not included in it.However, this hypothesis can neither be proved nor disproved within the widely accepted ZFC axiomatic set theory, if ZFC is consistent.The continuum hypothesis states that there is no cardinal number between the cardinality of the reals and the cardinality of the natural numbers, that is,If the axiom of choice holds, the law of trichotomy holds for cardinality. Thus we can make the following definitions:Assuming AC, the cardinalities of the infinite sets are denotedThe relation of having the same cardinality is called equinumerosity, and this is an equivalence relation on the class of all sets. The equivalence class of a set A under this relation then consists of all those sets which have the same cardinality as A. There are two ways to define the "cardinality of a set":Above, "cardinality" was defined functionally. That is, the "cardinality" of a set was not defined as a specific object itself. However, such an object can be defined as follows.If |A| ≤ |B| and |B| ≤ |A| then |A| = |B| (Cantor–Bernstein–Schroeder theorem). The axiom of choice is equivalent to the statement that |A| ≤ |B| or |B| ≤ |A| for every A, B.[3][4]While the cardinality of a finite set is just the number of its elements, extending the notion to infinite sets usually starts with defining the notion of comparison of arbitrary (in particular infinite) sets.The cardinality of a set A is usually denoted | A |, with a vertical bar on each side; this is the same notation as absolute value and the meaning depends on context. Alternatively, the cardinality of a set A may be denoted by n(A), A, card(A), or # A.In mathematics, the cardinality of a set is a measure of the "number of elements of the set". For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. There are two approaches to cardinality – one which compares sets directly using bijections and injections, and another which uses cardinal numbers.[1] The cardinality of a set is also called its size, when no confusion with other notions of size[2] is possible.
Dimension (vector space)
Alternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of "dimension of the operator". These fall under the rubric of "trace class operators" on a Hilbert space, or more generally nuclear operators on a Banach space.The Krull dimension of a commutative ring, named after Wolfgang Krull (1899–1971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.One can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.Some simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself. If V is a vector space over a field F then, denoting the dimension of V by dim V, we have:In particular, every complex vector space of dimension n is a real vector space of dimension 2n.If F/K is a field extension, then F is in particular a vector space over K. Furthermore, every F-vector space V is also a K-vector space. The dimensions are related by the formulaAn important result about dimensions is given by the rank–nullity theorem for linear maps.Any two vector spaces over F having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If B is some set, a vector space with dimension |B| over F can be constructed as follows: take the set F(B) of all functions f : B → F such that f(b) = 0 for all but finitely many b in B. These functions can be added and multiplied with elements of F, and we obtain the desired F-vector space.Rn has the standard basis {e1, ..., en}, where ei is the i-th column of the corresponding identity matrix. Therefore Rn has dimension n.To show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if V is a finite-dimensional vector space and W is a linear subspace of V with dim(W) = dim(V), then W = V.If W is a linear subspace of V, then dim(W) ≤ dim(V).The only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.The complex numbers C are both a real and complex vector space; we have dimR(C) = 2 and dimC(C) = 1. So the dimension depends on the base field.as a basis, and therefore we have dimR(R3) = 3. More generally, dimR(Rn) = n, and even more generally, dimF(Fn) = n for any field F.The vector space R3 hasThe dimension of the vector space V over the field F can be written as dimF(V) or as [V : F], read "dimension of V over F". When F can be inferred from context, dim(V) is typically written.For every vector space there exists a basis,[a] and all bases of a vector space have equal cardinality;[b] as a result, the dimension of a vector space is uniquely defined. We say V is finite-dimensional if the dimension of V is finite, and infinite-dimensional if its dimension is infinite.In mathematics, the dimension of a vector space V is the cardinality (i.e. the number of vectors) of a basis of V over its base field.[1] It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.
Well-defined
A solution to a partial differential equation is said to be well-defined if it is determined by the boundary conditions in a continuous way as the boundary conditions are changed.[1]Unlike with functions, the notational ambiguities can be overcome more or less easily by means of additional definitions, i. e. rules of precedence, and/or associativity of the operators. In the programming language C e. g. the operator - for subtraction is left-to-right-associative which means that a-b-c is defined as (a-b)-c and the operator = for assignment is right-to-left-associative which means that a=b=c is defined as a=(b=c). In the programming language APL there is only one rule: from right to left − but parentheses first.In particular, the term well-defined is used with respect to (binary) operations on cosets. In this case one can view the operation as a function of two variables and the property of being well-defined is the same as that for a function. For example, addition on the integers modulo some n can be defined naturally in terms of integer addition.The function f is well-defined, becauseFor example, consider the following functionThe question of well-definedness of a function classically arises when the defining equation of a function does not (only) refer to the arguments themselves, but (also) to elements of the arguments. This is sometimes unavoidable when the arguments are cosets and the equation refers to coset representatives.Despite these subtle logical problems, it is quite common to anticipatorily use the term definition (without apostrophes) for "definitions" of this kind, firstly because it is sort of a short-hand of the two-step approach, secondly because the relevant mathematical reasoning (step 2.) is the same in both cases, and finally because in mathematical texts the assertion is «up to 100%» true.A function that is not well-defined is not the same as a function that is undefined. For example, if f(x) = 1/x, then f(0) is undefined, but this has nothing to do with the question of whether f(x) = 1/x is well-defined. It is; 0 is simply not in the domain of the function.In mathematics, an expression is called well-defined or unambiguous if its definition assigns it a unique interpretation or value. Otherwise, the expression is said to be not well-defined or ambiguous.[1] A function is well-defined if it gives the same result when the representation of the input is changed without changing the value of the input. For instance if f takes real numbers as input, and if f(0.5) does not equal f(1/2) then f is not well-defined (and thus: not a function).[2] The term well-defined is also used to indicate whether a logical statement is unambiguous.
Dimension theorem for vector spaces

Matrix (mathematics)
Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toA number λ and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n×n-matrix A is called positive-definite if for all nonzero vectors x ∈ Rn the associated quadratic form given bywhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2×2 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generallywhere 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:
Coordinate system
In geometry and kinematics, coordinate systems are used not only to describe the (linear) position of points, but also to describe the angular position of axes, planes, and rigid bodies.[15] In the latter case, the orientation of a second (typically referred to as "local") coordinate system, fixed to the node, is defined based on the first (typically referred to as "global" or "world" coordinate system). For instance, the orientation of a rigid body can be represented by an orientation matrix, which includes, in its three columns, the Cartesian coordinates of three points. These points are used to define the orientation of the axes of the local system; they are the tips of three unit vectors aligned with those axes.The concept of a coordinate map, or coordinate chart is central to the theory of manifolds. A coordinate map is essentially a coordinate system for a subset of a given space with the property that each point has exactly one set of coordinates. More precisely, a coordinate map is a homeomorphism from an open subset of a space X to an open subset of Rn.[14] It is often not possible to provide one consistent coordinate system for an entire space. In this case, a collection of coordinate maps are put together to form an atlas covering the space. A space equipped with such an atlas is called a manifold and additional structure can be defined on a manifold if the structure is consistent where the coordinate maps overlap. For example, a differentiable manifold is a manifold where the change of coordinates from one coordinate map to another is always a differentiable function.Similarly, coordinate hypersurfaces are the (n − 1)-dimensional spaces resulting from fixing a single coordinate of an n-dimensional coordinate system.[13]In three-dimensional space, if one coordinate is held constant and the other two are allowed to vary, then the resulting surface is called a coordinate surface. For example, the coordinate surfaces obtained by holding ρ constant in the spherical coordinate system are the spheres with center at the origin. In three-dimensional space the intersection of two coordinate surfaces is a coordinate curve. In the Cartesian coordinate system we may speak of coordinate planes.In two dimensions, if one of the coordinates in a point coordinate system is held constant and the other coordinate is allowed to vary, then the resulting curve is called a coordinate curve. In the Cartesian coordinate system the coordinate curves are, in fact, straight lines, thus coordinate lines. Specifically, they are the lines parallel to one of the coordinate axes. For other coordinate systems the coordinates curves may be general curves. For example, the coordinate curves in polar coordinates obtained by holding r constant are the circles with center at the origin. Coordinates systems for Euclidean space other than the Cartesian coordinate system are called curvilinear coordinate systems.[12] This procedure does not always make sense, for example there are no coordinate curves in a homogeneous coordinate system.For example, in 1D, if the mapping is a translation of 3 to the right, the first moves the origin from 0 to 3, so that the coordinate of each point becomes 3 less, while the second moves the origin from 0 to −3, so that the coordinate of each point becomes 3 more.With every bijection from the space to itself two coordinate transformations can be associated:Because there are often many different possible coordinate systems for describing geometrical figures, it is important to understand how they are related. Such relations are described by coordinate transformations which give formulas for the coordinates in one system in terms of the coordinates in another system. For example, in the plane, if Cartesian coordinates (x, y) and polar coordinates (r, θ) have the same origin, and the polar axis is the positive x axis, then the coordinate transformation from polar to Cartesian coordinates is given by x = r cosθ and y = r sinθ.It may occur that systems of coordinates for two different sets of geometric figures are equivalent in terms of their analysis. An example of this is the systems of homogeneous coordinates for points and lines in the projective plane. The two systems in a case like this are said to be dualistic. Dualistic systems have the property that results from one system can be carried over to the other since these results are only different interpretations of the same analytical result; this is known as the principle of duality.[11]Coordinates systems are often used to specify the position of a point, but they may also be used to specify the position of more complex figures such as lines, planes, circles or spheres. For example, Plücker coordinates are used to determine the position of a line in space.[10] When there is a need, the type of figure being described is used to distinguish the type of coordinate system, for example the term line coordinates is used for any coordinate system that specifies the position of a line.There are ways of describing curves without coordinates, using intrinsic equations that use invariant quantities such as curvature and arc length. These include:Some other common coordinate systems are the following:A point in the plane may be represented in homogeneous coordinates by a triple (x, y, z) where x/z and y/z are the Cartesian coordinates of the point.[9] This introduces an "extra" coordinate since only two are needed to specify a point on the plane, but this system is useful in that it represents any point on the projective plane without the use of infinity. In general, a homogeneous coordinate system is one where only the ratios of the coordinates are significant and not the actual values.There are two common methods for extending the polar coordinate system to three dimensions. In the cylindrical coordinate system, a z-coordinate with the same meaning as in Cartesian coordinates is added to the r and θ polar coordinates giving a triple (r, θ, z).[7] Spherical coordinates take this a step further by converting the pair of cylindrical coordinates (r, z) to polar coordinates (ρ, φ) giving a triple (ρ, θ, φ).[8]Another common coordinate system for the plane is the polar coordinate system.[6] A point is chosen as the pole and a ray from this point is taken as the polar axis. For a given angle θ, there is a single line through the pole whose angle with the polar axis is θ (measured counterclockwise from the axis to the line). Then there is a unique point on this line whose signed distance from the origin is r for given number r. For a given pair of coordinates (r, θ) there is a single point, but any point is represented by many pairs of coordinates. For example, (r, θ), (r, θ+2π) and (−r, θ+π) are all polar coordinates for the same point. The pole is represented by (0, θ) for any value of θ.Depending on the direction and order of the coordinate axis the system may be a right-hand or a left-hand system. This is one of many coordinate systems.In three dimensions, three perpendicular planes are chosen and the three coordinates of a point are the signed distances to each of the planes.[5] This can be generalized to create n coordinates for any point in n-dimensional Euclidean space.The prototypical example of a coordinate system is the Cartesian coordinate system. In the plane, two perpendicular lines are chosen and the coordinates of a point are taken to be the signed distances to the lines. The simplest example of a coordinate system is the identification of points on a line with real numbers using the number line. In this system, an arbitrary point O (the origin) is chosen on a given line. The coordinate of a point P is defined as the signed distance from O to P, where the signed distance is the distance taken as positive or negative depending on which side of the line P lies. Each point is given a unique coordinate and each real number is the coordinate of a unique point.[4]In geometry, a coordinate system is a system which uses one or more numbers, or coordinates, to uniquely determine the position of the points or other geometric elements on a manifold such as Euclidean space.[1][2] The order of the coordinates is significant, and they are sometimes identified by their position in an ordered tuple and sometimes by a letter, as in "the x-coordinate". The coordinates are taken to be real numbers in elementary mathematics, but may be complex numbers or elements of a more abstract system such as a commutative ring. The use of a coordinate system allows problems in geometry to be translated into problems about numbers and vice versa; this is the basis of analytic geometry.[3]
Linear combination

Matrix (mathematics)
Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toA number λ and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n×n-matrix A is called positive-definite if for all nonzero vectors x ∈ Rn the associated quadratic form given bywhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2×2 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generallywhere 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:
Matrix similarity
In the definition of similarity, if the matrix P can be chosen to be a permutation matrix then A and B are permutation-similar; if P can be chosen to be a unitary matrix then A and B are unitarily equivalent. The spectral theorem says that every normal matrix is unitarily equivalent to some diagonal matrix. Specht's theorem states that two matrices are unitarily equivalent if and only if they satisfy certain trace equalities.Similarity of matrices does not depend on the base field: if L is a field containing K as a subfield, and A and B are two matrices over K, then A and B are similar as matrices over K if and only if they are similar as matrices over L. This is so because the rational canonical form over K is also the rational canonical form over L. This means that one may use Jordan forms that only exist over a larger field to determine whether the given matrices are similar.Because of this, for a given matrix A, one is interested in finding a simple "normal form" B which is similar to A—the study of A then reduces to the study of the simpler matrix B. For example, A is called diagonalizable if it is similar to a diagonal matrix. Not all matrices are diagonalizable, but at least over the complex numbers (or any algebraically closed field), every matrix is similar to a matrix in Jordan form. Neither of these forms is unique (diagonal entries or Jordan blocks may be permuted) so they are not really normal forms; moreover their determination depends on being able to factor the minimal or characteristic polynomial of A (equivalently to find its eigenvalues). The rational canonical form does not have these drawbacks: it exists over any field, is truly unique, and it can be computed using only arithmetic operations in the field; A and B are similar if and only if they have the same rational canonical form. The rational canonical form is determined by the elementary divisors of A; these can be immediately read off from a matrix in Jordan form, but they can also be determined directly for any matrix by computing the Smith normal form, over the ring of polynomials, of the matrix (with polynomial entries) XIn − A (the same one whose determinant defines the characteristic polynomial). Note that this Smith normal form is not a normal form of A itself; moreover it is not similar to XIn − A either, but obtained from the latter by left and right multiplications by different invertible matrices (with polynomial entries).Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator:Similarity is an equivalence relation on the space of square matrices.A transformation A ↦ P−1AP is called a similarity transformation or conjugation of the matrix A. In the general linear group, similarity is therefore the same as conjugacy, and similar matrices are also called conjugate; however in a given subgroup H of the general linear group, the notion of conjugacy may be more restrictive than similarity, since it requires that P be chosen to lie in H.for some invertible n-by-n matrix P. Similar matrices represent the same linear operator under two (possibly) different bases, with P being the change of basis matrix.[1][2]In linear algebra, two n-by-n matrices A and B are called similar if
Standard basis
In physics, the standard basis vectors for a given Euclidean space are sometimes referred to as the versors of the axes of the corresponding Cartesian coordinate system.Gröbner bases are also sometimes called standard bases.The existence of other 'standard' bases has become a topic of interest in algebraic geometry, beginning with work of Hodge from 1943 on Grassmannians. It is now a part of representation theory called standard monomial theory. The idea of standard basis in the universal enveloping algebra of a Lie algebra is established by the Poincaré–Birkhoff–Witt theorem.from I into a ring R, which are zero except for a finite number of indices, if we interpret 1 as 1R, the unit in R.of all familiesAll of the preceding are special cases of the familyThere is a standard basis also for the ring of polynomials in n indeterminates over a field, namely the monomials.are also orthogonal unit vectors, but they are not aligned with the axes of the Cartesian coordinate system, so the basis with these vectors does not meet the definition of standard basis.However, an ordered orthonormal basis is not necessarily a standard basis. For instance the two vectors representing a 30° rotation of the 2D standard basis described above, i.e.By definition, the standard basis is a sequence of orthogonal unit vectors. In other words, it is an ordered and orthonormal basis.the scalars vx, vy, vz being the scalar components of the vector v.These vectors are a basis in the sense that any other vector can be expressed uniquely as a linear combination of these. For example, every vector v in three-dimensional space can be written uniquely asHere the vector ex points in the x direction, the vector ey points in the y direction, and the vector ez points in the z direction. There are several common notations for these vectors, including {ex, ey, ez}, {e1, e2, e3}, {i, j, k}, and {x, y, z}. These vectors are sometimes written with a hat to emphasize their status as unit vectors. Each of these vectors is sometimes referred to as the versor of the corresponding Cartesian axis.and the standard basis for three-dimensional space is formed by vectorsIn mathematics, the standard basis (also called natural basis) for a Euclidean space is the set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. For example, the standard basis for the Euclidean plane is formed by vectors
Determinant
where ωj is an nth root of 1.where ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]Third orderSecond orderwhere the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.In general, the nth-order Vandermonde determinant is[33]The third order Vandermonde determinant isThe Jacobian also occurs in the inverse function theorem.Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given bythe Jacobian matrix is the n × n matrix whose entries are given byFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. ForBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Another infinite-dimensional notion of determinant is the functional determinant.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,holds. In other words, the following diagram commutes:between the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityThe determinant defines a mappingThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.from the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isThe determinant can also be characterized as the unique functionThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn). As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple. We call this scalar the determinant of T.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.As ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThe determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant is therefore also called a similarity invariant. The determinant of a linear transformationThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThis identity is used in describing the tangent space of certain matrix Lie groups.Yet another equivalent formulation isExpressed in terms of the entries of A, these arewhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveBy definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]When D is a 1×1 matrix, B is a column vector, and C is a row vector thenWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]as can be seen by employing the decompositionWhen A is invertible, one hasThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenIt has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.the solution is given by Cramer's rule:For a matrix equationThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.Also,with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantis expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).where I is the identity matrix. More generally, ifAn important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way asThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l – 1)! tr(Al) aswhere the sum is taken over the set of all integers kl ≥ 0 satisfying the equationIn the general case, this may also be obtained from[9]cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromFor example, for n = 2, n = 3, and n = 4, respectively,the determinant of A is given byHere exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingor, for real matrices A,The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,being positive, for all k between 1 and n.A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatriceswhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationThe product of all non-zero eigenvalues is referred to as pseudo-determinant.From this general result several consequences follow.where Im and In are the m × m and n × n identity matrices, respectively.Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):In terms of the adjugate matrix, Laplace's expansion can be written as[7]The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,However, Laplace expansion is efficient for small matrices only.along the second column (j = 2 and the sum runs over i) is given by,Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i+jMi,j is known as a cofactor. The determinant of A is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant of a matrix product of square matrices equals the product of their determinants:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.can be computed using the following matrices:For example, the determinant ofProperty 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.Property 2 above implies that properties for columns have their counterparts in terms of rows:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.The determinant has many properties. Some basic properties of determinants arewhere now each ir and each jr should be summed over 1, …, n.or using two epsilon symbols asFor example, the determinant of a 3 × 3 matrix A (n = 3) isis notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:Here the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.The Leibniz formula for the determinant of an n × n matrix A isThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.which is the Leibniz formula for the determinant of a 3 × 3 matrix.this can be expanded out to giveThe Laplace formula for the determinant of a 3 × 3 matrix isThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (-b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The Leibniz formula for the determinant of a 2 × 2 matrix isThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.Assume A is a square matrix with n rows and n columns, so that it can be written asEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n × n matrix A in terms of its column vectorsThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 × 4 matrix:When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 × 4 matrix, the determinant of a 5 × 5 matrix, and so forth.Similarly, suppose we have a 3 × 3 matrix A, and we want the specific formula for its determinant |A|:In the case of a 2 × 2 matrix the specific formula for the determinant is:In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.
Invertible matrix
Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any m-by-n matrix.Therefore,then,More generally, ifSuppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given byIf it is also the case that A − X has rank 1 then this simplifies tothen A is nonsingular and its inverse isMore generally, if A is "near" the invertible matrix X in the sense thatTruncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfiesthen A is nonsingular and its inverse may be expressed by a Neumann series:[11]If a matrix A has the property thatSince a blockwise inversion of an n × n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is Ω(n2 log n).[10]where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.Equating Equations (1) and (2) leads toThe inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A − BD−1C are nonsingular,[8] the result isThe nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D − CA−1B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D − CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.Matrices can also be inverted blockwise by using the following analytic inversion formula:With increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley–Hamilton method leads to an expression that is still tractable:The Cayley–Hamilton decomposition givesThe determinant of A can be computed by applying the rule of Sarrus as follows:(where the scalar A is not to be confused with the matrix A). If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given byA computationally efficient 3 × 3 matrix inversion is given byThe Cayley–Hamilton method givesThis is possible because 1/(ad − bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.The cofactor equation listed above yields the following result for 2 × 2 matrices. Inversion of these matrices can be done as follows:[6]where |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.so thatWriting the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:where L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.If matrix A is positive definite, then its inverse can be obtained asIf matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given byThe formula can be rewritten in terms of complete Bell polynomials of arguments tl = - (l - 1)! tr(Al) aswhere n is dimension of A, and tr(A) is the trace of matrix A given by the sum of the main diagonal. The sum is taken over s and the sets of all kl ≥ 0 satisfying the linear Diophantine equationThe Cayley–Hamilton theorem allows the inverse of A to be expressed in terms of det(A), traces and powers of A [5]Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:Gauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.As an example of a non-invertible, or singular, matrix, consider the matrixConsider the following 2-by-2 matrix:In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices. Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.Over the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn×n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.for finite square matrices A and B, then alsoIt follows from the theory of matrices that ifA matrix that is its own inverse, i.e. such that A = A−1 and A2 = I, is called an involutory matrix.Furthermore, the following properties hold for an invertible matrix A:Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:The set of n × n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined. The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse. If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In. If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.where In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that
Inverse element
which is a singular matrix, and cannot be inverted.The left inverse doesn't exist, becauseAs an example of matrix inverses, consider:No rank deficient matrix has any (even one-sided) inverse. However, the Moore–Penrose inverse exists for all matrices, and coincides with the left or right (or true) inverse when it exists.Non-square matrices of full rank have several one-sided inverses:[3]The lower and upper adjoints in a (monotone) Galois connection, L and G are quasi-inverses of each other, i.e. LGL = L and GLG = G and one uniquely determines the other. They are not left or right inverses of each other however.All examples in this section involve associative operators, thus we shall use the terms left/right inverse for the unital magma-based definition, and quasi-inverse for its more general version.Clearly a group is both an I-semigroup and a *-semigroup. A class of semigroups important in semigroup theory are completely regular semigroups; these are I-semigroups in which one additionally has aa° = a°a; in other words every element has commuting pseudoinverse a°. There are few concrete examples of such semigroups however; most are completely simple semigroups. In contrast, a subclass of *-semigroups, the *-regular semigroups (in the sense of Drazin), yield one of best known examples of a (unique) pseudoinverse, the Moore–Penrose inverse. In this case however the involution a* is not the pseudoinverse. Rather, the pseudoinverse of x is the unique element y such that xyx = x, yxy = y, (xy)* = xy, (yx)* = yx. Since *-regular semigroups generalize inverse semigroups, the unique element defined this way in a *-regular semigroup is called the generalized inverse or Penrose–Moore inverse.A natural generalization of the inverse semigroup is to define an (arbitrary) unary operation ° such that (a°)° = a for all a in S; this endows S with a type ⟨2,1⟩ algebra. A semigroup endowed with such an operation is called a U-semigroup. Although it may seem that a° will be the inverse of a, this is not necessarily the case. In order to obtain interesting notion(s), the unary operation must somehow interact with the semigroup operation. Two classes of U-semigroups have been studied:[2]Outside semigroup theory, a unique inverse as defined in this section is sometimes called a quasi-inverse. This is generally justified because in most applications (e.g., all examples in this article) associativity holds, which makes this notion a generalization of the left/right inverse relative to an identity.In a monoid, the notion of inverse as defined in the previous section is strictly narrower than the definition given in this section. Only elements in the Green class H1 have an inverse from the unital magma perspective, whereas for any idempotent e, the elements of He have an inverse as defined in this section. Under this more general definition, inverses need not be unique (or exist) in an arbitrary semigroup or monoid. If all elements are regular, then the semigroup (or monoid) is called regular, and every element has at least one inverse. If every element has exactly one inverse as defined in this section, then the semigroup is called an inverse semigroup. Finally, an inverse semigroup with only one idempotent is a group. An inverse semigroup may have an absorbing element 0 because 000 = 0, whereas a group may not.In a semigroup S an element x is called (von Neumann) regular if there exists some element z in S such that xzx = x; z is sometimes called a pseudoinverse. An element y is called (simply) an inverse of x if xyx = x and y = yxy. Every regular element has at least one inverse: if x = xzx then it is easy to verify that y = zxz is an inverse of x as defined in this section. Another easy to prove fact: if y is an inverse of x then e = xy and f = yx are idempotents, that is ee = e and ff = f. Thus, every pair of (mutually) inverse elements gives rise to two idempotents, and ex = xf = x, ye = fy = y, and e acts as a left identity on x, while f acts a right identity, and the left/right roles are reversed for y. This simple observation can be generalized using Green's relations: every idempotent e in an arbitrary semigroup is a left identity for Re and right identity for Le.[1] An intuitive description of this fact is that every pair of mutually inverse elements produces a local left identity, and respectively, a local right identity.The definition in the previous section generalizes the notion of inverse in group relative to the notion of identity. It's also possible, albeit less obvious, to generalize the notion of an inverse by dropping the identity element but keeping associativity, i.e., in a semigroup.A left-invertible element is left-cancellative, and analogously for right and two-sided.The word 'inverse' is derived from Latin: inversus that means 'turned upside down', 'overturned'.In abstract algebra, the idea of an inverse element generalises concepts of a negation (sign reversal) in relation to addition, and a reciprocal in relation to multiplication. The intuition is of an element that can 'undo' the effect of combination with another given element. While the precise definition of an inverse element varies depending on the algebraic structure involved, these definitions coincide in a group.
Kernel (linear algebra)
Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]The problem of computing the kernel on a computer depends on the nature of the coefficients.are a basis of the kernel of A.The last three columns of B are zero columns. Therefore, the three last vectors of C,Putting the upper part in column echelon form by column operations on the whole matrix givesThenFor example, suppose thatIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.A basis of the kernel of a matrix may be computed by Gaussian elimination.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.Note also that the following dot products are zero:The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.Since c is a free variable, this can be expressed equally well as,for c a scalar.Now we can express an element of the kernel:Rewriting yields:Gauss–Jordan elimination reduces this to:which can be written in matrix form as:which can be expressed as a homogeneous system of linear equations involving x, y, and z:The kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichConsider the matrixWe give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Geometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax = b isThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.If u and v are two possible solutions to the above equation, thenThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The product Ax can be written in terms of the dot product of vectors as follows:The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The matrix equation is equivalent to a homogeneous system of linear equations:Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.This implies the rank–nullity theorem:It follows that the image of L is isomorphic to the quotient of V by the kernel:The kernel of L is a linear subspace of the domain V.[1] In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,
Cramer's rule

Gaussian elimination
Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]The Gaussian elimination can be performed over any field, not just the real numbers.One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 − 5n)/6 multiplications, and (2n3 + 3n2 − 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).All of this applies also to the reduced row echelon form, which is a particular row echelon form.One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:For example, consider the following matrixA variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.Computationally, for a n×n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = ∏diag(B) / d.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Suppose the goal is to find and describe the set of solutions to the following system of linear equations:A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.There are three types of elementary row operations which may be performed on the rows of a matrix:Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
Eigenvalues and eigenvectors
The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.leads to a so-called quadratic eigenvalue problem,orEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byPrincipal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.This matrix equation is equivalent to two linear equationsOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]A similar procedure is used for solving a differential equation of the formThe solution of this equation for x in terms of t is found by using its characteristic equationThe simplest difference equations have the formThe representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.So, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u+v,αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenBy definition of a linear transformation,which is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.Given an eigenvalue λ, consider the setThis equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such thatThe concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,The main eigenfunction article gives other examples.is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationOn the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 −1 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity γA is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.has a characteristic polynomial that is the product of its diagonal elements,As in the previous example, the lower triangular matrixrespectively, as well as scalar multiples of these vectors.These eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isConsider the lower triangular matrix,A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.respectively, as well as scalar multiples of these vectors.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isMatrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixandThenFor the complex conjugate pair of imaginary eigenvalues, note thatFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areConsider the cyclic permutation matrixThe characteristic polynomial of A isConsider the matrixThus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Any non-zero vector with v1 = v2 solves this equation. Therefore,For λ = 3, Equation (2) becomesis an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.Any non-zero vector with v1 = −v2 solves this equation. Therefore,For λ = 1, Equation (2) becomes,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.Taking the determinant to find characteristic polynomial of A,The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Consider the matrixA matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.or by instead left multiplying both sides by Q−1,Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenSince each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues λ1, λ2, ..., λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.where κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation isMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),Let A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenSuppose A has d ≤ n distinct eigenvalues λ1, λ2, ..., λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,The condition that γA(λ) ≤ μA(λ) can be proven by considering a particular eigenvalue ξ of A and diagonalizing the first γA(ξ) columns of A with respect to the eigenvectors of ξ, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix ξIγA(ξ). As a result, the characteristic polynomial of B will have a factor of (ξ − λ)γA(ξ). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of ξ as an eigenvalue of B is no less than the geometric multiplicity of ξ as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number, (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),If μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asSuppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofTaking the determinant of (M − λI), the characteristic polynomial of M isAs a brief example, which is described in more detail in the examples section later, consider the matrixwhere each λi may be real but in general is a complex number. The numbers λ1, λ2, ... λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equationwhere I is the n by n identity matrix.Equation (1) can be stated equivalently asthen v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.If it occurs that v and w are scalar multiples, that is ifwhere, for each row,orNow consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,In this case λ = −1/20.These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatConsider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationThe Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.referred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationwhere λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation
Invariant (mathematics)
Thirdly, if one is studying an object which varies in a family, as is common in algebraic geometry and differential geometry, one may ask if the property is unchanged under perturbation – if an object is constant on families or invariant under change of metric, for instance.The most common examples are:Secondly, a function may be defined in terms of some presentation or decomposition of a mathematical object; for instance, the Euler characteristic of a cell complex is defined as the alternating sum of the number of cells in each dimension. One may forget the cell complex structure and look only at the underlying topological space (the manifold) – as different cell complexes give the same underlying manifold, one may ask if the function is independent of choice of presentation, in which case it is an intrinsically defined invariant. This is the case for the Euler characteristic, and a general method for defining and computing invariants is to define them for a given presentation and then show that they are independent of the choice of presentation. Note that there is no notion of a group action in this sense.These are connected as follows: invariants are constant on coinvariants (for example, congruent triangles have the same perimeter), while two objects which agree in the value of one invariant may or may not be congruent (two triangles with the same perimeter need not be congruent). In classification problems, one seeks to find a complete set of invariants, such that if two objects have the same values for this set of invariants, they are congruent. For example, triangles such that all three sides are equal are congruent, via SSS congruence, and thus the lengths of all three sides form a complete set of invariants for triangles.Dual to the notion of invariants are coinvariants, also known as orbits, which formalizes the notion of congruence: objects which can be taken to each other by a group action. For example, under the group of rigid motions of the plane, the perimeter of a triangle is an invariant, while the set of triangles congruent to a given triangle is a coinvariant.More importantly, one may define a function on a set, such as "radius of a circle in the plane" and then ask if this function is invariant under a group action, such as rigid motions.Very frequently one will have a group acting on a set X and ask which objects in an associated set F(X) are invariant. For example, rotation in the plane about a point leaves the point about which it rotates invariant, while translation in the plane does not leave any points invariant, but does leave all lines parallel to the direction of translation invariant as lines. Formally, define the set of lines in the plane P as L(P); then a rigid motion of the plane takes lines to lines – the group of rigid motions acts on the set of lines – and one may ask which lines are unchanged by an action.Firstly, if one has a group G acting on a mathematical object (or set of objects) X, then one may ask which points x are unchanged, "invariant" under the group action, or under an element g of the group.The notion of invariance is formalized in three different ways in mathematics: via group actions, presentations, and deformation.When T is a screw displacement, the screw axis is an invariant line, though if the pitch is non-zero, T has no fixed points.An invariant set of an operation T is also said to be stable under T. For example, the normal subgroups that are so important in group theory are those subgroups that are stable under the inner automorphisms of the ambient group.[6][7][8] Other examples occur in linear algebra. Suppose a linear transformation T has an eigenvector v. Then the line through 0 and v is an invariant set under T. The eigenvectors span an invariant subspace which is stable under T.Some more complicated examples:Angles and ratios of distances are invariant under scalings, rotations, translations and reflections. These transformations produce similar shapes, which is the basis of trigonometry. For example, no matter how a triangle is transformed, the sum of its interior angles is always 180°. As another example, all circles are similar: they can be transformed into each other and the ratio of the circumference to the diameter is invariant (denoted by the Greek letter pi).The distance between two points on a number line is not changed by adding the same quantity to both numbers. On the other hand, multiplication does not have this same property as addition, so distance is not invariant under multiplication.An identity is an equation that remains true for all values of its variables. There are also inequalities that remain true when the values of their variables change.A simple example of invariance is expressed in our ability to count. For a finite set of objects of any kind, there is a number to which we always arrive, regardless of the order in which we count the objects in the set. The quantity—a cardinal number—is associated with the set, and is invariant under the process of counting.Invariants are used in diverse areas of mathematics such as geometry, topology and algebra. Some important classes of transformations are defined by an invariant they leave unchanged, for example conformal maps are defined as transformations of the plane that preserve angles. The discovery of invariants is an important step in the process of classifying mathematical objects.In mathematics, an invariant is a property, held by a class of mathematical objects, which remains unchanged when transformations of a certain type are applied to the objects. The particular class of objects and type of transformations are usually indicated by the context in which the term is used. For example, the area of a triangle is an invariant with respect to isometries of the Euclidean plane. The phrases "invariant under" and "invariant to" a transformation are both used. More generally, an invariant with respect to an equivalence relation is a property that is constant on each equivalence class.
Eigenvalues and eigenvectors
The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.leads to a so-called quadratic eigenvalue problem,orEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byPrincipal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.This matrix equation is equivalent to two linear equationsOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]A similar procedure is used for solving a differential equation of the formThe solution of this equation for x in terms of t is found by using its characteristic equationThe simplest difference equations have the formThe representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.So, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u+v,αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenBy definition of a linear transformation,which is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.Given an eigenvalue λ, consider the setThis equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such thatThe concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,The main eigenfunction article gives other examples.is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationOn the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 −1 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity γA is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.has a characteristic polynomial that is the product of its diagonal elements,As in the previous example, the lower triangular matrixrespectively, as well as scalar multiples of these vectors.These eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isConsider the lower triangular matrix,A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.respectively, as well as scalar multiples of these vectors.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isMatrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixandThenFor the complex conjugate pair of imaginary eigenvalues, note thatFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areConsider the cyclic permutation matrixThe characteristic polynomial of A isConsider the matrixThus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Any non-zero vector with v1 = v2 solves this equation. Therefore,For λ = 3, Equation (2) becomesis an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.Any non-zero vector with v1 = −v2 solves this equation. Therefore,For λ = 1, Equation (2) becomes,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.Taking the determinant to find characteristic polynomial of A,The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Consider the matrixA matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.or by instead left multiplying both sides by Q−1,Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenSince each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues λ1, λ2, ..., λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.where κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation isMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),Let A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenSuppose A has d ≤ n distinct eigenvalues λ1, λ2, ..., λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,The condition that γA(λ) ≤ μA(λ) can be proven by considering a particular eigenvalue ξ of A and diagonalizing the first γA(ξ) columns of A with respect to the eigenvectors of ξ, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix ξIγA(ξ). As a result, the characteristic polynomial of B will have a factor of (ξ − λ)γA(ξ). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of ξ as an eigenvalue of B is no less than the geometric multiplicity of ξ as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number, (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),If μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asSuppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofTaking the determinant of (M − λI), the characteristic polynomial of M isAs a brief example, which is described in more detail in the examples section later, consider the matrixwhere each λi may be real but in general is a complex number. The numbers λ1, λ2, ... λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equationwhere I is the n by n identity matrix.Equation (1) can be stated equivalently asthen v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.If it occurs that v and w are scalar multiples, that is ifwhere, for each row,orNow consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,In this case λ = −1/20.These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatConsider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationThe Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.referred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationwhere λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation
Eigenvalues and eigenvectors
The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.leads to a so-called quadratic eigenvalue problem,orEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byPrincipal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.This matrix equation is equivalent to two linear equationsOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]A similar procedure is used for solving a differential equation of the formThe solution of this equation for x in terms of t is found by using its characteristic equationThe simplest difference equations have the formThe representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.So, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u+v,αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenBy definition of a linear transformation,which is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.Given an eigenvalue λ, consider the setThis equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such thatThe concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,The main eigenfunction article gives other examples.is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationOn the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 −1 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity γA is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.has a characteristic polynomial that is the product of its diagonal elements,As in the previous example, the lower triangular matrixrespectively, as well as scalar multiples of these vectors.These eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isConsider the lower triangular matrix,A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.respectively, as well as scalar multiples of these vectors.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isMatrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixandThenFor the complex conjugate pair of imaginary eigenvalues, note thatFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areConsider the cyclic permutation matrixThe characteristic polynomial of A isConsider the matrixThus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Any non-zero vector with v1 = v2 solves this equation. Therefore,For λ = 3, Equation (2) becomesis an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.Any non-zero vector with v1 = −v2 solves this equation. Therefore,For λ = 1, Equation (2) becomes,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.Taking the determinant to find characteristic polynomial of A,The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Consider the matrixA matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.or by instead left multiplying both sides by Q−1,Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenSince each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues λ1, λ2, ..., λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.where κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation isMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),Let A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenSuppose A has d ≤ n distinct eigenvalues λ1, λ2, ..., λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,The condition that γA(λ) ≤ μA(λ) can be proven by considering a particular eigenvalue ξ of A and diagonalizing the first γA(ξ) columns of A with respect to the eigenvectors of ξ, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix ξIγA(ξ). As a result, the characteristic polynomial of B will have a factor of (ξ − λ)γA(ξ). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of ξ as an eigenvalue of B is no less than the geometric multiplicity of ξ as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number, (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),If μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asSuppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofTaking the determinant of (M − λI), the characteristic polynomial of M isAs a brief example, which is described in more detail in the examples section later, consider the matrixwhere each λi may be real but in general is a complex number. The numbers λ1, λ2, ... λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equationwhere I is the n by n identity matrix.Equation (1) can be stated equivalently asthen v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.If it occurs that v and w are scalar multiples, that is ifwhere, for each row,orNow consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,In this case λ = −1/20.These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatConsider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationThe Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.referred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationwhere λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation
Identity matrix
The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.[3]The identity matrix of a given size is the only idempotent matrix of that size having full rank. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.It can also be written using the Kronecker delta notation:Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:The ith column of an identity matrix is the unit vector ei. It follows that the determinant of the identity matrix is 1 and the trace is n.Where n×n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.In particular, the identity matrix serves as the unit of the ring of all n×n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n×n matrices. (The identity matrix itself is invertible, being its own inverse.)When A is m×n, it is a property of matrix multiplication thatIn linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning "unit matrix"[1] and the German word "Einheitsmatrix",[2] respectively.
Polynomial
The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, − for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. René Descartes, in La géometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.[23]Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write 3x + 2y + z = 29.The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.Polynomials serve to approximate other functions,[22] such as the use of splines.Analogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, "non-constant" must be replaced by "non-constant or non-unit" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.and such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[x] is a Euclidean domain.If F is a field and f and g are polynomials in F[x] with g ≠ 0, then there exist unique polynomials q and r in F[x] withIn commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[x], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[x] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[x] and r is an element of R such that f(r) = 0, then the polynomial (x − r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.[20][21]If R is commutative, then one can associate to every polynomial P in R[x], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for x.Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[x] over the real numbers by factoring out the ideal of multiples of the polynomial x2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).One can think of the ring R[x] as arising from R by adding one new element x to R, and extending in a minimal way to a ring in which x satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is xr = rx). To do this, one must add all powers of x and their linear combinations as well.Thus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[x]. The map from R to R[x] sending r to rx0 is an injective homomorphism of rings, by which R is viewed as a subring of R[x]. If R is commutative, then R[x] is an algebra over R.where n is a natural number, the coefficients a0, . . ., an are elements of R, and x is a formal symbol, whose powers xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aixi is interpreted as a polynomial that has zero coefficients at all powers of x other than xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the ruleIn abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate x over a ring R is defined as a formal expression of the formand the indefinite integral isthe derivative with respect to x isCalculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial functionThe simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone–Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).where I is the identity matrix.[19]this polynomial evaluated at a matrix A isA matrix polynomial is a polynomial with square matrices as variables.[18] Given an ordinary, scalar-valued polynomialTrigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.For complex coefficients, there is no difference between such a function and a finite Fourier series.If sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.A trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers.[17] The coefficients may be taken as real numbers, for real-valued functions. There are several generalizations of the concept of polynomials.A polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.For polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of "roots". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.When there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions.[16] There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).Some polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x − a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.A number a is a root of a polynomial P if and only if the linear polynomial x − a divides P, that is if there is another polynomial Q such that P = (x – a) Q. It may happen that x − a divides P more than once: if (x − a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x − a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P.[15] The relation between the coefficients of a polynomial and its roots is described by Vieta's formulas.The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.In elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel–Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.When considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x − y) = x2 − y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.is a polynomial equation.For example,A polynomial equation, also called algebraic equation, is an equation of the form[14]Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.A non-constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).A polynomial function in one real variable can be represented by a graph.Every polynomial function is continuous, smooth, and entire.is a polynomial function of one variable. Polynomial functions of multiple variables are similarly defined, using polynomials in multiple indeterminates, as inFor example, the function f, defined byGenerally, unless otherwise specified, polynomial functions have real or complex coefficients and have real or complex arguments and values. In particular, a polynomial with real coefficients defines a function from the complex numbers to the complex numbers, whose restriction to the reals maps reals to reals.for all arguments x, where n is a non-negative integer and a0, a1, a2, ..., an are constant coefficients.A polynomial function is a function that can be defined by evaluating a polynomial. A function f of one argument is thus a polynomial function if it satisfies.Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.The computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.over the complex numbers.over the integers and the reals andisAll polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree.[13] For example, the factored form ofAs for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b ≠ 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.[12]Polynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of f(x) by (x − a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.which can be simplified tothenTo work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.[8] For example, ifwhich can be simplified tothenPolynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.[8][10] For example, ifThe evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:A polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in x, y, and z", listing the indeterminates allowed.A real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms. A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.[8] Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,[9] a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term "quadrinomial" is occasionally used for a four-term polynomial.The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of x", with the term of largest degree first, or in "ascending powers of x". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is −5. The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.[7]In the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its non-zero terms have degree n. The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.[6] For example, x3y2 + 7x2y3 − 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞).[5] These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, f(x) = 0, is the X-axis.Polynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:is a term. The coefficient is −5, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.For example:A term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.[3] The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).[4]That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number—called the coefficient of the term[2]—and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because x = x1, the degree of an indeterminate without a written exponent is one.This can be expressed more concisely by using summation notation:A polynomial in a single indeterminate x can always be written (or rewritten) in the formA polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.This equality allows writing "let P(x) be a polynomial" as a shorthand for "let P be a polynomial in the indeterminate x". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.Frequently, when using this function, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (any ring). In particular, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,which is the polynomial function associated to P.Normally, the name of the polynomial is P, not P(x). However, if a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting x by a in P. Thus, the polynomial P defines the functionIt may be confusing that a polynomial P in the indeterminate x may appear in the formulas either as P or as P(x).[citation needed]It is a common convention to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.[citation needed]The x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is "indeterminate"). It is thus more correct to call it an "indeterminate".[citation needed] However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a "variable". Many authors use these two words interchangeably.The word polynomial joins two diverse roots: the Greek poly, meaning "many," and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.[1] Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate x is x2 − 4x + 7. An example in three variables is x3 + 2xyz2 − yz + 1.
Algebraically closed field

Complex number
The fields R and Qp and their finite field extensions, including C, are local fields.Hypercomplex numbers also generalize R, C, H, and O. For example, this notion contains the split-complex numbers, which are elements of the ring R[x]/(x2 − 1) (as opposed to R[x]/(x2 + 1)). In this ring, the equation a2 = 1 has four solutions.is also isomorphic to the field C, and gives an alternative complex structure on R2. This is generalized by the notion of a linear complex structure.has the property that its square is the negative of the identity matrix: J2 = −I. Theni.e., the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of C in the 2 × 2 real matrices, it is not the only one. Any matrixfor some fixed complex number w can be represented by a 2 × 2 matrix (once a basis has been chosen). With respect to the basis (1, i), this matrix isThe Cayley–Dickson construction is closely related to the regular representation of C, thought of as an R-algebra (an R-vector space with a multiplication), with respect to the basis (1, i). This means the following: the R-linear mapReals, complex numbers, quaternions and octonions are all normed division algebras over R. However, by Hurwitz's theorem they are the only ones. The next step in the Cayley–Dickson construction, the sedenions, in fact fails to have this structure.However, just as applying the construction to reals loses the property of ordering, more properties familiar from real and complex numbers vanish with increasing dimension. The quaternions are only a skew field, i.e. for some x, y: x·y ≠ y·x for two quaternions, the multiplication of octonions fails (in addition to not being commutative) to be associative: for some x, y, z: (x·y)·z ≠ x·(y·z).The process of extending the field R of reals to C is known as the Cayley–Dickson construction. It can be carried further to higher dimensions, yielding the quaternions H and octonions O which (as a real vector space) are of dimension 4 and 8, respectively. In this context the complex numbers have been called the binarions.[37]Later classical writers on the general theory include Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass and many others.The English mathematician G. H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise.[36] Augustin Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case.Wessel's memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Carl Friedrich Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about "the true metaphysics of the square root of −1". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology. In the beginning of the 19th century, other mathematicians discovered independently the geometrical representation of the complex numbers: Buée, Mourey, Warren, Français and his brother, Bellavitis.[35]The idea of a complex number as a point in the complex plane (above) was first described by Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's De Algebra tractatus.by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.In 1748 Leonhard Euler went further and obtained Euler's formula of complex analysis:In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:The term "imaginary" for these quantities was coined by René Descartes in 1637, although he was at pains to stress their imaginary nature[34]Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function ζ(s) is related to the distribution of prime numbers.Another example are Gaussian integers, that is, numbers of the form x + iy, where x and y are integers, which can be used to classify sums of squares.As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in C. A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers – they are a principal object of study in algebraic number theory. Compared to Q, the algebraic closure of Q, which also contains all algebraic numbers, C has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge – a purely geometric problem.Certain fractals are plotted in the complex plane, e.g. the Mandelbrot set and Julia sets.In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics—the Schrödinger equation and Heisenberg's matrix mechanics—make use of complex numbers.Another example, relevant to the two side bands of amplitude modulation of AM radio, is:This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.where ω represents the angular frequency and the complex number A encodes the phase and amplitude as explained above.andIf Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex valued functions of the formComplex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value | z | of the corresponding z is the amplitude and the argument arg(z) is the phase.To obtain the measurable quantity, the real part is taken:Since the voltage in an AC circuit is oscillating, it can be represented asIn electrical engineering, the imaginary unit is denoted by j, to avoid confusion with I, which is generally in use to denote electric current, or, more particularly, i, which is generally in use to denote instantaneous electric current.In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.In differential equations, it is common to first find all complex roots r of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form f(t) = ert. Likewise, in difference equations, the complex roots r of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form f(t) = rt.In fluid dynamics, complex functions are used to describe potential flow in two dimensions.In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.If a system has zeros in the right half plane, it is a nonminimum phase system.In the root locus method, it is important whether zeros and poles are in the left or right half planes, i.e. have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that areIn control theory, systems are often transformed from the time domain to the frequency domain using the Laplace transform. The system's zeros and poles are then analyzed in the complex plane. The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane.Complex numbers have essential concrete applications in a variety of scientific and related areas such as signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some applications of complex numbers are:Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions f and g that agree on an arbitrarily small open subset of C necessarily agree everywhere. Meromorphic functions, functions that can locally be written as f(z)/(z − z0)n with a holomorphic function f, still share some of the features of holomorphic functions. Other functions have essential singularities, such as sin(1/z) at z = 0.A function f : C → C is called holomorphic if it satisfies the Cauchy–Riemann equations. For example, any R-linear map C → C can be written in the formBoth sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when naïvely treated as single-valued functions; see failure of power and logarithm identities. For example, they do not satisfyComplex exponentiation zω is defined aswhere arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2π, log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval (−π,π].for any complex number w ≠ 0. It can be shown that any such solution z—called complex logarithm of w—satisfiesUnlike in the situation of real numbers, there is an infinitude of complex solutions z of the equationfor any real number φ, in particularEuler's formula states:The series defining the real trigonometric functions sine and cosine, as well as the hyperbolic functions sinh and cosh, also carry over to complex arguments without change. For the other trigonometric and hyperbolic functions, such as tangent, things are slightly more complicated, as the defining series do not converge for all complex values. Therefore, one must define them either in terms of sine, cosine and exponential, or, equivalently, by using the method of analytic continuation.Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the exponential function exp(z), also written ez, is defined as the infinite seriesfor any two complex numbers z1 and z2.is a complete metric space, which notably includes the triangle inequalityThe notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (ε, δ)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, C, endowed with the metricThe study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane.The geometric description of the multiplication of complex numbers can also be expressed in terms of rotation matrices by using this correspondence between complex numbers and such matrices. Moreover, the square of the absolute value of a complex number expressed as a matrix is equal to the determinant of that matrix:Here the entries a and b are real numbers. The sum and product of two such matrices is again of this form, and the sum and product of complex numbers corresponds to the sum and product of such matrices, the product being:Complex numbers a + bi can also be represented by 2 × 2 matrices that have the following form:Accepting that C is algebraically closed, since it is an algebraic extension of R in this approach, C is therefore the algebraic closure of R.The formulas for addition and multiplication in the ring R[X], modulo the relation (X2 = 1 correspond to the formulas for addition and multiplication of complex numbers defined as ordered pairs. So the two definitions of the field C are isomorphic (as fields).The set of complex numbers is defined as the quotient ring R[X]/(X 2 + 1).[28] This extension field contains two square roots of −1, namely (the cosets of) X and −X, respectively. (The cosets of) 1 and X form a basis of R[X]/(X 2 + 1) as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs (a, b) of real numbers. The quotient ring is a field, because the (X2 + 1) is a prime ideal in R[X], a principal ideal domain, and therefore is a maximal ideal.where the a0, ..., an are real numbers. The usual addition and multiplication of polynomials endows the set R[X] of all such polynomials with a ring structure. This ring is called the polynomial ring over the real numbers.must hold for any three elements x, y and z of a field. The set R of real numbers does form a field. A polynomial p(X) with real coefficients is an expression of the formThough this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of C more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive lawIt is then just a matter of notation to express (a, b) as a + bi.The set C of complex numbers can be defined as the set R2 of ordered pairs (a, b) of real numbers, in which the following rules for addition and multiplication are imposed:[27]The only connected locally compact topological fields are R and C. This gives another characterization of C as a topological field, since C can be distinguished from R because the nonzero complex numbers are connected, while the nonzero real numbers are not.[26]Any field F with these properties can be endowed with a topology by taking the sets B(x, p) = { y | p − (y − x)(y − x)* ∈ P }  as a base, where x ranges over the field and p ranges over P. With this topology F is isomorphic as a topological field to C.Moreover, C has a nontrivial involutive automorphism x ↦ x* (namely the complex conjugation), such that x x* is in P for any nonzero x in C.The preceding characterization of C describes only the algebraic aspects of C. That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of C as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. C contains a subset P (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:The field C has the following three properties: first, it has characteristic 0. This means that 1 + 1 + ⋯ + 1 ≠ 0 for any number of summands (all of which equal one). Second, its transcendence degree over Q, the prime field of C, is the cardinality of the continuum. Third, it is algebraically closed (see above). It can be shown that any field having these properties is isomorphic (as a field) to C. For example, the algebraic closure of Qp also satisfies these three properties, so these two fields are isomorphic (as fields, but not as topological fields).[25] Also, C is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that C contains many proper subfields that are isomorphic to C.Because of this fact, theorems that hold for any algebraically closed field, apply to C. For example, any non-empty complex square matrix has at least one (complex) eigenvalue.There are various proofs of this theorem, either by analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of odd degree has at least one real root.has at least one complex solution z, provided that at least one of the higher coefficients a1, …, an is nonzero.[24] This is the statement of the fundamental theorem of algebra, of Carl Friedrich Gauss and Jean le Rond d'Alembert. Because of this fact, C is called an algebraically closed field. This property does not hold for the field of rational numbers Q (the polynomial x2 − 2 does not have a rational root, since √2 is not a rational number) nor the real numbers R (the polynomial x2 + a does not have a real root for a > 0, since the square of x is positive for any real number x).Given any complex numbers (called coefficients) a0, …, an, the equationWhen the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra.Unlike the reals, C is not an ordered field, that is to say, it is not possible to define a relation z1 < z2 that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so i2 = −1 precludes the existence of an ordering on C.[23]These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.The set C of complex numbers is a field.[22] Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number z, its additive inverse −z is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers z1 and z2:(which holds for positive real numbers), do in general not hold for complex numbers.for any integer k satisfying 0 ≤ k ≤ n − 1. Here n√r is the usual (positive) nth root of the positive real number r. While the nth root of a positive real number r is chosen to be the positive real number c satisfying cn = r there is no natural way of distinguishing one particular complex nth root of a complex number. Therefore, the nth root of z is considered as a multivalued function (in z), as opposed to a usual function f, for which f(z) is a uniquely defined number. Formulas such asThe nth roots of z are given byWhen n is an integer, this simplifies to de Moivre's formula:to define complex exponentiation, which is likewise multi-valued:We may use the identityAlternatively, a branch cut can be used to define a single-valued "branch" of the complex logarithm.To deal with the existence of more the one possible value for a given input, the complex logarithm may be considered a multi-valued function, withwhere r is a non-negative real number, one possible value for the complex logarithm of z isIt follows from Euler's formula that, for any complex number z written in polar form,The rearrangement of terms is justified because each series is absolutely convergent.and so on, and by considering the Taylor series expansions of eix, cos x and sin x:where e is the base of the natural logarithm. This can be proved through induction by observing thatEuler's formula states that, for any real number x,Similarly, division is given byholds. As the arctan function can be approximated highly efficiently, formulas like this—known as Machin-like formulas—are used for high-precision approximations of π.Since the real and imaginary part of 5 + 5i are equal, the argument of that number is 45 degrees, or π/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formulaIn other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by i corresponds to a quarter-turn counter-clockwise, which gives back i2 = −1. The picture at the right illustrates the multiplication ofwe may deriveFormulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers z1 = r1(cos φ1 + i sin φ1) and z2 = r2(cos φ2 + i sin φ2), because of the well-known trigonometric identitiesIn angle notation, often used in electronics to represent a phasor with amplitude r and phase φ, it is written as[21]Using the cis function, this is sometimes abbreviated toUsing Euler's formula this can be written asTogether, r and φ give another way of representing complex numbers, the polar form, as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called trigonometric formThe value of φ equals the result of atan2:Normally, as given above, the principal value in the interval (−π,π] is chosen. Values in the range [0,2π) are obtained by adding 2π if the value is negative. The value of φ is expressed in radians in this article. It can increase by any integer multiple of 2π and still give the same angle. Hence, the arg function is sometimes considered as multivalued. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the angle 0 is common.The square of the absolute value isBy Pythagoras' theorem, the absolute value of complex number is the distance to the origin of the point representing the complex number in the complex plane.If z is a real number (that is, if y = 0), then r = | x |. That is, the absolute value of a real number equals its absolute value as a complex number.The absolute value (or modulus or magnitude) of a complex number z = x + yi is[19]An alternative way of defining a point P in the complex plane, other than using the x- and y-coordinates, is to use the distance of the point from O, the point whose coordinates are (0, 0) (the origin), together with the angle subtended between the positive real axis and the line segment OP in a counterclockwise direction. This idea leads to the polar form of complex numbers.andThis formula can be used to compute the multiplicative inverse of a complex number if it is given in rectangular coordinates. Inversive geometry, a branch of geometry studying reflections more general than ones about a line, can also be expressed in terms of complex numbers. In the network analysis of electrical circuits, the complex conjugate is used in finding the equivalent impedance when the maximum power transfer theorem is used.The reciprocal of a nonzero complex number z = x + yi is given byAs shown earlier, c − di is the complex conjugate of the denominator c + di. At least one of the real part c and the imaginary part d of the denominator must be nonzero for division to be defined. This is called "rationalization" of the denominator (although the denominator in the final expression might be an irrational real number).Division can be defined in this way because of the following observation:The division of two complex numbers is defined in terms of complex multiplication, which is described above, and real division. When at least one of c and d is non-zero, we haveThe preceding definition of multiplication of general complex numbers follows naturally from this fundamental property of i. Indeed, if i is treated as a number so that di means d times i, the above multiplication rule is identical to the usual rule for multiplying two sums of two terms.In particular, the square of i is −1:The multiplication of two complex numbers is defined by the following formula:Using the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers A and B, interpreted as points of the complex plane, is the point X obtained by building a parallelogram, three of whose vertices are O, A and B. Equivalently, X is the point such that the triangles with vertices O, A, B, and X, B, A, are congruent.Similarly, subtraction is defined byComplex numbers are added by separately adding the real and imaginary parts of the summands. That is to say:Conjugation distributes over the standard arithmetic operations:Moreover, a complex number is real if and only if it equals its own conjugate.The real and imaginary parts of a complex number z can be extracted using the conjugate:Because complex numbers are naturally thought of as existing on a two-dimensional plane, there is no natural linear ordering on the set of complex numbers. Furthermore, there is no linear ordering on the complex numbers that is compatible with addition and multiplication – the complex numbers cannot have the structure of an ordered field. This is because any square in an ordered field is at least 0, but i2 = −1.Because it is a polynomial in the indeterminate i, a + ib may be written instead of a + bi, which is often expedient when b is a radical.[13] In some disciplines, in particular electromagnetism and electrical engineering, j is used instead of i,[14] since i is frequently used for electric current. In these cases complex numbers are written as a + bj or a + jb.Many mathematicians contributed to the full development of complex numbers. The rules for addition, subtraction, multiplication, and division of complex numbers were developed by the Italian mathematician Rafael Bombelli.[12] A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.The solution in radicals (without trigonometric functions) of a general cubic equation contains the square roots of negative numbers when all three roots are real numbers, a situation that cannot be rectified by factoring aided by the rational root test if the cubic is irreducible (the so-called casus irreducibilis). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545,[11] though his understanding was rudimentary.A position vector may also be defined in terms of its magnitude and direction relative to the origin. These are emphasized in a complex number's polar form. Using the polar form of the complex number in calculations may lead to a more intuitive interpretation of mathematical results. Notably, the operations of addition and multiplication take on a very natural geometric character when complex numbers are viewed as position vectors: addition corresponds to vector addition while multiplication corresponds to multiplying their magnitudes and adding their arguments (i.e. the angles they make with the x axis). Viewed in this way the multiplication of a complex number by i corresponds to rotating the position vector counterclockwise by a quarter turn (90°) about the origin: (a+bi)i = ai+bi2 = -b+ai.A complex number can be viewed as a point or position vector in a two-dimensional Cartesian coordinate system called the complex plane or Argand diagram (see Pedoe 1988 and Solomentsev 2001), named after Jean-Robert Argand. The numbers are conventionally plotted using the real part as the horizontal component, and imaginary part as vertical (see Figure 1). These two values used to identify a given complex number are therefore called its Cartesian, rectangular, or algebraic form.A complex number can thus be identified with an ordered pair (Re(z),Im(z)) in the Cartesian plane, an identification sometimes known as the Cartesian form of z. In fact, a complex number can be defined as an ordered pair (a,b), but then rules for addition and multiplication must also be included as part of the definition (see below).[9] William Rowan Hamilton introduced this approach to define the complex number system.[10]A real number a can be regarded as a complex number a + 0i whose imaginary part is 0. A purely imaginary number bi is a complex number 0 + bi whose real part is zero. It is common to write a for a + 0i and bi for 0 + bi. Moreover, when the imaginary part is negative, it is common to write a − bi with b > 0 instead of a + (−b)i, for example 3 − 4i instead of 3 + (−4)i.The real number a is called the real part of the complex number a + bi; the real number b is called the imaginary part of a + bi. By this convention, the imaginary part does not include a factor of i: hence b, not bi, is the imaginary part.[7][8] The real part of a complex number z is denoted by Re(z) or ℜ(z); the imaginary part of a complex number z is denoted by Im(z) or ℑ(z). For example,A complex number is a number of the form a + bi, where a and b are real numbers and i is an indeterminate satisfying i2 = −1. For example, 2 + 3i is a complex number.[5]According to the fundamental theorem of algebra, all polynomial equations with real or complex coefficients in a single variable have a solution in complex numbers.has no real solution, since the square of a real number cannot be negative. Complex numbers provide a solution to this problem. The idea is to extend the real numbers with an indeterminate i (sometimes called the imaginary unit) that is taken to satisfy the relation i2 = −1, so that solutions to equations like the preceding one can be found. In this case the solutions are −1 + 3i and −1 − 3i, as can be verified using the fact that i2 = −1:Complex numbers allow solutions to certain equations that have no solutions in real numbers. For example, the equationGeometrically, complex numbers extend the concept of the one-dimensional number line to the two-dimensional complex plane by using the horizontal axis for the real part and the vertical axis for the imaginary part. The complex number a + bi can be identified with the point (a, b) in the complex plane. A complex number whose real part is zero is said to be purely imaginary; the points for these numbers lie on the vertical axis of the complex plane. A complex number whose imaginary part is zero can be viewed as a real number; its point lies on the horizontal axis of the complex plane. Complex numbers can also be represented in polar form, which associates each complex number with its distance from the origin (its magnitude) and with a particular angle known as the argument of this complex number.Most importantly the complex numbers give rise to the fundamental theorem of algebra: every non-constant polynomial equation with complex coefficients has a complex solution. This property is true of the complex numbers, but not the reals. The 16th century Italian mathematician Gerolamo Cardano is credited with introducing complex numbers in his attempts to find solutions to cubic equations.[4]The complex number system can be defined as the algebraic extension of the ordinary real numbers by an imaginary number i.[3] This means that complex numbers can be added, subtracted, and multiplied, as polynomials in the variable i, with the rule i2 = −1 imposed. Furthermore, complex numbers can also be divided by nonzero complex numbers. Overall, the complex number system is a field.A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers, and i is a solution of the equation x2 = −1, which is called an imaginary number because there is no real number that satisfies this equation. For the complex number a + bi, a is called the real part, and b is called the imaginary part. Despite the historical nomenclature "imaginary", complex numbers are regarded in the mathematical sciences as just as "real" as the real numbers, and are fundamental in many aspects of the scientific description of the natural world.[1][2]
Diagonalizable matrix
In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schrödinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schrödinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.thereby explaining the above phenomenon.The preceding relations, expressed in matrix form, areSwitching back to the standard basis, we haveThus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have thatStraightforward calculations show thatwhere ei denotes the standard basis of Rn. The reverse change of basis is given byThe above phenomenon can be explained by diagonalizing M. To accomplish this, we need a basis of R2 consisting of eigenvectors of M. One such eigenvector basis is given byCalculating the various powers of M reveals a surprising pattern:For example, consider the following matrix:This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found thatThen P diagonalizes A, as a simple computation confirms, having calculated P −1 using any suitable method:Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]Now, let P be the matrix with these eigenvectors as its columns:The eigenvectors of A areThese eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.A is a 3×3 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n×n matrix then this matrix is diagonalizable.This matrix has eigenvaluesConsider a matrixNote that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.then Q−1BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle θ=3π/2The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q−1BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we takeSome real matrices are not diagonalizable over the reals. Consider for instance the matrixThis matrix is not diagonalizable: there is no matrix U such that U−1CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, considerIn general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to "do the best one can", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.are diagonalizable but not simultaneously diagonalizable because they do not commute.The set of all n×n diagonalisable matrices (over C) with n > 1 is not simultaneously diagonalisable. For instance, the matricesA set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P−1AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.[2]In practice, matrices are diagonalized numerically using computers. Many algorithms exist to accomplish this.When the matrix A is a Hermitian matrix (resp. symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of Cn (resp. Rn). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P−1 equals the conjugate transpose (resp. transpose) of P.So the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P−1 are the left eigenvectors of A.the above equation can be rewritten asthen:If a matrix A can be diagonalized, that is,The Jordan–Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each "block" is a one-by-one matrix.As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n×n matrices that are not diagonalizable over C, considered as a subset of Cn×n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.The following sufficient (but not necessary) condition is often useful.Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)The fundamental fact about diagonalizable maps and matrices is expressed by the following:Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling) — it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P−1AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T : V → V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.
Diagonal matrix
Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function–the values of the function at each point correspond to the diagonal entries of a matrix.In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates–in the language of operators, an integral transform–which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA∗ = A∗A then there exists a unitary matrix U such that UAU∗ is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV∗ is diagonal with positive entries.In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X−1AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.A symmetric diagonal matrix can be defined as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.Any square diagonal matrix is also a symmetric matrix.A square matrix is diagonal if and only if it is triangular and normal.The adjugate of a diagonal matrix is again diagonal.The determinant of diag(a1, ..., an) is the product a1...an.In other words, the eigenvalues of diag(λ1, ..., λn) are λ1, ..., λn with associated eigenvectors of e1, ..., en.Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i.In particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.The diagonal matrix diag(a1, ..., an) is invertible if and only if the entries a1, ..., an are all non-zero. In this case, we haveand for matrix multiplication,The operations of matrix addition and matrix multiplication are especially simple for symmetric diagonal matrices. Write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1, ..., an. Then, for addition, we haveThe scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.A square diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple λI of the identity matrix I. Its effect on a vector is scalar multiplication by λ. For example, a 3×3 scalar matrix has the form:In the remainder of this article we will consider only square matrices.If the entries are real numbers or complex numbers, then it is a normal matrix as well.The following matrix is a symmetric diagonal matrix:The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. For example:However, the main diagonal entries need not be zero.As stated above, the off-diagonal entries are zero. That is, the matrix D = (di,j) with n columns and n rows is diagonal if
Inner product space
As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the "outer (alternatively, wedge) product". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.More abstractly, the outer product is the bilinear map W × V∗ → Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V∗ × V → F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V → V∗) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 × n covector with an n × 1 vector, yielding a 1 × 1 matrix (a scalar), while the outer product is the product of an m × 1 vector with a 1 × n covector, yielding an m × n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V → V∗) and thus hold more generally.Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that ⟨x,y⟩ ≠ 0, though y need not equal x; in other words, the induced map to the dual space V → V∗ is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "−" to them differs depending on conventions).This construction is used in numerous contexts. The Gelfand–Naimark–Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x : ||x|| = 0}. The sesquilinear form ⟨·,·⟩ factors through W.If V is a vector space and ⟨·,···⟩ a semi-definite sesquilinear form, then the function:Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.Several types of linear maps A from an inner product space V to an inner product space W are of relevance:Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [−π,π] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.Orthogonality of the sequence {ek}k follows immediately from the fact that if k ≠ j, thenis an isometric linear map with dense image.is an orthonormal basis of the space C[−π,π] with the L2 inner product. The mappingTheorem. Let V be the inner product space C[−π,π]. Then the sequence (indexed on set of all integers) of continuous functionsThis theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:is an isometric linear map V → l2 with a dense image.Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of V. Then the mapParseval's identity leads immediately to the following theorem:The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]Theorem. Any complete inner product space V has an orthonormal basis.Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show thatTheorem. Any separable inner product space V has an orthonormal basis.Using an infinite-dimensional analog of the Gram-Schmidt process one may show:if α ≠ β and ⟨eα,eα⟩ = ||eα|| = 1 for all α, β ∈ A.is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis andThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collectionLet V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram–Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if ⟨ei,ej⟩ = 0 for every i ≠ j and ⟨ei,ei⟩ = ||ei|| = 1 for each i.This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]A linear space with a norm such as:is an inner product.For real matrices of the same size, ⟨A,B⟩ := tr(ABT) with transpose as conjugationis an inner product.[6][7][8] In this case, ⟨X,X⟩ = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.For real random variables X and Y, the expected value of their productThis sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.This space is not complete; consider for example, for the interval [−1,1] the sequence of continuous "step" functions, { fk}k, defined by:The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product iswhere M is any Hermitian positive-definite matrix and y† is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.The general form of an inner product on Cn is known as the Hermitian form and is given bywhere xT is the transpose of x.More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.A simple example is the real numbers with the standard multiplication as the inner productis also known as additivity.The property of an inner product space V thatAssuming the underlying field to be R, the inner product becomes symmetric, and we obtainCombining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:From the linearity property it is derived that x = 0 implies ⟨x,x⟩ = 0. while from the positive-definiteness axiom we obtain the converse, ⟨x,x⟩ = 0 implies x = 0. Combining these two, we have the property that ⟨x,x⟩ = 0 if and only if x = 0.In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.Conjugate symmetry and linearity in the first variable givesMoreover, sesquilinearity (see below) implies thatNotice that conjugate symmetry implies that ⟨x,x⟩ is real for all x, since we have:When F = R, conjugate symmetry reduces to symmetry. That is, ⟨x,y⟩ = ⟨y,x⟩ for F = R; while for F = C, ⟨x,y⟩ is equal to the complex conjugate.In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that ⟨x,x⟩ is only required to be non-negative. We show how to treat these below.There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product ⟨x,y⟩ as ⟨y|x⟩ (the bra–ket notation of quantum mechanics), respectively y†x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V∗, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking ⟨x,y⟩ to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both ⟨·,·⟩ and ⟨·|·⟩ as distinct notations differing only in which argument is conjugate linear.that satisfies the following three axioms for all vectors x, y, z ∈ V and all scalars a ∈ F:[2][3]Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a mapIn this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]
Inner product space
As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the "outer (alternatively, wedge) product". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.More abstractly, the outer product is the bilinear map W × V∗ → Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V∗ × V → F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V → V∗) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 × n covector with an n × 1 vector, yielding a 1 × 1 matrix (a scalar), while the outer product is the product of an m × 1 vector with a 1 × n covector, yielding an m × n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V → V∗) and thus hold more generally.Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that ⟨x,y⟩ ≠ 0, though y need not equal x; in other words, the induced map to the dual space V → V∗ is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "−" to them differs depending on conventions).This construction is used in numerous contexts. The Gelfand–Naimark–Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x : ||x|| = 0}. The sesquilinear form ⟨·,·⟩ factors through W.If V is a vector space and ⟨·,···⟩ a semi-definite sesquilinear form, then the function:Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.Several types of linear maps A from an inner product space V to an inner product space W are of relevance:Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [−π,π] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.Orthogonality of the sequence {ek}k follows immediately from the fact that if k ≠ j, thenis an isometric linear map with dense image.is an orthonormal basis of the space C[−π,π] with the L2 inner product. The mappingTheorem. Let V be the inner product space C[−π,π]. Then the sequence (indexed on set of all integers) of continuous functionsThis theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:is an isometric linear map V → l2 with a dense image.Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of V. Then the mapParseval's identity leads immediately to the following theorem:The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]Theorem. Any complete inner product space V has an orthonormal basis.Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show thatTheorem. Any separable inner product space V has an orthonormal basis.Using an infinite-dimensional analog of the Gram-Schmidt process one may show:if α ≠ β and ⟨eα,eα⟩ = ||eα|| = 1 for all α, β ∈ A.is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis andThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collectionLet V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram–Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if ⟨ei,ej⟩ = 0 for every i ≠ j and ⟨ei,ei⟩ = ||ei|| = 1 for each i.This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]A linear space with a norm such as:is an inner product.For real matrices of the same size, ⟨A,B⟩ := tr(ABT) with transpose as conjugationis an inner product.[6][7][8] In this case, ⟨X,X⟩ = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.For real random variables X and Y, the expected value of their productThis sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.This space is not complete; consider for example, for the interval [−1,1] the sequence of continuous "step" functions, { fk}k, defined by:The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product iswhere M is any Hermitian positive-definite matrix and y† is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.The general form of an inner product on Cn is known as the Hermitian form and is given bywhere xT is the transpose of x.More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.A simple example is the real numbers with the standard multiplication as the inner productis also known as additivity.The property of an inner product space V thatAssuming the underlying field to be R, the inner product becomes symmetric, and we obtainCombining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:From the linearity property it is derived that x = 0 implies ⟨x,x⟩ = 0. while from the positive-definiteness axiom we obtain the converse, ⟨x,x⟩ = 0 implies x = 0. Combining these two, we have the property that ⟨x,x⟩ = 0 if and only if x = 0.In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.Conjugate symmetry and linearity in the first variable givesMoreover, sesquilinearity (see below) implies thatNotice that conjugate symmetry implies that ⟨x,x⟩ is real for all x, since we have:When F = R, conjugate symmetry reduces to symmetry. That is, ⟨x,y⟩ = ⟨y,x⟩ for F = R; while for F = C, ⟨x,y⟩ is equal to the complex conjugate.In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that ⟨x,x⟩ is only required to be non-negative. We show how to treat these below.There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product ⟨x,y⟩ as ⟨y|x⟩ (the bra–ket notation of quantum mechanics), respectively y†x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V∗, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking ⟨x,y⟩ to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both ⟨·,·⟩ and ⟨·|·⟩ as distinct notations differing only in which argument is conjugate linear.that satisfies the following three axioms for all vectors x, y, z ∈ V and all scalars a ∈ F:[2][3]Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a mapIn this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]
Bilinear form
A linear map S : M∗ → M∗ : u ↦ S(u) induces the bilinear form B : M∗ × M → R : (u, x) ↦ ⟨S(u), x⟩, and a linear map T : M → M : x ↦ T(x) induces the bilinear form B : M∗ × M → R : (u, x) ↦ ⟨u, T(x))⟩. Conversely, a bilinear form B : M∗ × M → R induces the R-linear maps S : M∗ → M∗ : u ↦ (x ↦ B(u, x)) and T′ : M → M∗∗ : x ↦ (u ↦ B(u, x)). Here, M∗∗ denotes the double dual of M.The mapping ⟨⋅,⋅⟩ : M∗ × M → R : (u, x) ↦ u(x) is known as the natural pairing, also called the canonical bilinear form on M∗ × M.[7]for all u, v ∈ M∗, x, y ∈ M, α, β ∈ R.Given a ring R and a right R-module M and its dual module M∗, a mapping B : M∗ × M → R is called a bilinear form ifLikewise, symmetric bilinear forms may be thought of as elements of Sym2(V∗) (the second symmetric power of V∗), and alternating bilinear forms as elements of Λ2V∗ (the second exterior power of V∗).The set of all linear maps V ⊗ V → K is the dual space of V ⊗ V, so bilinear forms may be thought of as elements ofBy the universal property of the tensor product, bilinear forms on V are in 1-to-1 correspondence with linear maps V ⊗ V → K. If B is a bilinear form on V the corresponding linear map is given byis called the real symmetric case and labeled R(p, q), where p + q = n. Then he articulates the connection to traditional terminology:[6]Terminology varies in coverage of bilinear forms. For example, F. Reese Harvey discusses "eight types of inner product".[5] To define them he uses diagonal matrices Aij having only +1 or −1 for non-zero elements. Some of the "inner products" are symplectic forms and some are sesquilinear forms or Hermitian forms. Rather than a general field K, the instances with real numbers R, complex numbers C, and quaternions H are spelled out. The bilinear formIn finite dimensions, this is equivalent to the pairing being nondegenerate (the spaces necessarily having the same dimensions). For modules (instead of vector spaces), just as how a nondegenerate form is weaker than a unimodular form, a nondegenerate pairing is a weaker notion than a perfect pairing. A pairing can be nondegenerate without being a perfect pairing, for instance Z × Z → Z via (x,y) ↦ 2xy is nondegenerate, but induces multiplication by 2 on the map Z → Z∗.Here we still have induced linear mappings from V to W∗, and from W to V∗. It may happen that these mappings are isomorphisms; assuming finite dimensions, if one is an isomorphism, the other must be. When this occurs, B is said to be a perfect pairing.Much of the theory is available for a bilinear mapping from two vector spaces over the same base field to that fieldFor a non-degenerate form on a finite dimensional space, the map V/W → W⊥ is bijective, and the dimension of W⊥ is dim(V) − dim(W).Suppose W is a subspace. Define the orthogonal complement[4]A bilinear form B is reflexive if and only if it is either symmetric or alternating.[3] In the absence of reflexivity we have to distinguish left and right orthogonality. In a reflexive space the left and right radicals agree and are termed the kernel or the radical of the bilinear form: the subspace of all vectors orthogonal with every other vector. A vector v, with matrix representation x, is in the radical of a bilinear form with matrix representation A, if and only if Ax = 0 ⇔ xTA = 0. The radical is always a subspace of V. It is trivial if and only if the matrix A is nonsingular, and thus if and only if the bilinear form is nondegenerate.When char(K) = 2 and dim V > 1, this correspondence between quadratic forms and symmetric bilinear forms breaks down.When char(K) ≠ 2, the quadratic form Q is determined by the symmetric part of the bilinear form B and is independent of the antisymmetric part. In this case there is a one-to-one correspondence between the symmetric part of the bilinear form and the quadratic form, and it makes sense to speak of the symmetric bilinear form associated with a quadratic form.For any bilinear form B : V × V → K, there exists an associated quadratic form Q : V → K defined by Q : V → K : v ↦ B(v, v).where tB is the transpose of B (defined above).A bilinear form is symmetric if and only if the maps B1, B2: V → V∗ are equal, and skew-symmetric if and only if they are negatives of one another. If char(K) ≠ 2 then one can decompose a bilinear form into a symmetric and a skew-symmetric part as followsA bilinear form is symmetric (resp. skew-symmetric) if and only if its coordinate matrix (relative to any basis) is symmetric (resp. skew-symmetric). A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero (which follows from skew-symmetry when char(K) ≠ 2).If the characteristic of K is not 2 then the converse is also true: every skew-symmetric form is alternating. If, however, char(K) = 2 then a skew-symmetric form is the same as a symmetric form and there exist symmetric/skew-symmetric forms that are not alternating.We define a bilinear form to beIf V is finite-dimensional then, relative to some basis for V, a bilinear form is degenerate if and only if the determinant of the associated matrix is zero. Likewise, a nondegenerate form is one for which the determinant of the associated matrix is non-zero (the matrix is non-singular). These statements are independent of the chosen basis. For a module over a commutative ring, a unimodular form is one for which the determinant of the associate matrix is a unit (for example 1), hence the term; note that a form whose matrix is non-zero but not a unit will be nondegenerate but not unimodular, for example B(x, y) = 2xy over the integers.This form will be nondegenerate if and only if A is an isomorphism.Given any linear map A : V → V∗ one can obtain a bilinear form B on V viaIf V is finite-dimensional then the rank of B1 is equal to the rank of B2. If this number is equal to dim(V) then B1 and B2 are linear isomorphisms from V to V∗. In this case B is nondegenerate. By the rank–nullity theorem, this is equivalent to the condition that the left and equivalently right radicals be trivial. For finite-dimensional spaces, this is often taken as the definition of nondegeneracy:The left radical and right radical of the form B are the kernels of B1 and B2 respectively;[1] they are the vectors orthogonal to the whole space on the left and on the right.[2]If V is finite-dimensional then one can identify V with its double dual V∗∗. One can then show that B2 is the transpose of the linear map B1 (if V is infinite-dimensional then B2 is the transpose of B1 restricted to the image of V in V∗∗). Given B one can define the transpose of B to be the bilinear form given byThe corresponding notion for a module over a commutative ring is that a bilinear form is unimodular if V → V∗ is an isomorphism. Given a finitely generated module over a commutative ring, the pairing may be injective (hence "nondegenerate" in the above sense) but not unimodular. For example, over the integers, the pairing B(x, y) = 2xy is nondegenerate but not unimodular, as the induced map from V = Z to V∗ = Z is multiplication by 2.For a finite-dimensional vector space V, if either of B1 or B2 is an isomorphism, then both are, and the bilinear form B is said to be nondegenerate. More concretely, for a finite-dimensional vector space, non-degenerate means that every non-zero element pairs non-trivially with some other element:where the dot ( ⋅ ) indicates the slot into which the argument for the resulting linear functional is to be placed (see Currying).This is often denoted asEvery bilinear form B on V defines a pair of linear maps from V to its dual space V∗. Define B1, B2: V → V∗ byNow the new matrix representation for the bilinear form is given by: STAS.where S ∈ GL(n, K).Suppose {f1, ..., fn} is another basis for V, such that:If the n × 1 matrix x represents a vector v with respect to this basis, and analogously, y represents w, then:Define the n × n matrix A by Aij = B(ei, ej).Let V ≅ Kn be an n-dimensional vector space with basis {e1, ..., en}.When K is the field of complex numbers C, one is often more interested in sesquilinear forms, which are similar to bilinear forms but are conjugate linear in one argument.The definition of a bilinear form can be extended to include modules over a ring, with linear maps replaced by module homomorphisms.In mathematics, more specifically in abstract algebra and linear algebra, a bilinear form on a vector space V is a bilinear map V × V → K, where K is the field of scalars. In other words, a bilinear form is a function B : V × V → K that is linear in each argument separately:
Axiom
Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.There is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms. The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.The objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.The Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.[12]This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.[citation needed]This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.Non-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought[citation needed] that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not tautologies. Another name for a non-logical axiom is postulate.[11]Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.[10]Other axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed.[9]These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms at least some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.In the field of mathematical logic, a clear distinction is made between two notions of axioms: logical and non-logical (somewhat similar to the ancient distinction between "axioms" and "postulates" respectively).Regardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither "proves" nor "disproves" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.As a consequence, it is not necessary to explicitly cite Einstein's axioms, the more so since they concern subtle points on the "reality" and "locality" of experiments.Another paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of "theorems" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics "hidden variables" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein's axioms compared to using Bohr's axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr's axioms, not Einstein's. (Bohr's axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)In 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity.Axioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment.It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.The formalist project suffered a decisive setback, when in 1931 Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of naïve set theory raised the possibility that any such system could turn out to be inconsistent.It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms.In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development.It is not correct to say that the axioms of field theory are "propositions that are regarded as true without proof." Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without any particular application in mind. The distinction between an "axiom" and a "postulate" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like "line" and "parallel" with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience.A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.The classical approach is well-illustrated by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a postulate. While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.[8]An "axiom", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion thatThe ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that, "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property."[7] Boethius translated 'postulate' as petitio and called the axioms notiones communes but in later manuscripts this usage was not always strictly kept.The root meaning of the word postulate is to "demand"; for instance, Euclid demands that one agree that some things can be done, e.g. any two points can be joined by a straight line, etc.[6]The word axiom comes from the Greek word ἀξίωμα (axíōma), a verbal noun from the verb ἀξιόειν (axioein), meaning "to deem worthy", but also "to require", which in turn comes from ἄξιος (áxios), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof.In both senses, an axiom is any mathematical statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom, or any mathematical statement, to be "true" is an open question[citation needed] in the philosophy of mathematics.[5]As used in mathematics, the term axiom is used in two related but distinguishable senses: "logical axioms" and "non-logical axioms". Logical axioms are usually statements that are taken to be true within the system of logic they define (e.g., (A and B) implies A), often shown in symbolic form, while non-logical axioms (e.g., a + b = b + a) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, "axiom", "postulate", and "assumption" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain.The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] As used in modern logic, an axiom is simply a premise or starting point for reasoning.[4]An axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek axíōma (ἀξίωμα) 'that which is thought worthy or fit' or 'that which commends itself as evident.'[1][2]
Complex conjugate
Since the multiplication of planar real algebras is commutative, this reversal is not needed there.Note that all these generalizations are multiplicative only if the factors are reversed:Taking the conjugate transpose (or adjoint) of complex matrices generalizes complex conjugation. Even more general is the concept of adjoint operator for operators on (possibly infinite-dimensional) complex Hilbert spaces. All this is subsumed by the *-operations of C*-algebras.The other planar real algebras, dual numbers, and split-complex numbers are also analyzed using complex conjugation.These uses of the conjugate of z as a variable are illustrated in Frank Morley's book Inversive Geometry (1933), written with his son Frank Vigor Morley.The penultimate relation is involution; i.e., the conjugate of the conjugate of a complex number z is z. The ultimate relation is the method of choice to compute the inverse of a complex number if it is given in rectangular coordinates.For any two complex numbers w,z:A significant property of the complex conjugate is that a complex number is equal to its complex conjugate if its imaginary part is zero, that is, if the complex number is real.The following properties apply for all complex numbers z and w, unless stated otherwise, and can be proved by writing z and w in the form a + ib.Complex conjugates are important for finding roots of polynomials. According to the complex conjugate root theorem, if a complex number is a root to a polynomial in one variable with real coefficients (such as the quadratic equation or the cubic equation), so is its conjugate.In mathematics, the complex conjugate of a complex number is the number with an equal real part and an imaginary part equal in magnitude but opposite in sign.[1][2] For example, the complex conjugate of 3 + 4i is 3 − 4i.
Linearity
In measurement, the term "linear foot" refers to the number of feet in a straight line of material (such as lumber or fabric) generally without regard to the width. It is sometimes incorrectly referred to as "lineal feet"; however, "lineal" is typically reserved for usage when referring to ancestry or heredity.[1] The words "linear"[2] & "lineal" [3] both descend from the same root meaning, the Latin word for line, which is "linea".In music the linear aspect is succession, either intervals or melody, as opposed to simultaneity or the vertical aspect.Linear is one of the five categories proposed by Swiss art historian Heinrich Wölfflin to distinguish "Classic", or Renaissance art, from the Baroque. According to Wölfflin, painters of the fifteenth and early sixteenth centuries (Leonardo da Vinci, Raphael or Albrecht Dürer) are more linear than "painterly" Baroque painters of the seventeenth century (Peter Paul Rubens, Rembrandt, and Velázquez) because they primarily use outline to create shape.[7] Linearity in art can also be referenced in digital art. For example, hypertext fiction can be an example of nonlinear narrative, but there are also websites designed to go in a specified, organized manner, following a linear path.In military tactical formations, "linear formations" were adapted from phalanx-like formations of pike protected by handgunners towards shallow formations of handgunners protected by progressively fewer pikes. This kind of formation would get thinner until its extreme in the age of Wellington with the 'Thin Red Line'. It would eventually be replaced by skirmish order at the time of the invention of the breech-loading rifle that allowed soldiers to move and fire independently of the large-scale formations and fight in small, mobile units.For an electronic device (or other physical device) that converts a quantity to another quantity, Bertram S. Kolts writes:[5][6]In most scientific and technological, as distinct from mathematical, applications, something may be described as linear if the characteristic is approximately but not exactly a straight line; and linearity may be valid only within a certain operating region—for example, a high-fidelity amplifier may distort a small signal, but sufficiently little to be acceptable (acceptable but imperfect linearity); and may distort very badly if the input exceeds a certain value, taking it away from the approximately linear part of the transfer function.[4]In electronics, the linear operating region of a device, for example a transistor, is where a dependent variable (such as the transistor collector current) is directly proportional to an independent variable (such as the base current). This ensures that an analog output is an accurate representation of an input, typically with higher amplitude (amplified). A typical example of linear equipment is a high fidelity audio amplifier, which must amplify a signal without changing its waveform. Others are linear filters, linear regulators, and linear amplifiers in general.In instrumentation, linearity means that for every change in the variable you are observing, you get the same change in the output of the measurement apparatus - this is highly desirable in scientific work. In general, instruments are close to linear over a useful certain range, and most useful within that range. In contrast, human senses are highly nonlinear- for instance, the brain totally ignores incoming light unless it exceeds a certain absolute threshold number of photons.Linearity of a differential equation means that if two functions f and g are solutions of the equation, then any linear combination af + bg is, too.In physics, linearity is a property of the differential equations governing many systems; for instance, the Maxwell equations or the diffusion equation.[3]Negation, Logical biconditional, exclusive or, tautology, and contradiction are linear functions.Another way to express this is that each variable always makes a difference in the truth value of the operation or it never makes a difference.A Boolean function is linear if one of the following holds for the function's truth table:Note that this usage of the term linear is not the same as in the section above, because linear polynomials over the real numbers do not in general satisfy either additivity or homogeneity. In fact, they do so if and only if b = 0. Hence, if b ≠ 0, the function is often called an affine function (see in greater generality affine transformation).where m is often called the slope or gradient; b the y-intercept, which gives the point of intersection between the graph of the function and the y-axis.Over the reals, a linear equation is one of the forms:In a different usage to the above definition, a polynomial of degree 1 is said to be linear, because the graph of a function of that form is a line.[2]The word linear comes from the Latin word linearis, which means pertaining to or resembling a line. For a description of linear and nonlinear equations, see linear equation. Nonlinear equations and functions are of interest to physicists and mathematicians because they can be used to represent many natural phenomena, including chaos.Linear algebra is the branch of mathematics concerned with the study of vectors, vector spaces (also called linear spaces), linear transformations (also called linear maps), and systems of linear equations.The concept of linearity can be extended to linear operators. Important examples of linear operators include the derivative considered as a differential operator, and many constructed from it, such as del and the Laplacian. When a differential equation can be expressed in linear form, it is generally straightforward to solve by breaking the equation up into smaller pieces, solving each of those pieces, and summing the solutions.In this definition, x is not necessarily a real number, but can in general be a member of any vector space. A more specific definition of linear function, not coinciding with the definition of linear map, is used in elementary mathematics.The homogeneity and additivity properties together are called the superposition principle. It can be shown that additivity implies homogeneity in all cases where α is rational; this is done by proving the case where α is a natural number by mathematical induction and then extending the result to arbitrary rational numbers. If f is assumed to be continuous as well, then this can be extended to show homogeneity for any real number α, using the fact that rationals form a dense subset of the reals.In mathematics, a linear map or linear function f(x) is a function that satisfies the following two properties:[1]Linearity is the property of a mathematical relationship or function which means that it can be graphically represented as a straight line. Examples are the relationship of voltage and current across a resistor (Ohm's law), or the mass and weight of an object. Proportionality implies linearity, but linearity does not imply proportionality.
Definite quadratic form
An important example of such an optimization arises in multiple regression, in which a vector of estimated parameters is sought which minimizes the sum of squared deviations from a perfect fit within the dataset.assuming A is nonsingular. If the quadratic form, and hence A, is positive definite, the second-order conditions for a minimum are met at this point. If the quadratic form is negative definite, the second-order conditions for a maximum are met.givingwhere b is an n×1 vector of constants. The first-order conditions for a maximum or minimum are found by setting the matrix derivative to the zero vector:Definite quadratic forms lend themselves readily to optimization problems. Suppose the matrix quadratic form is augmented with linear terms, asPositive or negative definiteness or semi-definiteness, or indefiniteness, of this quadratic form is equivalent to the same property of A, which can be checked by considering all eigenvalues of A or by checking the signs of all of its principal minors.A quadratic form can be written in terms of matrices asThe square of the Euclidean norm in n-dimensional space, the most commonly used measure of distance, isIn general a quadratic form in two variables will also involve a cross-product term in x1x2:Quadratic forms correspond one-to-one to symmetric bilinear forms over the same space.[2] A symmetric bilinear form is also described as definite, semidefinite, etc. according to its associated quadratic form. A quadratic form Q and its associated symmetric bilinear form B are related by the following equations:More generally, the definition applies to a vector space over an ordered field.[1]A semidefinite (or semi-definite) quadratic form is defined in the same way, except that "positive" and "negative" are replaced by "not negative" and "not positive", respectively. An indefinite quadratic form is one that takes on both positive and negative values.In mathematics, a definite quadratic form is a quadratic form over some real vector space V that has the same sign (always positive or always negative) for every nonzero vector of V. According to that sign, the quadratic form is called positive definite or negative definite.
Cauchy–Schwarz inequality
The next two theorems are further examples in operator algebra.which extends verbatim to positive functionals on C*-algebras:Various generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.then the Cauchy–Schwarz inequality becomesAfter defining an inner product on the set of random variables using the expectation of their product,Let X, Y be random variables, then the covariance inequality[13][14] is given byThe Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[11][12] as is done when extracting a metric from quantum fidelity.The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner-product space by defining:[9][10]The Cauchy–Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[7][8]Taking square roots gives the triangle inequality.The triangle inequality for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors x and y:A generalization of this is the Hölder inequality.For the inner product space of square-integrable complex-valued functions, one haswhich yields the Cauchy–Schwarz inequality.This establishes the theorem.which givesThen, by linearity of the inner product in its first argument, one hasLetorThe inequality for sums was published by Augustin-Louis Cauchy (1821), while the corresponding inequality for integrals was first proved by Viktor Bunyakovsky (1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz (1888).[1]In mathematics, the Cauchy–Schwarz inequality, also known as the Cauchy–Bunyakovsky–Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]
Gram–Schmidt process

Hermitian adjoint
is formally similar to the defining properties of pairs of adjoint functors in category theory, and this is where adjoint functors got their name from.The equationFor an antilinear operator the definition of adjoint needs to be adjusted in order to compensate for the complex conjugation. An adjoint operator of the antilinear operator A on a complex Hilbert space H is an antilinear operator A∗ : H → H with the property:In some sense, these operators play the role of the real numbers (being equal to their own "complex conjugate") and form a real vector space. They serve as the model of real-valued observables in quantum mechanics. See the article on self-adjoint operators for a full treatment.which is equivalent toA bounded operator A : H → H is called Hermitian or self-adjoint ifThe second equation follows from the first by taking the orthogonal complement on both sides. Note that in general, the image need not be closed, but the kernel of a continuous operator[7] always is.[clarification needed]Proof of the first equation:[6][clarification needed]The relationship between the image of A and the kernel of its adjoint is given by:Properties 1.–5. hold with appropriate clauses about domains and codomains.[clarification needed] For instance, the last property now states that (AB)∗ is an extension of B∗A∗ if A, B and AB are densely defined operators.[5]and A∗(y) is defined to be the z thus found.[4]A densely defined operator A from a complex Hilbert space H to itself is a linear operator whose domain D(A) is a dense linear subspace of H and whose values lies in H.[3] By definition, the domain D(A∗) of its adjoint A∗ is the set of all y ∈ H for which there is a z ∈ H satisfyingThe set of bounded linear operators on a complex Hilbert space H together with the adjoint operation and the operator norm form the prototype of a C*-algebra.One says that a norm that satisfies this condition behaves like a "largest value", extrapolating from the case of self-adjoint operators.Moreover,thenIf we define the operator norm of A byThe following properties of the Hermitian adjoint of bounded operators are immediate:[2]This can be seen as a generalization of the adjoint matrix of a square matrix which has a similar property involving the standard complex inner product.Existence and uniqueness of this operator follows from the Riesz representation theorem.[2]The fundamental defining identity is thusThe adjoint of an operator A may also be called the Hermitian adjoint, Hermitian conjugate or Hermitian transpose[1] (after Charles Hermite) of A and is denoted by A∗ or A† (the latter especially when used in conjunction with the bra–ket notation).In a similar sense there can be defined an adjoint operator for linear (and possibly unbounded) operators between Banach spaces.In mathematics, specifically in functional analysis, each bounded linear operator on a complex Hilbert space has a corresponding adjoint operator. Adjoints of operators generalize conjugate transposes of square matrices to (possibly) infinite-dimensional situations. If one thinks of operators on a complex Hilbert space as "generalized complex numbers", then the adjoint of an operator plays the role of the complex conjugate of a complex number.
Normal matrix

Linear map
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank–nullity theorem:If f : V → W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n × n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol ∗ denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f : V → W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.
Isomorphism
In the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects X and Y, however, one asks if they are equal or not (they are both elements of the set Hom(X, Y), hence equality is the proper relationship), particularly in commutative diagrams.are three different descriptions for a mathematical object, all of which are isomorphic, but not equal because they are not all subsets of a single space: the first is a subset of R3, the second is C ≅ R2[note 4] plus an additional point, and the third is a subquotient of C2which can be presented as the one-point compactification of the complex plane C ∪ {∞} or as the complex projective line (a quotient space)Generally, saying that two objects are equal is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional spaceIf one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write ≈ for an unnatural isomorphism and ≅ for a natural isomorphism, as in V ≈ V* and V ≅ V**. This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of "the set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.This corresponds to transforming a column vector (element of V) to a row vector (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis". More subtly, there is a map from a vector space V to its double dual V** = { x: V* → K} that does not depend on the choice of basis: For all v ∈ V and φ ∈ V*,Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word isomorphism (Greek iso-, "same," and -morph, "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.and no one isomorphism is intrinsically better than any other.[note 2][note 3] On this view and in this sense, these two sets are not equal because one cannot consider them identical: one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.are equal; they are merely different presentations—the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {A,B,C} and {1,2,3} are not equal—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism isIn certain areas of mathematics, notably category theory, it is valuable to distinguish between equality on the one hand and isomorphism on the other.[3] Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the setsIn cybernetics, the good regulator or Conant–Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's Introduction to Mathematical Philosophy.In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.In graph theory, an isomorphism between two graphs G and H is a bijective map f from the vertices of G to the vertices of H that preserves the "edge structure" in the sense that there is an edge from vertex u to vertex v in G if and only if there is an edge from ƒ(u) to ƒ(v) in H. See graph isomorphism.In category theory, let the category C consist of two classes, one of objects and the other of morphisms. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism ƒ: a → b that has an inverse, i.e. there exists a morphism g: b → a with ƒg = 1b and gƒ = 1a. For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.In abstract algebra, two basic isomorphisms are defined:In a concrete category (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets. In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets. However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).If X = Y, then this is a relation-preserving automorphism.Such an isomorphism is called an order isomorphism or (less commonly) an isotone isomorphism.S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.If one object consists of a set X with a binary relation R and the other object consists of a set Y with a binary relation S then an isomorphism from X to Y is a bijective function ƒ: X → Y such that:[2]For example, (1,1) + (1,0) = (0,1), which translates in the other system as 1 + 3 = 4.or in general (a,b) ↦ (3a + 4b) mod 6.These structures are isomorphic under addition, under the following scheme:Isomorphisms are formalized using category theory. A morphism f : X → Y in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism g : Y → X in that category such that gf = 1X and fg = 1Y, where 1X and 1Y are the identity morphisms of X and Y, respectively.[1]A canonical isomorphism is a canonical map that is an isomorphism. Two objects are said to be canonically isomorphic if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space V to its second dual space is a canonical isomorphism; on the other hand, V is isomorphic to its dual space but not canonically in general.In topology, where the morphisms are continuous functions, isomorphisms are also called homeomorphisms or bicontinuous functions. In mathematical analysis, where the morphisms are differentiable functions, isomorphisms are also called diffeomorphisms.For most algebraic structures, including groups and rings, a homomorphism is an isomorphism if and only if it is bijective.In mathematics, an isomorphism (from the Ancient Greek: ἴσος isos "equal", and μορφή morphe "form" or "shape") is a homomorphism or morphism (i.e. a mathematical mapping) that admits an inverse.[note 1] Two mathematical objects are isomorphic if an isomorphism exists between them. An automorphism is an isomorphism whose source and target coincide. The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.
Isomorphism
In the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects X and Y, however, one asks if they are equal or not (they are both elements of the set Hom(X, Y), hence equality is the proper relationship), particularly in commutative diagrams.are three different descriptions for a mathematical object, all of which are isomorphic, but not equal because they are not all subsets of a single space: the first is a subset of R3, the second is C ≅ R2[note 4] plus an additional point, and the third is a subquotient of C2which can be presented as the one-point compactification of the complex plane C ∪ {∞} or as the complex projective line (a quotient space)Generally, saying that two objects are equal is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional spaceIf one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write ≈ for an unnatural isomorphism and ≅ for a natural isomorphism, as in V ≈ V* and V ≅ V**. This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of "the set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.This corresponds to transforming a column vector (element of V) to a row vector (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis". More subtly, there is a map from a vector space V to its double dual V** = { x: V* → K} that does not depend on the choice of basis: For all v ∈ V and φ ∈ V*,Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word isomorphism (Greek iso-, "same," and -morph, "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.and no one isomorphism is intrinsically better than any other.[note 2][note 3] On this view and in this sense, these two sets are not equal because one cannot consider them identical: one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.are equal; they are merely different presentations—the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {A,B,C} and {1,2,3} are not equal—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism isIn certain areas of mathematics, notably category theory, it is valuable to distinguish between equality on the one hand and isomorphism on the other.[3] Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the setsIn cybernetics, the good regulator or Conant–Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's Introduction to Mathematical Philosophy.In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.In graph theory, an isomorphism between two graphs G and H is a bijective map f from the vertices of G to the vertices of H that preserves the "edge structure" in the sense that there is an edge from vertex u to vertex v in G if and only if there is an edge from ƒ(u) to ƒ(v) in H. See graph isomorphism.In category theory, let the category C consist of two classes, one of objects and the other of morphisms. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism ƒ: a → b that has an inverse, i.e. there exists a morphism g: b → a with ƒg = 1b and gƒ = 1a. For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.In abstract algebra, two basic isomorphisms are defined:In a concrete category (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets. In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets. However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).If X = Y, then this is a relation-preserving automorphism.Such an isomorphism is called an order isomorphism or (less commonly) an isotone isomorphism.S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.If one object consists of a set X with a binary relation R and the other object consists of a set Y with a binary relation S then an isomorphism from X to Y is a bijective function ƒ: X → Y such that:[2]For example, (1,1) + (1,0) = (0,1), which translates in the other system as 1 + 3 = 4.or in general (a,b) ↦ (3a + 4b) mod 6.These structures are isomorphic under addition, under the following scheme:Isomorphisms are formalized using category theory. A morphism f : X → Y in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism g : Y → X in that category such that gf = 1X and fg = 1Y, where 1X and 1Y are the identity morphisms of X and Y, respectively.[1]A canonical isomorphism is a canonical map that is an isomorphism. Two objects are said to be canonically isomorphic if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space V to its second dual space is a canonical isomorphism; on the other hand, V is isomorphic to its dual space but not canonically in general.In topology, where the morphisms are continuous functions, isomorphisms are also called homeomorphisms or bicontinuous functions. In mathematical analysis, where the morphisms are differentiable functions, isomorphisms are also called diffeomorphisms.For most algebraic structures, including groups and rings, a homomorphism is an isomorphism if and only if it is bijective.In mathematics, an isomorphism (from the Ancient Greek: ἴσος isos "equal", and μορφή morphe "form" or "shape") is a homomorphism or morphism (i.e. a mathematical mapping) that admits an inverse.[note 1] Two mathematical objects are isomorphic if an isomorphism exists between them. An automorphism is an isomorphism whose source and target coincide. The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.
Isomorphism
In the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects X and Y, however, one asks if they are equal or not (they are both elements of the set Hom(X, Y), hence equality is the proper relationship), particularly in commutative diagrams.are three different descriptions for a mathematical object, all of which are isomorphic, but not equal because they are not all subsets of a single space: the first is a subset of R3, the second is C ≅ R2[note 4] plus an additional point, and the third is a subquotient of C2which can be presented as the one-point compactification of the complex plane C ∪ {∞} or as the complex projective line (a quotient space)Generally, saying that two objects are equal is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional spaceIf one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write ≈ for an unnatural isomorphism and ≅ for a natural isomorphism, as in V ≈ V* and V ≅ V**. This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of "the set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.This corresponds to transforming a column vector (element of V) to a row vector (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis". More subtly, there is a map from a vector space V to its double dual V** = { x: V* → K} that does not depend on the choice of basis: For all v ∈ V and φ ∈ V*,Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word isomorphism (Greek iso-, "same," and -morph, "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.and no one isomorphism is intrinsically better than any other.[note 2][note 3] On this view and in this sense, these two sets are not equal because one cannot consider them identical: one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.are equal; they are merely different presentations—the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {A,B,C} and {1,2,3} are not equal—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism isIn certain areas of mathematics, notably category theory, it is valuable to distinguish between equality on the one hand and isomorphism on the other.[3] Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the setsIn cybernetics, the good regulator or Conant–Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's Introduction to Mathematical Philosophy.In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.In graph theory, an isomorphism between two graphs G and H is a bijective map f from the vertices of G to the vertices of H that preserves the "edge structure" in the sense that there is an edge from vertex u to vertex v in G if and only if there is an edge from ƒ(u) to ƒ(v) in H. See graph isomorphism.In category theory, let the category C consist of two classes, one of objects and the other of morphisms. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism ƒ: a → b that has an inverse, i.e. there exists a morphism g: b → a with ƒg = 1b and gƒ = 1a. For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.In abstract algebra, two basic isomorphisms are defined:In a concrete category (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets. In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets. However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).If X = Y, then this is a relation-preserving automorphism.Such an isomorphism is called an order isomorphism or (less commonly) an isotone isomorphism.S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.If one object consists of a set X with a binary relation R and the other object consists of a set Y with a binary relation S then an isomorphism from X to Y is a bijective function ƒ: X → Y such that:[2]For example, (1,1) + (1,0) = (0,1), which translates in the other system as 1 + 3 = 4.or in general (a,b) ↦ (3a + 4b) mod 6.These structures are isomorphic under addition, under the following scheme:Isomorphisms are formalized using category theory. A morphism f : X → Y in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism g : Y → X in that category such that gf = 1X and fg = 1Y, where 1X and 1Y are the identity morphisms of X and Y, respectively.[1]A canonical isomorphism is a canonical map that is an isomorphism. Two objects are said to be canonically isomorphic if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space V to its second dual space is a canonical isomorphism; on the other hand, V is isomorphic to its dual space but not canonically in general.In topology, where the morphisms are continuous functions, isomorphisms are also called homeomorphisms or bicontinuous functions. In mathematical analysis, where the morphisms are differentiable functions, isomorphisms are also called diffeomorphisms.For most algebraic structures, including groups and rings, a homomorphism is an isomorphism if and only if it is bijective.In mathematics, an isomorphism (from the Ancient Greek: ἴσος isos "equal", and μορφή morphe "form" or "shape") is a homomorphism or morphism (i.e. a mathematical mapping) that admits an inverse.[note 1] Two mathematical objects are isomorphic if an isomorphism exists between them. An automorphism is an isomorphism whose source and target coincide. The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.
Determinant
where ωj is an nth root of 1.where ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]Third orderSecond orderwhere the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.In general, the nth-order Vandermonde determinant is[33]The third order Vandermonde determinant isThe Jacobian also occurs in the inverse function theorem.Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given bythe Jacobian matrix is the n × n matrix whose entries are given byFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. ForBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Another infinite-dimensional notion of determinant is the functional determinant.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,holds. In other words, the following diagram commutes:between the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityThe determinant defines a mappingThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.from the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isThe determinant can also be characterized as the unique functionThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn). As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple. We call this scalar the determinant of T.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.As ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThe determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant is therefore also called a similarity invariant. The determinant of a linear transformationThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThis identity is used in describing the tangent space of certain matrix Lie groups.Yet another equivalent formulation isExpressed in terms of the entries of A, these arewhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveBy definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]When D is a 1×1 matrix, B is a column vector, and C is a row vector thenWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]as can be seen by employing the decompositionWhen A is invertible, one hasThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenIt has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.the solution is given by Cramer's rule:For a matrix equationThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.Also,with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantis expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).where I is the identity matrix. More generally, ifAn important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way asThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l – 1)! tr(Al) aswhere the sum is taken over the set of all integers kl ≥ 0 satisfying the equationIn the general case, this may also be obtained from[9]cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromFor example, for n = 2, n = 3, and n = 4, respectively,the determinant of A is given byHere exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingor, for real matrices A,The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,being positive, for all k between 1 and n.A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatriceswhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationThe product of all non-zero eigenvalues is referred to as pseudo-determinant.From this general result several consequences follow.where Im and In are the m × m and n × n identity matrices, respectively.Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):In terms of the adjugate matrix, Laplace's expansion can be written as[7]The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,However, Laplace expansion is efficient for small matrices only.along the second column (j = 2 and the sum runs over i) is given by,Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i+jMi,j is known as a cofactor. The determinant of A is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant of a matrix product of square matrices equals the product of their determinants:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.can be computed using the following matrices:For example, the determinant ofProperty 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.Property 2 above implies that properties for columns have their counterparts in terms of rows:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.The determinant has many properties. Some basic properties of determinants arewhere now each ir and each jr should be summed over 1, …, n.or using two epsilon symbols asFor example, the determinant of a 3 × 3 matrix A (n = 3) isis notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:Here the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.The Leibniz formula for the determinant of an n × n matrix A isThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.which is the Leibniz formula for the determinant of a 3 × 3 matrix.this can be expanded out to giveThe Laplace formula for the determinant of a 3 × 3 matrix isThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (-b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The Leibniz formula for the determinant of a 2 × 2 matrix isThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.Assume A is a square matrix with n rows and n columns, so that it can be written asEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n × n matrix A in terms of its column vectorsThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 × 4 matrix:When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 × 4 matrix, the determinant of a 5 × 5 matrix, and so forth.Similarly, suppose we have a 3 × 3 matrix A, and we want the specific formula for its determinant |A|:In the case of a 2 × 2 matrix the specific formula for the determinant is:In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.
System of linear equations
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:where A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.A homogeneous system is equivalent to a matrix equation of the formA system of linear equations is homogeneous if all of the constant terms are zero:There is also a quantum algorithm for linear systems of equations.[3]A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.is given byCramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemThe last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsFor example, consider the following system:The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:Here x is the free variable, and y and z are dependent.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.The solution set to this system can be described by the following equations:For example, consider the following system:To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.There are several algorithms for solving a system of linear equations.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.For example, the equationsA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.For a more complicated example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For example, the equationsThe equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.The following pictures illustrate this trichotomy in the case of two variables:In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.A linear system may behave in any one of three possible ways:A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.The number of vectors in a basis for the span is now expressed as the rank of the matrix.where A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The vector equation is equivalent to a matrix equation of the formThis allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.A general system of m linear equations with n unknowns can be written asNow substitute this expression for x into the bottom equation:The simplest kind of linear system involves two equations and two variables:Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given byIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,
Triangular matrix
Forward substitution is used in financial bootstrapping to construct a yield curve.A matrix equation with an upper triangular matrix U can be solved in an analogous way, only working backwards.The resulting formulas are:The matrix equation Lx = b can be written as a system of linear equationsNotice that this does not require inverting the matrix.The group of 2 by 2 upper unitriangular matrices is isomorphic to the additive group of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic Möbius transformations; the 3 by 3 upper unitriangular matrices form the Heisenberg group.The stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are not all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called parabolic subgroups.The upper triangular matrices are precisely those that stabilize the standard flag. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are Borel subgroups. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.The set of invertible triangular matrices of a given kind (upper or lower) forms a group, indeed a Lie group, which is a subgroup of the general linear group of all invertible matrices. Note that a triangular matrix is invertible precisely when its diagonal entries are invertible (non-zero).A non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a trapezoid.Because the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an algebra. Algebras of upper triangular matrices have a natural generalization in functional analysis which yields nest algebras on Hilbert spaces.This is generalized by Lie's theorem, which shows that any representation of a solvable Lie algebra is simultaneously upper triangularizable, the case of commuting matrices being the abelian Lie algebra case, abelian being a fortiori solvable.In the case of complex matrices, it is possible to say more about triangularization, namely, that any square matrix A has a Schur decomposition. This means that A is unitarily equivalent (i.e. similar, using a unitary matrix as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.A more precise statement is given by the Jordan normal form theorem, which states that in this situation, A is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.[1][2]Any complex square matrix is triangularizable.[1] In fact, a matrix A over a field containing all of the eigenvalues of A (for example, any matrix over an algebraically closed field) is similar to a triangular matrix. This can be proven by using induction on the fact that A has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that A stabilises a flag, and is thus triangularizable with respect to a basis for that flag.The transpose of an upper triangular matrix is a lower triangular matrix and vice versa.A matrix which is simultaneously triangular and normal is also diagonal. This can be seen by looking at the diagonal entries of A*A and AA*, where A is a normal, triangular matrix.is atomic lower triangular. Its inverse isThe matrixi.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.The inverse of an atomic triangular matrix is again atomic triangular. Indeed, we haveAn atomic (upper or lower) triangular matrix is a special form of unitriangular matrix, where all of the off-diagonal elements are zero, except for the entries in a single column. Such a matrix is also called a Frobenius matrix, a Gauss matrix, or a Gauss transformation matrix. So an atomic lower triangular matrix is of the formIn fact, by Engel's theorem, any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable.The set of unitriangular matrices forms a Lie group.If the entries on the main diagonal of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) unitriangular. All unitriangular matrices are unipotent. Other names used for these matrices are unit (upper or lower) triangular (of which "unitriangular" might be a contraction), or very rarely normed (upper or lower) triangular. However a unit triangular matrix is not the same as the unit matrix, and a normed triangular matrix has nothing to do with the notion of matrix norm. The identity matrix is the only matrix which is both upper and lower unitriangular.is lower triangular.is upper triangular and this matrixThis matrixAll these results hold if "upper triangular" is replaced by "lower triangular" throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.Together these facts mean that the upper triangular matrices form a subalgebra of the associative algebra of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the Lie algebra of square matrices of a fixed size, where the Lie bracket [a,b] given by the commutator ab-ba. The Lie algebra of all upper triangular matrices is a solvable Lie algebra. It is often referred to as a Borel subalgebra of the Lie algebra of all square matrices.Many operations on upper triangular matrices preserve the shape:Matrices that are similar to triangular matrices are called triangularisable.is called an upper triangular matrix or right triangular matrix. The variable L (standing for lower or left) is commonly used to represent a lower triangular matrix, while the variable U (standing for upper) or R (standing for right) is commonly used for upper triangular matrix. A matrix that is both upper and lower triangular is diagonal.is called a lower triangular matrix or left triangular matrix, and analogously a matrix of the formA matrix of the formBecause matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix L and an upper triangular matrix U if and only if all its leading principal minors are non-zero.In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. A square matrix is called lower triangular if all the entries above the main diagonal are zero. Similarly, a square matrix is called upper triangular if all the entries below the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.
Linear least squares (mathematics)
Matrix calculations, like any other, are affected by rounding errors. An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson.[14]Fitting of linear models by least squares often, but not always, arise in the context of statistical analysis. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can beThe numerical methods for linear least squares are important because linear regression models are among the most important types of model, both as formal statistical models and for exploration of data-sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to round-off error.and the best fit can be found by solving the normal equations.so to minimize the functionIdeally, the model function fits the data exactly, soOften it is of interest to solve a linear least squares problem with an additional constraint on the solution. With constrained linear least squares, the original equationThese values can be used for a statistical criterion as to the goodness of fit. When unit weights are used, the numbers should be divided by the variance of an observation.The optimal value of the objective function, found by substituting in the optimal expression for the coefficient vector, can be written as (assuming unweighted observations)If experimental error follows a normal distribution, then, because of the linear relationship between residuals and observations, so should residuals,[9] but since the observations are only a sample of the population of all possible observations, the residuals should belong to a Student's t-distribution. Studentized residuals are useful in making a statistical test for an outlier when a particular residual appears to be excessively large.Thus, in the motivational example, above, the fact that the sum of residual values is equal to zero it is not accidental but is a consequence of the presence of the constant term, α, in the model.The sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by XT:Thus the residuals are correlated, even if the observations are not.and I is the identity matrix. The variance-covariance matrix of the residuals, Mr is given bywhere H is the idempotent matrix known as the hat matrix:The residuals are related to the observations byWhen the number of observations is relatively small, Chebychev's inequality can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively.The assumption is not unreasonable when m >> n. If the experimental errors are normally distributed the parameters will belong to a Student's t-distribution with m − n degrees of freedom. When m >> n Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to sampling error.[8]where S is the minimum value of the (weighted) objective function:When W = M−1, this simplifies toTherefore, an expression for the residuals (i.e., the estimated errors in the parameters) can be obtained by error propagation from the errors in the observations. Let the variance-covariance matrix for the observations be denoted by M and that of the parameters by Mβ. ThenThe estimated parameter values are linear combinations of the observed valuesThis method is used in iteratively reweighted least squares.The weights should, ideally, be equal to the reciprocal of the variance of the measurement.[6][7] The normal equations are then:where wi > 0 is the weight of the ith observation, and W is the diagonal matrix of such weights.In some cases the observations may be weighted—for example, they may not be equally reliable. In this case, one can minimize the weighted sum of squares:An assumption underlying the treatment given above is that the independent variable, x, is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares or more generally errors-in-variables models, or rigorous least squares, should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure.[4][5]These properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.However, in the case that the experimental errors do belong to a normal distribution, the least-squares estimator is also a maximum likelihood estimator.[3]For example, it is easy to show that the arithmetic mean of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss–Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.The equation and solution of linear least squares are thus described as follows:The gradient equations at the minimum can be written asis a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, XTX, is very ill-conditioned (i.e. if its condition number multiplied by the machine's relative round-off error is appreciably large). In that case, including the smallest singular values in the inversion merely adds numerical noise to the solution. This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to factor analysis.and thus,An alternative decomposition of X is the singular value decomposition (SVD)[2]These equations are easily solved as R is upper triangular.Since v doesn't depend on β, the minimum value of s is attained when the upper block, u, is zero. Therefore, the parameters are found by solving:Because Q is orthogonal, the sum of squares of the residuals, s, may be written as:The residual vector is left-multiplied by QT.The matrix X is subjected to an orthogonal decomposition, e.g., the QR decomposition as follows.The residuals are written in matrix notation asOrthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more numerically stable because they avoid forming the product XTX.Both substitutions are facilitated by the triangular nature of R.The solution is obtained in two stages, a forward substitution step, solving for z:If the matrix XTX is well-conditioned and positive definite, implying that it has full rank, the normal equations can be solved directly by using the Cholesky decomposition RTR, where R is an upper triangular matrix, giving:where X+ is the Moore–Penrose pseudoinverse of X. Although this equation is correct and can work in many applications, it is not computationally efficient to invert the normal-equations matrix (the Gramian matrix). An exception occurs in numerical smoothing and differentiation where an analytical expression is required.The algebraic solution of the normal equations with a full-rank matrix XTX can be written assimply becauseIn matrix form:and the derivatives change intoand therefore minimized exactly whencan be written asThe normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimizeThe normal equations are written in matrix notation asUpon rearrangement, we obtain the normal equations:Substitution of the expressions for the residuals and the derivatives into the gradient equations givesThe derivatives areGiven that S is convex, it is minimized when its gradient vector is zero (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further – see maxima and minima.) The elements of the gradient vector are the partial derivatives of S with respect to the parameters:where the objective function S is given bywhereConsider an overdetermined systemand solvedThe partial derivatives with respect to the parameters (this time there is only one) are again computed and set to 0:This results in a system of two equations in two unknowns, called the normal equations, which when solved giveThe "error", at each point, between the curve fit and the data is the difference between the right- and left-hand sides of the equations above. The least squares approach to solving this problem is to try to make the sum of the squares of these errors as small as possible; that is, to find the minimum of the functionof four equations in two unknowns in some "best" sense.In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See Outline of regression analysis for an outline of the topic.Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called linear least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator.In statistics and mathematics, linear least squares is an approach to fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system.
Fourier series
In 1922, Andrey Kolmogorov published an article titled "Une série de Fourier-Lebesgue divergente presque partout" in which he gave an example of a Lebesgue-integrable function whose Fourier series diverges almost everywhere. He later constructed an example of an integrable function whose Fourier series diverges everywhere (Katznelson 1976).Since Fourier series have such good convergence properties, many are often surprised by some of the negative results. For example, the Fourier series of a continuous T-periodic function need not converge pointwise. The uniform boundedness principle yields a simple non-constructive proof of this fact.These theorems, and informal variations of them that don't specify the convergence conditions, are sometimes referred to generically as "Fourier's theorem" or "the Fourier theorem".[14][15][16][17]Many other results concerning the convergence of Fourier series are known, ranging from the moderately simple result that the series converges at x if f is differentiable at x, to Lennart Carleson's much more sophisticated result that the Fourier series of an L2 function actually converges almost everywhere.Because of the least squares property, and because of the completeness of the Fourier basis, we obtain an elementary convergence result.Note that fN is a trigonometric polynomial of degree N. Parseval's theorem implies thatWe say that p is a trigonometric polynomial of degree N when it is of the formThis is called a partial sum. We would like to know, in which sense does fN(x) converge to f(x) as N → ∞.This generalizes the Fourier transform to L1(G) or L2(G), where G is an LCA group. If G is compact, one also obtains a Fourier series, which converges similarly to the [−π, π] case, but if G is noncompact, one obtains instead a Fourier integral. This generalization yields the usual Fourier transform when the underlying locally compact Abelian group is R.The generalization to compact groups discussed above does not generalize to noncompact, nonabelian groups. However, there is a straightforward generalization to Locally Compact Abelian (LCA) groups.If the domain is not a group, then there is no intrinsically defined convolution. However, if X is a compact Riemannian manifold, it has a Laplace–Beltrami operator. The Laplace–Beltrami operator is the differential operator that corresponds to Laplace operator for the Riemannian manifold X. Then, by analogy, one can consider heat equations on X. Since Fourier arrived at his basis by attempting to solve the heat equation, the natural generalization is to use the eigensolutions of the Laplace–Beltrami operator as a basis. This generalizes Fourier series to spaces of the type L2(X), where X is a Riemannian manifold. The Fourier series converges in ways similar to the [−π, π] case. A typical example is to take X to be the sphere with the usual metric, in which case the Fourier basis consists of spherical harmonics.An alternative extension to compact groups is the Peter–Weyl theorem, which proves results about representations of compact groups analogous to those about finite groups.One of the interesting properties of the Fourier transform which we have mentioned, is that it carries convolutions to pointwise products. If that is the property which we seek to preserve, one can produce Fourier series on any compact group. Typical examples include those classical groups that are compact. This generalizes the Fourier transform to all spaces of the form L2(G), where G is a compact group, in such a way that the Fourier transform carries convolutions to pointwise products. The Fourier series exists and converges in similar ways to the [−π,π] case.furthermore, the sines and cosines are orthogonal to the constant function 1. An orthonormal basis for L2([−π,π]) consisting of real functions is formed by the functions 1 and √2 cos(nx),  √2 sin(nx) with n = 1, 2,...  The density of their span is a consequence of the Stone–Weierstrass theorem, but follows also from the properties of classical kernels like the Fejér kernel.(where δmn is the Kronecker delta), andThis corresponds exactly to the complex exponential formulation given above. The version with sines and cosines is also justified with the Hilbert space interpretation. Indeed, the sines and cosines form an orthogonal set:The basic Fourier series result for Hilbert spaces can be written asWe can write now h(K) as an integral with the traditional coordinate system over the volume of the primitive cell, instead of with the x1, x2 and x3 variables:(it may be advantageous for the sake of simplifying calculations, to work in such a cartesian coordinate system, in which it just so happens that a1 is parallel to the x axis, a2 lies in the x-y plane, and a3 has components of all three axes). The denominator is exactly the volume of the primitive unit cell which is enclosed by the three primitive-vectors a1, a2 and a3. In particular, we now know thatwhich after some calculation and applying some non-trivial cross-product identities can be shown to be equal to:we can solve this system of three linear equations for x, y, and z in terms of x1, x2 and x3 in order to calculate the volume element in the original cartesian coordinate system. Once we have x, y, and z in terms of x1, x2 and x3, we can calculate the Jacobian determinant:AssumingwhereAnd so it is clear that in our expansion, the sum is actually over reciprocal lattice vectors:Re-arranging:We write g as:Finally applying the same for the third coordinate, we define:We can write g once again as:Further defining:And then we can write:If we write a series for g on the interval [0, a1] for x1, we can define the following:Thus we can define a new function,where ai = |ai|.where ni are integers and ai are three linearly independent vectors. Assuming we have some function, f(r), such that it obeys the following condition for any Bravais lattice vector R: f(r) = f(r + R), we could make a Fourier series of it. This kind of function can be, for example, the effective potential that one electron "feels" inside a periodic crystal. It is useful to make a Fourier series of the potential then when applying Bloch's theorem. First, we may write any arbitrary vector r in the coordinate-system of the lattice:The three-dimensional Bravais lattice is defined as the set of vectors of the form:Aside from being useful for solving partial differential equations such as the heat equation, one notable application of Fourier series on the square is in image compression. In particular, the jpeg image compression standard uses the two-dimensional discrete cosine transform, which is a Fourier transform using the cosine basis functions.We can also define the Fourier series for functions of two variables x and y in the square [−π, π] × [−π, π]:Many other Fourier-related transforms have since been defined, extending the initial idea to other applications. This general area of inquiry is now sometimes called harmonic analysis. A Fourier series, however, can be used only for periodic functions, or for functions on a bounded (compact) interval.Since Fourier's time, many different approaches to defining and understanding the concept of Fourier series have been discovered, all of which are consistent with one another, but each of which emphasizes different aspects of the topic. Some of the more powerful and elegant approaches are based on mathematical ideas and tools that were not available at the time Fourier completed his original work. Fourier originally defined the Fourier series for real-valued functions of real arguments, and using the sine and cosine functions as the basis set for the decomposition.When Fourier submitted a later competition essay in 1811, the committee (which included Lagrange, Laplace, Malus and Legendre, among others) concluded: ...the manner in which the author arrives at these equations is not exempt of difficulties and...his analysis to integrate them still leaves something to be desired on the score of generality and even rigour.[citation needed]In these few lines, which are close to the modern formalism used in Fourier series, Fourier revolutionized both mathematics and physics. Although similar trigonometric series were previously used by Euler, d'Alembert, Daniel Bernoulli and Gauss, Fourier believed that such trigonometric series could represent any arbitrary function. In what sense that is actually true is a somewhat subtle issue and the attempts over many years to clarify this idea have led to important discoveries in the theories of convergence, function spaces, and harmonic analysis.This immediately gives any coefficient ak of the trigonometrical series for φ(y) for any function which has such an expansion. It works because if φ has such an expansion, then (under suitable convergence assumptions) the integralThe constructed function S(f) is therefore commonly referred to as a Fourier transform, even though the Fourier integral of a periodic function is not convergent at the harmonic frequencies.[nb 2]Another commonly used frequency domain representation uses the Fourier series coefficients to modulate a Dirac comb:In engineering, particularly when the variable x represents time, the coefficient sequence is called a frequency domain representation. Square brackets are often used to emphasize that the domain of this function is a discrete set of frequencies.Another application of this Fourier series is to solve the Basel problem by using Parseval's theorem. The example generalizes and one may compute ζ(2n), for any positive integer n.Here, sinh is the hyperbolic sine function. This solution of the heat equation is obtained by multiplying each term of  Eq.1 by sinh(ny)/sinh(nπ). While our example function s(x) seems to have a needlessly complicated Fourier series, the heat distribution T(x, y) is nontrivial. The function T cannot be written as a closed-form expression. This method of solving the heat problem was made possible by Fourier's work.The Fourier series expansion of our function in Example 1 looks more complicated than the simple formula s(x) = x/π, so it is not immediately apparent why one would need the Fourier series. While there are many applications, Fourier's motivation was in solving the heat equation. For example, consider a metal plate in the shape of a square whose side measures π meters, with coordinates (x, y) ∈ [0, π] × [0, π]. If there is no heat source within the plate, and if three of the four sides are held at 0 degrees Celsius, while the fourth side, given by y = π, is maintained at the temperature gradient T(x, π) = x degrees Celsius, for x in (0, π), then one can show that the stationary heat distribution (or the heat distribution after a long period of time has elapsed) is given byThis example leads us to a solution to the Basel problem.When x = π, the Fourier series converges to 0, which is the half-sum of the left- and right-limit of s at x = π. This is a particular instance of the Dirichlet theorem for Fourier series.It can be proven that Fourier series converges to s(x) at every point x where s is differentiable, and therefore:In this case, the Fourier coefficients are given byWe now use the formula above to give a Fourier series expansion of a very simple function. Consider a sawtooth waveIn engineering applications, the Fourier series is generally presumed to converge everywhere except at discontinuities, since the functions encountered in engineering are more well behaved than the ones that mathematicians can provide as counter-examples to this presumption. In particular, if s is continuous and the derivative of s(x) (which may not exist everywhere) is square integrable, then the Fourier series of s converges absolutely and uniformly to s(x).[11]  If a function is square-integrable on the interval [x0, x0+P], then the Fourier series converges to the function at almost every point. Convergence of Fourier series also depends on the finite number of maxima and minima in a function which is popularly known as one of the Dirichlet's condition for Fourier series. See Convergence of Fourier series. It is possible to define Fourier coefficients for more general functions or distributions, in such cases convergence in norm or weak convergence is usually of interest.This is the same formula as before except cn and c−n are no longer complex conjugates. The formula for cn is also unchanged:Both components of a complex-valued function are real-valued functions that can be represented by a Fourier series. The two sets of coefficients and the partial sum are given by:When the coefficients (known as Fourier coefficients) are computed as follows:[10]The inverse relationships between the coefficients are:where:we can also write the function in these equivalent forms:In this section, s(x) denotes a function of the real variable x, and s is integrable on an interval [x0, x0 + P], for real numbers x0 and P. We will attempt to represent  s  in that interval as an infinite sum, or series, of harmonically related sinusoidal functions. Outside the interval, the series is periodic with period P (frequency 1/P). It follows that if s also has that property, the approximation is valid on the entire real line. We can begin with a finite summation (or partial sum):Although the original motivation was to solve the heat equation, it later became obvious that the same techniques could be applied to a wide array of mathematical and physical problems, and especially those involving linear differential equations with constant coefficients, for which the eigensolutions are sinusoids. The Fourier series has many such applications in electrical engineering, vibration analysis, acoustics, optics, signal processing, image processing, quantum mechanics, econometrics,[8] thin-walled shell theory,[9] etc.From a modern point of view, Fourier's results are somewhat informal, due to the lack of a precise notion of function and integral in the early nineteenth century. Later, Peter Gustav Lejeune Dirichlet[4] and Bernhard Riemann[5][6][7] expressed Fourier's results with greater precision and formality.The heat equation is a partial differential equation. Prior to Fourier's work, no solution to the heat equation was known in the general case, although particular solutions were known if the heat source behaved in a simple way, in particular, if the heat source was a sine or cosine wave. These simple solutions are now sometimes called eigensolutions. Fourier's idea was to model a complicated heat source as a superposition (or linear combination) of simple sine and cosine waves, and to write the solution as a superposition of the corresponding eigensolutions. This superposition or linear combination is called the Fourier series.The Fourier series is named in honour of Jean-Baptiste Joseph Fourier (1768–1830), who made important contributions to the study of trigonometric series, after preliminary investigations by Leonhard Euler, Jean le Rond d'Alembert, and Daniel Bernoulli.[nb 1] Fourier introduced the series for the purpose of solving the heat equation in a metal plate, publishing his initial results in his 1807 Mémoire sur la propagation de la chaleur dans les corps solides (Treatise on the propagation of heat in solid bodies), and publishing his Théorie analytique de la chaleur (Analytical theory of heat) in 1822. The Mémoire introduced Fourier analysis, specifically Fourier series. Through Fourier's research the fact was established that an arbitrary (continuous)[2] function can be represented by a trigonometric series. The first announcement of this great discovery was made by Fourier in 1807, before the French Academy.[3] Early ideas of decomposing a periodic function into the sum of simple oscillating functions date back to the 3rd century BC, when ancient astronomers proposed an empiric model of planetary motions, based on deferents and epicycles.In mathematics, a Fourier series (English: /ˈfʊəriˌeɪ/)[1] is a way to represent a function as the sum of simple sine waves. More formally, it decomposes any periodic function or periodic signal into the sum of a (possibly infinite) set of simple oscillating functions, namely sines and cosines (or, equivalently, complex exponentials). The discrete-time Fourier transform is a periodic function, often defined in terms of a Fourier series. The Z-transform, another example of application, reduces to a Fourier series for the important case |z|=1. Fourier series are also central to the original proof of the Nyquist–Shannon sampling theorem. The study of Fourier series is a branch of Fourier analysis.
Partial differential equation
Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. "Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods conserve mass by design.Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.The finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge–Kutta, etc.The three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM), as well other kind of methods called Meshfree methods, which were made to solve problems where the before mentioned methods are limited. The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other hybrid versions of FEM and Meshfree methods include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.The adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality.Symmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.A general approach to solving PDE's uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, Bäcklund transform and finally finding exact analytic solutions to the PDE.From 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.In some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.The method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.Nevertheless, some techniques can be used for several types of equations. The h-principle is the most powerful method to solve underdetermined equations. The Riquier–Janet theory is an effective method for obtaining information about many analytic overdetermined systems.There are no generally applicable methods to solve nonlinear PDEs. Still, existence and uniqueness results (such as the Cauchy–Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schrödinger equation.This is analogous in signal processing to understanding a filter by its impulse response.Inhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.by the change of variables (for complete details see Solution of the Black Scholes Equation at the Wayback Machine (archived April 11, 2008))is reducible to the heat equationOften a PDE can be reduced to a simpler form with a known solution by a suitable change of variables. For example, the Black–Scholes PDEIf the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example of the use of a Fourier integral.An important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.An integral transform may transform the PDE to a simpler one, in particular, a separable PDE. This corresponds to diagonalizing an operator.More generally, one may find characteristic surfaces.In special cases, one can find characteristic curves on which the equation reduces to an ODE – changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.This generalizes to the method of characteristics, and is also used in integral transforms.This is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices – thinking of "the value for fixed x" as a coordinate, each coordinate can be understood separately.In the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable – these are in turn easier to solve.Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can find any solution that solves the equation and satisfies the boundary conditions, then it is the solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.[2]In the phase space formulation of quantum mechanics, one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of ħ. The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function.which is called elliptic-hyperbolic because it is elliptic in the region x < 0, hyperbolic in the region x > 0, and degenerate parabolic on the line x = 0.If a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler–Tricomi equationThe geometric interpretation of this condition is as follows: if data for u are prescribed on the surface S, then it may be possible to determine the normal derivative of u on S from the differential equation. If the data on S and the differential equation determine the normal derivative of u on S, then S is non-characteristic. If the data on S and the differential equation do not determine the normal derivative of u on S, then the surface is characteristic, and the differential equation restricts the data on S: the differential equation is internal to S.where φ has a non-zero gradient, then S is a characteristic surface for the operator L at a given point if the characteristic form vanishes:where the coefficient matrices Aν and the vector B may depend upon x and u. If a hypersurface S is given in the implicit formThe classification of partial differential equations can be extended to systems of first-order equations, where the unknown u is now a vector with m components, and the coefficient matrices Aν are m by m matrices for ν = 1, ..., n. The partial differential equation takes the formThe classification depends upon the signature of the eigenvalues of the coefficient matrix ai,j..If there are n independent variables x1, x2 , ..., xn, a general linear partial differential equation of second order has the formMore precisely, replacing ∂x by X, and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.Some linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others such as the Euler–Tricomi equation have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions, and to the smoothness of the solutions.where Δ is the Laplace operator.orIn PDEs, it is common to denote partial derivatives using subscripts. That is:This solution approaches infinity if nx is not an integer multiple of π for any non-zero value of y. The Cauchy problem for the Laplace equation is called ill-posed or not well-posed, since the solution does not continuously depend on the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.where n is an integer. The derivative of u with respect to y approaches 0 uniformly in x as n increases, but the solution iswith boundary conditionsAn example of pathological behavior is the sequence (depending upon n) of Cauchy problems for the Laplace equationAlthough the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard–Lindelöf theorem, that is far from the case for partial differential equations. The Cauchy–Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties. The mathematical study of these questions is usually in the more powerful context of weak solutions.where c is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function f(y) can be determined if u is specified on the line x = 0.which has the solutionwhere f is an arbitrary function of y. The analogous ordinary differential equation isThis relation implies that the function u(x,y) is independent of x. However, the equation gives no information on the function's dependence on the variable y. Hence the general solution of this equation isA relatively simple PDE isIf f is a linear function of u and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein–Gordon equation, and Poisson's equation.Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six parameters[1], but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the fluid occur in an infinite-dimensional configuration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again, there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid dynamics, electrodynamics, and heat transfer.PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.In mathematics, a partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.
Dirichlet conditions

Inner product space
As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the "outer (alternatively, wedge) product". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.More abstractly, the outer product is the bilinear map W × V∗ → Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V∗ × V → F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V → V∗) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 × n covector with an n × 1 vector, yielding a 1 × 1 matrix (a scalar), while the outer product is the product of an m × 1 vector with a 1 × n covector, yielding an m × n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V → V∗) and thus hold more generally.Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that ⟨x,y⟩ ≠ 0, though y need not equal x; in other words, the induced map to the dual space V → V∗ is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "−" to them differs depending on conventions).This construction is used in numerous contexts. The Gelfand–Naimark–Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x : ||x|| = 0}. The sesquilinear form ⟨·,·⟩ factors through W.If V is a vector space and ⟨·,···⟩ a semi-definite sesquilinear form, then the function:Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.Several types of linear maps A from an inner product space V to an inner product space W are of relevance:Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [−π,π] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.Orthogonality of the sequence {ek}k follows immediately from the fact that if k ≠ j, thenis an isometric linear map with dense image.is an orthonormal basis of the space C[−π,π] with the L2 inner product. The mappingTheorem. Let V be the inner product space C[−π,π]. Then the sequence (indexed on set of all integers) of continuous functionsThis theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:is an isometric linear map V → l2 with a dense image.Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of V. Then the mapParseval's identity leads immediately to the following theorem:The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]Theorem. Any complete inner product space V has an orthonormal basis.Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show thatTheorem. Any separable inner product space V has an orthonormal basis.Using an infinite-dimensional analog of the Gram-Schmidt process one may show:if α ≠ β and ⟨eα,eα⟩ = ||eα|| = 1 for all α, β ∈ A.is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis andThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collectionLet V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram–Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if ⟨ei,ej⟩ = 0 for every i ≠ j and ⟨ei,ei⟩ = ||ei|| = 1 for each i.This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]A linear space with a norm such as:is an inner product.For real matrices of the same size, ⟨A,B⟩ := tr(ABT) with transpose as conjugationis an inner product.[6][7][8] In this case, ⟨X,X⟩ = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.For real random variables X and Y, the expected value of their productThis sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.This space is not complete; consider for example, for the interval [−1,1] the sequence of continuous "step" functions, { fk}k, defined by:The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product iswhere M is any Hermitian positive-definite matrix and y† is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.The general form of an inner product on Cn is known as the Hermitian form and is given bywhere xT is the transpose of x.More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.A simple example is the real numbers with the standard multiplication as the inner productis also known as additivity.The property of an inner product space V thatAssuming the underlying field to be R, the inner product becomes symmetric, and we obtainCombining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:From the linearity property it is derived that x = 0 implies ⟨x,x⟩ = 0. while from the positive-definiteness axiom we obtain the converse, ⟨x,x⟩ = 0 implies x = 0. Combining these two, we have the property that ⟨x,x⟩ = 0 if and only if x = 0.In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.Conjugate symmetry and linearity in the first variable givesMoreover, sesquilinearity (see below) implies thatNotice that conjugate symmetry implies that ⟨x,x⟩ is real for all x, since we have:When F = R, conjugate symmetry reduces to symmetry. That is, ⟨x,y⟩ = ⟨y,x⟩ for F = R; while for F = C, ⟨x,y⟩ is equal to the complex conjugate.In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that ⟨x,x⟩ is only required to be non-negative. We show how to treat these below.There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product ⟨x,y⟩ as ⟨y|x⟩ (the bra–ket notation of quantum mechanics), respectively y†x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V∗, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking ⟨x,y⟩ to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both ⟨·,·⟩ and ⟨·|·⟩ as distinct notations differing only in which argument is conjugate linear.that satisfies the following three axioms for all vectors x, y, z ∈ V and all scalars a ∈ F:[2][3]Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a mapIn this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]
Quantum mechanics
Each term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.andwith coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy viaandThe solutions are superpositions of left- and right-moving waves:The potential in this case is given by:This is another example illustrating the quantification of energy for bound states.and the corresponding energy levels arewhere Hn are the Hermite polynomialsThis problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given byAs in the classical case, the potential for the quantum harmonic oscillator is given byThis is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.The quantization of energy levels follows from this constraint on k, sincein which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of π,and D = 0. At x = L,The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where ψ must be zero. Thus, at x = 0,or, from Euler's formula,The general solutions of the Schrödinger equation for the particle in a box arethe previous equation is evocative of the classic kinetic energy analogue,With the differential operator defined byFor example, consider a free particle. In quantum mechanics, a free matter is described by a wave function. The particle properties of the matter become apparent when we measure its position and velocity. The wave properties of the matter become apparent when we measure its wave properties like interference. The wave–particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics. Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position—or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.[86] In an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.[87]Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures.[84] Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms.[85] Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement.[83] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.A more distant goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.[82]An inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.[81]Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.Many electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum mechanics is necessary to understanding and designing such electronic devices.Many modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.In many aspects modern technology operates at a scale where quantum effects are significant.Quantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry. Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved.[80] Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.Quantum mechanics has had enormous[79] success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).The Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[77] This is not accomplished by introducing some "new axiom" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical - not just formally mathematical, as in other interpretations - quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these "parallel universes" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation.[78] Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.John Bell showed that this "EPR" paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables.[76] Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement.Albert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, "God does not play with dice".[75] He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein–Podolsky–Rosen paradox.The Copenhagen interpretation — due largely to Niels Bohr and Werner Heisenberg — remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of "causality." It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born's basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics."[73] According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[74]Another popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete. More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616×10−35 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.The quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or "quantum electromagnetism"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity,[70][71] has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field.[72] Beyond this "grand unification", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However — and while special relativity is parsimoniously incorporated into quantum electrodynamics — the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely "compactified" (or infinitely curved) and not readily amenable to measurement or probing.Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th and 21st century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering Gödel's Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture "Gödel and the End of Physics" (2002).[69]Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.[68]Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this.[67] For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed.For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.[65][66]In Niels Bohr's mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics.[55][56][57][58] The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or "state" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition.[59][60] In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous "state" in the classical sense of that word.[61][62] Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined.[59] Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.[63][64]A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.[54]Quantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein–Podolsky–Rosen (EPR) paradox — an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism.[49] Quantum interference involves adding together probability amplitudes, whereas classical "waves" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems.[50] Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically.[51] This is in accordance with the following observations:Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[46] According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles).[47] The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers.[48] However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.Classical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.[45]It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.[44]When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.The rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space—although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This "high energy" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.[42] In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or "observables". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom).[43] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics - matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger).[41]There exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the "semi-classical equation of motion" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.The Schrödinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the "wave-like" behavior of quantum states. As it turns out, analytic solutions of the Schrödinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom—which contains just one more electron than does the hydrogen atom—has defied all attempts at a fully analytic treatment.Some wave functions produce probability distributions that are constant, or independent of time—such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).[40]Wave functions change as time progresses. The Schrödinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schrödinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.[39]During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.[37][38]The time evolution of a quantum state is described by the Schrödinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that - given a wave function at an initial time - it makes a definite prediction of what the wave function will be at any later time.[36]In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs); rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates). Usually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or "generalized" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process[34] that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates. For example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result.[30] It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.[35]Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction.[31][32] Naturally, these probabilities will depend on the quantum state at the "instant" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable ("eigen" can be translated from German as meaning "inherent" or "characteristic").[33]The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of "wave function collapse" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[30]According to one interpretation, as the result of a measurement the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable—which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute.In the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space.[28] This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability, often referred to as "clouds", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.[29]In the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac,[23] David Hilbert,[24] John von Neumann,[25] and Hermann Weyl,[26] the possible states of a quantum mechanical system are symbolized[27] as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space—variously called the state space or the associated Hilbert space of the system—that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system—for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave–particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.[22]The word quantum derives from the Latin, meaning "how great" or "how much".[19] In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics.[20][better source needed] Some fundamental aspects of the theory are still actively studied.[21]While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,[17] and superfluids.[18]By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann[16] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.[citation needed]It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave–particle duality.[citation needed]In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). In 1926 Erwin Schrödinger suggested a partial differential equation for the wave functions of particles like electrons. And when effectively restricted to a finite region, this equation allowed only certain modes, corresponding to discrete quantum states—whose properties turned out to be exactly the same as implied by matrix mechanics.[15] From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[citation needed]The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schrödinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.[14]Planck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself.[12] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[13] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.where h is Planck's constant.According to Planck, each energy element (E) is proportional to its frequency (ν):Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld.[11] This phase is known as old quantum theory.Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887). Around 1900-1910, the atomic theory and the corpuscular theory of light[10] first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation,[9] known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck.[8] Planck's hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets) precisely matched the observed patterns of black-body radiation.Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[7] In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.Important applications of quantum theory[5] include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.[6]Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schrödinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.Classical physics (the physics existing before quantum mechanics) is a set of fundamental theories which describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that: energy, momentum and other quantities of a system may be restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to the precision with which quantities can be known (uncertainty principle).[note 1]Quantum mechanics (QM; also known as quantum physics or quantum theory), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.[2]
Observable
A crucial difference between classical quantities and quantum mechanical observables is that the latter may not be simultaneously measurable. This is mathematically expressed by non-commutativity of the corresponding operators, to the effect thatTo be more precise, the dynamical variable/observable is a (not necessarily bounded) Hermitian operator in a Hilbert space and thus is represented by a Hermitian matrix if the space is finite-dimensional. In an infinite-dimensional Hilbert space, the observable is represented by a symmetric operator, which may not be defined everywhere (i.e. its domain is not the whole space - there exist some states that are not in the domain of the operator). The reason for such a change is that in an infinite-dimensional Hilbert space, the operator becomes unbounded, which means that it no longer has a largest eigenvalue. This is not the case in a finite-dimensional Hilbert space, where every operator is bounded - it has a largest eigenvalue. For example, if we consider the position of a point particle moving along a line, this particle's position variable can take on any number on the real-line, which is uncountably infinite. Since the eigenvalue of an observable represents a real physical quantity for that particular dynamical variable, then we must conclude that there is no largest eigenvalue for the position observable in this uncountably infinite-dimensional Hilbert space, since the field we're working over consists of the real-line. Nonetheless, whether we are working in an infinite-dimensional or finite-dimensional Hilbert space, the role of an observable in quantum mechanics is to assign real numbers to outcomes of particular measurements; this means that only certain measurements can determine the value of an observable for some state of a quantum system. In classical mechanics, any measurement can be made to determine the value of an observable.In quantum mechanics, measurement of observables exhibits some seemingly unintuitive properties. Specifically, if a system is in a state described by a vector in a Hilbert space, the measurement process affects the state in a non-deterministic, but statistically predictable way. In particular, after a measurement is applied, the state description by a single vector may be destroyed, being replaced by a statistical ensemble. The irreversible nature of measurement operations in quantum physics is sometimes referred to as the measurement problem and is described mathematically by quantum operations. By the structure of quantum operations, this description is mathematically equivalent to that offered by relative state interpretation where the original system is regarded as a subsystem of a larger system and the state of the original system is given by the partial trace of the state of the larger system.In the case of transformation laws in quantum mechanics, the requisite automorphisms are unitary (or antiunitary) linear transformations of the Hilbert space V. Under Galilean relativity or special relativity, the mathematics of frames of reference is particularly simple, and in fact restricts considerably the set of physically meaningful observables.In quantum physics, the relation between system state and the value of an observable requires some basic linear algebra for its description. In the mathematical formulation of quantum mechanics, states are given by non-zero vectors in a Hilbert space V (where two vectors are considered to specify the same state if, and only if, they are scalar multiples of each other) and observables are given by self-adjoint operators on V. However, as indicated below, not every self-adjoint operator corresponds to a physically meaningful observable. For the case of a system of particles, the space V consists of functions called wave functions or state vectors.Physically meaningful observables must also satisfy transformation laws which relate observations performed by different observers in different frames of reference. These transformation laws are automorphisms of the state space, that is bijective transformations which preserve some mathematical property.In physics, an observable is a dynamic variable[clarification needed] that can be measured. Examples include position and momentum. In systems governed by classical mechanics, it is a real-valued function on the set of all possible system states. In quantum physics, it is an operator, or gauge, where the property of the system state can be determined by some sequence of physical operations. For example, these operations might involve submitting the system to various electromagnetic fields and eventually reading a value.
Momentum
The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, Mechanica sive De Motu, Tractatus Geometricus: "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result".[58] Wallis uses momentum and vis for force. Newton's Philosophiæ Naturalis Principia Mathematica, when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines quantitas motus, "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum.[59] Thus when in Law II he refers to mutatio motus, "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion.[60] It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jenning's Miscellanea in 1721, five years before the final edition of Newton's Principia Mathematica, momentum M or "quantity of motion" was being defined for students as "a rectangle", the product of Q and V, where Q is "quantity of material" and V is "velocity", s/t.[61]Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.[57]René Descartes believed that the total "quantity of motion" (Latin: quantitas motus) in the universe is conserved,[54] where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more importantly he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion.[55][56] Galileo, later, in his Two New Sciences, used the Italian word impeto.In about 530 AD, working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's Physics. Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it.[48] Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in The Book of Healing in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it.[49][50][51] The work of Philoponus, and possibly that of Ibn Sīnā,[51] was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.[52][53]The flux, or transport per unit area, of a momentum component ρvj by a velocity vi is equal to ρ vjvj. In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average.[46] It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.[47]where c is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).[45]A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure p can often be described by the acoustic wave equation:The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).where f is the body force.[44]The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction i and force in direction j, there is a stress component σij. The nine components make up the Cauchy stress tensor σ, which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:These are known as the Navier–Stokes equations.[43]Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid arewhere μ is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.[42]Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress τ, exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the x direction varies with z, the tangential force in direction x per unit area normal to the z direction isApplied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to ρDv/Dt. This is equal to the net force on the droplet.If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative ∂v/∂t because the fluid in a given volume changes with time. Instead, the material derivative is needed:[41]Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is ρg, where g is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure p. The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is[40]In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density ρ and velocity v that depend on time t and position r. The momentum per unit volume is ρv.[39]Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).[37][38]where the operator p acting on a wave function ψ(p) yields that wave function multiplied by the value p, in an analogous fashion to the way that the position operator acting on a wave function ψ(x) yields that wave function multiplied by the value x.where ∇ is the gradient operator, ħ is the reduced Planck constant, and i is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented asFor a single particle described in the position basis the momentum operator can be written asIn quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.The electromagnetic stress tensor depends on the properties of the media.[34]where the H-field H is related to the B-field and the magnetization M byThe above results are for the microscopic Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified toThe term on the right is an integral over the surface area Σ of the surface σ representing momentum flow into and out of the volume, and nj is a component of the surface normal of S. The quantity Tij is called the Maxwell stress tensor, defined asand the equation for conservation of each component i of the momentum isThe electromagnetic momentum isIf momentum is to be conserved over the volume V over a region Q, changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If Pmech is the momentum of all the particles in Q, and the particles are treated as a continuum, then Newton's second law giveswhere μ0 is the vacuum permeability and c is the speed of light. The momentum density is proportional to the Poynting vector S which gives the directional rate of energy transfer per unit area:[34][35]In a vacuum, the momentum per unit volume isThe Lorentz force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.[34]In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions.[33] Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.while in relativistic mechanics this becomes(in SI units).[32]:2 It has an electric potential φ(r, t) and magnetic vector potential A(r, t).[28] In the non-relativistic regime, its generalized momentum isIn Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force (Lorentz force) on a particle with charge q due to a combination of electric field E and magnetic field B isConservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.[31]As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.[30]where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are[29]In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined asEven if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.This is the generalization of the conservation of momentum.[6]Now if a given coordinate qi does not appear in the Lagrangian (although its time derivative might appear), thenEach component pj is said to be the conjugate momentum for the coordinate qj.In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined asIf a coordinate qi is not a Cartesian coordinate, the associated generalized momentum component pi does not necessarily have the dimensions of linear momentum. Even if qi is a Cartesian coordinate, pi will not be the same as the mechanical momentum if the potential depends on velocity.[6] Some sources represent the kinematic momentum by the symbol Π.[28]If the generalized coordinates are represented as a vector q = (q1, q2, ... , qN) and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of N equations:[27]In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy T and the potential energy V:Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by constraints. For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of generalized coordinates that may be fewer in number.[24] Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a generalized momentum, also known as the canonical or conjugate momentum, that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as mechanical, kinetic or kinematic momentum.[6][25][26] The two main methods are described below.For a particle, the relationship between temporal components, E = ħ ω, is the Planck–Einstein relation, and the relation between spatial components, p= ħ k, describes a de Broglie matter wave.The four-momentum of a planar wave can be related to a wave four-vector[23]In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.[22]The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting m0 = 0 it follows thatand is invariant across all reference frames.The magnitude of the momentum four-vector is equal to m0c:Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.Using Einstein's mass-energy equivalence, E = mc2, this can be rewritten aswhere m0 is the invariant mass. If R = (ct,x,y,z) (in Minkowski space), thenand the (contravariant) four-momentum isIn all the coordinate systems, the (contravariant) relativistic four-velocity is defined byis invariant under Lorentz transformations (in this expression and in what follows the (+ − − −) metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by √−1; or by keeping time a real quantity and embedding the vectors in a Minkowski space.[21] In a Minkowski space, the scalar product of two four-vectors U = (U0,U1,U2,U3) and V = (V0,V1,V2,V3) is defined asIn the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example R for position. The expression for the four-momentum depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, τ, defined by[20]Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, γm0v is approximately equal to m0v, the Newtonian expression for momentum.obeys Newton's second law:The modified momentum,m0 is the object's invariant mass.[19]Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the inertial mass m of an object a function of velocity:where γ is the Lorentz factor:while the Lorentz transformation gives[18]Consider, for example, a reference frame moving relative to another at velocity v in the x direction. The Galilean transformation gives the coordinates of the moving frame asNewtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light c is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.[17]This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass (dm). When considered together, the object and the mass (dm) constitute a closed system in which total momentum is conserved.where u is the velocity of the ejected/accreted mass as seen in the object's rest frame.[16] This is distinct from v, which is the velocity of the object itself as seen in an inertial frame.This equation does not correctly describe the motion of variable-mass objects. The correct equation isThe concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: m(t). The momentum of the object at time t is therefore p(t) = m(t)v(t). One might then try to invoke Newton's second law of motion by saying that the external force F on the object is related to its momentum p(t) by F = dp/dt, but this is incorrect, as is the related expression found by applying the product rule to d(mv)/dt:[16]A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).[15]Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.[14]The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,represents three equations:[14]The equations in the previous sections, work in vector form if the scalars p and v are replaced by vectors p and v. Each vector equation represents three scalar equations. For example,Similarly, the momentum is a vector quantity and is represented by a boldface symbol:Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with x, y, z axes, velocity has components vx in the x-direction, vy in the y-direction, vz in the z-direction. The vector is represented by a boldface symbol:[14]The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.[13]One measure of the inelasticity of the collision is the coefficient of restitution CR, defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to a ball bouncing from a solid surface, this can be easily measured using the following formula:[12]In a frame of reference moving at the speed v), the objects are brought to rest by the collision and 100% of the kinetic energy is converted to other forms of energy.soIn a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. If one body is motionless to begin with, the equation for conservation of momentum isIn an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions,[10] in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment);[11] and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.In general, when the initial velocities are known, the final velocities are given by[9]A change of reference frame can simplify analysis of a collision. For example, suppose there are two bodies of equal mass m, one stationary and one approaching the other at a speed v (as in the figure). The center of mass is moving at speed v/2 and both bodies are moving towards it at speed v/2. Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed v. The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by[4]A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are u1 and u2 before the collision and v1 and v2 after, the equations expressing conservation of momentum and kinetic energy are:An elastic collision is one in which no kinetic energy is absorbed in the collision. Perfectly elastic "collisions" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision. A collision between two pool balls is a good example of an almost totally elastic collision, due to their high rigidity, but when bodies come in contact there is always some dissipation.[8]By itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an elastic collision; if not, it is an inelastic collision.A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame – one that is moving with the center of mass. In this frame, the total momentum is zero.Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.[7]Since u does not change, the accelerations are the same:This is called a Galilean transformation. If the particle is moving at speed dx/dt = v in the first frame of reference, in the second, it is moving at speedSuppose a particle has position x in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed u, the position (represented by a primed coordinate) changes with time asMomentum is a measurable quantity, and the measurement depends on the motion of the observer. For example: if an apple is sitting in a glass elevator that is descending, an outside observer, looking into the elevator, sees the apple moving, so, to that observer, the apple has a non-zero momentum. To someone inside the elevator, the apple does not move, so, it has zero momentum. The two observers each have a frame of reference, in which, they observe motions, and, if the elevator is descending steadily, they will see behavior that is consistent with those same physical laws.This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces.[4] It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.[6]If the velocities of the particles are u1 and u2 before the interaction, and afterwards they are v1 and v2, thenwith the negative sign indicating that the forces oppose. Equivalently,In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the law of conservation of momentum, is implied by Newton's laws of motion.[4][5] Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that F1 = dp1/dt and F2 = dp2/dt. Therefore,Example: A model airplane of 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg⋅m/s. The rate of change of momentum is 3 (kg⋅m/s)/s = 3 N.hence the force is equal to mass times acceleration.[1]Under the assumption of constant mass m, it is equivalent to writeImpulse is measured in the derived units of the newton second (1 N⋅s = 1 kg⋅m/s) or dyne second (1 dyne⋅s = 1 g⋅m/s)If the force depends on time, the change in momentum (or impulse J) between times t1 and t2 isIn differential form, this is Newton's second law; the rate of change of the momentum of a particle is proportional to the force F acting on it,[1]If a force F is applied to a particle for a time interval Δt, the momentum of the particle changes by an amountThis is known as Euler's first law.[2][3]If all the particles are moving, the center of mass will generally be moving as well (unless the system is in pure rotation around it). If the center of mass is moving at velocity vcm, the momentum is:A system of particles has a center of mass, a point determined by the weighted sum of their positions:The momenta of more than two particles can be added more generally with the following:The momentum of a system of particles is the sum of their momenta. If two particles have respective masses m1 and m2, and velocities v1 and v2, the total momentum isBeing a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg⋅m/s due north measured with reference to the ground.The unit of momentum is the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity is in meters per second then the momentum is in kilogram meters per second (kg⋅m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second, then the momentum is in gram centimeters per second (g⋅cm/s).The momentum of a particle is conventionally represented by the letter p. It is the product of two quantities, the particle's mass (represented by the letter m) and its velocity (v):[1]Momentum is a vector quantity: it has both magnitude and direction. Since momentum has a direction, it can be used to predict the resulting direction and speed of motion of objects after they collide. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).In continuous systems such as electromagnetic fields, fluids and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.Momentum depends on the frame of reference, but in any inertial frame it is a conserved quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity, (with a modified formula) and, in a modified form, in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.In SI units, it is measured in kilogram meters per second (kg⋅m/s). Newton's second law of motion states that a body's rate of change in momentum is equal to the net force acting on it.In Newtonian mechanics, linear momentum, translational momentum, or simply momentum (pl. momenta) is the product of the mass and velocity of an object. It is a three-dimensional vector quantity, possessing a magnitude and a direction. If m is an object's mass and v is the velocity (also a vector), then the momentum is
Energy
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics. The second law of thermodynamics is valid only for systems which are near or in equilibrium state. For non-equilibrium systems, the laws governing system’s behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production.[18][19] It states that nonequilibrium systems behave in such a way to maximize its entropy production.[20]The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form bywhere the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and the change dS is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system).The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved[17] and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given asInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.[16]This simplified equation is the one used to define the joule, for example.Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:[note 6]Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat.[note 4] Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy,[note 5] and the conductive transfer of thermal energy.In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since H and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics).In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is byEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time,[15] a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.Most kinds of energy (with gravitational energy being a notable exception)[14] are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.[12][13]Richard Feynman said during a 1961 lecture:[13]While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations.[12] The total energy of a system can be calculated by adding up all forms of energy in the system.The fact that energy can be neither created nor be destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out by work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.[11]As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula E = mc², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being "released" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector).[10] In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).Energy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system ("mass manifestations"), and is also responsible for the potential ability of the system to perform work or heating ("energy manifestations"), subject to the limitations of other physical laws.In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.[10]For example, consider electron–positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from the radiant energy of two (or more) annihilating photons.whereWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior,[9] while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings").[note 3] Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants,[8] i.e. reconverted into carbon dioxide and heat.The rest of the chemical energy in O2[7] and the carbohydrate or fat is converted into heat: the ATP is used as a sort of "energy currency", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:[note 2]and some of the energy is used to convert ADP into ATP.Any living organism relies on an external source of energy—radiant energy from the Sun in the case of green plants, chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondriaSunlight's radiant energy is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum.[5] The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy.[6]In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor e−E/kT – that is the probability of molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.[4]Work, a function of energy, is force times distance.In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.The SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.In the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.In 1843, James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the "Joule apparatus": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time.[3] Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.In 1807, Thomas Young was possibly the first to use the term "energy" instead of vis viva, in its modern sense.[2] Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two.The word energy derives from the Ancient Greek: ἐνέργεια, translit. energeia, lit. 'activity, operation',[1] which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.While these two categories are sufficient to describe all forms of energy, it is often convenient refer to particular combinations of potential and kinetic energy as its own form. For example, macroscopic mechanical energy is the sum of translational and rotational kinetic and potential energy in a system neglects the kinetic energy due to temperature, and nuclear energy which combines utilize potentials from the nuclear force and the weak force), among others.The total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object -- or the composite motion of the components of an object - and potential energy reflects the potential of an object to have motion, and generally is a function of the position of an object within a field or may stored in the field itself.Living organisms require available energy to stay alive, such as the energy humans get from food. Human civilization requires energy to function, which it gets from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.Mass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy (in that frame of reference), and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as an increase in mass, with a sensitive enough scale.Common forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.In physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object.[note 1] Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.
Angular momentum
In an 1872 edition of the same book, Rankine stated that "The term angular momentum was introduced by Mr. Hayward,"[40] probably referring to R.B. Hayward's article On a Direct Method of estimating Velocities, Accelerations, and all similar Quantities with respect to Axes moveable in any manner in Space with Applications,[41] which was introduced in 1856, and published in 1864. Rankine was mistaken, as numerous publications feature the term starting in the late 18th to early 19th centuries.[42] However, Hayward's article apparently was the first use of the term and the concept seen by much of the English-speaking world. Before this, angular momentum was typically referred to as "momentum of rotation" in English.[43]William J. M. Rankine's 1858 Manual of Applied Mechanics defined angular momentum in the modern sense for the first time:In 1852 Léon Foucault used a gyroscope in an experiment to display the Earth's rotation.Louis Poinsot in 1803 began representing rotations as a line segment perpendicular to the rotation, and elaborated on the "conservation of moments".In 1799, Pierre-Simon Laplace first realized that a fixed plane was associated with rotation — his invariable plane.Bernoulli wrote in a 1744 letter of a "moment of rotational motion", possibly the first conception of angular momentum as we now understand it.[39]In 1736 Euler, like Newton, touched on some of the equations of angular momentum in his Mechanica without further developing them.[38]Leonhard Euler, Daniel Bernoulli, and Patrick d'Arcy all understood angular momentum in terms of conservation of areal velocity, a result of their analysis of Kepler's Second Law of planetary motion. It is unlikely that they realized the implications for ordinary rotating matter.[37]In the case of triangle SBC, area is equal to 1/2(SB)(VC). Wherever C is eventually located due to the impulse applied at B, the product (SB)(VC), and therefore rmv⊥ remain constant. Similarly so for each of the triangles.The proportionality of angular momentum to the area swept out by a moving object can be understood by realizing that the bases of the triangles, that is, the lines from S to the object, are equivalent to the radius r, and that the heights of the triangles are proportional to the perpendicular component of velocity v⊥. Hence, if the area swept per unit time is constant, then by the triangular area formula 1/2(base)(height), the product (base)(height) and therefore the product rv⊥ are constant: if r and the base length are decreased, v⊥ and height must increase proportionally. Mass is constant, therefore angular momentum rmv⊥ is conserved by this exchange of distance and velocity.Note that because this derivation is geometric, and no specific force is applied, it proves a more general law than Kepler's Second Law of Planetary Motion. It shows that the Law of Areas applies to any central force, attractive or repulsive, continuous or non-continuous, or zero.At point C, the object receives another impulse toward S, again deflecting its path during the third interval from d to D. Thus it continues to E and beyond, the triangles SAB, SBc, SBC, SCd, SCD, SDe, SDE all having the same area. Allowing the time intervals to become ever smaller, the path ABCDE approaches indefinitely close to a continuous curve.During the first interval of time, an object is in motion from point A to point B. Undisturbed, it would continue to point c during the second interval. When the object arrives at B, it receives an impulse directed toward point S. The impulse gives it a small added velocity toward S, such that if this were its only velocity, it would move from B to V during the second interval. By the rules of velocity composition, these two velocities add, and point C is found by construction of parallelogram BcCV. Thus the object's path is deflected by the impulse so that it arrives at point C at the end of the second interval. Because the triangles SBc and SBC have the same base SB and the same height Bc or VC, they have the same area. By symmetry, triangle SBc also has the same area as triangle SAB, therefore the object has swept out equal areas SAB and SBC in equal times.As a planet orbits the Sun, the line between the Sun and the planet sweeps out equal areas in equal intervals of time. This had been known since Kepler expounded his Second Law of Planetary Motion. Newton derived a unique geometric proof, and went on to show that the attractive force of the Sun's gravity was the cause of all of Kepler's laws.However, his geometric proof of the Law of Areas is an outstanding example of Newton's genius, and indirectly proves angular momentum conservation in the case of a central force.He did not further investigate angular momentum directly in the Principia,Newton, in the Principia, hinted at angular momentum in his examples of the First Law of Motion,In classical Maxwell electrodynamics the Poynting vector is a linear momentum density of electromagnetic field.[33]The interplay with quantum mechanics is discussed further in the article on canonical commutation relations.where e is the electric charge of the particle and A the magnetic vector potential of the electromagnetic field. The gauge-invariant angular momentum, that is kinetic angular momentum, is given byWhen describing the motion of a charged particle in an electromagnetic field, the canonical momentum P (derived from the Lagrangian for this system) is not gauge invariant. As a consequence, the canonical angular momentum L = r × P is not gauge invariant either. Instead, the momentum that is physical, the so-called kinetic momentum (used throughout this article), is (in SI units)The relationship between the angular momentum operator and the rotation operators is the same as the relationship between Lie algebras and Lie groups in mathematics. The close relationship between angular momentum and rotations is reflected in Noether's theorem that proves that angular momentum is conserved whenever the laws of physics are rotationally invariant.Quantization of angular momentum was first postulated by Niels Bohr in his Bohr model of the atom and was later predicted by Erwin Schrödinger in his Schrödinger equation.(There are additional restrictions as well, see angular momentum operator for details.)Finally, there is total angular momentum J, which combines both the spin and orbital angular momentum of all particles and fields. (For one particle, J = L + S.) Conservation of angular momentum applies to J, but not to L or S; for example, the spin–orbit interaction allows angular momentum to transfer back and forth between L and S, with the total remaining constant. Electrons and photons need not have integer-based values for total angular momentum, but can also have fractional values.[31]However, in quantum physics, there is another type of angular momentum, called spin angular momentum, represented by the spin operator S. Almost all elementary particles have spin. Spin is often depicted as a particle literally spinning around an axis, but this is a misleading and inaccurate picture: spin is an intrinsic property of a particle, unrelated to any sort of motion in space and fundamentally different from orbital angular momentum. All elementary particles have a characteristic spin, for example electrons have "spin 1/2" (this actually means "spin ħ/2") while photons have "spin 1" (this actually means "spin ħ").Angular momentum in quantum mechanics differs in many profound respects from angular momentum in classical mechanics. In relativistic quantum mechanics, it differs even more, in which the above relativistic definition becomes a tensorial operator.In each of the above cases, for a system of particles, the total angular momentum is just the sum of the individual particle angular momenta, and the centre of mass is for the system.in the language of four-vectors, namely the four position X and the four momentum P, and absorbs the above L together with the motion of the centre of mass of the particle.In relativistic mechanics, the relativistic angular momentum of a particle is expressed as an antisymmetric tensor of second order:Again, this equation in L and ω as tensors is true in any number of dimensions. This equation also appears in the geometric algebra formalism, in which L and ω are bivectors, and the moment of inertia is a mapping between them.The angular velocity can also be defined as an antisymmetric second order tensor, with components ωij. The relation between the two antisymmetric tensors is given by the moment of inertia which must now be a fourth order tensor:[28]or more compactly in index notation:in which the exterior product ∧ replaces the cross product × (these products have similar characteristics but are nonequivalent). This has the advantage of a clearer geometric interpretation as a plane element, defined from the x and p vectors, and the expression is true in any number of dimensions (two or higher). In Cartesian coordinates:In classical mechanics, the angular momentum of a particle can be reinterpreted as a plane element:In modern (20th century) theoretical physics, angular momentum (not including any intrinsic angular momentum – see below) is described using a different formalism, instead of a classical pseudovector. In this formalism, angular momentum is the 2-form Noether charge associated with rotational invariance. As a result, angular momentum is not conserved for general curved spacetimes, unless it happens to be asymptotically rotationally invariant.[citation needed]For the case of the center of mass fixed in space with respect to the origin,In the case of a single particle moving about the arbitrary origin,Rearranging equation (2) by vector identities, multiplying both terms by "one", and grouping appropriately,The first term is the angular momentum of the center of mass relative to the origin. Similar to Single particle, below, it is the angular momentum of one particle of mass M at the center of mass moving with velocity V. The second term is the angular momentum of the particles moving relative to the center of mass, similar to Fixed center of mass, below. The result is general — the motion of the particles is not restricted to rotation or revolution about the origin or center of mass. The particles need not be individual masses, but can be elements of a continuous distribution, such as a solid body.and total angular momentum for the collection of particles is finally,[27]The first term can be rearranged,therefore the second and third terms vanish,It can be shown that (see sidebar),The total angular momentum of the collection of particles is the sum of the angular momentum of each particle,By inspection,The position vector of the center of mass is defined by,[26]The total mass of the particles is simply their sum,For a collection of particles in motion about an arbitrary origin, it is informative to develop the equation of angular momentum by resolving their motion into components about their own center of mass and about the origin. Given,In the derivation which follows, integrals similar to this can replace the sums for the case of continuous mass.and integrating this differential over the volume of the entire mass gives its total angular momentum:For a continuous mass distribution with density function ρ(r), a differential volume element dV with position vector r within the mass has a mass element dm = ρ(r)dV. Therefore, the infinitesimal angular momentum of this element is:In astrodynamics and celestial mechanics, a massless (or per unit mass) angular momentum is defined[25]Noether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.[24]Conservation is not always a full explanation for the dynamics of a system but is a key constraint. For example, a spinning top is subject to gravitational torque making it lean over and change the angular momentum about the nutation axis, but neglecting friction at the point of spinning contact, it has a conserved angular momentum about its spinning axis, and another about its precession axis. Also, in any planetary system, the planets, star(s), comets, and asteroids can all move in numerous complicated ways, but only so that the angular momentum of the system is conserved.The same phenomenon results in extremely fast spin of compact stars (like white dwarfs, neutron stars and black holes) when they are formed out of much larger and slower rotating stars. Decrease in the size of an object n times results in increase of its angular velocity by the factor of n2.The conservation of angular momentum explains the angular acceleration of an ice skater as she brings her arms and legs close to the vertical axis of rotation. By bringing part of the mass of her body closer to the axis she decreases her body's moment of inertia. Because angular momentum is the product of moment of inertia and angular velocity, if the angular momentum remains constant (is conserved), then the angular velocity (rotational speed) of the skater must increase.For a planet, angular momentum is distributed between the spin of the planet and its revolution in its orbit, and these are often exchanged by various mechanisms. The conservation of angular momentum in the Earth–Moon system results in the transfer of angular momentum from Earth to Moon, due to tidal torque the Moon exerts on the Earth. This in turn results in the slowing down of the rotation rate of Earth, at about 65.7 nanoseconds per day,[22] and in gradual increase of the radius of Moon's orbit, at about 3.82 centimeters per year.[23]A rotational analog of Newton's First Law of Motion might be written, "A body continues in a state of rest or of uniform rotation unless acted by an external torque."[18] Thus with no external influence to act upon it, the original angular momentum of the system is conserved.[21]Similarly, a rotational analog of Newton's Second law of Motion might be, "A change in angular momentum is proportional to the applied torque and occurs about the same axis as that torque."[18] Since a torque applied over time is equivalent to a change in angular momentum, then if torque is zero, angular momentum is constant. As above, a system with constant angular momentum is a closed system. Therefore, requiring the system to be closed is equivalent to requiring that no external influence, in the form of a torque, acts upon it.[20]A rotational analog of Newton's Third Law of Motion might be written, "In a closed system, no torque can be exerted on any matter without the exertion on some other matter of an equal and opposite torque."[18] Hence, angular momentum can be exchanged between objects in a closed system, but total angular momentum before and after an exchange remains constant (is conserved).[19]or Angular momentum = moment of inertia × angular velocity, and its time derivative isor force = mass × acceleration. The rotational equivalent for point particles isNewton's Second Law of Motion can be expressed mathematically,The plane perpendicular to the axis of angular momentum and passing through the center of mass[15] is sometimes called the invariable plane, because the direction of the axis remains fixed if only the interactions of the bodies within the system, free from outside influences, are considered.[16] One such plane is the invariable plane of the Solar System.Angular momentum's dependence on position and shape is reflected in its units versus linear momentum: kg·m2/s, N·m·s or J·s for angular momentum versus kg·m/s or N·s for linear momentum. Angular momentum's units can be interpreted as torque·seconds, work·seconds, or energy·seconds. An object with angular momentum of L N·m·s can be reduced to zero rotation (all of the energy can be transferred out of it) by an angular impulse of L N·m·s[13] or equivalently, by torque or work of L N·m for one second, or energy of L J for one second.[14]In brief, the more mass and the farther it is from the center of rotation (the longer the moment arm), the greater the moment of inertia, and therefore the greater the angular momentum for a given angular velocity. In many cases the moment of inertia, and hence the angular momentum, can be simplified by,[12]For a collection of objects revolving about a center, for instance all of the bodies of the Solar System, the orientations may be somewhat organized, as is the Solar System, with most of the bodies' axes lying close to the system's axis. Their orientations may also be completely random.For a rigid body, for instance a wheel or an asteroid, the orientation of rotation is simply the position of the rotation axis versus the matter of the body. It may or may not pass through the center of mass, or it may lie completely outside of the body. For the same body, angular momentum may take a different value for every possible axis about which rotation may take place.[10] It reaches a minimum when the axis passes through the center of mass.[11]Because rotational inertia is a part of angular momentum, it necessarily includes all of the complications of moment of inertia, which is calculated by multiplying elementary bits of the mass by the squares of their distances from the center of rotation.[9] Therefore, the total moment of inertia, and the angular momentum, is a complex function of the configuration of the matter about the center of rotation and the orientation of the rotation for the various bits.is the matter's momentum.[7] Referring this momentum to a central point introduces a complication: the momentum is not applied to the point directly. For instance, a particle of matter at the outer edge of a wheel is, in effect, at the end of a lever of the same length as the wheel's radius, its momentum turning the lever about the center point. This imaginary lever is known as the moment arm. It has the effect of multiplying the momentum's effort in proportion to its length, an effect known as a moment. Hence, the particle's momentum referred to a particular point,Many problems in physics involve matter in motion about some certain point in space, be it in actual rotation about it, or simply moving past it, where it is desired to know what effect the moving matter has on the point — can it exert energy upon it or perform work about it? Energy, the ability to do work, can be stored in matter by setting it in motion — a combination of its inertia and its displacement. Inertia is measured by its mass, and displacement by its velocity. Their product,Angular momentum can be described as the rotational analog of linear momentum. Like linear momentum it involves elements of mass and displacement. Unlike linear momentum it also involves elements of position and shape.The two-dimensional scalar equations of the previous section can thus be given direction:To completely define angular momentum in three dimensions, it is required to know the angle swept out in unit time, the direction perpendicular to the instantaneous plane of angular displacement, and the sense (right- or left-handed) of the angular velocity, as well as the mass involved.[6] By retaining this vector nature of angular momentum, the general nature of the equations is also retained, and can describe any sort of three-dimensional motion about the center of rotation – circular, linear, or otherwise. In vector notation, the angular momentum of a point particle in motion about the origin is defined as:Then the Lagrangian isAnd the potential energy isThis simple analysis can also apply to non-circular motion if only the component of the motion which is perpendicular to the radius vector is considered. In that case,In quantum mechanics, angular momentum is an operator with quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, meaning that at any time, only one component can be measured with definite precision; the other two cannot. Also, the "spin" of elementary particles does not correspond to literal spinning motion.[1]Torque can be defined as the rate of change of angular momentum, analogous to force. The conservation of angular momentum helps explain many observed phenomena, for example the increase in rotational speed of a spinning figure skater as the skater's arms are contracted, the high rotational rates of neutron stars, the Coriolis effect, and precession of tops and gyroscopes. Applications include the gyrocompass, control moment gyroscope, inertial guidance systems, reaction wheels, flying discs or Frisbees, and Earth's rotation to name a few. In general, conservation does limit the possible motion of a system, but does not uniquely determine what the exact motion is.Angular momentum is additive; the total angular momentum of a system is the (pseudo)vector sum of the angular momenta. For continua or fields one uses integration. The total angular momentum of any rigid body can be split into the sum of two main components: the angular momentum of the centre of mass (with a mass equal to the total mass) about the origin, plus the spin angular momentum of the object about the centre of mass.In three dimensions, the angular momentum for a point particle is a pseudovector r×p, the cross product of the particle's position vector r (relative to some origin) and its momentum vector p = mv. This definition can be applied to each point in continua like solids or fluids, or physical fields. Unlike momentum, angular momentum does depend on where the origin is chosen, since the particle's position is measured from it. The angular momentum vector of a point particle is parallel and directly proportional to the angular velocity vector ω of the particle (how fast its angular position changes), where the constant of proportionality depends on both the mass of the particle and its distance from origin. For continuous rigid bodies, though, the spin angular velocity ω is proportional but not always parallel to the spin angular momentum of the object, making the constant of proportionality I (called the moment of inertia) a second-rank tensor rather than a scalar.In physics, angular momentum (rarely, moment of momentum or rotational momentum) is the rotational equivalent of linear momentum. It is an important quantity in physics because it is a conserved quantity – the total angular momentum of a system remains constant unless acted on by an external torque.
Wave function
Whether the wave function really exists, and what it represents, are major questions in the interpretation of quantum mechanics. Many famous physicists of a previous generation puzzled over this problem, such as Schrödinger, Einstein and Bohr. Some advocate formulations or variants of the Copenhagen interpretation (e.g. Bohr, Wigner and von Neumann) while others, such as Wheeler or Jaynes, take the more classical approach[42] and regard the wave function as representing information in the mind of the observer, i.e. a measure of our knowledge of reality. Some, including Schrödinger, Bohm and Everett and others, argued that the wave function must have an objective, physical existence. Einstein thought that a complete description of physical reality should refer directly to physical space and time, as distinct from the wave function, which refers to an abstract mathematical space.[43]The normalization condition requires ρ dmω to be dimensionless, by dimensional analysis Ψ must have the same units as (ω1ω2...ωm)−1/2.must hold at all times during the evolution of the system.where dmω = dω1dω2...dωm is a "differential volume element" in the continuous degrees of freedom. Since the sum of all probabilities must be 1, the normalization conditionThe probability of finding system with α in some or all possible discrete-variable configurations, D ⊆ A, and ω in some or all possible continuous-variable configurations, C ⊆ Ω, is the sum and integral over the density,[nb 14]When interpreted as a probability amplitude (non-relativistic systems with constant number of particles), the probability density of finding the system at α, ω isThen, a component Ψ(α, ω, t) of the vector |Ψ⟩ is referred to as the "wave function" of the system.For example, for a single particle in 3d with spin s, neglecting other degrees of freedom, using Cartesian coordinates, we could take α = (sz) for the spin quantum number of the particle along the z direction, and ω = (x, y, z) for the particle's position coordinates. Here A = {−s, −s + 1, ..., s − 1, s} is the set of allowed spin quantum numbers and Ω = ℝ3 is the set of all possible particle positions throughout 3d position space. An alternative choice is α = (sy) for the spin quantum number along the y direction and ω = (px, py, pz) for the particle's momentum components. In this case A and Ω are the same.where α = (α1, α2, ..., αn) are (dimensionless) discrete quantum numbers, and ω = (ω1, ω2, ..., ωm) are continuous variables (not necessarily dimensionless). All of them index the components of the vector, and |α, ω⟩ are the basis vectors in this representation. All α are in an n-dimensional set A = A1 × A2 × ... An where each Ai is the set of allowed values for αi, likewise all ω are in an m-dimensional "volume" Ω ⊆ ℝm where Ω = Ω1 × Ω2 × ... Ωm and each Ωi ⊆ ℝ is the set of allowed values for ωi, a subset of the real numbers ℝ. For generality n and m are not necessarily equal.As has been demonstrated, the set of all possible wave functions in some representation for a system constitute an in general infinite-dimensional Hilbert space. Due to the multiple possible choices of representation basis, these Hilbert spaces are not unique. One therefore talks about an abstract Hilbert space, state space, where the choice of representation and basis is left undetermined. Specifically, each state is represented as an abstract vector in state space.[41] A quantum state |Ψ⟩ in any representation is generally expressed as a vectorThis does not alter the structure of the Hilbert space that these particular wave functions inhabit, but it should be pointed out that the subspace of the square-integrable functions L2, which is a Hilbert space, satisfying the second requirement is not closed in L2, hence not a Hilbert space in itself.[nb 11] The functions that does not meet the requirements are still needed for both technical and practical reasons.[nb 12][nb 13]It is possible to relax these conditions somewhat for special purposes.[nb 10] If these requirements are not met, it is not possible to interpret the wave function as a probability amplitude.[40]Not all introductory textbooks take the long route and introduce the full Hilbert space machinery, but the focus is on the non-relativistic Schrödinger equation in position representation for certain standard potentials. The following constraints on the wave function are sometimes explicitly formulated for the calculations and physical interpretation to make sense:[38][39]Due to the infinite-dimensional nature of the system, the appropriate mathematical tools are objects of study in functional analysis.With more particles, the situations is more complicated. One has to employ tensor products and use representation theory of the symmetry groups involved (the rotation group and the Lorentz group respectively) to extract from the tensor product the spaces in which the (total) spin wave functions reside. (Further problems arise in the relativistic case unless the particles are free.[37] See the Bethe–Salpeter equation.) Corresponding remarks apply to the concept of isospin, for which the symmetry group is SU(2). The models of the nuclear forces of the sixties (still useful today, see nuclear force) used the symmetry group SU(3). In this case as well, the part of the wave functions corresponding to the inner symmetries reside in some ℂn or subspaces of tensor products of such spaces.There occurs also finite-dimensional Hilbert spaces. The space ℂn is a Hilbert space of dimension n. The inner product is the standard inner product on these spaces. In it, the "spin part" of a single particle wave function resides.More generally, one may consider a unified treatment of all second order polynomial solutions to the Sturm–Liouville equations in the setting of Hilbert space. These include the Legendre and Laguerre polynomials as well as Chebyshev polynomials, Jacobi polynomials and Hermite polynomials. All of these actually appear in physical problems, the latter ones in the harmonic oscillator, and what is otherwise a bewildering maze of properties of special functions becomes an organized body of facts. For this, see Byron & Fuller (1992, Chapter 5).While the space of solutions as a whole is a Hilbert space there are many other Hilbert spaces that commonly occur as ingredients.The above description of the function space containing the wave functions is mostly mathematically motivated. The function spaces are, due to completeness, very large in a certain sense. Not all functions are realistic descriptions of any physical system. For instance, in the function space L2 one can find the function that takes on the value 0 for all rational numbers and -i for the irrationals in the interval [0, 1]. This is square integrable,[nb 8] but can hardly represent a physical state.Not all functions of interest are elements of some Hilbert space, say L2. The most glaring example is the set of functions e​2πip · x⁄h. These are plane wave solutions of the Schrödinger equation for a free particle, but are not normalizable, hence not in L2. But they are nonetheless fundamental for the description. One can, using them, express functions that are normalizable using wave packets. They are, in a sense, a basis (but not a Hilbert space basis, nor a Hamel basis) in which wave functions of interest can be expressed. There is also the artifact "normalization to a delta function" that is frequently employed for notational convenience, see further down. The delta functions themselves aren't square integrable either.In summary, the set of all possible normalizable wave functions for a system with a particular choice of basis, together with the null vector, constitute a Hilbert space.The above observations encapsulate the essence of the function spaces of which wave functions are elements. However the description is not yet complete. There is a further technical requirement on the function space, that of completeness, that allows one to take limits of sequences in the function space, and be ensured that, if the limit exists, it is an element of the function space. A complete inner product space is called a Hilbert space. The property of completeness is crucial in advanced treatments and applications of quantum mechanics. For instance, the existence of projection operators or orthogonal projections relies on the completeness of the space.[36] These projection operators, in turn, are essential for the statement and proof of many useful theorems, e.g. the spectral theorem. It is not very important in introductory quantum mechanics, and technical details and links may be found in footnotes like the one that follows.[nb 7] The space L2 is a Hilbert space, with inner product presented later. The function space of the example of the figure is a subspace of L2. A subspace of a Hilbert space is a Hilbert space if it is closed.where Φ and Ψ are assumed normalized. Consider a scattering experiment. In quantum field theory, if Φout describes a state in the "distant future" (an "out state") after interactions between scattering particles have ceased, and Ψin an "in state" in the "distant past", then the quantities (Φout, Ψin), with Φout and Ψin varying over a complete set of in states and out states respectively, is called the S-matrix or scattering matrix. Knowledge of it is, effectively, having solved the theory at hand, at least as far as predictions go. Measurable quantities such as decay rates and scattering cross sections are calculable from the S-matrix.[35]This motivates the introduction of an inner product on the vector space of abstract quantum states, compatible with the mathematical observations above when passing to a representation. It is denoted (Ψ, Φ), or in the Bra–ket notation ⟨Ψ|Φ⟩. It yields a complex number. With the inner product, the function space is an inner product space. The explicit appearance of the inner product (usually an integral or a sum of integrals) depends on the choice of representation, but the complex number (Ψ, Φ) does not. Much of the physical interpretation of quantum mechanics stems from the Born rule. It states that the probability p of finding upon measurement the state Φ given the system is in the state Ψ isThere is an additional algebraic structure on the vector spaces of wave functions and the abstract state space.Each choice of representation should be thought of as specifying a unique function space in which wave functions corresponding to that choice of representation lives. This distinction is best kept, even if one could argue that two such function spaces are mathematically equal, e.g. being the set of square integrable functions. One can then think of the function spaces as two distinct copies of that set.The abstract states are "abstract" only in that an arbitrary choice necessary for a particular explicit description of it is not given. This is the same as saying that no choice of maximal set of commuting observables has been given. This is analogous to a vector space without a specified basis. Wave functions corresponding to a state are accordingly not unique. This non-uniqueness reflects the non-uniqueness in the choice of a maximal set of commuting observables. For one spin particle in one dimension, to a particular state there corresponds two wave functions, Ψ(x, Sz) and Ψ(p, Sy), both describing the same state.Basic states are characterized by a set of quantum numbers. This is a set of eigenvalues of a maximal set of commuting observables. Physical observables are represented by linear operators, also called observables, on the vectors space. Maximality means that there can be added to the set no further algebraically independent observables that commute with the ones already present. A choice of such a set may be called a choice of representation.This similarity is of course not accidental. There are also a distinctions between the spaces to keep in mind.A wave function is an element of a function space partly characterized by the following concrete and abstract descriptions.The concept of function spaces enters naturally in the discussion about wave functions. A function space is a set of functions, usually with some defining requirements on the functions (in the present case that they are square integrable), sometimes with an algebraic structure on the set (in the present case a vector space structure with an inner product), together with a topology on the set. The latter will sparsely be used here, it is only needed to obtain a precise definition of what it means for a subset of a function space to be closed. It will be concluded below that the function space of wave functions is a Hilbert space. This observation is the foundation of the predominant mathematical formulation of quantum mechanics.The figure can serve to illustrate some further properties of the function spaces of wave functions.In the figure of the hydrogen orbitals, the 19 sub-images are images of wave functions in position space (their norm squared). The wave functions each represent the abstract state characterized by the triple of quantum numbers (n, l, m), in the lower right of each image. These are the principal quantum number, the orbital angular momentum quantum number and the magnetic quantum number. Together with one spin-projection quantum number of the electron, this is a complete set of observables.This solution does not take into account the spin of the electron.where a0 = 4πε0ħ2/mee2 is the Bohr radius, L2ℓ + 1
n − ℓ − 1 are the generalized Laguerre polynomials of degree n − ℓ − 1, n = 1, 2, ... is the principal quantum number, ℓ = 0, 1, ... n − 1 the azimuthal quantum number, m = −ℓ, −ℓ + 1, ..., ℓ − 1, ℓ the magnetic quantum number. Hydrogen-like atoms have very similar solutions.where R are radial functions and Ym
ℓ(θ, φ) are spherical harmonics of degree ℓ and order m. This is the only atom for which the Schrödinger equation has been solved for exactly. Multi-electron atoms require approximative methods. The family of solutions are:[33]It is convenient to use spherical coordinates, and the wavefunction can be separated into functions of each coordinate,[32]The wave functions of an electron in a Hydrogen atom are expressed in terms of spherical harmonics and generalized Laguerre polynomials (these are defined differently by different authors—see main article on them and the hydrogen atom).where n = 0,1,2,....The wave functions for the quantum harmonic oscillator can be expressed in terms of Hermite polynomials Hn, they areIn a semiconductor crystallite whose radius is smaller than the size of its exciton Bohr radius, the excitons are squeezed, leading to quantum confinement. The energy levels can then be modeled using the particle in a box model in which the energy of different states is dependent on the length of the box.The standard interpretation of this is as a stream of particles being fired at the step from the left (the direction of negative x): setting Ar = 1 corresponds to firing particles singly; the terms containing Ar and Cr signify motion to the right, while Al and Cl – to the left. Under this beam interpretation, put Cl = 0 since no particles are coming from the right. By applying the continuity of wave functions and their derivatives at the boundaries, it is hence possible to determine the constants above.Note that these wave functions are not normalized; see scattering theory for discussion.and the steady-state solutions to the wave equation have the form (for some constants k, κ)One of most prominent features of the wave mechanics is a possibility for a particle to reach a location with a prohibitive (in classical mechanics) force potential. A common model is the "potential barrier", the one-dimensional case has the potentialThe following are solutions to the Schrödinger equation for one nonrelativistic spinless particle.The time dependence of the quantum state and the operators can be placed according to unitary transformations on the operators and states. For any quantum state |Ψ⟩ and operator O, in the Schrödinger picture |Ψ(t)⟩ changes with time according to the Schrödinger equation while O is constant. In the Heisenberg picture it is the other way round, |Ψ⟩ is constant while O(t) evolves with time according to the Heisenberg equation of motion. The Dirac (or interaction) picture is intermediate, time dependence is places in both operators and states which evolve according to equations of motion. It is useful primarily in computing S-matrix elements.[31]where E is the energy eigenvalue of the system corresponding to the eigenstate Ψ. Wave functions of this form are called stationary states.For systems in time-independent potentials, the wave function can always be written as a function of the degrees of freedom multiplied by a time-dependent phase factor, the form of which is given by the Schrödinger equation. For N particles, considering their positions only and suppressing other degrees of freedom,and the probability that particle 1 is in region R1 with spin sz1 = m1 and particle 2 is in region R2 with spin sz2 = m2 etc. at time t is the integral of the probability density over these regions and evaluated at these spin numbers:For the general case of N particles with spin in 3d, if Ψ is interpreted as a probability amplitude, the probability density isThe multidimensional Fourier transforms of the position or position–spin space wave functions yields momentum or momentum–spin space wave functions.this is altogether N three-dimensional volume integrals and N sums over the spins. The differential volume elements d3ri are also written "dVi" or "dxi dyi dzi".The formulae for the inner products are integrals over all coordinates or momenta and sums over all spin quantum numbers. For the general case of N particles with spin in 3d,For identical particles, symmetry requirements apply to both position and spin arguments of the wave function so it has the overall correct symmetry.Accumulating all these components into a single vector,The wave function for N particles each with spin is the complex-valued functionAgain, there is no symmetry requirement for the distinguishable particle coordinates xi.For a collection of particles, some identical with coordinates r1, r2, ... and others distinguishable x1, x2, ... (not identical with each other, and not identical to the aforementioned identical particles), the wave function is symmetric or antisymmetric in the identical particle coordinates ri only:For N distinguishable particles (no two being identical, i.e. no two having the same set of quantum numbers), there is no requirement for the wave function to be either symmetric or antisymmetric.where the + sign occurs if the particles are all bosons and − sign if they are all fermions. In other words, the wave function is either totally symmetric in the positions of bosons, or totally antisymmetric in the positions of fermions.[30] The physical interchange of particles corresponds to mathematically switching arguments in the wave function. The antisymmetry feature of fermionic wave functions leads to the Pauli principle. Generally, bosonic and fermionic symmetry requirements are the manifestation of particle statistics and are present in other quantum state formalisms.In quantum mechanics there is a fundamental distinction between identical particles and distinguishable particles. For example, any two electrons are identical and fundamentally indistinguishable from each other; the laws of physics make it impossible to "stamp an identification number" on a certain electron to keep track of it.[27] This translates to a requirement on the wave function for a system of identical particles:where ri is the position of the ith particle in three-dimensional space, and t is time. Altogether, this is a complex-valued function of 3N + 1 real variables.If there are many particles, in general there is only one wave function, not a separate wave function for each particle. The fact that one wave function describes many particles is what makes quantum entanglement and the EPR paradox possible. The position-space wave function for N particles is written:[19]The preceding discussion is not limited to spin as a discrete variable, the total angular momentum J may also be used.[29] Other discrete degrees of freedom, like isospin, can expressed similarly to the case of spin above.The tensor product factorization is only possible if the orbital and spin angular momenta of the particle are separable in the Hamiltonian operator underlying the system's dynamics (in other words, the Hamiltonian can be split into the sum of orbital and spin terms[28]). The time dependence can be placed in either factor, and time evolution of each can be studied separately. The factorization is not possible for those interactions where an external field or any space-dependent quantity couples to the spin; examples include a particle in a magnetic field, and spin-orbit coupling.with the identificationsFor a single particle, the tensor product ⊗ of its position state vector |ψ⟩ and spin state vector |ξ⟩ gives the composite position-spin state vectorAll values of the wave function, not only for discrete but continuous variables also, collect into a single vectorin which the spin dependence is placed in indexing the entries, and the wave function is a complex vector-valued function of space and time only.and these can also be arranged into a column vectorMore generally, for a particle in 3d with any spin, the wave function can be written in "position–spin space" as:The entire vector ξ is a solution of the Schrödinger equation (with a suitable Hamiltonian), which unfolds to a coupled system of 2s + 1 ordinary differential equations with solutions ξ(s, t), ξ(s − 1, t), ..., ξ(−s, t). The term "spin function" instead of "wave function" is used by some authors. This contrasts the solutions to position space wave functions, the position coordinates being continuous degrees of freedom, because then the Schrödinger equation does take the form of a wave equation.In bra ket notation, these easily arrange into the components of a vector[nb 4]where sz is the spin projection quantum number along the z axis. (The z axis is an arbitrary choice; other axes can be used instead if the wave function is transformed appropriately, see below.) The sz parameter, unlike r and t, is a discrete variable. For example, for a spin-1/2 particle, sz can only be +1/2 or −1/2, and not any other value. (In general, for spin s, sz can be s, s − 1, ... , −s + 1, −s). Inserting each quantum number gives a complex valued function of space and time, there are 2s + 1 of them. These can be arranged into a column vector[nb 3]For a particle with spin, ignoring the position degrees of freedom, the wave function is a function of spin only (time is a parameter);All the previous remarks on inner products, momentum space wave functions, Fourier transforms, and so on extend to higher dimensions.where r is the position vector in three-dimensional space, and t is time. As always Ψ(r, t) is a complex-valued function of real variables. As a single vector in Dirac notationThe position-space wave function of a single particle without spin in three spatial dimensions is similar to the case of one spatial dimension above:Following are the general forms of the wave function for systems in higher dimensions and more particles, as well as including other degrees of freedom than position coordinates or momentum components.In practice, the position-space wave function is used much more often than the momentum-space wave function. The potential entering the relevant equation (Schrödinger, Dirac, etc.) determines in which basis the description is easiest. For the harmonic oscillator, x and p enter symmetrically, so there it doesn't matter which description one uses. The same equation (modulo constants) results. From this follows, with a little bit of afterthought, a factoid: The solutions to the wave equation of the harmonic oscillator are eigenfunctions of the Fourier transform in L2.[nb 2]The position-space and momentum-space wave functions are thus found to be Fourier transforms of each other.[27] The two wave functions contain the same information, and either one alone is sufficient to calculate any property of the particle. As representatives of elements of abstract physical Hilbert space, whose elements are the possible states of the system under consideration, they represent the same state vector, hence identical physical states, but they are not generally equal when viewed as square-integrable functions.Likewise, using eigenfunctions of position,one obtainsThen utilizing the known expression for suitably normalized eigenstates of momentum in the position representation solutions of the free Schrödinger equationNow take the projection of the state Ψ onto eigenfunctions of momentum using the last expression in the two equations,[26]The x and p representations areFor another thing, though they are linearly independent, there are too many of them (they form an uncountable set) for a basis for physical Hilbert space. They can still be used to express all functions in it using Fourier transforms as described next.forms what is called the momentum basis. This "basis" is not a basis in the usual mathematical sense. For one thing, since the functions aren't normalizable, they are instead normalized to a delta function,a plane wave, which can be used in the description of a particle with momentum exactly p, since it is an eigenfunction of the momentum operator. These functions are not normalizable to unity (they aren't square-integrable), so they are not really elements of physical Hilbert space. The setOne particular solution to the time-independent Schrödinger equation isAnalogous to the position case, the inner product of two wave functions Φ1(p, t) and Φ2(p, t) can be defined as:where p is the momentum in one dimension, which can be any value from −∞ to +∞, and t is time.The particle also has a wave function in momentum space:Finding the identity operator in a basis allows the abstract state to be expressed explicitly in a basis, and more (the inner product between two state vectors, and other operators for observables, can be expressed in the basis).which illuminates the identity operatorandthusThe time parameter is often suppressed, and will be in the following. The x coordinate is a continuous index. The |x⟩ are the basis vectors, which are orthonormal so their inner product is a delta function;and is referred to as a "quantum state vector", or simply "quantum state".There are several advantages to understanding wave functions as representing elements of an abstract vector space:At a particular instant of time, all values of the wave function Ψ(x, t) are components of a vector. There are uncountably infinitely many of them and integration is used in place of summation. In Bra–ket notation, this vector is writtenwhich, assuming both wave functions are normalized, is interpreted as the probability of the wave function Ψ2 "collapsing" to the new wave function Ψ1 upon measurement of an observable, whose eigenvalues are the possible results of the measurement, with Ψ1 being an eigenvector of the resulting eigenvalue. This is the Born rule,[8] and is one of the fundamental postulates of quantum mechanics.In the Copenhagen interpretation, the modulus squared of the inner product (a complex number) gives a real numberIf the wave functions Ψn were nonorthogonal, the coefficients would be less simple to obtain.If (Ψ, Ψ) = 1, then Ψ is normalized. If Ψ is not normalized, then dividing by its norm gives the normalized function Ψ/||Ψ||. Two wave functions Ψ1 and Ψ2 are orthogonal if (Ψ1, Ψ2) = 0. If they are normalized and orthogonal, they are orthonormal. Orthogonality (hence also orthonormality) of wave functions is not a necessary condition wave functions must satisfy, but is instructive to consider since this guarantees linear independence of the functions. In a linear combination of orthogonal wave functions Ψn we have,is always a positive real number. The number ||Ψ|| (not ||Ψ||2) is called the norm of the wave function Ψ, and is not the same as the modulus |Ψ|.More details are given below. Although the inner product of two wave functions is a complex number, the inner product of a wave function Ψ with itself,For a given system, the set of all possible normalizable wave functions (at any given time) forms an abstract mathematical vector space, meaning that it is possible to add together different wave functions, and multiply wave functions by complex numbers (see vector space for details). Technically, because of the normalization condition, wave functions form a projective space rather than an ordinary vector space. This vector space is infinite-dimensional, because there is no finite set of functions which can be added together in various combinations to create every possible function. Also, it is a Hilbert space, because the inner product of two wave functions Ψ1 and Ψ2 can be defined as the complex number (at time t)[nb 1]because if the particle is measured, there is 100% probability that it will be somewhere.where t is the time at which the particle was measured. This leads to the normalization condition:is interpreted as the probability density that the particle is at x. The asterisk indicates the complex conjugate. If the particle's position is measured, its location cannot be determined from the wave function, but is described by a probability distribution. The probability that its position x will be in the interval a ≤ x ≤ b is the integral of the density over this interval:For one spinless particle in 1d, if the wave function is interpreted as a probability amplitude, the square modulus of the wave function, the positive real numberwhere x is position and t is time. This is a complex-valued function of two real variables x and t.The state of such a particle is completely described by its wave function,For now, consider the simple case of a non-relativistic single particle, without spin, in one spatial dimension. More general cases are discussed below.In string theory, the situation remains analogous. For instance, a wave function in momentum space has the role of Fourier expansion coefficient in a general state of a particle (string) with momentum that is not sharply defined.[25]It should be emphasized that this applies to free field equations; interactions are not included. If a Lagrangian density (including interactions) is available, then the Lagrangian formalism will yield an equation of motion at the classical level. This equation may be very complex and not amenable to solution. Any solution would refer to a fixed number of particles and would not account for the term "interaction" as referred to in these theories, which involves the creation and annihilation of particles and not external potentials as in ordinary "first quantized" quantum theory.Thus the Klein-Gordon equation (spin 0) and the Dirac equation (spin ​1⁄2) in this guise remain in the theory. Higher spin analogues include the Proca equation (spin 1), Rarita–Schwinger equation (spin ​3⁄2), and, more generally, the Bargmann–Wigner equations. For massless free fields two examples are the free field Maxwell equation (spin 1) and the free field Einstein equation (spin 2) for the field operators.[23] All of them are essentially a direct consequence of the requirement of Lorentz invariance. Their solutions must transform under Lorentz transformation in a prescribed way, i.e. under a particular representation of the Lorentz group and that together with few other reasonable demands, e.g. the cluster decomposition principle,[24] with implications for causality is enough to fix the equations.Relativity makes it inevitable that the number of particles in a system is not constant. For full reconciliation, quantum field theory is needed.[22] In this theory, the wave equations and the wave functions have their place, but in a somewhat different guise. The main objects of interest are not the wave functions, but rather operators, so called field operators (or just fields where "operator" is understood) on the Hilbert space of states (to be described next section). It turns out that the original relativistic wave equations and their solutions are still needed to build the Hilbert space. Moreover, the free fields operators, i.e. when interactions are assumed not to exist, turn out to (formally) satisfy the same equation as do the fields (wave functions) in many cases.The Klein-Gordon equation and the Dirac equation, while being relativistic, do not represent full reconciliation of quantum mechanics and special relativity. The branch of quantum mechanics where these equations are studied the same way as the Schrödinger equation, often called relativistic quantum mechanics, while very successful, has its limitations (see e.g. Lamb shift) and conceptual problems (see e.g. Dirac sea).All these wave equations are of enduring importance. The Schrödinger equation and the Pauli equation are under many circumstances excellent approximations of the relativistic variants. They are considerably easier to solve in practical problems than the relativistic counterparts.In 1927, Pauli phenomenologically found a non-relativistic equation to describe spin-1/2 particles in electromagnetic fields, now called the Pauli equation.[21] Pauli found the wave function was not described by a single complex function of space and time, but needed two complex numbers, which respectively correspond to the spin +1/2 and −1/2 states of the fermion. Soon after in 1928, Dirac found an equation from the first successful unification of special relativity and quantum mechanics applied to the electron, now called the Dirac equation. In this, the wave function is a spinor represented by four complex-valued components:[19] two for the electron and two for the electron's antiparticle, the positron. In the non-relativistic limit, the Dirac wave function resembles the Pauli wave function for the electron. Later, other relativistic wave equations were found.Schrödinger did encounter an equation for the wave function that satisfied relativistic energy conservation before he published the non-relativistic one, but discarded it as it predicted negative probabilities and negative energies. In 1927, Klein, Gordon and Fock also found it, but incorporated the electromagnetic interaction and proved that it was Lorentz invariant. De Broglie also arrived at the same equation in 1928. This relativistic wave equation is now most commonly known as the Klein–Gordon equation.[20]In 1926, Schrödinger published the famous wave equation now named after him, indeed the Schrödinger equation, based on classical Conservation of energy using quantum operators and the de Broglie relations such that the solutions of the equation are the wave functions for the quantum system.[15] However, no one was clear on how to interpret it.[16] At first, Schrödinger and others thought that wave functions represent particles that are spread out with most of the particle being where the wave function is large.[17] This was shown to be incompatible with the elastic scattering of a wave packet (representing a particle) off a target; it spreads out in all directions.[8] While a scattered particle may scatter in any direction, it does not break up and take off in all directions. In 1926, Born provided the perspective of probability amplitude.[8][9][18] This relates calculations of quantum mechanics directly to probabilistic experimental observations. It is accepted as part of the Copenhagen interpretation of quantum mechanics. There are many other interpretations of quantum mechanics. In 1927, Hartree and Fock made the first step in an attempt to solve the N-body wave function, and developed the self-consistency cycle: an iterative algorithm to approximate the solution. Now it is also known as the Hartree–Fock method.[19] The Slater determinant and permanent (of a matrix) was part of the method, provided by John C. Slater.In the 1920s and 1930s, quantum mechanics was developed using calculus and linear algebra. Those who used the techniques of calculus included Louis de Broglie, Erwin Schrödinger, and others, developing "wave mechanics". Those who applied the methods of linear algebra included Werner Heisenberg, Max Born, and others, developing "matrix mechanics". Schrödinger subsequently showed that the two approaches were equivalent.[14]In Born's statistical interpretation in non-relativistic quantum mechanics,[8][9][10] the squared modulus of the wave function, |ψ|2, is a real number interpreted as the probability density of measuring a particle's being detected at a given place – or having a given momentum – at a given time, and possibly having definite values for discrete degrees of freedom. The integral of this quantity, over all the system's degrees of freedom, must be 1 in accordance with the probability interpretation. This general requirement that a wave function must satisfy is called the normalization condition. Since the wave function is complex valued, only its relative phase and relative magnitude can be measured—its value does not, in isolation, tell anything about the magnitudes or directions of measurable observables; one has to apply quantum operators, whose eigenvalues correspond to sets of possible results of measurements, to the wave function ψ and calculate the statistical distributions for measurable quantities.According to the superposition principle of quantum mechanics, wave functions can be added together and multiplied by complex numbers to form new wave functions and form a Hilbert space. The inner product between two wave functions is a measure of the overlap between the corresponding physical states, and is used in the foundational probabilistic interpretation of quantum mechanics, the Born rule, relating transition probabilities to inner products. The Schrödinger equation determines how wave functions evolve over time, and a wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation. This explains the name "wave function," and gives rise to wave–particle duality. However, the wave function in quantum mechanics describes a kind of physical phenomenon, still open to different interpretations, which fundamentally differs from that of classic mechanical waves.[1][2][3][4][5][6][7]For a given system, the choice of which commuting degrees of freedom to use is not unique, and correspondingly the domain of the wave function is also not unique. For instance it may be taken to be a function of all the position coordinates of the particles over position space, or the momenta of all the particles over momentum space; the two are related by a Fourier transform. Some particles, like electrons and photons, have nonzero spin, and the wave function for such particles includes spin as an intrinsic, discrete degree of freedom; other discrete variables can also be included, such as isospin. When a system has internal degrees of freedom, the wave function at each point in the continuous degrees of freedom (e.g., a point in space) assigns a complex number for each possible value of the discrete degrees of freedom (e.g., z-component of spin) -- these values are often displayed in a column matrix (e.g., a 2 × 1 column vector for a non-relativistic electron with spin ​1⁄2).The wave function is a function of the degrees of freedom corresponding to some maximal set of commuting observables. Once such a representation is chosen, the wave function can be derived from the quantum state.A wave function in quantum physics is a mathematical description of the quantum state of an isolated quantum system. The wave function is a complex-valued probability amplitude, and the probabilities for the possible results of measurements made on the system can be derived from it. The most common symbols for a wave function are the Greek letters ψ or Ψ (lower-case and capital psi, respectively).
Lp space
One may also define spaces Lp(M) on a manifold, called the intrinsic Lp spaces of the manifold, using densities.As Lp-spaces, the weighted spaces have nothing special, since Lp(S, w dμ) is equal to Lp(S, dν). But they are the natural framework for several results in harmonic analysis (Grafakos 2004); they appear for example in the Muckenhoupt theorem: for 1 < p < ∞, the classical Hilbert transform is defined on Lp(T, λ) where T denotes the unit circle and λ the Lebesgue measure; the (nonlinear) Hardy–Littlewood maximal operator is bounded on Lp(Rn, λ). Muckenhoupt's theorem describes weights w such that the Hilbert transform remains bounded on Lp(T, w dλ) and the maximal operator on Lp(Rn, w dλ).or, in terms of the Radon–Nikodym derivative, w = dν/dμ  the norm for Lp(S, w dμ) is explicitlyAs before, consider a measure space (S, Σ, μ). Let w : S → [0, ∞) be a measurable function. The w-weighted Lp space is defined as Lp(S, w dμ), where w dμ means the measure ν defined byA major result that uses the Lp,w-spaces is the Marcinkiewicz interpolation theorem, which has broad applications to harmonic analysis and the study of singular integrals.is comparable to the Lp,w-norm. Further in the case p > 1, this expression defines a norm if r = 1. Hence for p > 1 the weak Lp spaces are Banach spaces (Grafakos 2004).For any 0 < r < p the expressionUnder the convention that two functions are equal if they are equal μ almost everywhere, then the spaces Lp,w are complete (Grafakos 2004).In fact, one hasand in particular Lp(S, μ) ⊂ Lp,w(S, μ).The Lp,w-norm is not a true norm, since the triangle inequality fails to hold. Nevertheless, for f in Lp(S, μ),The weak Lp coincide with the Lorentz spaces Lp,∞, so this notation is also used to denote them.The best constant C for this inequality is the Lp,w-norm of f, and is denoted byA function f is said to be in the space weak Lp(S, μ), or Lp,w(S, μ), if there is a constant C > 0 such that, for all t > 0,If f is in Lp(S, μ) for some p with 1 ≤ p < ∞, then by Markov's inequality,Let (S, Σ, μ) be a measure space, and f a measurable function with real or complex values on S. The distribution function of f is defined for t > 0 byThe resulting space L0(Rn, λ) coincides as topological vector space with L0(Rn, g(x) dλ(x)), for any positive λ–integrable density g.For the infinite Lebesgue measure λ on Rn, the definition of the fundamental system of neighborhoods could be modified as followswhere φ is bounded continuous concave and non-decreasing on [0, ∞), with φ(0) = 0 and φ(t) > 0 when t > 0 (for example, φ(t) = min(t, 1)). Such a metric is called Lévy-metric for L0. Under this metric the space L0 is complete (it is again an F-space). The space L0 is in general not locally bounded, and not locally convex.The topology can be defined by any metric d of the formThe description is easier when μ is finite. If μ is a finite measure on (S, Σ), the 0 function admits for the convergence in measure the following fundamental system of neighborhoodsThe vector space of (equivalence classes of) measurable functions on (S, Σ, μ) is denoted L0(S, Σ, μ) (Kalton, Peck & Roberts 1984). By definition, it contains all the Lp, and is equipped with the topology of convergence in measure. When μ is a probability measure (i.e., μ(S) = 1), this mode of convergence is named convergence in probability.The situation of having no linear functionals is highly undesirable for the purposes of doing analysis. In the case of the Lebesgue measure on Rn, rather than work with Lp for 0 < p < 1, it is common to work with the Hardy space H p whenever possible, as this has quite a few linear functionals: enough to distinguish points from one another. However, the Hahn–Banach theorem still fails in H p for p < 1 (Duren 1970, §7.5).The only nonempty convex open set in Lp([0, 1]) is the entire space (Rudin 1991, §1.47). As a particular consequence, there are no nonzero linear functionals on Lp([0, 1]): the dual space is the zero space. In the case of the counting measure on the natural numbers (producing the sequence space Lp(μ) = ℓ p), the bounded linear functionals on ℓ p are exactly those that are bounded on ℓ 1, namely those given by sequences in ℓ ∞. Although ℓ p does contain non-trivial convex open sets, it fails to have enough of them to give a base for the topology.The space Lp for 0 < p < 1 is an F-space: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ≥ 1. It is the prototypical example of an F-space that, for most reasonable measure spaces, is not locally convex: in ℓ p or Lp([0, 1]), every open convex set containing the 0 function is unbounded for the p-quasi-norm; therefore, the 0 vector does not possess a fundamental system of convex neighborhoods. Specifically, this is true if the measure space S contains an infinite family of disjoint measurable sets of finite positive measure.This result may be used to prove Clarkson's inequalities, which are in turn used to establish the uniform convexity of the spaces Lp for 1 < p < ∞ (Adams & Fournier 2003).In this setting Lp satisfies a reverse Minkowski inequality, that is for u, v in Lpis a metric on Lp(μ). The resulting metric space is complete; the verification is similar to the familiar case when p ≥ 1.and so the functionAs before, we may introduce the p-norm || f ||p = Np( f )1/p, but || · || p does not satisfy the triangle inequality in this case, and defines only a quasi-norm. The inequality (a + b) p ≤ a p + b p, valid for a, b ≥ 0 implies that (Rudin 1991, §1.47)Let (S, Σ, μ) be a measure space. If 0 < p < 1, then Lp(μ) can be defined as above: it is the vector space of those measurable functions  f  such thatwhereSeveral properties of general functions in Lp(Rd) are first proved for continuous and compactly supported functions (sometimes for step functions), then extended by density to all functions. For example, it is proved this way that translations are continuous on Lp(Rd), in the following sense:This applies in particular when S = Rd and when μ is the Lebesgue measure. The space of continuous and compactly supported functions is dense in Lp(Rd). Similarly, the space of integrable step functions is dense in Lp(Rd); this space is the linear span of indicator functions of bounded intervals when d = 1, of bounded rectangles when d = 2 and more generally of products of bounded intervals.If S can be covered by an increasing sequence (Vn) of open sets that have finite measure, then the space of p–integrable continuous functions is dense in Lp(S, Σ, μ). More precisely, one can use bounded continuous functions that vanish outside one of the open sets Vn.It follows that there exists φ continuous on S such thatSuppose V ⊂ S is an open set with μ(V) < ∞. It can be proved that for every Borel set A ∈ Σ contained in V, and for every ε > 0, there exist a closed set F and an open set U such thatMore can be said when S is a metrizable topological space and Σ its Borel σ–algebra, i.e., the smallest σ–algebra of subsets of S containing the open sets.Let (S, Σ, μ) be a measure space. An integrable simple function  f  on S is one of the formThroughout this section we assume that: 1 ≤ p < ∞.the case of equality being achieved exactly when  f  = 1 μ-a.e.The constant appearing in the above inequality is optimal, in the sense that the operator norm of the identity I : Lq(S, μ) → Lp(S, μ) is preciselyleading toNeither condition holds for the real line with the Lebesgue measure. In both cases the embedding is continuous, in that the identity operator is a bounded linear map from Lq to Lp in the first case, and Lp to Lq in the second. (This is a consequence of the closed graph theorem and properties of Lp spaces.) Indeed, if the domain S has finite measure, one can make the following explicit calculation using Hölder's inequalityColloquially, if 1 ≤ p < q ≤ ∞, then Lp(S, μ) contains functions that are more locally singular, while elements of Lq(S, μ) can be more spread out. Consider the Lebesgue measure on the half line (0, ∞). A continuous function in L1 might blow up near 0 but must decay sufficiently fast toward infinity. On the other hand, continuous functions in L∞ need not decay at all but no blow-up is allowed. The precise technical result is the following.[6] Suppose that 0 < p < q ≤ ∞. Then:The dual of L∞ is subtler. Elements of L∞(μ)∗ can be identified with bounded signed finitely additive measures on S that are absolutely continuous with respect to μ. See ba space for more details. If we assume the axiom of choice, this space is much bigger than L1(μ) except in some trivial cases. However, Saharon Shelah proved that there are relatively consistent extensions of Zermelo–Fraenkel set theory (ZF + DC + "Every subset of the real numbers has the Baire property") in which the dual of ℓ∞ is ℓ1.[5]If the measure μ on S is sigma-finite, then the dual of L1(μ) is isometrically isomorphic to L∞(μ) (more precisely, the map κ1 corresponding to p = 1 is an isometry from L∞(μ) onto L1(μ)∗).This map coincides with the canonical embedding J of Lp(μ) into its bidual. Moreover, the map jp is onto, as composition of two onto isometries, and this proves reflexivity.For 1 < p < ∞, the space Lp(μ) is reflexive. Let κp be as above and let κq : Lp(μ) → Lq(μ)∗ be the corresponding linear isometry. Consider the map from Lp(μ) to Lp(μ)∗∗, obtained by composing κq with the transpose (or adjoint) of the inverse of κp:The fact that κp(g) is well defined and continuous follows from Hölder's inequality. κp : Lq(μ) → Lp(μ)∗ is a linear mapping which is an isometry by the extremal case of Hölder's inequality. It is also possible to show (for example with the Radon–Nikodym theorem, see[4]) that any G ∈ Lp(μ)∗ can be expressed this way: i.e., that κp is onto. Since κp is onto and isometric, it is an isomorphism of Banach spaces. With this (isometric) isomorphism in mind, it is usual to say simply that Lq is the dual Banach space of Lp.For 1 ≤ p ≤ ∞ the ℓp spaces are a special case of Lp spaces, when S = N, and μ is the counting measure on N. More generally, if one considers any set S with the counting measure, the resulting Lp space is denoted ℓp(S). For example, the space ℓp(Z) is the space of all sequences indexed by the integers, and when defining the p-norm on such a space, one sums over all the integers. The space ℓp(n), where n is the set with n elements, is Rn with its p-norm as defined above. As any Hilbert space, every space L2 is linearly isometric to a suitable ℓ2(I), where the cardinality of the set I is the cardinality of an arbitrary Hilbertian basis for this particular L2.If we use complex-valued functions, the space L∞ is a commutative C*-algebra with pointwise multiplication and conjugation. For many measure spaces, including all sigma-finite ones, it is in fact a commutative von Neumann algebra. An element of L∞ defines a bounded operator on any Lp space by multiplication.The additional inner product structure allows for a richer theory, with applications to, for instance, Fourier series and quantum mechanics. Functions in L2 are sometimes called quadratically integrable functions, square-integrable functions or square-summable functions, but sometimes these terms are reserved for functions that are square-integrable in some other sense, such as in the sense of a Riemann integral (Titchmarsh 1976).Similar to the ℓp spaces, L2 is the only Hilbert space among Lp spaces. In the complex case, the inner product on L2 is defined byWhen the underlying measure space S is understood, Lp(S, μ) is often abbreviated Lp(μ), or just Lp. The above definitions generalize to Bochner spaces.For 1 ≤ p ≤ ∞, Lp(S, μ) is a Banach space. The fact that Lp is complete is often referred to as the Riesz-Fischer theorem. Completeness can be checked using the convergence theorems for Lebesgue integrals.As before, if there exists q < ∞ such that  f  ∈ L∞(S, μ) ∩ Lq(S, μ), thenFor p = ∞, the space L∞(S, μ) is defined as follows. We start with the set of all measurable functions from S to C or R which are bounded. Again two such functions are identified if they are equal almost everywhere. Denote this set by L∞(S, μ). For a function  f  in this set, the essential supremum of its absolute value serves as an appropriate norm:In the quotient space, two functions  f  and g are identified if  f  = g almost everywhere. The resulting normed vector space is, by definition, This can be made into a normed vector space in a standard way; one simply takes the quotient space with respect to the kernel of || · ||p. Since for any measurable function  f , we have that || f ||p = 0 if and only if  f  = 0 almost everywhere, the kernel of || · ||p does not depend upon p,That the sum of two p-th power integrable functions is again p-th power integrable follows from the inequalityfor every scalar λ.The set of such functions forms a vector space, with the following natural operations:An Lp space may be defined as a space of functions for which the p-th power of the absolute value is Lebesgue integrable,[3] where functions which agree almost everywhere are identified. More generally, let 1 ≤ p < ∞ and (S, Σ, μ) be a measure space. Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, thatThe p-norm thus defined on ℓ p is indeed a norm, and ℓ p together with this norm is a Banach space. The fully general Lp space is obtained—as seen below — by considering vectors, not only with finitely or countably-infinitely many components, but with "arbitrarily many components"; in other words, functions. An integral instead of a sum is used to define the p-norm.if the right-hand side is finite, or the left-hand side is infinite. Thus, we will consider ℓ p spaces for 1 ≤ p ≤ ∞.and the corresponding space ℓ ∞ of all bounded sequences. It turns out that[2]One also defines the ∞-norm using the supremum:diverges for p = 1 (the harmonic series), but is convergent for p > 1.is not in ℓ 1, but it is in ℓ p for p > 1, as the seriesOne can check that as p increases, the set ℓ p grows larger. For example, the sequenceHere, a complication arises, namely that the series on the right is not always convergent, so for example, the sequence made up of only ones, (1, 1, 1, ...), will have an infinite p-norm for 1 ≤ p < ∞. The space ℓ p is then defined as the set of all infinite sequences of real (or complex) numbers such that the p-norm is finite.Define the p-norm:The space of sequences has a natural vector space structure by applying addition and scalar multiplication coordinate by coordinate. Explicitly, the vector sum and the scalar action for infinite sequences of real (or complex) numbers are given by:The p-norm can be extended to vectors that have an infinite number of components, which yields the space ℓ p. This contains as special cases:This is not a norm because it is not homogeneous. Despite these defects as a mathematical norm, the non-zero counting "norm" has uses in scientific computing, information theory, and statistics–notably in compressed sensing in signal processing and computational harmonic analysis.Another function was called the ℓ0 "norm" by David Donoho—whose quotation marks warn that this function is not a proper norm—is the number of non-zero entries of the vector x. Many authors abuse terminology by omitting the quotation marks. Defining 00 = 0, the zero "norm" of x is equal towhich is discussed by Stefan Rolewicz in Metric Linear Spaces.[1] The ℓ0-normed space is studied in functional analysis, probability theory, and harmonic analysis.The mathematical definition of the ℓ0 norm was established by Banach's Theory of Linear Operations. The space of sequences has a complete metric topology provided by the F-normThere is one ℓ0 norm and another function called the ℓ0 "norm" (with quotation marks).shows that the infinite-dimensional sequence space ℓp defined below, is no longer locally convex.[citation needed]Although the p-unit ball Bnp around the origin in this metric is "concave", the topology defined on Rn by the metric dp is the usual vector space topology of Rn, hence ℓnp is a locally convex topological vector space. Beyond this qualitative statement, a quantitative way to measure the lack of convexity of ℓnp is to denote by Cp(n) the smallest constant C such that the multiple C Bnp of the p-unit ball contains the convex hull of Bnp, equal to Bn1. The fact that for fixed p < 1 we havedefines a metric. The metric space (Rn, dp) is denoted by ℓnp.Hence, the functiondefines a subadditive function at the cost of losing absolute homogeneity. It does define an F-norm, though, which is homogeneous of degree p.defines an absolutely homogeneous function for 0 < p < 1; however, the resulting function does not define a norm, because it is not subadditive. On the other hand, the formulaIn Rn for n > 1, the formulaIn general, for vectors in Cn where 0 < r < p:This inequality depends on the dimension n of the underlying vector space and follows directly from the Cauchy–Schwarz inequality.For the opposite direction, the following relation between the 1-norm and the 2-norm is known:This fact generalizes to p-norms in that the p-norm ||x||p of any given vector x does not grow with p:The grid distance or rectilinear distance (sometimes called the "Manhattan distance") between two points is never shorter than the length of the line segment between them (the Euclidean or "as the crow flies" distance). Formally, this means that the Euclidean norm of any vector is bounded by its 1-norm:Abstractly speaking, this means that Rn together with the p-norm is a Banach space. This Banach space is the Lp-space over Rn.For all p ≥ 1, the p-norms and maximum norm as defined above indeed satisfy the properties of a "length function" (or norm), which are that:See L-infinity.The L∞-norm or maximum norm (or uniform norm) is the limit of the Lp-norms for p → ∞. It turns out that this limit is equivalent to the following definition:The Euclidean norm from above falls into this class and is the 2-norm, and the 1-norm is the norm that corresponds to the rectilinear distance.Of course the absolute value bars are unnecessary when p is a rational number and, in reduced form, has an even numerator.For a real number p ≥ 1, the p-norm or Lp-norm of x is defined byThe Euclidean distance between two points x and y is the length ||x − y||2 of the straight line between the two points. In many situations, the Euclidean distance is insufficient for capturing the actual distances in a given space. An analogy to this is suggested by taxi drivers in a grid street plan who should measure distance not in terms of the length of the straight line to their destination, but in terms of the rectilinear distance, which takes into account that streets are either orthogonal or parallel to each other. The class of p-norms generalizes these two examples and has an abundance of applications in many parts of mathematics, physics, and computer science.
The length of a vector x = (x1, x2, ..., xn) in the n-dimensional real vector space Rn is usually given by the Euclidean norm:Hilbert spaces are central to many applications, from quantum mechanics to stochastic calculus. The spaces L2 and ℓ2 are both Hilbert spaces. In fact, by choosing a Hilbert basis (i.e., a maximal orthonormal subset of L2 or any Hilbert space), one sees that all Hilbert spaces are isometric to ℓ2(E), where E is a set with an appropriate cardinality.By contrast, if p > 2, the Fourier transform does not map into Lq.The Fourier transform for the real line (or, for periodic functions, see Fourier series), maps Lp(R) to Lq(R) (or Lp(T) to ℓq) respectively, where 1 ≤ p ≤ 2 and 1/p + 1/q = 1. This is a consequence of the Riesz–Thorin interpolation theorem, and is made precise with the Hausdorff–Young inequality.In penalized regression, 'L1 penalty' and 'L2 penalty' refer to penalizing either the L1 norm of a solution's vector of parameter values (i.e. the sum of its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector.In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as solutions to variational problems.In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910). Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces. Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, finance, engineering, and other disciplines.
Schrödinger equation
The 3 dimensional version of the equation is given byThe Schrödinger equation can also be derived from a first order form[43][44][45] similar to the manner in which the Klein-Gordon equation can be derived from the Dirac equation. In 1D the first order equation is given byThe general equation is also valid and used in quantum field theory, both in relativistic and nonrelativistic situations. However, the solution ψ is no longer interpreted as a "wave", but should be interpreted as an operator acting on states existing in a Fock space.[citation needed]In general, the Hamiltonian to be substituted in the general Schrödinger equation is not just a function of the position and momentum operators (and possibly time), but also of spin matrices. Also, the solutions to a relativistic wave equation, for a massive particle of spin s, are complex-valued 2(2s + 1)-component spinor fields.For the Klein–Gordon equation, the general form of the Schrödinger equation is inconvenient to use, and in practice the Hamiltonian is not expressed in an analogous way to the Dirac Hamiltonian. The equations for relativistic quantum fields can be obtained in other ways, such as starting from a Lagrangian density and using the Euler–Lagrange equations for fields, or use the representation theory of the Lorentz group in which certain representations can be used to fix the equation for a free particle of given spin (and mass).in which the γ = (γ1, γ2, γ3) and γ0 are the Dirac gamma matrices related to the spin of the particle. The Dirac equation is true for all spin-​1⁄2 particles, and the solutions to the equation are 4-component spinor fields with two components corresponding to the particle and the other two for the antiparticle.The general form of the Schrödinger equation remains true in relativity, but the Hamiltonian is less obvious. For example, the Dirac Hamiltonian for a particle of mass m and electric charge q in an electromagnetic field (described by the electromagnetic potentials φ and A) is:was the first such equation to be obtained, even before the nonrelativistic one, and applies to massive spinless particles. The Dirac equation arose from taking the "square root" of the Klein–Gordon equation by factorizing the entire relativistic wave operator into a product of two operators – one of these is the operator for the entire Dirac equation. Entire Dirac equation:instead of classical energy equations. The Klein–Gordon equation and the Dirac equation are two such equations. The Klein–Gordon equation,Relativistic quantum mechanics is obtained where quantum mechanics and special relativity simultaneously apply. In general, one wishes to build relativistic wave equations from the relativistic energy–momentum relationwhich has the same form as the diffusion equation, with diffusion coefficient ħ/2m. In that case, the diffusivity yields the De Broglie relation in accordance with the Markov process.[42]The above properties (positive definiteness of energy) allow the analytic continuation of the Schrödinger equation to be identified as a stochastic process. This can be interpreted as the Huygens–Fresnel principle applied to De Broglie waves; the spreading wavefronts are diffusive probability amplitudes.[40] For a free particle (not subject to a potential) in a random walk, substituting τ = it into the time-dependent Schrödinger equation gives:[41]The lack of sign changes also shows that the ground state is nondegenerate, since if there were two ground states with common energy E, not proportional to each other, there would be a linear combination of the two that would also be a ground state resulting in a zero solution.For potentials which are bounded below and are not infinite over a region, there is a ground state which minimizes the integral above. This lowest energy wavefunction is real and positive definite – meaning the wavefunction can increase and decrease, but is positive for all positions. It physically cannot be negative: if it were, smoothing out the bends at the sign change (to minimize the wavefunction) rapidly reduces the gradient contribution to the integral and hence the kinetic energy, while the potential energy changes linearly and less quickly. The kinetic and potential energy are both changing at different rates, so the total energy is not constant, which can't happen (conservation). The solutions are consistent with Schrödinger equation if this wavefunction is positive definite.(using integration by parts). Due to the complex modulus of ψ2 (which is positive definite), the right hand side is always greater than the lowest value of V(x). In particular, the ground state energy is positive when V(x) is everywhere positive.over all ψ which are normalized.[40] In this way, the smallest eigenvalue is expressed through the variational principle. For the Schrödinger Hamiltonian Ĥ bounded from below, the smallest eigenvalue is called the ground state energy. That energy is the minimum value ofFor any linear operator Â bounded from below, the eigenvector with the smallest eigenvalue is the vector ψ that minimizes the quantityIf the potential is bounded from below, meaning there is a minimum value of potential energy, the eigenfunctions of the Schrödinger equation have energy which is also bounded from below. This can be seen most easily by using the variational principle, as follows. (See also below).Hence predictions from the Schrödinger equation do not violate probability conservation.is the probability current (flow per unit area).is the probability density (probability per unit volume, * denotes complex conjugate), andwhereThe Schrödinger equation is consistent with probability conservation. Multiplying the Schrödinger equation on the right by the complex conjugate wavefunction, and multiplying the wavefunction to the left of the complex conjugate of the Schrödinger equation, and subtracting, gives the continuity equation for probability:[40]On the contrary, wave equations in physics are usually second order in time, notable are the family of classical wave equations and the quantum Klein–Gordon equation.As the first order derivatives are arbitrary, the wavefunction can be a continuously differentiable function of space, since at any boundary the gradient of the wavefunction can be matched.are all arbitrary constants at a given set of points, where xb, yb, zb are a set of points describing boundary b (derivatives are evaluated at the boundaries). Typically there are one or two boundaries, such as the step potential and particle in a box respectively.is an arbitrary constant. Likewise – the second order derivatives with respect to space implies the wavefunction and its first order spatial derivativesThe first time partial derivative implies the initial value (at t = 0) of the wavefunctionExplicitly for one particle in 3-dimensional Cartesian coordinates – the equation isThe Schrödinger equation is first order in time and second in space, which describes the time evolution of a quantum state (meaning it determines the future amplitude from the present).In the time-dependent equation, complex conjugate waves move in opposite directions. If Ψ(x, t) is one solution, then so is Ψ*(x, –t). The symmetry of complex conjugation is called time-reversal symmetry.In an arbitrary potential, if a wave function ψ solves the time-independent equation, so does its complex conjugate, denoted ψ*. By taking linear combinations, the real and imaginary parts of ψ are each solutions. If there is no degeneracy they can only differ by a factor.Two different solutions with the same energy are called degenerate.[31]For the time-independent equation, an additional feature of linearity follows: if two wave functions ψ1 and ψ2 are solutions to the time-independent equation with the same energy E, then so is any linear combination:This is much more convenient than having to verify thatcan be normalized by ensuring thatAdditionally, the ability to scale solutions allows one to solve for a wave function without normalizing it first. If one has a set of normalized solutions ψn, thenthen a valid general solution iswhere a and b are any complex numbers (the sum can be extended for any number of wavefunctions). This property allows superpositions of quantum states to be solutions of the Schrödinger equation. Even more generally, it holds that a general solution to the Schrödinger equation can be found by taking a weighted sum over all single state solutions achievable. For example, consider a wave function Ψ(x, t) such that the wave function is a product of two functions: one time independent, and one time dependent. If states of definite energy found using the time independent Schrödinger equation are given by ψE(x) with amplitude An and time dependent phase factor is given byIn the development above, the Schrödinger equation was made to be linear for generality, though this has other implications. If two wave functions ψ1 and ψ2 are solutions, then so is any linear combination of the two:The Schrödinger equation has the following properties: some are useful, but there are shortcomings. Ultimately, these properties arise from the Hamiltonian used, and the solutions to the equation.This last equation is in a very high dimension, so the solutions are not easy to visualize.where the position of particle n is rn, generating the equation:[5]:141For N particles in three dimensions, the Hamiltonian is:generating the equation:For one particle in three dimensions, the Hamiltonian is:where the position of particle n is xn, generating the equation:For N particles in one dimension, the Hamiltonian is:generates the equation:For one particle in one dimension, the Hamiltonianand the solution, the wavefunction, is a function of all the particle coordinates of the system and time. Following are specific cases.This is the equation of motion for the quantum state. In the most general form, it is written:[5]:143ffThere is no closed form solution for this equation.is known as the mass polarization term, which arises due to the motion of atomic nuclei. The wavefunction is a function of the two electron's positions:The cross-term of two laplaciansand Z is the atomic number for the element (not a quantum number).μ is again the two-body reduced mass of an electron with respect to the nucleus of mass M, so this timewhere r1 is the position of one electron (r1 = |r1| is its magnitude), r2 is the position of the other electron (r2 = |r2| is the magnitude), r12 = |r12| is the magnitude of the separation between them given byThe equation for any two-electron system, such as the neutral helium atom (He, Z = 2), the negative hydrogen ion (H−, Z = 1), or the positive lithium ion (Li+, Z = 3) is:[29]NB: generalized Laguerre polynomials are defined differently by different authors—see main article on them and the hydrogen atom.where:where R are radial functions and Ym
ℓ(θ, φ) are spherical harmonics of degree ℓ and order m. This is the only atom for which the Schrödinger equation has been solved for exactly. Multi-electron atoms require approximative methods. The family of solutions are:[39]The wavefunction for hydrogen is a function of the electron's coordinates, and in fact can be separated into functions of each coordinate.[38] Usually this is done in spherical polar coordinates:is the 2-body reduced mass of the hydrogen nucleus (just a proton) of mass mp and the electron of mass me. The negative sign arises in the potential term since the proton and electron are oppositely charged. The reduced mass in place of the electron mass is used since the electron and proton together orbit each other about a common centre of mass, and constitute a two-body problem to solve. The motion of the electron is of principle interest here, so the equivalent one-body problem is the motion of the electron using the reduced mass.where e is the electron charge, r is the position of the electron (r = |r| is the magnitude of the position), the potential term is due to the Coulomb interaction, wherein ε0 is the electric constant (permittivity of free space) andThis form of the Schrödinger equation can be applied to the hydrogen atom:[26][28]Following are examples where exact solutions are known. See the main articles for further details.For non-interacting identical particles, the potential is a sum but the wavefunction is a sum over permutations of products. The previous two equations do not apply to interacting particles.and the wavefunction is a product of the particle wavefunctionsAgain, for non-interacting distinguishable particles the potential is the sum of particle potentialswith stationary state solutions:The Schrödinger equation is:where the position of particle n is rn and the gradient operators are partial derivatives with respect to the particle's position coordinates. In Cartesian coordinates, for particle n, the position vector is rn = (xn, yn, zn) while the gradient and Laplacian operator are respectively:For N particles in three dimensions, the Hamiltonian is:where the position of the particle is r. Two useful coordinate systems for solving the Schrödinger equation are Cartesian coordinates so that r = (x, y, z) and spherical polar coordinates so that r = (r, θ, φ), although other orthogonal coordinates are useful for solving the equation for systems with certain geometric symmetries.with stationary state solutions of the form:generating the equation:The Hamiltonian for one particle in three dimensions is:The extension from one dimension to three dimensions is straightforward, all position and momentum operators are replaced by their three-dimensional expressions and the partial derivative with respect to space is replaced by the gradient operator.where n = 0,1,2,..., and the functions Hn are the Hermite polynomials.There is a family of solutions – in the position basis they areIt is a notable quantum system to solve for; since the solutions are exact (but complicated – in terms of Hermite polynomials), and it can describe or at least approximate a wide variety of other systems, including vibrating atoms, molecules,[36] and atoms or ions in lattices,[37] and approximating other potentials near equilibrium points. It is also the basis of perturbation methods in quantum mechanics.The Schrödinger equation for this situation isFor a constant potential, V = V0, the solution is oscillatory for E > V0 and exponential for E < V0, corresponding to energies that are allowed or disallowed in classical mechanics. Oscillatory solutions have a classically allowed energy and correspond to actual classical motions, while the exponential solutions have a disallowed energy and describe a small amount of quantum bleeding into the classically disallowed region, due to quantum tunneling. If the potential V0 grows to infinity, the motion is classically confined to a finite region. Viewed far enough away, every solution is reduced to an exponential; the condition that the exponential is decreasing restricts the energy levels to a discrete set, called the allowed energies.[31]See also free particle and wavepacket for more discussion on the free particle.The exponentially growing solutions have an infinite norm, and are not physical. They are not allowed in a finite volume with periodic or fixed boundary conditions.and exponential solutions for E < 0which has oscillatory solutions for E > 0 (the Cn are arbitrary constants):For no potential, V = 0, so the particle is free and the equation reads:[5]:151ffFor non-interacting identical particles, the potential is still a sum, but wavefunction is a bit more complicated – it is a sum over the permutations of products of the separate wavefunctions to account for particle exchange. In general for interacting particles, the above decompositions are not possible.and the wavefunction can be written as a product of the wavefunctions for each particle:For non-interacting distinguishable particles,[35] the potential of the system only influences each particle separately, so the total potential energy is the sum of potential energies for each particle:so the general solutions have the form:where the position of particle n is xn. The corresponding Schrödinger equation is:For N particles in one dimension, the Hamiltonian is:This is the only case the Schrödinger equation is an ordinary differential equation, rather than a partial differential equation. The general solutions are always of the form:and substituting this into the general Schrödinger equation gives:For a particle in one dimension, the Hamiltonian is:The energy eigenvalues from this equation form a discrete spectrum of values, so mathematically energy must be quantized. More specifically, the energy eigenstates form a basis – any wavefunction may be written as a sum over the discrete energy states or an integral over continuous energy states, or more generally as an integral over a measure. This is the spectral theorem in mathematics, and in a finite state space it is just a statement of the completeness of the eigenvectors of a Hermitian matrix.This is true for any number of particles in any number of dimensions (in a time independent potential). This case describes the standing wave solutions of the time-dependent equation, which are the states with definite energy (instead of a probability distribution of different energies). In physics, these standing waves are called "stationary states" or "energy eigenstates"; in chemistry they are called "atomic orbitals" or "molecular orbitals". Superpositions of energy eigenstates change their properties according to the relative phases between the energy levels.Since the time dependent phase factor is always the same, only the spatial part needs to be solved for in time independent problems. Additionally, the energy operator Ê = iħ∂/∂t can always be replaced by the energy eigenvalue E, thus the time independent Schrödinger equation is an eigenvalue equation for the Hamiltonian operator:[5]:143ffSubstituting for ψ into the Schrödinger equation for the relevant number of particles in the relevant number of dimensions, solving by separation of variables implies the general solution of the time-dependent equation has the form:[15]where ψ(space coords) is a function of all the spatial coordinate(s) of the particle(s) constituting the system only, and τ(t) is a function of time only.If the Hamiltonian is not an explicit function of time, the equation is separable into a product of spatial and temporal parts. In general, the wavefunction takes the form:In actuality, the particles constituting the system do not have the numerical labels used in theory. The language of mathematics forces us to label the positions of particles one way or another, otherwise there would be confusion between symbols representing which variables are for which particle.[30]The quantum mechanics of particles without accounting for the effects of special relativity, for example particles propagating at speeds much less than light, is known as nonrelativistic quantum mechanics. Following are several forms of Schrödinger's equation in this context for different situations: time independence and dependence, one and three spatial dimensions, and one and N particles.The implications are as follows:where ρ is the probability density, into the Schrödinger equation and then taking the limit ħ → 0 in the resulting equation, yields the Hamilton–Jacobi equation.Substitutingwhere S is action and H is the Hamiltonian function (not operator). Here the generalized coordinates qi for i = 1, 2, 3 (used in the context of the HJE) can be set to the position in Cartesian coordinates as r = (q1, q2, q3) = (x, y, z).[32]is closely related to the Hamilton–Jacobi equation (HJE)The Schrödinger equation in its general formwhere σ denotes the (root mean square) measurement uncertainty in x and px (and similarly for the y and z directions) which implies the position and momentum can only be known to arbitrary precision in this limit.The limiting short-wavelength is equivalent to ħ tending to zero because this is limiting case of increasing the wave packet localization to the definite position of the particle (see images right). Using the Heisenberg uncertainty principle for position and momentum, the products of uncertainty in position and momentum become zero as ħ → 0:Schrödinger required that a wave packet solution near position r with wavevector near k will move along the trajectory determined by classical mechanics for times short enough for the spread in k (and hence in velocity) not to substantially increase the spread in r. Since, for a given spread in k, the spread in velocity is proportional to Planck's constant ħ, it is sometimes said that in the limit as ħ approaches zero, the equations of classical mechanics are restored from quantum mechanics.[32] Great care is required in how that limit is taken, and in what cases.As the curvature increases, the amplitude of the wave alternates between positive and negative more rapidly, and also shortens the wavelength. So the inverse relation between momentum and wavelength is consistent with the energy the particle has, and so the energy of the particle has a connection to a wave, all in the same mathematical formulation.[28]The kinetic energy is also proportional to the second spatial derivatives, so it is also proportional to the magnitude of the curvature of the wave, in terms of operators:Wave–particle duality can be assessed from these equations as follows. The kinetic energy T is related to the square of momentum p. As the particle's momentum increases, the kinetic energy increases more rapidly, but since the wavenumber |k| increases the wavelength λ decreases. In terms of ordinary scalar and vector quantities (not operators):so in terms of derivatives with respect to time and space, acting this operator on the wavefunction Ψ immediately led Schrödinger to his equation:[citation needed]Substituting the energy and momentum operators into the classical energy conservation equation obtains the operator:where p is a vector of the momentum eigenvalues. In the above, the "hats" ( ˆ ) indicate these observables are operators, not simply ordinary numbers or vectors. The energy and momentum operators are differential operators, while the potential energy function V is just a multiplicative factor.where E are the energy eigenvalues, and the momentum operator, corresponding to the spatial derivatives (the gradient ∇),Another postulate of quantum mechanics is that all observables are represented by linear Hermitian operators which act on the wavefunction, and the eigenvalues of the operator are the values the observable takes. The previous derivatives are consistent with the energy operator, corresponding to the time derivative,with respect to time:with respect to space:and to realize that the first order partial derivatives were:Schrödinger's insight,[citation needed] late in 1925, was to express the phase of a plane wave as a complex phase factor using these relations:The Planck–Einstein and de Broglie relations illuminate the deep connections between energy with time, and space with momentum, and express wave–particle duality. In practice, natural units comprising ħ = 1 are used, as the De Broglie equations reduce to identities: allowing momentum, wavenumber, energy and frequency to be used interchangeably, to prevent duplication of quantities, and reduce the number of dimensions of related quantities. For familiarity SI units are still used in this article.while in three dimensions, wavelength λ is related to the magnitude of the wavevector k:Likewise De Broglie's hypothesis (1924) states that any particle can be associated with a wave, and that the momentum p of the particle is inversely proportional to the wavelength λ of such a wave (or proportional to the wavenumber, k = 2π/λ), in one dimension, by:Einstein's light quanta hypothesis (1905) states that the energy E of a photon is proportional to the frequency ν (or angular frequency, ω = 2πν) of the corresponding quantum wavepacket of light:where d3k = dkxdkydkz is the differential volume element in k-space, and the integrals are taken over all k-space. The momentum wavefunction Φ(k) arises in the integrand since the position and momentum space wavefunctions are Fourier transforms of each other.for some real amplitude coefficients An, and for continuous k the sum becomes an integral, the Fourier transform of a momentum space wavefunction:[31]For discrete k the sum is a superposition of plane waves:where the A is the amplitude, k the wavevector, and ω the angular frequency, of the plane wave. In general, physical situations are not purely described by plane waves, so for generality the superposition principle is required; any wave can be made by superposition of sinusoidal plane waves. So if the equation is linear, a linear combination of plane waves is also an allowed solution. Hence a necessary and separate requirement is that the Schrödinger equation is a linear differential equation.The simplest wavefunction is a plane wave of the form:This formalism can be extended to any fixed number of particles: the total energy of the system is then the total kinetic energies of the particles, plus the total potential energy, again the Hamiltonian. However, there can be interactions between the particles (an N-body problem), so the potential energy V can change as the spatial configuration of particles changes, and possibly with time. The potential energy, in general, is not the sum of the separate potential energies for each particle, it is a function of all the spatial positions of the particles. Explicitly:For three dimensions, the position vector r and momentum vector p must be used:Explicitly, for a particle in one dimension with position x, mass m and momentum p, and potential energy V which generally varies with position and time t:The total energy E of a particle is the sum of kinetic energy T and potential energy V, this sum is also the frequent expression for the Hamiltonian H in classical mechanics:The Schrödinger equation was developed principally from the De Broglie hypothesis, a wave equation that would describe particles,[28] and can be constructed as shown informally in the following sections.[29] For a more rigorous description of Schrödinger's equation, see also Resnick et al.[30]The foundation of the equation is structured to be a linear differential equation based on classical energy conservation, and consistent with the De Broglie relations. The solution is the wave function ψ, which contains all the information that can be known about the system. In the Copenhagen interpretation, the modulus of ψ is related to the probability the particles are in some spatial configuration at some instant of time. Solving the equation for ψ can be used to predict how the particles will behave under the influence of the specified potential and with each other.The Schrödinger equation is a diffusion equation,[25] the solutions are functions which describe wave-like motions. Wave equations in physics can normally be derived from other physical laws – the wave equation for mechanical vibrations on strings and in matter can be derived from Newton's laws, where the wave function represents the displacement of matter, and electromagnetic waves from Maxwell's equations, where the wave functions are electric and magnetic fields. The basis for Schrödinger's equation, on the other hand, is the energy of the system and a separate postulate of quantum mechanics: the wave function is a description of the system.[26] The Schrödinger equation is therefore a new concept in itself; as Feynman put it:Louis de Broglie in his later years proposed a real valued wave function connected to the complex wave function by a proportionality constant and developed the De Broglie–Bohm theory.The Schrödinger equation details the behavior of Ψ but says nothing of its nature. Schrödinger tried to interpret it as a charge density in his fourth paper, but he was unsuccessful.[23]:219 In 1926, just a few days after Schrödinger's fourth and final paper was published, Max Born successfully interpreted Ψ as the probability amplitude, whose absolute square is equal to probability density.[23]:220 Schrödinger, though, always opposed a statistical or probabilistic approach, with its associated discontinuities—much like Einstein, who believed that quantum mechanics was a statistical approximation to an underlying deterministic theory—and never reconciled with the Copenhagen interpretation.[24]This 1926 paper was enthusiastically endorsed by Einstein, who saw the matter-waves as an intuitive depiction of nature, as opposed to Heisenberg's matrix mechanics, which he considered overly formal.[22]While at the cabin, Schrödinger decided that his earlier nonrelativistic calculations were novel enough to publish, and decided to leave off the problem of relativistic corrections for the future. Despite the difficulties in solving the differential equation for hydrogen (he had sought help from his friend the mathematician Hermann Weyl[19]:3) Schrödinger showed that his nonrelativistic version of the wave equation produced the correct spectral energies of hydrogen in a paper published in 1926.[19]:1[20] In the equation, Schrödinger computed the hydrogen spectral series by treating a hydrogen atom's electron as a wave Ψ(x, t), moving in a potential well V, created by the proton. This computation accurately reproduced the energy levels of the Bohr model. In a paper, Schrödinger himself explained this equation as follows:He found the standing waves of this relativistic equation, but the relativistic corrections disagreed with Sommerfeld's formula. Discouraged, he put away his calculations and secluded himself in an isolated mountain cabin in December 1925.[18]However, by that time, Arnold Sommerfeld had refined the Bohr model with relativistic corrections.[16][17] Schrödinger used the relativistic energy momentum relation to find what is now known as the Klein–Gordon equation in a Coulomb potential (in natural units):Following up on de Broglie's ideas, physicist Peter Debye made an offhand comment that if particles behaved as waves, they should satisfy some sort of wave equation. Inspired by Debye's remark, Schrödinger decided to find a proper 3-dimensional wave equation for the electron. He was guided by William R. Hamilton's analogy between mechanics and optics, encoded in the observation that the zero-wavelength limit of optics resembles a mechanical system—the trajectories of light rays become sharp tracks that obey Fermat's principle, an analog of the principle of least action.[14] A modern version of his reasoning is reproduced below. The equation he found is:[15]In 1921, prior to de Broglie, Arthur C. Lunn at the University of Chicago had used the same argument based on the completion of the relativistic energy–momentum 4-vector to derive what we now call the de Broglie relation.[12] Unlike de Broglie, Lunn went on to formulate the differential equation now known as the Schrödinger equation, and solve for its energy eigenvalues for the hydrogen atom. Unfortunately the paper was rejected by the Physical Review, as recounted by Kamen.[13]This approach essentially confined the electron wave in one dimension, along a circular orbit of radius r.According to de Broglie the electron is described by a wave and a whole number of wavelengths must fit along the circumference of the electron's orbit:where h is Planck's constant and ħ is the reduced Planck constant, h/2π. Louis de Broglie hypothesized that this is true for all particles, even particles which have mass such as electrons. He showed that, assuming that the matter waves propagate along with their particle counterparts, electrons form standing waves, meaning that only certain discrete rotational frequencies about the nucleus of an atom are allowed.[11] These quantized orbits correspond to discrete energy levels, and de Broglie reproduced the Bohr model formula for the energy levels. The Bohr model was based on the assumed quantization of angular momentum L according to:Following Max Planck's quantization of light (see black body radiation), Albert Einstein interpreted Planck's quanta to be photons, particles of light, and proposed that the energy of a photon is proportional to its frequency, one of the first signs of wave–particle duality. Since energy and momentum are related in the same way as frequency and wavenumber in special relativity, it followed that the momentum p of a photon is inversely proportional to its wavelength λ, or proportional to its wavenumber k:An important aspect is the relationship between the Schrödinger equation and wavefunction collapse. In the oldest Copenhagen interpretation, particles follow the Schrödinger equation except during wavefunction collapse, during which they behave entirely differently. The advent of quantum decoherence theory allowed alternative approaches (such as the Everett many-worlds interpretation and consistent histories), wherein the Schrödinger equation is always satisfied, and wavefunction collapse should be explained as a consequence of the Schrödinger equation.The Schrödinger equation provides a way to calculate the wave function of a system and how it changes dynamically in time. However, the Schrödinger equation does not directly say what, exactly, the wave function is. Interpretations of quantum mechanics address questions such as what the relation is between the wave function, the underlying reality, and the results of experimental measurements.In Dublin in 1952 Erwin Schrödinger gave a lecture in which at one point he jocularly warned his audience that what he was about to say might "seem lunatic". It was that, when his equations seem to be describing several different histories, they are "not alternatives but all really happen simultaneously". This is the earliest known reference to the Many-worlds interpretation of quantum mechanics.[10]The superposition property allows the particle to be in a quantum superposition of two or more quantum states at the same time. However, it is noted that a "quantum state" in quantum mechanics means the probability that a system will be, for example at a position x, not that the system will actually be at position x. It does not imply that the particle itself may be in two classical states at once. Indeed, quantum mechanics is generally unable to assign values for properties prior to measurement at all.Related to diffraction, particles also display superposition and interference.However, since the Schrödinger equation is a wave equation, a single particle fired through a double-slit does show this same pattern (figure on right). Note: The experiment must be repeated many times for the complex pattern to emerge. Although this is counterintuitive, the prediction is correct; in particular, electron diffraction and neutron diffraction are well understood and widely used in science and engineering.Two-slit diffraction is a famous example of the strange behaviors that waves regularly display, that are not intuitively associated with particles. The overlapping waves from the two slits cancel each other out in some locations, and reinforce each other in other locations, causing a complex pattern to emerge. Intuitively, one would not expect this pattern from firing a single particle at the slits, because the particle should pass through one slit or the other, not a complex overlap of both.The nonrelativistic Schrödinger equation is a type of partial differential equation called a wave equation. Therefore, it is often said particles can exhibit behavior usually attributed to waves. In some modern interpretations this description is reversed – the quantum state, i.e. wave, is the only genuine physical reality, and under the appropriate conditions it can show features of particle-like behavior. However, Ballentine[9]:Chapter 4, p.99 shows that such an interpretation has problems. Ballentine points out that whilst it is arguable to associate a physical wave with a single particle, there is still only one Schrödinger wave equation for many particles. He points out:In classical physics, when a ball is rolled slowly up a large hill, it will come to a stop and roll back, because it doesn't have enough energy to get over the top of the hill to the other side. However, the Schrödinger equation predicts that there is a small probability that the ball will get to the other side of the hill, even if it has too little energy to reach the top. This is called quantum tunneling. It is related to the distribution of energy: although the ball's assumed position seems to be on one side of the hill, there is a chance of finding it on the other side.The Schrödinger equation describes the (deterministic) evolution of the wave function of a particle. However, even if the wave function is known exactly, the result of a specific measurement on the wave function is uncertain.The Heisenberg uncertainty principle is the statement of the inherent measurement uncertainty in quantum mechanics. It states that the more precisely a particle's position is known, the less precisely its momentum is known, and vice versa.In classical mechanics, a particle has, at every moment, an exact position and an exact momentum. These values change deterministically as the particle moves according to Newton's laws. Under the Copenhagen interpretation of quantum mechanics, particles do not have exactly determined properties, and when they are measured, the result is randomly drawn from a probability distribution. The Schrödinger equation predicts what the probability distributions are, but fundamentally cannot predict the exact result of each measurement.Another result of the Schrödinger equation is that not every measurement gives a quantized result in quantum mechanics. For example, position, momentum, time, and (in some situations) energy can have any value across a continuous range.[8]:165–167The Schrödinger equation predicts that if certain properties of a system are measured, the result may be quantized, meaning that only specific discrete values can occur. One example is energy quantization: the energy of an electron in an atom is always one of the quantized energy levels, a fact discovered via atomic spectroscopy. (Energy quantization is discussed below.) Another example is quantization of angular momentum. This was an assumption in the earlier Bohr model of the atom, but it is a prediction of the Schrödinger equation.The overall form of the equation is not unusual or unexpected, as it uses the principle of the conservation of energy. The terms of the nonrelativistic Schrödinger equation can be interpreted as total energy of the system, equal to the system kinetic energy plus the system potential energy. In this respect, it is just the same as in classical physics.The Schrödinger equation and its solutions introduced a breakthrough in thinking about physics. Schrödinger's equation was the first of its type, and solutions led to consequences that were very unusual and unexpected for the time.So far, H is only an abstract Hermitian operator. However using the correspondence principle it is possible to show that, in the classical limit, the expectation value of H is indeed the classical energy. The correspondence principle does not completely fix the form of the quantum Hamiltonian due to the uncertainty principle and therefore the precise form of the quantum Hamiltonian must be fixed empirically.The time-independent Schrödinger equation is discussed further below.with definitions as above.As before, the most common manifestation is the nonrelativistic Schrödinger equation for a single particle moving in an electric field (but not a magnetic field):In linear algebra terminology, this equation is an eigenvalue equation and in this sense the wave function is an eigenfunction of the Hamiltonian operator.In words, the equation states:where E is a constant equal to the total energy of the system. This is only used when the Hamiltonian itself is not dependent on time explicitly. However, even in this case the total wave function still has a time dependency.The time-dependent Schrödinger equation described above predicts that wave functions can form standing waves, called stationary states (also called "orbitals", as in atomic orbitals or molecular orbitals). These states are particularly important as their individual study later simplifies the task of solving the time-dependent Schrödinger equation for any state. Stationary states can also be described by a simpler form of the Schrödinger equation, the time-independent Schrödinger equation (TISE).To apply the Schrödinger equation, the Hamiltonian operator is set up for the system, accounting for the kinetic and potential energy of the particles constituting the system, then inserted into the Schrödinger equation. The resulting partial differential equation is solved for the wave function, which contains information about the system.The term "Schrödinger equation" can refer to both the general equation, or the specific nonrelativistic version. The general equation is indeed quite general, used throughout quantum mechanics, for everything from the Dirac equation to quantum field theory, by plugging in diverse expressions for the Hamiltonian. The specific nonrelativistic version is a strictly classical approximation to reality and yields accurate results in many situations, but only to a certain extent (see relativistic quantum mechanics and relativistic quantum field theory).Given the particular differential operators involved, this is a linear partial differential equation. It is also a diffusion equation, but unlike the heat equation, this one is also a wave equation given the imaginary unit present in the transient term.where μ is the particle's "reduced mass", V is its potential energy, ∇2 is the Laplacian (a differential operator), and Ψ is the wave function (more precisely, in this context, it is called the "position-space wave function"). In plain language, it means "total energy equals kinetic energy plus potential energy", but the terms take unfamiliar forms for reasons explained below.The most famous example is the nonrelativistic Schrödinger equation for a single particle moving in an electric field (but not a magnetic field; see the Pauli equation):[6]The form of the Schrödinger equation depends on the physical situation (see below for special cases). The most general form is the time-dependent Schrödinger equation (TDSE), which gives a description of a system evolving with time:[5]:143The Schrödinger equation is not the only way to study quantum mechanical systems and make predictions, as there are other quantum mechanical formulations such as matrix mechanics, introduced by Werner Heisenberg, and path integral formulation, developed chiefly by Richard Feynman. Paul Dirac incorporated matrix mechanics and the Schrödinger equation into a single formulation.In the Copenhagen interpretation of quantum mechanics, the wave function is the most complete description that can be given of a physical system. Solutions to Schrödinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe.[4]:292ff Schrödinger's equation is central to all applications of quantum mechanics including quantum field theory which combines special relativity with quantum mechanics. Theories of quantum gravity, such as string theory, also do not modify Schrödinger's equation.The concept of a wavefunction is a fundamental postulate of quantum mechanics. Using these postulates, Schrödinger's equation can be derived from the fact that the time-evolution operator must be unitary and must therefore be generated by the exponential of a self-adjoint operator, which is the quantum Hamiltonian. This derivation is explained below.In classical mechanics, Newton's second law (F = ma) is used to make a mathematical prediction as to what path a given system will take following a set of known initial conditions. In quantum mechanics, the analogue of Newton's law is Schrödinger's equation for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localised). It is not an algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a "state function").[3]:1–2In quantum mechanics, the Schrödinger equation is a mathematical equation that describes the changes over time of a physical system in which quantum effects, such as wave–particle duality, are significant. The equation is a mathematical formulation for studying quantum mechanical systems. It is considered a central result in the study of quantum systems and its derivation was a significant landmark in developing the theory of quantum mechanics. It was named after Erwin Schrödinger, who derived the equation in 1925 and published it in 1926, forming the basis for his work that resulted in his being awarded the Nobel Prize in Physics in 1933.[1][2] The equation is a type of differential equation known as a wave-equation, which serves as a mathematical model of the movement of waves.
Potential energy

Hamiltonian (quantum mechanics)
then the above equations becomeIf we define "conjugate momentum" variables πn bySimilarly, one can show thatBy applying Schrödinger's equation and using the orthonormality of the basis states, this further reduces toEach an(t) actually corresponds to two independent degrees of freedom, since the variable has a real part and an imaginary part. We now perform the following trick: instead of using the real and imaginary parts as the independent variables, we use an(t) and its complex conjugate an*(t). With this choice of independent variables, we can calculate the partial derivativeThe expectation value of the Hamiltonian of this state, which is also the mean energy, isThe coefficients an(t) are complex variables. We can treat them as coordinates which specify the state of the system, like the position and momentum coordinates which specify a classical system. Like classical coordinates, they are generally not constant in time, and their time dependence gives rise to the time dependence of the system as a whole.whereNote that these basis states are assumed to be independent of time. We will assume that the Hamiltonian is also independent of time.Thus, the expected value of the observable G is conserved for any state of the system. In the case of the free particle, the conserved quantity is the angular momentum.In obtaining this result, we have used the Schrödinger equation, as well as its dual,Therefore,It is straightforward to show that if U commutes with H, then so does G:The existence of a symmetry operator implies the existence of a conserved observable. Let G be the Hermitian generator of U:In many systems, two or more energy eigenstates have the same energy. A simple example of this is a free particle, whose energy eigenstates have wavefunctions that are propagating plane waves. The energy of each of these plane waves is inversely proportional to the square of its wavelength. A wave propagating in the x direction is a different state from one propagating in the y direction, but if they have the same wavelength, then their energies will be the same. When this happens, the states are said to be degenerate.Casting all of these into the Hamiltonian gives:and the potential energy, which is due to the φ field:so the corresponding kinetic energy operator is:For a charged particle q in an electromagnetic field, described by the scalar potential φ and vector potential A, there are two parts to the Hamiltonian to substitute for.[1] The momentum operator must be replaced by the kinetic momentum operator, which includes a contribution from the A field:where gs is the spin gyromagnetic ratio (a.k.a. "spin g-factor"), e is the electron charge, S is the spin operator vector, whose components are the Pauli matrices, henceFor a Spin-½ particle, the corresponding spin magnetic moment is:[4]Since the particle is stationary, there is no translational kinetic energy of the dipole, so the Hamiltonian of the dipole is just the potential energy:For a magnetic dipole moment μ in a uniform, magnetostatic field (time-independent) B, positioned in one place, the potential is:Since the particle is stationary, there is no translational kinetic energy of the dipole, so the Hamiltonian of the dipole is just the potential energy:the dipole moment itself is the operatorFor an electric dipole moment d constituting charges of magnitude q, in a uniform, electrostatic field (time-independent) E, positioned in one place, the potential is:so the Hamiltonian is:where φ(ri) is the electrostatic potential of charge qj at ri. The total potential of the system is then the sum over j:However, this is only the potential for one point charge due to another. If there are many charged particles, each charge has a potential energy due to every other point charge (except itself). For N charges, the potential energy of charge qj due to all other charges is (see also Electrostatic potential energy stored in a configuration of discrete point charges):[3]The Coulomb potential energy for two point charges q1 and q2 (i.e. charged particles, since particles have no spatial extent), in three dimensions, is (in SI units - rather than Gaussian units which are frequently used in electromagnetism):For a rigid rotor – i.e. system of particles which can rotate freely about any axes, not bound in any potential (such as free molecules with negligible vibrational degrees of freedom, say due to double or triple chemical bonds), Hamiltonian is:Writing the Hamiltonian out in full shows it is simply the sum of the one-dimensional Hamiltonians in each direction:where the three-dimensional position vector r using cartesian coordinates is (x, y, z), its magnitude isFor three dimensions, this becomesso the Hamiltonian is:For a simple harmonic oscillator in one dimension, the potential varies with position (but not time), according to:This applies to the elementary "particle in a box" problem, and step potentials.in three dimensionsFor a particle in a region of constant potential V = V0 (no dependence on space or time), in one dimension, the Hamiltonian is:and in three dimensions:The particle is not bound by any potential energy, so the potential is zero and this Hamiltonian is the simplest. For one dimension:Following are expressions for the Hamiltonian in a number of situations.[2] Typical ways to classify the expressions are the number of particles, number of dimensions, and the nature of the potential energy function - importantly space and time dependence. Masses are denoted by m, and charges by q.From a mathematically rigorous point of view, care must be taken with the above assumptions. Operators on infinite-dimensional Hilbert spaces need not have eigenvalues (the set of eigenvalues does not necessarily coincide with the spectrum of an operator). However, all routine quantum mechanical calculations can be done using the physical formulation.[clarification needed]Since H is a Hermitian operator, the energy is always a real number.However, in the more general formalism of Dirac, the Hamiltonian is typically implemented as an operator on a Hilbert space in the following way:is a unitary operator. It is the time evolution operator, or propagator, of a closed quantum system. If the Hamiltonian is time-independent, {U(t)} form a one parameter unitary group (more than a semigroup); this gives rise to the physical principle of detailed balance.By the *-homomorphism property of the functional calculus, the operatorThe exponential operator on the right hand side of the Schrödinger equation is usually defined by the corresponding power series in H. One might notice that taking polynomials or power series of unbounded operators that are not defined everywhere may not make mathematical sense. Rigorously, to take functions of unbounded operators, a functional calculus is required. In the case of the exponential function, the continuous, or just the holomorphic functional calculus suffices. We note again, however, that for common calculations the physicists' formulation is quite sufficient.This equation is the Schrödinger equation. It takes the same form as the Hamilton–Jacobi equation, which is one of the reasons H is also called the Hamiltonian. Given the state at some initial time (t = 0), we can solve it to obtain the state at any subsequent time. In particular, if H is independent of time, thenwhere the sum is taken over all particles and their corresponding potentials; the result is that the Hamiltonian of the system is the sum of the separate Hamiltonians for each particle. This is an idealized situation - in practice the particles are almost always influenced by some potential, and there are many-body interactions. One illustrative example of a two-body interaction where this form would not apply is for electrostatic potentials due to charged particles, because they interact with each other by Coulomb interaction (electrostatic force), as shown below.The general form of the Hamiltonian in this case is:For non-interacting particles, i.e. particles which do not interact mutually and move independently, the potential of the system is the sum of the separate potential energy for each particle,[1] that isFor N interacting particles, i.e. particles which interact mutually and constitute a many-body situation, the potential energy function V is not simply a sum of the separate potentials (and certainly not a product, as this is dimensionally incorrect). The potential energy function can only be written as above: a function of all the spatial positions of each particle.where M denotes the mass of the collection of particles resulting in this extra kinetic energy. Terms of this form are known as mass polarization terms, and appear in the Hamiltonian of many electron atoms (see below).However, complications can arise in the many-body problem. Since the potential energy depends on the spatial arrangement of the particles, the kinetic energy will also depend on the spatial configuration to conserve energy. The motion due to any one particle will vary due to the motion of all the other particles in the system. For this reason cross terms for kinetic energy may appear in the Hamiltonian; a mix of the gradients for two particles:Combining these yields the Schrödinger Hamiltonian for the N-particle case:is the kinetic energy operator of particle n, and ∇n is the gradient for particle n, ∇n2 is the Laplacian for particle using the coordinates:is the potential energy function, now a function of the spatial configuration of the system and time (a particular set of spatial positions at some instant of time defines a configuration) and;whereThe formalism can be extended to N particles:One can also make substitutions to certain variables to fit specific cases, such as some involving electromagnetic fields.which allows one to apply the Hamiltonian to systems described by a wave function Ψ(r, t). This is the approach commonly taken in introductory treatments of quantum mechanics, using the formalism of Schrödinger's wave mechanics.Although this is not the technical definition of the Hamiltonian in classical mechanics, it is the form it most commonly takes. Combining these together yields the familiar form used in the Schrödinger equation:is the momentum operator where an ∇ is the del operator. The dot product of ∇ with itself is the Laplacian ∇2. In three dimensions using Cartesian coordinates the Laplace operator isis the kinetic energy operator in which m is the mass of the particle, the dot denotes the dot product of vectors, andis the potential energy operator andwhereBy analogy with classical mechanics, the Hamiltonian is commonly expressed as the sum of operators corresponding to the kinetic and potential energies of a system in the formThe Hamiltonian is the sum of the kinetic energies of all the particles, plus the potential energy of the particles associated with the system. For different situations or number of particles, the Hamiltonian is different since it includes the sum of kinetic energies of the particles, and the potential energy function corresponding to the situation.The Hamiltonian is named after William Rowan Hamilton, who also created a revolutionary reformation of Newtonian mechanics, now called Hamiltonian mechanics, that is important in quantum physics.In quantum mechanics, a Hamiltonian is an operator corresponding to the total energy of the system in most of the cases. It is usually denoted by H, also Ȟ or Ĥ. Its spectrum is the set of possible outcomes when one measures the total energy of a system. Because of its close relation to the time-evolution of a system, it is of fundamental importance in most formulations of quantum theory.
Linear equation
If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n – 1)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).In other words, if ai ≠ 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 ≠ 0, and rewrite the equationIf all the coefficients are zero, then either b ≠ 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewrittenAn everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point–slope form or slope–intercept form.where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.andA linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.Ergo,Thus,One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.andThese are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:andSince this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.becomes:Further, this representation extends to systems of linear equations.one can rewrite the equation in matrix form:Using the order of the standard formwhere a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.Using a determinant, one gets a determinant form, easy to remember:Expanding the products and regrouping the terms leads to the general form:Multiplying both sides of this equation by (x2 − x1) yields a form of the line generally referred to as the symmetric form:where (x1, y1) and (x2, y2) are two points on the line with x2 ≠ x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2 − y1)/(x2 − x1).The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y − y1) is proportional to the difference in the x coordinate (that is, x − x1). The proportionality constant is m (the slope of the line).where m is the slope of the line and (x1,y1) is any point on the line.where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.where A and B are not both equal to zero. The equation is usually written so that A ≥ 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is −A/B. The general form is sometimes written as:In the general (or standard[1]) form the linear equation is written as:Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and θ are variables; other letters represent constants (fixed numbers).Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.A common form of a linear equation in the two variables x and y isIf a = 0, then, if b = 0, every number is a solution of the equation, and, if b ≠ 0, there are no solutions (and the equation is said to be inconsistent).If a ≠ 0, there is a unique solutionA linear equation in one unknown x may always be rewrittenThis article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a ≠ 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a ≠ 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).
Homogeneous coordinates

System of linear equations
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:where A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.A homogeneous system is equivalent to a matrix equation of the formA system of linear equations is homogeneous if all of the constant terms are zero:There is also a quantum algorithm for linear systems of equations.[3]A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.is given byCramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemThe last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsFor example, consider the following system:The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:Here x is the free variable, and y and z are dependent.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.The solution set to this system can be described by the following equations:For example, consider the following system:To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.There are several algorithms for solving a system of linear equations.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.For example, the equationsA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.For a more complicated example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For example, the equationsThe equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.The following pictures illustrate this trichotomy in the case of two variables:In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.A linear system may behave in any one of three possible ways:A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.The number of vectors in a basis for the span is now expressed as the rank of the matrix.where A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The vector equation is equivalent to a matrix equation of the formThis allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.A general system of m linear equations with n unknowns can be written asNow substitute this expression for x into the bottom equation:The simplest kind of linear system involves two equations and two variables:Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given byIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,
Cramer's rule

Linear map
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank–nullity theorem:If f : V → W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n × n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol ∗ denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f : V → W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.
Linear form
In higher dimensions, this generalizes as followsSo each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.  Thenwhere δ is the Kronecker delta.  Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.Or, more succinctly,In an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.  There is a mapping V → V∗ into the continuous dual space V∗.  However, this mapping is antilinear rather than linear.The above defined vector v∗ ∈ V∗ is said to be the dual vector of v ∈ V.The inverse isomorphism is V∗ → V : v∗ ↦ v, where v is the unique element of V such thatwhere the bilinear form on V is denoted ⟨ , ⟩ (for instance, in Euclidean space ⟨v, w⟩ = v ⋅ w is the dot product of v and w).Every non-degenerate bilinear form on a finite-dimensional vector space V induces an isomorphism V → V∗ : v ↦ v∗ such thatIn finite dimensions, a linear functional can be visualized in terms of its level sets.  In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.  This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as Gravitation by Misner, Thorne & Wheeler (1973).In the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.Linear functionals are particularly important in quantum mechanics.  Quantum mechanical systems are represented by Hilbert spaces, which are anti–isomorphic to their own dual spaces.  A state of a quantum mechanical system can be identified with a linear functional.  For more information see bra–ket notation.This follows from the fact that the linear functionals evxi : f → f(xi) defined above form a basis of the dual space of Pn.[1]for all f ∈ Pn. This forms the foundation of the theory of numerical quadrature.The integration functional I defined above defines a linear functional on the subspace Pn of polynomials of degree ≤ n. If x0, ..., xn are n + 1 distinct points in [a, b], then there are coefficients a0, ..., an for whichIf x0, ..., xn are n + 1 distinct points in [a, b], then the evaluation functionals evxi, i = 0, 1, ..., n form a basis of the dual space of Pn.  (Lax (1996) proves this last fact using Lagrange interpolation.)The mapping f → f(c) is linear sinceLet Pn denote the vector space of real-valued polynomial functions of degree ≤n defined on an interval [a, b].  If c ∈ [a, b], then let evc : Pn → R be the evaluation functionalis a linear functional from the vector space C[a, b] of continuous functions on the interval [a, b] to the real numbers. The linearity of I follows from the standard facts about the integral:Linear functionals first appeared in functional analysis, the study of vector spaces of functions.  A typical example of a linear functional is integration: the linear transformation defined by the Riemann integraland each linear functional can be expressed in this form.For each row vector [a1 … an] there is a linear functional f defined bySuppose that vectors in the real coordinate space Rn are represented as column vectorsIf V is a topological vector space, the space of continuous linear functionals — the continuous dual — is often simply called the dual space.  If V is a Banach space, then so is its (continuous) dual.  To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the algebraic dual space.  In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.The set of all linear functionals from V to k, Homk(V,k), forms a vector space over k with the addition of the operations of addition and scalar multiplication (defined pointwise).  This space is called the dual space of V, or sometimes the algebraic dual space, to distinguish it from the continuous dual space.  It is often written V∗, V′, or Vᐯ when the field k is understood.In linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In ℝn, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.  In general, if V is a vector space over a field k, then a linear functional f is a function from V to k that is linear:
Module (mathematics)
Over near-rings, one can consider near-ring modules, a nonabelian generalization of modules.[citation needed]One can also consider modules over a semiring. Modules over rings are abelian groups, but modules over semirings are only commutative monoids. Most applications of modules are still possible. In particular, for any semiring S the matrices over S form a semiring over which the tuples of elements from S are a module (in this generalized sense only). This allows a further generalization of the concept of vector space incorporating the semirings from theoretical computer science.Modules over commutative rings can be generalized in a different direction: take a ringed space (X, OX) and consider the sheaves of OX-modules; see sheaf of modules for more. These form a category OX-Mod, and play an important role in modern algebraic geometry. If X has only a single point, then this is a module category in the old sense over the commutative ring OX(X).Any ring R can be viewed as a preadditive category with a single object. With this understanding, a left R-module is nothing but a (covariant) additive functor from R to the category Ab of abelian groups. Right R-modules are contravariant additive functors. This suggests that, if C is any preadditive category, a covariant additive functor from C to Ab should be considered a generalized left module over C; these functors form a functor category C-Mod which is the natural generalization of the module category R-Mod.A representation is called faithful if and only if the map R → EndZ(M) is injective. In terms of modules, this means that if r is an element of R such that rx = 0 for all x in M, then r = 0. Every abelian group is a faithful module over the integers or over some modular arithmetic Z/nZ.Such a ring homomorphism R → EndZ(M) is called a representation of R over the abelian group M; an alternative and equivalent way of defining left R-modules is to say that a left R-module is an abelian group M together with a representation of R over it.If M is a left R-module, then the action of an element r in R is defined to be the map M → M that sends each x to rx (or xr in the case of a right module), and is necessarily a group endomorphism of the abelian group (M, +). The set of all group endomorphisms of M is denoted EndZ(M) and forms a ring under addition and composition, and sending a ring element r of R to its action actually defines a ring homomorphism from R to EndZ(M).Uniform. A uniform module is a module in which all pairs of nonzero submodules have nonzero intersection.Graded. A graded module is a module with a decomposition as a direct sum M = ⨁x Mx over a graded ring R = ⨁x Rx such that RxMy ⊂ Mx+y for all x and y.Artinian. An Artinian module is a module which satisfies the descending chain condition on submodules, that is, every decreasing chain of submodules becomes stationary after finitely many steps.Noetherian. A Noetherian module is a module which satisfies the ascending chain condition on submodules, that is, every increasing chain of submodules becomes stationary after finitely many steps. Equivalently, every submodule is finitely generated.Torsion-free. A torsion-free module is a module over a ring such that 0 is the only element annihilated by a regular element (non zero-divisor) of the ring.Faithful. A faithful module M is one where the action of each r ≠ 0 in R on M is nontrivial (i.e. r ⋅ x ≠ 0 for some x in M). Equivalently, the annihilator of M is the zero ideal.Indecomposable. An indecomposable module is a non-zero module that cannot be written as a direct sum of two non-zero submodules. Every simple module is indecomposable, but there are indecomposable modules which are not simple (e.g. uniform modules).Semisimple. A semisimple module is a direct sum (finite or not) of simple modules. Historically these modules are also called completely reducible.Simple. A simple module S is a module that is not {0} and whose only submodules are {0} and S. Simple modules are sometimes called irreducible.[3]Torsionless module. A module is called torsionless if it embeds into its algebraic dual.Flat. A module is called flat if taking the tensor product of it with any exact sequence of R-modules preserves exactness.Injective. Injective modules are defined dually to projective modules.Projective. Projective modules are direct summands of free modules and share many of their desirable properties.Free. A free R-module is a module that has a basis, or equivalently, one that is isomorphic to a direct sum of copies of the ring R. These are the modules that behave very much like vector spaces.Cyclic. A module is called a cyclic module if it is generated by one element.Finitely generated. An R-module M is finitely generated if there exist finitely many elements x1, ..., xn in M such that every element of M is a linear combination of those elements with coefficients from the ring R.The left R-modules, together with their module homomorphisms, form a category, written as R-Mod (see category of modules for more.) This is an abelian category.The kernel of a module homomorphism f : M → N is the submodule of M consisting of all elements that are sent to zero by f. The isomorphism theorems familiar from groups and vector spaces are also valid for R-modules.A bijective module homomorphism is an isomorphism of modules, and the two modules are called isomorphic. Two isomorphic modules are identical for all practical purposes, differing solely in the notation for their elements.This, like any homomorphism of mathematical objects, is just a mapping which preserves the structure of the objects. Another name for a homomorphism of modules over R is an R-linear map.If M and N are left R-modules, then a map f : M → N is a homomorphism of R-modules if, for any m, n in M and r, s in R,The set of submodules of a given module M, together with the two binary operations + and ∩, forms a lattice which satisfies the modular law: Given submodules U, N1, N2 of M such that N1 ⊂ N2, then the following two submodules are equal: (N1 + U) ∩ N2 = N1 + (U ∩ N2).Suppose M is a left R-module and N is a subgroup of M. Then N is a submodule (or R-submodule, to be more explicit) if, for any n in N and any r in R, the product r ⋅ n is in N (or n ⋅ r for a right module).If R is commutative, then left R-modules are the same as right R-modules and are simply called R-modules.A bimodule is a module that is a left module and a right module such that the two multiplications are compatible.If one writes the scalar action as fr so that fr(x) = r ⋅ x, and f for the map that takes each r to its corresponding map fr , then the first axiom states that every fr is a group endomorphism of M, and the other three axioms assert that the map f : R → End(M) given by r ↦ fr is a ring homomorphism from R to the endomorphism ring End(M).[2] Thus a module is a ring action on an abelian group (cf. group action. Also consider monoid action of multiplicative structure of R). In this sense, module theory generalizes representation theory, which deals with group actions on vector spaces, or equivalently group ring actions.Authors who do not require rings to be unital omit condition 4 above in the definition of an R-module, and so would call the structures defined above "unital left R-modules". In this article, consistent with the glossary of ring theory, all rings and modules are assumed to be unital.[1]The operation of the ring on M is called scalar multiplication, and is usually written by juxtaposition, i.e. as rx for r in R and x in M, though here it is denoted as r ⋅ x to distinguish it from the ring multiplication operation, denoted here by juxtaposition. The notation RM indicates a left R-module M. A right R-module M or MR is defined similarly, except that the ring acts on the right; i.e., scalar multiplication takes the form ⋅ : M × R → M, and the above axioms are written with scalars r and s on the right of x and y.Suppose that R is a ring and 1R is its multiplicative identity. A left R-module M consists of an abelian group (M, +) and an operation ⋅ : R × M → M such that for all r, s in R and x, y in M, we have:Much of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a "well-behaved" ring, such as a principal ideal domain. However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis, and even those that do, free modules, need not have a unique rank if the underlying ring does not satisfy the invariant basis number condition, unlike vector spaces, which always have a (possibly infinite) basis whose cardinality is then unique. (These last two assertions require the axiom of choice in general, but not in the case of finite-dimensional spaces, or certain well-behaved infinite-dimensional spaces such as Lp spaces.)In a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law. In a module, the scalars need only be a ring, so the module concept represents a significant generalization. In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules. In non-commutative algebra the distinction between left ideals, ideals, and modules becomes more pronounced, though some ring-theoretic conditions can be expressed either about left ideals or left modules.Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is compatible with the ring multiplication.In mathematics, a module is one of the fundamental algebraic structures used in abstract algebra. A module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary given ring (with identity) and a multiplication (on the left and/or on the right) is defined between elements of the ring and elements of the module.
Field (mathematics)

Free module
See local ring, perfect ring and Dedekind ring.Many statements about free modules, which are wrong for general modules over rings, are still true for certain generalisations of free modules. Projective modules are direct summands of free modules, so one can choose an injection in a free module and use the basis of this one to prove something for the projective module. Even weaker generalisations are flat modules, which still have the property that tensoring with them preserves exact sequences, and torsion-free modules. If the ring has special properties, this hierarchy may collapse, e.g., for any perfect local Dedekind ring, every torsion-free module is flat, projective and free as well. A finitely generated torsion-free module of a commutative PID is free. A finitely generated Z-module is free if and only if it is flat.and the scalar multiplication by: for r in R and x in E,We equip it with a structure of a left module such that the addition is defined by: for x in E,Given a ring R and a set E, first as a set we letThe free module R(E) may also be constructed in the following equivalent way.A similar argument shows that every free left (resp. right) R-module is isomorphic to a direct sum of copies of R as left (resp. right) module.Given a set E and ring R, there is a free R-module that has E as a basis: namely, the direct sum of copies of R indexed by ELet R be a ring.An immediate consequence of the second half of the definition is that the coefficients in the first half are unique for each element of M.A free module is a module with a basis.[2]A free abelian group is precisely a free module over the ring Z of integers.Given any set S and ring R, there is a free R-module with basis S, which is called free module on S or module of formal linear combinations of the elements of S.In mathematics, a free module is a module that has a basis – that is, a generating set consisting of linearly independent elements. Every vector space is a free module,[1] but, if the ring of the coefficients is not a division ring (not a field in the commutative case), then there exist non-free modules.
Multilinear algebra

Dual space
be continuous for the chosen topology on V′. Further, there is still a choice of a topology on V′′, and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V′, so that the continuous double dual V′′ is not uniquely defined as a set. Saying that Ψ maps from V to V′′, or in other words, that Ψ(x) is continuous on V′ for every x ∈ V, is a reasonable minimal requirement on the topology of V′, namely that the evaluation mappingsWhen V is a topological vector space, one can still define Ψ(x) by the same formula, for every x ∈ V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial. However, if V is Hausdorff and locally convex, the map Ψ is injective from V to the algebraic dual V′∗ of the continuous dual, again as a consequence of the Hahn–Banach theorem.[15]As a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning ‖ Ψ(x) ‖ = ‖ x ‖ for all x in V. Normed spaces for which the map Ψ is a bijection are called reflexive.In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator Ψ : V → V′′ from a normed space V into its continuous double dual V′′, defined byThe topology of V and the topology of real or complex numbers can be used to induce on V′ a dual space topology.If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space ℓ 1 is separable, but its dual ℓ ∞ is not.and it follows from the Hahn–Banach theorem that j′ induces an isometric isomorphism V′ / W⊥ → W′.Then, the dual of the quotient V / W  can be identified with W⊥, and the dual of W can be identified with the quotient V′ / W⊥.[14] Indeed, let P denote the canonical surjection from V onto the quotient V / W ; then, the transpose P′ is an isometric isomorphism from (V / W )′ into V′, with range equal to W⊥. If j denotes the injection map from W into V, then the kernel of the transpose j′ is the annihilator of W:Assume that W is a closed linear subspace of a normed space V, and consider the annihilator of W in V′,When T is a continuous linear map between two topological vector spaces V and W, then the transpose T′ is continuous when W′ and V′ are equipped with"compatible" topologies: for example when, for X = V and X = W, both duals X′ have the strong topology β(X′, X) of uniform convergence on bounded sets of X, or both have the weak-∗ topology σ(X′, X) of pointwise convergence on X. The transpose T′ is continuous from β(W′, W) to β(V′, V), or from σ(W′, W) to σ(V′, V).When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V′. For every bounded linear map T on V, the transpose and the adjoint operators are linked byWhen T is a compact linear map between two Banach spaces V and W, then the transpose T′ is compact. This can be proved using the Arzelà–Ascoli theorem.When V and W are normed spaces, the norm of the transpose in L(W′, V′) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T′ is injective.The resulting functional T′(φ) is in V′. The assignment T → T′ produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W′ to V′. When T and U are composable continuous linear maps, thenIf T : V → W is a continuous linear map between two topological vector spaces, then the (continuous) transpose T′ : W′ → V′ is defined by the same formula as before:By the Riesz–Markov–Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.In a similar manner, the continuous dual of ℓ 1 is naturally identified with ℓ ∞ (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with ℓ 1.is finite. Define the number q by 1/p + 1/q = 1. Then the continuous dual of ℓ p is naturally identified with ℓ q: given an element φ ∈ (ℓ p)′, the corresponding element of ℓ q is the sequence (φ(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) ∈ ℓ q, the corresponding continuous linear functional φ on ℓ p is defined by φ(b) = ∑n anbn for all b = (bn) ∈ ℓ p (see Hölder's inequality).Let 1 < p < ∞ be a real number and consider the Banach space ℓ p of all sequences a = (an) for whichHere are the three most important special cases.form its local base.As a particular consequence, if V is a direct sum of two subspaces A and B, then V∗ is a direct sum of A0 and B0.If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional f : V → F factors through V/W if and only if W is in the kernel of f. There is thus an isomorphismafter identifying W with its image in the second dual space under the double duality isomorphism V ≈ V∗∗. Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.If V is finite-dimensional, and W is a vector subspace, thenIn particular if A and B are subspaces of V, it follows thatand equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, thenMoreover, if A and B are two subsets of V, thenThe annihilator of a subset is itself a vector space. In particular, ∅0 = V∗ is all of V∗ (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S ⊂ T ⊂ V, thenLet S be a subset of V. The annihilator of S in V∗, denoted here S0, is the collection of linear functionals f ∈ V∗ such that [f, s] = 0 for all s ∈ S. That is, S0 consists of all linear functionals f : V → F such that the restriction to S vanishes: f|S = 0.If the linear map f is represented by the matrix A with respect to two bases of V and W, then f∗ is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f∗ is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.The assignment f ↦ f∗ produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)∗ = g∗f∗. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify (f∗)∗ with f using the natural injection into the double dual.where the bracket [·,·] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual. This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.The following identity holds for all φ ∈ W∗ and v ∈ V:for every φ ∈ W∗. The resulting functional f∗(φ) in V∗ is called the pullback of φ along f.If f : V → W is a linear map, then the transpose (or dual) f∗ : W∗ → V∗ is defined byThe conjugate space V∗ can be identified with the set of all additive complex-valued functionals f: V → C such thatIf the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form ⟨·,·⟩ determines an isomorphism of V with the complex conjugate of the dual spaceThus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V∗ and nondegenerate bilinear forms on V.defined bywhere the right hand side is defined as the functional on V taking each w ∈ V to ⟨v,w⟩. In other words, the bilinear form determines a linear mappingIf V is finite-dimensional, then V is isomorphic to V∗. But there is in general no natural isomorphism between these two spaces.[7] Any bilinear form ⟨·,·⟩ on V gives a mapping of V into its dual space viaThus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.is a special case of a general result relating direct sums (of modules) to direct products.On the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identificationNote that (FA)0 may be identified (essentially by definition) with the direct sum of infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphismsAgain the sum is finite because fα is nonzero for only finitely many α.The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values θα = T(eα) it takes on the basis of V, and any function θ : A → F (with θ(α) = θα) defines a linear functional T on V byin V (the sum is finite by the assumption on f, and any v ∈ V may be written in this way by the definition of the basis).This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {eα : α ∈ A} identifies V with the space (FA)0 of functions f : A → F such that fα = f(α) is nonzero for only finitely many α ∈ A, where such a function f is identified with the vectorConsider, for instance, the space R∞, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i ∈ N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R∞ is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R∞ to give the number ∑anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R∞ is countably infinite, whereas RN does not have a countable basis.If V is not finite-dimensional but has a basis[6] eα indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements eα (α ∈ A) of the dual space, but they will not form a basis.If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V∗ form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V∗ are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]In particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n × 1 matrix and a 1 × 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 × n matrix, i.e., M must be a row vector.for any choice of coefficients ci ∈ F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equationsIf V is finite-dimensional, then V∗ has the same dimension as V. Given a basis {e1, ..., en} in V, it is possible to construct a specific basis in V∗, called the dual basis. This dual basis is a set {e1, ..., en} of linear functionals on V, defined by the relationThe pairing of a functional φ in the dual space V∗ and an element x of V is sometimes denoted by a bracket: φ(x) = [x,φ] [2] or φ(x) = ⟨φ,x⟩.[3] This pairing defines a nondegenerate bilinear mapping[4] ⟨·,·⟩ : V∗ × V → F called the natural pairing.for all φ and ψ ∈ V∗, x ∈ V, and a ∈ F. Elements of the algebraic dual space V∗ are sometimes called covectors or one-forms.Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space. When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.
Tensor product
However, these kinds of notation are not universally present in array languages. Other array languages may require explicit treatment of indices (for example, MATLAB), and/or may not support higher-order functions such as the Jacobian derivative (for example, Fortran/APL).Note that J's treatment also allows the representation of some tensor fields, as a and b may be functions instead of constants. This product of two functions is a derived function, and if a and b are differentiable, then a */ b is differentiable.Array programming languages may have this pattern built in. For example, in APL the tensor product is expressed as ○.× (for example A ○.× B or A ○.× B ○.× C). In J the tensor product is the dyadic form of */ (for example a */ b or a */ b */ c).That is, in the symmetric algebra two adjacent vectors (and therefore all of them) can be interchanged. The resulting objects are called symmetric tensors.The symmetric algebra is constructed in a similar manner:Note that when the underlying field of V does not have characteristic 2, then this definition is equivalent tois defined asTwo notable constructions in linear algebra can be constructed as quotients of the tensor product: the exterior algebra and the symmetric algebra. For example, given a vector space V, the exterior productA general context for tensor product is that of a monoidal category.It should be mentioned that, though called "tensor product", this is not a tensor product of graphs in the above sense; actually it is the category-theoretic product in the category of graphs and graph homomorphisms. However it is actually the Kronecker tensor product of the adjacency matrices of the graphs. Compare also the section Tensor product of linear maps above.This is a special case of the product of tensors if they are seen as multilinear maps (see also tensors as multilinear maps). Thus the components of the tensor product of multilinear forms can be computed by the Kronecker product.is isomorphic (as an A-algebra) to the Adeg(f).where now f is interpreted as the same polynomial, but with its coefficients regarded as elements of B. In the larger field B, the polynomial may become reducible, which brings in Galois theory. For example, if A = B is a Galois extension of R, thenA particular example is when A and B are fields containing a common subfield R. The tensor product of fields is closely related to Galois theory: if, say, A = R[x] / f(x), where f is some irreducible polynomial with coefficients in R, the tensor product can be calculated asFor example,Let R be a commutative ring. The tensor product of R-modules applies, in particular, if A and B are R-algebras. In this case, the tensor product A ⊗R B is an R-algebra itself by puttingis not usually injective. For example, tensoring the (injective) map given by multiplication with n, n : Z → Z with Z/nZ yields the zero map 0 : Z/nZ → Z/nZ, which is not injective. Higher Tor functors measure the defect of the tensor product being not left exact. All higher Tor functors are assembled in the derived tensor product.Here NJ := ⨁j ∈ J N and the map is determined by sending some n ∈ N in the jth copy of NJ to ajin (in NI). Colloquially, this may be rephrased by saying that a presentation of M gives rise to a presentation of M ⊗R N. This is referred to by saying that the tensor product is a right exact functor. It is not in general left exact, that is, given an injective map of R-modules M1 → M2, the tensor productFor vector spaces, the tensor product V ⊗ W is quickly computed since bases of V of W immediately determine a basis of V ⊗ W, as was mentioned above. For modules over a general (commutative) ring, not every module is free. For example, Z/nZ is not a free abelian group (= Z-module). The tensor product with Z/nZ is given byLet A be a right R-module and B be a left R-module B. Then the tensor product of A and B is an abelian group defined byThe universal property also carries over, slightly modified: the map φ : A × B → A ⊗R B defined by (a, b) ↦ a ⊗ b is a middle linear map (referred to as "the canonical middle linear map".[13]); that is,[14] it satisfies:is imposed. If R is non-commutative, this is no longer an R-module, but just an abelian group.More generally, the tensor product can be defined even if the ring is non-commutative (ab ≠ ba). In this case A has to be a right-R-module and B is a left-R-module, and instead of the last two relations above, the relationwhere now F(A × B) is the free R-module generated by the cartesian product and G is the R-module generated by the same relations as above.The tensor product of two modules A and B over a commutative ring R is defined in exactly the same way as the tensor product of vector spaces over a field:where u∗ in End(V∗) is the transpose of u, that is, in terms of the obvious pairing on V ⊗ V∗,Here Hom(-,-) denotes the K-vector space of all linear maps. This is an example of adjoint functors: the tensor product is "left adjoint" to Hom.Furthermore, given three vector spaces U, V, W the tensor product is linked to the vector space of all linear maps, as follows:This result impliesGiven two finite dimensional vector spaces U, V, denote the dual space of U as U*, we have the following relation:The interplay of evaluation and coevaluation map can be used to characterize finite-dimensional vector spaces without referring to bases.[12]where v1, ..., vn is any basis of V, and vi∗ is its dual basis. Surprisingly, this map does not depend on our choice of basis.[11]On the other hand, if V is finite-dimensional, there is a canonical map in the other direction (called the coevaluation map)is called tensor contraction (for r, s > 0).The resulting mapwhich on elementary tensors is defined byA particular example is the tensor product of some vector space V with its dual vector space V∗ (which consists of all linear maps f from V to the ground field K). In this case, there is a canonical evaluation mapand[10] Thus, the components of the tensor product of two tensors are the ordinary product of the components of each tensor. Another example: let U be a tensor of type (1, 1) with components Uαβ, and let V be a tensor of type (1, 0) with components V γ. ThenPicking a basis of V and the corresponding dual basis of V∗ naturally induces a basis for Tr
s(V) (this basis is described in the article on Kronecker products). In terms of these bases, the components of a (tensor) product of two (or more) tensors can be computed. For example, if F and G are two covariant tensors of rank m and n respectively (i.e. F ∈ T 0
m, and G ∈ T 0
n), then the components of their tensor product are given byIt is defined by grouping all occurring "factors" V together: writing vi for an element of V and fi for elements of the dual space,There is a product map, called the (tensor) product of tensors[9]Here V∗ is the dual vector space (which consists of all linear maps f from V to the ground field K).For non-negative integers r and s a type (r,s) tensor on a vector space V is an element ofThe isomorphism τσ is called the braiding map associated to the permutation σ.such thatbe the natural multilinear embedding of the Cartesian power of V into the tensor power of V. Then, by the universal property, there is a unique isomorphismLetA permutation σ of the set {1, 2, ..., n} determines a mapping of the nth Cartesian power of V as follows:Let n be a non-negative integer. The nth tensor power of the vector space V is the n-fold tensor product of V with itself. That isThe universal-property definition of a tensor product is valid in more categories that just the category of vector spaces. Instead of using multilinear (bilinear) maps, the general tensor product definition uses multimorphisms.[8]The category of vector spaces with tensor product is an example of a symmetric monoidal category.Similar reasoning can be used to show that the tensor product is associative, that is, there are natural isomorphismsThis characterization can simplify proofs about the tensor product. For example, the tensor product is symmetric, meaning there is a canonical isomorphism:A dyadic product is the special case of the tensor product between two vectors of the same dimension.The resultant rank is at most 4, and thus the resultant dimension is 4. Here rank denotes the tensor rank (number of requisite indices), while the matrix rank counts the number of degrees of freedom in the resulting array.respectively, then the tensor product of these two matrices isBy choosing bases of all vector spaces involved, the linear maps S and T can be represented by matrices. Then, the matrix describing the tensor product S ⊗ T is the Kronecker product of the two matrices. For example, if V, X, W, and Y above are all two-dimensional and bases have been fixed for all of them, and S and T are given by the matricesIf S and T are both injective, surjective, or continuous then S ⊗ T is, respectively, injective, surjective, continuous.In this way, the tensor product becomes a bifunctor from the category of vector spaces to itself, covariant in both arguments.[7]defined byThe tensor product also operates on linear maps between vector spaces. Specifically, given two linear maps S : V → X and T : W → Y between vector spaces, the tensor product of the two linear maps S and T is a linear mapGiven bases {vi} and {wj} for V and W respectively, the tensors {vi ⊗ wj} form a basis for V ⊗ W. Therefore, if V and W are finite-dimensional, the dimension of the tensor product is the product of dimensions of the original spaces; for instance Rm ⊗ Rn is isomorphic to Rmn.Elements of V ⊗ W are often referred to as tensors, although this term refers to many other related concepts as well.[5] If v belongs to V and w belongs to W, then the equivalence class of (v, w) is denoted by v ⊗ w, which is called the tensor product of v with w. In physics and engineering, this use of the "⊗" symbol refers specifically to the outer product operation; the result of the outer product v ⊗ w is one of the standard ways of representing the equivalence class v ⊗ w.[6] An element of V ⊗ W that can be written in the form v ⊗ w is called a pure or simple tensor. In general, an element of the tensor product space is not a pure tensor, but rather a finite linear combination of pure tensors. For example, if v1 and v2 are linearly independent, and w1 and w2 are also linearly independent, then v1 ⊗ w1 + v2 ⊗ w2 cannot be written as a pure tensor. The number of simple tensors required to express an element of a tensor product is called the tensor rank (not to be confused with tensor order, which is the number of spaces one has taken the product of, in this case 2; in notation, the number of indices), and for linear operators or matrices, thought of as (1, 1) tensors (elements of the space V ⊗ V∗), it agrees with matrix rank.all hold (unlike in F(V × W)), which is exactly what is desired. In these latter expressions, the (v1, w), etc., are images in the quotient of vectors in the free product under the quotient map. Usually, some other notation is employed for them, see below.In the quotient, where N is mapped to the zero vector, the following equalities,The following expression explicitly gives the subspace N:[4]The result can be proven to be independent of which representatives of the involved classes have been chosen. In other words, the operations are well-defined.in the involved equivalence classes outputting the one equivalence class of the result.The operations of V ⊗ W, i.e. the map of vector addition + : U × U → U and scalar multiplication ⋅ : K × U → U are defined to be the respective operations +F and ⋅F from F(V × W), acting on any representativesFrom the Cartesian product V × W, the free vector space F(V × W) over K is formed. The vectors of V ⊗ W are then defined to be the equivalence classes of the congruence generated by the following relations on F(V × W):In general, given two vector spaces V and W over a field K, the tensor product U of V and W, denoted as U = V ⊗ W is defined as the vector space whose elements and operations are constructed as follows:Let us first consider a special case: let us say V, W are free vector spaces for the sets S, T respectively. That is, V = F(S), W = F(T). In this special case, the tensor product is defined as F(S) ⊗ F(T) = F(S × T). In most typical cases, any vector space can be immediately understood as the free vector space for some set, so this definition suffices. However, there is also an explicit way of constructing the tensor product directly from V, W, without appeal to S, T.By construction, the (possibly infinite) dimension of the vector space F(S) equals the cardinality of the set S.Then {δs | s ∈ S} is a basis for F(S), since each element g of F(S) can be uniquely written as a linear combination of δs, and because of the restriction that g has finite support, this linear combination consists of finitely many terms. Because of this explicit expression, an element of F(S) is often called a formal sum of symbols in S.The definition of ⊗ requires the notion of the free vector space F(S) on some set S, a vector space whose basis is indexed by S. F(S) is defined as the set of all functions g from S to a given field K that have finite support; i.e., g is identically zero outside some finite subset of S. It is a vector space over K with the usual addition and scalar multiplication of functions. It has a basis parameterized by S. Indeed, for each s in S we define[1]The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below). Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over V × W. This approach is described below.The tensor product of two vector spaces V and W over a field K is another vector space over K. It is denoted V ⊗K W, or V ⊗ W when the underlying field K is understood.More generally, the tensor product can be extended to other categories of mathematical objects in addition to vector spaces, such as to matrices, tensors, algebras, topological vector spaces, and modules. In each such case the tensor product is characterized by a similar universal property: it is the freest bilinear operation. The general concept of a "tensor product" is captured by monoidal categories; that is, the class of all things that have a tensor product is a monoidal category.In particular, this distinguishes the tensor product from the direct sum vector space, whose dimension is the sum of the dimensions of the two summands:The tensor product of (finite dimensional) vector spaces has dimension equal to the product of the dimensions of the two factors:In mathematics, the tensor product V ⊗ W of two vector spaces V and W (over the same field) is itself a vector space, together with an operation of bilinear composition, denoted by ⊗, from ordered pairs in the Cartesian product V × W into V ⊗ W, in a way that generalizes the outer product. The tensor product of V and W is the vector space generated by the symbols v ⊗ w, with v ∈ V and w ∈ W, in which the relations of bilinearity are imposed for the product operation ⊗, and no other relations are assumed to hold. The tensor product space is thus the "freest" (or most general) such vector space, in the sense of having the fewest constraints.
Algebra over a field
In some areas of mathematics, such as commutative algebra, it is common to consider the more general concept of an algebra over a ring, where a commutative unital ring R replaces the field K. The only part of the definition that changes is that A is assumed to be an R-module (instead of a vector space over K).The fourth algebra is non-commutative, others are commutative.There exist five three-dimensional algebras. Each algebra consists of linear combinations of three basis elements, 1 (the identity element), a and b. Taking into account the definition of an identity element, it is sufficient to specifyIt remains to specifyThere exist two two-dimensional algebras. Each algebra consists of linear combinations (with complex coefficients) of two basis elements, 1 (the identity element) and a. According to the definition of an identity element,Two-dimensional, three-dimensional and four-dimensional unital associative algebras over the field of complex numbers were completely classified up to isomorphism by Eduard Study.[4]If K is only a commutative ring and not a field, then the same process works if A is a free module over K. If it isn't, then the multiplication is still completely determined by its action on a set that spans A; however, the structure constants can't be specified arbitrarily in this case, and knowing only the structure constants does not specify the algebra up to isomorphism.If you apply this to vectors written in index notation, then this becomesWhen the algebra can be endowed with a metric, then the structure coefficients are generally written with upper and lower indices, so as to distinguish their transformation properties under coordinate transformations. Specifically, lower indices are covariant indices, and transform via pullbacks, while upper indices are contravariant, transforming under pushforwards. Thus, in mathematical physics, the structure coefficients are often written ci,jk, and their defining rule is written using the Einstein notation asNote however that several different sets of structure coefficients can give rise to isomorphic algebras.where e1,...,en form a basis of A.Thus, given the field K, any finite-dimensional algebra can be specified up to isomorphism by giving its dimension (say n), and specifying n3 structure coefficients ci,j,k, which are scalars. These structure coefficients determine the multiplication in A via the following rule:For algebras over a field, the bilinear multiplication from A × A to A is completely determined by the multiplication of basis elements of A. Conversely, once a basis for A has been chosen, the products of basis elements can be set arbitrarily, and then extended in a unique way to a bilinear operator on A, i.e., so the resulting multiplication satisfies the algebra laws.Given two such associative unital K-algebras A and B, a unital K-algebra morphism f: A → B is a ring morphism that commutes with the scalar multiplication defined by η, which one may write asgiven bywhere Z(A) is the center of A. Since η is a ring morphism, then one must have either that A is the zero ring, or that η is injective. This definition is equivalent to that above, with scalar multiplicationThe definition of an associative K-algebra with unit is also frequently given in an alternative way. In this case, an algebra over a field K is a ring A together with a ring homomorphismExamples detailed in the main article include:These unital zero algebras may be more generally useful, as they allow to translate any general property of the algebras to properties of vector spaces or modules. For example, the theory of Gröbner bases was introduced by Bruno Buchberger for ideals in a polynomial ring R = K[x1, ..., xn] over a field. The construction of the unital zero algebra over a free R-module allows extending this theory as a Gröbner basis theory for sub modules of a free module. This extension allows, for computing a Gröbner basis of a submodule, to use, without any modification, any algorithm and any software for computing Gröbner bases of ideals.An example of unital zero algebra is the algebra of dual numbers, the unital zero R-algebra built from a one dimensional real vector space.One may define a unital zero algebra by taking the direct sum of modules of a field (or more generally a ring) K and a K-vector space (or module) V, and defining the product of every pair of elements of V to be zero. That is, if λ, μ ∈ k and u, v ∈ V, then (λ + u) (μ + v) = λμ + (λv + μu). If e1, ... ed is a basis of V, the unital zero algebra is the quotient of the polynomial ring K[E1, ..., En] by the ideal generated by the EiEj for every pair (i, j).An algebra is called zero algebra if uv = 0 for all u, v in the algebra,[2] not to be confused with the algebra with one element. It is inherently non-unital (except in the case of only one element), associative and commutative.An algebra is unital or unitary if it has a unit or identity element I with Ix = x = xI for all x in the algebra.Algebras over fields come in many different types. These types are specified by insisting on some further axioms, such as commutativity or associativity of the multiplication operation, which are not required in the broad definition of an algebra. The theories corresponding to the different types of algebras are often very different.It is important to notice that this definition is different from the definition of an ideal of a ring, in that here we require the condition (2). Of course if the algebra is unital, then condition (3) implies condition (2).If (3) were replaced with x · z is in L, then this would define a right ideal. A two-sided ideal is a subset that is both a left and a right ideal. The term ideal on its own is usually taken to mean a two-sided ideal. Of course when the algebra is commutative, then all of these notions of ideal are equivalent. Notice that conditions (1) and (2) together are equivalent to L being a linear subspace of A. It follows from condition (3) that every left or right ideal is a subalgebra.A left ideal of a K-algebra is a linear subspace that has the property that any element of the subspace multiplied on the left by any element of the algebra produces an element of the subspace. In symbols, we say that a subset L of a K-algebra A is a left ideal if for every x and y in L, z in A and c in K, we have the following three statements.In the above example of the complex numbers viewed as a two-dimensional algebra over the real numbers, the one-dimensional real line is a subalgebra.A subalgebra of an algebra over a field K is a linear subspace that has the property that the product of any two of its elements is again in the subspace. In other words, a subalgebra of an algebra is a subset of elements that is closed under addition, multiplication, and scalar multiplication. In symbols, we say that a subset L of a K-algebra A is a subalgebra if for every x, y in L and c in K, we have that x · y, x + y, and cx are all in L.A K-algebra isomorphism is a bijective K-algebra homomorphism. For all practical purposes, isomorphic algebras differ only by notation.Given K-algebras A and B, a K-algebra homomorphism is a K-linear map f: A → B such that f(xy) = f(x) f(y) for all x,y in A. The space of all K-algebra homomorphisms between A and B is frequently written asPrevious examples are associative algebras. An example of a non-associative algebra is a three dimensional vector space equipped with the cross product. This is a simple example of a class of nonassociative algebras, which is widely used in mathematics and physics, the Lie algebras.The quaternions were soon followed by several other hypercomplex number systems, which were the early examples of algebras over a field.The real numbers may be viewed as a one-dimensional vector space with a compatible multiplication, and hence a one-dimensional algebra over itself. Likewise, as we saw above, the complex numbers form a two-dimensional vector space over the field of real numbers, and hence form a two dimensional algebra over the reals. In both these examples, every non-zero vector has an inverse, making them both division algebras. Although there are no division algebras in 3 dimensions, in 1843, the quaternions were defined and provided the now famous 4-dimensional example of an algebra over the real numbers, where one can not only multiply vectors, but also divide. Any quaternion may be written as (a, b, c, d) = a + bi + cj + dk. Unlike the complex numbers, the quaternions are an example of a non-commutative algebra: for instance, (0,1,0,0) · (0,0,1,0) = (0,0,0,1) but (0,0,1,0) · (0,1,0,0) = (0,0,0,−1).Notice that when a binary operation on a vector space is commutative, as in the above example of the complex numbers, it is left distributive exactly when it is right distributive. But in general, for non-commutative operations (such as the next example of the quaternions), they are not equivalent, and therefore require separate axioms.These three axioms are another way of saying that the binary operation is bilinear. An algebra over K is sometimes also called a K-algebra, and K is called the base field of A. The binary operation is often referred to as multiplication in A. The convention adopted in this article is that multiplication of elements of an algebra is not necessarily associative, although some authors use the term algebra to refer to an associative algebra.Let K be a field, and let A be a vector space over K equipped with an additional binary operation from A × A to A, denoted here by · (i.e. if x and y are any two elements of A, x · y is the product of x and y). Then A is an algebra over K if the following identities hold for all elements x, y, and z of A, and all elements (often called scalars) a and b of K:This example fits into the following definition by taking the field K to be the real numbers, and the vector space A to be the complex numbers.The following statements are basic properties of the complex numbers. If x, y, z are complex numbers and a, b are real numbers, thenAny complex number may be written a + bi, where a and b are real numbers and i is the imaginary unit. In other words, a complex number is represented by the vector (a, b) over the field of real numbers. So the complex numbers form a two-dimensional real vector space, where addition is given by (a, b) + (c, d) = (a + c, b + d) and scalar multiplication is given by c(a, b) = (ca, cb), where all of a, b, c and d are real numbers. We use the symbol · to multiply two vectors together, which we use complex multiplication to define: (a, b) · (c, d) = (ac − bd, ad + bc).Replacing the field of scalars by a commutative ring leads to the more general notion of an algebra over a ring. Algebras are not to be confused with vector spaces equipped with a bilinear form, like inner product spaces, as, for such a space, the result of a product is not in the space, but rather in the field of coefficients.Many authors use the term algebra to mean associative algebra, or unital associative algebra, or in some subjects such as algebraic geometry, unital associative commutative algebra.An algebra is unital or unitary if it has an identity element with respect to the multiplication. The ring of real square matrices of order n forms a unital algebra since the identity matrix of order n is the identity element with respect to matrix multiplication. It is an example of a unital associative algebra, a (unital) ring that is also a vector space.The multiplication operation in an algebra may or may not be associative, leading to the notions of associative algebras and nonassociative algebras. Given an integer n, the ring of real square matrices of order n is an example of an associative algebra over the field of real numbers under matrix addition and matrix multiplication since matrix multiplication is associative. Three-dimensional Euclidean space with multiplication given by the vector cross product is an example of a nonassociative algebra over the field of real numbers since the vector cross product is nonassociative, satisfying the Jacobi identity instead.In mathematics, an algebra over a field (often simply called an algebra) is a vector space equipped with a bilinear product. Thus, an algebra is an algebraic structure, which consists of a set, together with operations of multiplication, addition, and scalar multiplication by elements of the underlying field, and satisfies the axioms implied by "vector space" and "bilinear".[1]
Functional analysis
Functional analysis in its present form[update] includes the following tendencies:Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn–Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice.List of functional analysis topics.The closed graph theorem states the following: If X is a topological space and Y is a compact Hausdorff space, then the graph of a linear map T from X to Y is closed if and only if T is continuous.[3]The proof uses the Baire category theorem, and completeness of both X and Y is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if X and Y are taken to be Fréchet spaces.The open mapping theorem, also known as the Banach–Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:[2]then there exists a linear extension ψ : V → R of φ to the whole space V, i.e., there exists a linear functional ψ such thatHahn–Banach theorem:[2] If p : V → R is a sublinear function, and φ : U → R is a linear functional on a linear subspace U ⊆ V which is dominated by p on U, i.e.The Hahn–Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are "enough" continuous linear functionals defined on every normed vector space to make the study of the dual space "interesting".This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.where T is the multiplication operator:Theorem:[1] Let A be a bounded self-adjoint operator on a Hilbert space H. Then there is a measure space (X, Σ, μ) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H → L2μ(X) such thatThere are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let A be the operator of multiplication by t on L2[0, 1], that isThe theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.The uniform boundedness principle or Banach–Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn–Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.Important results of functional analysis include:Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fréchet derivative article.In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.More generally, functional analysis includes the study of Fréchet spaces and other topological vector spaces not endowed with a norm.The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra. The theory of nonlinear functionals was continued by students of Hadamard, in particular Fréchet and Lévy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.
Mathematical analysis
Techniques from analysis are used in many areas of mathematics, including:When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal. A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[24]Functional analysis is also a major factor in quantum mechanics.The vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schrödinger equation, and the Einstein field equations.Techniques from analysis are also found in other areas such as:Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.Modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).[22]Differential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly.A differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders.[18][19][20] Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense.[16][17] The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.Complex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers.[15] It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable.[13][14] In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.One of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n → ∞, denotedA sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.Much of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).In mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.Also, "monsters" (nowhere continuous functions, continuous but nowhere differentiable functions, space-filling curves) began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue solved the problem of measure, and Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.In the middle of the 19th century Riemann introduced his theory of integration. The last third of the century saw the arithmetization of analysis by Weierstrass, who thought that geometric reasoning was inherently misleading, and introduced the "epsilon-delta" definition of limit. Then, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the "gaps" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the "size" of the set of discontinuities of real functions.In the 18th century, Euler introduced the notion of mathematical function.[11] Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816,[12] but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler. Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals. Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y. He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis. The contributions of these mathematicians and others, such as Weierstrass, developed the (ε, δ)-definition of limit approach, thus founding the modern field of mathematical analysis.The modern foundations of mathematical analysis were established in 17th century Europe.[3] Descartes and Fermat independently developed analytic geometry, and a few decades later Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18th century, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.In the 14th century, Madhava of Sangamagrama developed infinite series expansions, like the power series and the Taylor series, of functions such as sine, cosine, tangent and arctangent.[10] Alongside his development of the Taylor series of the trigonometric functions, he also estimated the magnitude of the error terms created by truncating these series and gave a rational approximation of an infinite series. His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.Mathematical analysis formally developed in the 17th century during the Scientific Revolution,[3] but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy.[4] Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids.[5] The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century.[6] In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century AD to find the area of a circle.[7] Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century.[8] The Indian mathematician Bhāskara II gave examples of the derivative and used what is now known as Rolle's theorem in the 12th century.[9]These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis. Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects (a metric space).Mathematical analysis is the branch of mathematics dealing with limits and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2]
Lp space
One may also define spaces Lp(M) on a manifold, called the intrinsic Lp spaces of the manifold, using densities.As Lp-spaces, the weighted spaces have nothing special, since Lp(S, w dμ) is equal to Lp(S, dν). But they are the natural framework for several results in harmonic analysis (Grafakos 2004); they appear for example in the Muckenhoupt theorem: for 1 < p < ∞, the classical Hilbert transform is defined on Lp(T, λ) where T denotes the unit circle and λ the Lebesgue measure; the (nonlinear) Hardy–Littlewood maximal operator is bounded on Lp(Rn, λ). Muckenhoupt's theorem describes weights w such that the Hilbert transform remains bounded on Lp(T, w dλ) and the maximal operator on Lp(Rn, w dλ).or, in terms of the Radon–Nikodym derivative, w = dν/dμ  the norm for Lp(S, w dμ) is explicitlyAs before, consider a measure space (S, Σ, μ). Let w : S → [0, ∞) be a measurable function. The w-weighted Lp space is defined as Lp(S, w dμ), where w dμ means the measure ν defined byA major result that uses the Lp,w-spaces is the Marcinkiewicz interpolation theorem, which has broad applications to harmonic analysis and the study of singular integrals.is comparable to the Lp,w-norm. Further in the case p > 1, this expression defines a norm if r = 1. Hence for p > 1 the weak Lp spaces are Banach spaces (Grafakos 2004).For any 0 < r < p the expressionUnder the convention that two functions are equal if they are equal μ almost everywhere, then the spaces Lp,w are complete (Grafakos 2004).In fact, one hasand in particular Lp(S, μ) ⊂ Lp,w(S, μ).The Lp,w-norm is not a true norm, since the triangle inequality fails to hold. Nevertheless, for f in Lp(S, μ),The weak Lp coincide with the Lorentz spaces Lp,∞, so this notation is also used to denote them.The best constant C for this inequality is the Lp,w-norm of f, and is denoted byA function f is said to be in the space weak Lp(S, μ), or Lp,w(S, μ), if there is a constant C > 0 such that, for all t > 0,If f is in Lp(S, μ) for some p with 1 ≤ p < ∞, then by Markov's inequality,Let (S, Σ, μ) be a measure space, and f a measurable function with real or complex values on S. The distribution function of f is defined for t > 0 byThe resulting space L0(Rn, λ) coincides as topological vector space with L0(Rn, g(x) dλ(x)), for any positive λ–integrable density g.For the infinite Lebesgue measure λ on Rn, the definition of the fundamental system of neighborhoods could be modified as followswhere φ is bounded continuous concave and non-decreasing on [0, ∞), with φ(0) = 0 and φ(t) > 0 when t > 0 (for example, φ(t) = min(t, 1)). Such a metric is called Lévy-metric for L0. Under this metric the space L0 is complete (it is again an F-space). The space L0 is in general not locally bounded, and not locally convex.The topology can be defined by any metric d of the formThe description is easier when μ is finite. If μ is a finite measure on (S, Σ), the 0 function admits for the convergence in measure the following fundamental system of neighborhoodsThe vector space of (equivalence classes of) measurable functions on (S, Σ, μ) is denoted L0(S, Σ, μ) (Kalton, Peck & Roberts 1984). By definition, it contains all the Lp, and is equipped with the topology of convergence in measure. When μ is a probability measure (i.e., μ(S) = 1), this mode of convergence is named convergence in probability.The situation of having no linear functionals is highly undesirable for the purposes of doing analysis. In the case of the Lebesgue measure on Rn, rather than work with Lp for 0 < p < 1, it is common to work with the Hardy space H p whenever possible, as this has quite a few linear functionals: enough to distinguish points from one another. However, the Hahn–Banach theorem still fails in H p for p < 1 (Duren 1970, §7.5).The only nonempty convex open set in Lp([0, 1]) is the entire space (Rudin 1991, §1.47). As a particular consequence, there are no nonzero linear functionals on Lp([0, 1]): the dual space is the zero space. In the case of the counting measure on the natural numbers (producing the sequence space Lp(μ) = ℓ p), the bounded linear functionals on ℓ p are exactly those that are bounded on ℓ 1, namely those given by sequences in ℓ ∞. Although ℓ p does contain non-trivial convex open sets, it fails to have enough of them to give a base for the topology.The space Lp for 0 < p < 1 is an F-space: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ≥ 1. It is the prototypical example of an F-space that, for most reasonable measure spaces, is not locally convex: in ℓ p or Lp([0, 1]), every open convex set containing the 0 function is unbounded for the p-quasi-norm; therefore, the 0 vector does not possess a fundamental system of convex neighborhoods. Specifically, this is true if the measure space S contains an infinite family of disjoint measurable sets of finite positive measure.This result may be used to prove Clarkson's inequalities, which are in turn used to establish the uniform convexity of the spaces Lp for 1 < p < ∞ (Adams & Fournier 2003).In this setting Lp satisfies a reverse Minkowski inequality, that is for u, v in Lpis a metric on Lp(μ). The resulting metric space is complete; the verification is similar to the familiar case when p ≥ 1.and so the functionAs before, we may introduce the p-norm || f ||p = Np( f )1/p, but || · || p does not satisfy the triangle inequality in this case, and defines only a quasi-norm. The inequality (a + b) p ≤ a p + b p, valid for a, b ≥ 0 implies that (Rudin 1991, §1.47)Let (S, Σ, μ) be a measure space. If 0 < p < 1, then Lp(μ) can be defined as above: it is the vector space of those measurable functions  f  such thatwhereSeveral properties of general functions in Lp(Rd) are first proved for continuous and compactly supported functions (sometimes for step functions), then extended by density to all functions. For example, it is proved this way that translations are continuous on Lp(Rd), in the following sense:This applies in particular when S = Rd and when μ is the Lebesgue measure. The space of continuous and compactly supported functions is dense in Lp(Rd). Similarly, the space of integrable step functions is dense in Lp(Rd); this space is the linear span of indicator functions of bounded intervals when d = 1, of bounded rectangles when d = 2 and more generally of products of bounded intervals.If S can be covered by an increasing sequence (Vn) of open sets that have finite measure, then the space of p–integrable continuous functions is dense in Lp(S, Σ, μ). More precisely, one can use bounded continuous functions that vanish outside one of the open sets Vn.It follows that there exists φ continuous on S such thatSuppose V ⊂ S is an open set with μ(V) < ∞. It can be proved that for every Borel set A ∈ Σ contained in V, and for every ε > 0, there exist a closed set F and an open set U such thatMore can be said when S is a metrizable topological space and Σ its Borel σ–algebra, i.e., the smallest σ–algebra of subsets of S containing the open sets.Let (S, Σ, μ) be a measure space. An integrable simple function  f  on S is one of the formThroughout this section we assume that: 1 ≤ p < ∞.the case of equality being achieved exactly when  f  = 1 μ-a.e.The constant appearing in the above inequality is optimal, in the sense that the operator norm of the identity I : Lq(S, μ) → Lp(S, μ) is preciselyleading toNeither condition holds for the real line with the Lebesgue measure. In both cases the embedding is continuous, in that the identity operator is a bounded linear map from Lq to Lp in the first case, and Lp to Lq in the second. (This is a consequence of the closed graph theorem and properties of Lp spaces.) Indeed, if the domain S has finite measure, one can make the following explicit calculation using Hölder's inequalityColloquially, if 1 ≤ p < q ≤ ∞, then Lp(S, μ) contains functions that are more locally singular, while elements of Lq(S, μ) can be more spread out. Consider the Lebesgue measure on the half line (0, ∞). A continuous function in L1 might blow up near 0 but must decay sufficiently fast toward infinity. On the other hand, continuous functions in L∞ need not decay at all but no blow-up is allowed. The precise technical result is the following.[6] Suppose that 0 < p < q ≤ ∞. Then:The dual of L∞ is subtler. Elements of L∞(μ)∗ can be identified with bounded signed finitely additive measures on S that are absolutely continuous with respect to μ. See ba space for more details. If we assume the axiom of choice, this space is much bigger than L1(μ) except in some trivial cases. However, Saharon Shelah proved that there are relatively consistent extensions of Zermelo–Fraenkel set theory (ZF + DC + "Every subset of the real numbers has the Baire property") in which the dual of ℓ∞ is ℓ1.[5]If the measure μ on S is sigma-finite, then the dual of L1(μ) is isometrically isomorphic to L∞(μ) (more precisely, the map κ1 corresponding to p = 1 is an isometry from L∞(μ) onto L1(μ)∗).This map coincides with the canonical embedding J of Lp(μ) into its bidual. Moreover, the map jp is onto, as composition of two onto isometries, and this proves reflexivity.For 1 < p < ∞, the space Lp(μ) is reflexive. Let κp be as above and let κq : Lp(μ) → Lq(μ)∗ be the corresponding linear isometry. Consider the map from Lp(μ) to Lp(μ)∗∗, obtained by composing κq with the transpose (or adjoint) of the inverse of κp:The fact that κp(g) is well defined and continuous follows from Hölder's inequality. κp : Lq(μ) → Lp(μ)∗ is a linear mapping which is an isometry by the extremal case of Hölder's inequality. It is also possible to show (for example with the Radon–Nikodym theorem, see[4]) that any G ∈ Lp(μ)∗ can be expressed this way: i.e., that κp is onto. Since κp is onto and isometric, it is an isomorphism of Banach spaces. With this (isometric) isomorphism in mind, it is usual to say simply that Lq is the dual Banach space of Lp.For 1 ≤ p ≤ ∞ the ℓp spaces are a special case of Lp spaces, when S = N, and μ is the counting measure on N. More generally, if one considers any set S with the counting measure, the resulting Lp space is denoted ℓp(S). For example, the space ℓp(Z) is the space of all sequences indexed by the integers, and when defining the p-norm on such a space, one sums over all the integers. The space ℓp(n), where n is the set with n elements, is Rn with its p-norm as defined above. As any Hilbert space, every space L2 is linearly isometric to a suitable ℓ2(I), where the cardinality of the set I is the cardinality of an arbitrary Hilbertian basis for this particular L2.If we use complex-valued functions, the space L∞ is a commutative C*-algebra with pointwise multiplication and conjugation. For many measure spaces, including all sigma-finite ones, it is in fact a commutative von Neumann algebra. An element of L∞ defines a bounded operator on any Lp space by multiplication.The additional inner product structure allows for a richer theory, with applications to, for instance, Fourier series and quantum mechanics. Functions in L2 are sometimes called quadratically integrable functions, square-integrable functions or square-summable functions, but sometimes these terms are reserved for functions that are square-integrable in some other sense, such as in the sense of a Riemann integral (Titchmarsh 1976).Similar to the ℓp spaces, L2 is the only Hilbert space among Lp spaces. In the complex case, the inner product on L2 is defined byWhen the underlying measure space S is understood, Lp(S, μ) is often abbreviated Lp(μ), or just Lp. The above definitions generalize to Bochner spaces.For 1 ≤ p ≤ ∞, Lp(S, μ) is a Banach space. The fact that Lp is complete is often referred to as the Riesz-Fischer theorem. Completeness can be checked using the convergence theorems for Lebesgue integrals.As before, if there exists q < ∞ such that  f  ∈ L∞(S, μ) ∩ Lq(S, μ), thenFor p = ∞, the space L∞(S, μ) is defined as follows. We start with the set of all measurable functions from S to C or R which are bounded. Again two such functions are identified if they are equal almost everywhere. Denote this set by L∞(S, μ). For a function  f  in this set, the essential supremum of its absolute value serves as an appropriate norm:In the quotient space, two functions  f  and g are identified if  f  = g almost everywhere. The resulting normed vector space is, by definition, This can be made into a normed vector space in a standard way; one simply takes the quotient space with respect to the kernel of || · ||p. Since for any measurable function  f , we have that || f ||p = 0 if and only if  f  = 0 almost everywhere, the kernel of || · ||p does not depend upon p,That the sum of two p-th power integrable functions is again p-th power integrable follows from the inequalityfor every scalar λ.The set of such functions forms a vector space, with the following natural operations:An Lp space may be defined as a space of functions for which the p-th power of the absolute value is Lebesgue integrable,[3] where functions which agree almost everywhere are identified. More generally, let 1 ≤ p < ∞ and (S, Σ, μ) be a measure space. Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, thatThe p-norm thus defined on ℓ p is indeed a norm, and ℓ p together with this norm is a Banach space. The fully general Lp space is obtained—as seen below — by considering vectors, not only with finitely or countably-infinitely many components, but with "arbitrarily many components"; in other words, functions. An integral instead of a sum is used to define the p-norm.if the right-hand side is finite, or the left-hand side is infinite. Thus, we will consider ℓ p spaces for 1 ≤ p ≤ ∞.and the corresponding space ℓ ∞ of all bounded sequences. It turns out that[2]One also defines the ∞-norm using the supremum:diverges for p = 1 (the harmonic series), but is convergent for p > 1.is not in ℓ 1, but it is in ℓ p for p > 1, as the seriesOne can check that as p increases, the set ℓ p grows larger. For example, the sequenceHere, a complication arises, namely that the series on the right is not always convergent, so for example, the sequence made up of only ones, (1, 1, 1, ...), will have an infinite p-norm for 1 ≤ p < ∞. The space ℓ p is then defined as the set of all infinite sequences of real (or complex) numbers such that the p-norm is finite.Define the p-norm:The space of sequences has a natural vector space structure by applying addition and scalar multiplication coordinate by coordinate. Explicitly, the vector sum and the scalar action for infinite sequences of real (or complex) numbers are given by:The p-norm can be extended to vectors that have an infinite number of components, which yields the space ℓ p. This contains as special cases:This is not a norm because it is not homogeneous. Despite these defects as a mathematical norm, the non-zero counting "norm" has uses in scientific computing, information theory, and statistics–notably in compressed sensing in signal processing and computational harmonic analysis.Another function was called the ℓ0 "norm" by David Donoho—whose quotation marks warn that this function is not a proper norm—is the number of non-zero entries of the vector x. Many authors abuse terminology by omitting the quotation marks. Defining 00 = 0, the zero "norm" of x is equal towhich is discussed by Stefan Rolewicz in Metric Linear Spaces.[1] The ℓ0-normed space is studied in functional analysis, probability theory, and harmonic analysis.The mathematical definition of the ℓ0 norm was established by Banach's Theory of Linear Operations. The space of sequences has a complete metric topology provided by the F-normThere is one ℓ0 norm and another function called the ℓ0 "norm" (with quotation marks).shows that the infinite-dimensional sequence space ℓp defined below, is no longer locally convex.[citation needed]Although the p-unit ball Bnp around the origin in this metric is "concave", the topology defined on Rn by the metric dp is the usual vector space topology of Rn, hence ℓnp is a locally convex topological vector space. Beyond this qualitative statement, a quantitative way to measure the lack of convexity of ℓnp is to denote by Cp(n) the smallest constant C such that the multiple C Bnp of the p-unit ball contains the convex hull of Bnp, equal to Bn1. The fact that for fixed p < 1 we havedefines a metric. The metric space (Rn, dp) is denoted by ℓnp.Hence, the functiondefines a subadditive function at the cost of losing absolute homogeneity. It does define an F-norm, though, which is homogeneous of degree p.defines an absolutely homogeneous function for 0 < p < 1; however, the resulting function does not define a norm, because it is not subadditive. On the other hand, the formulaIn Rn for n > 1, the formulaIn general, for vectors in Cn where 0 < r < p:This inequality depends on the dimension n of the underlying vector space and follows directly from the Cauchy–Schwarz inequality.For the opposite direction, the following relation between the 1-norm and the 2-norm is known:This fact generalizes to p-norms in that the p-norm ||x||p of any given vector x does not grow with p:The grid distance or rectilinear distance (sometimes called the "Manhattan distance") between two points is never shorter than the length of the line segment between them (the Euclidean or "as the crow flies" distance). Formally, this means that the Euclidean norm of any vector is bounded by its 1-norm:Abstractly speaking, this means that Rn together with the p-norm is a Banach space. This Banach space is the Lp-space over Rn.For all p ≥ 1, the p-norms and maximum norm as defined above indeed satisfy the properties of a "length function" (or norm), which are that:See L-infinity.The L∞-norm or maximum norm (or uniform norm) is the limit of the Lp-norms for p → ∞. It turns out that this limit is equivalent to the following definition:The Euclidean norm from above falls into this class and is the 2-norm, and the 1-norm is the norm that corresponds to the rectilinear distance.Of course the absolute value bars are unnecessary when p is a rational number and, in reduced form, has an even numerator.For a real number p ≥ 1, the p-norm or Lp-norm of x is defined byThe Euclidean distance between two points x and y is the length ||x − y||2 of the straight line between the two points. In many situations, the Euclidean distance is insufficient for capturing the actual distances in a given space. An analogy to this is suggested by taxi drivers in a grid street plan who should measure distance not in terms of the length of the straight line to their destination, but in terms of the rectilinear distance, which takes into account that streets are either orthogonal or parallel to each other. The class of p-norms generalizes these two examples and has an abundance of applications in many parts of mathematics, physics, and computer science.
The length of a vector x = (x1, x2, ..., xn) in the n-dimensional real vector space Rn is usually given by the Euclidean norm:Hilbert spaces are central to many applications, from quantum mechanics to stochastic calculus. The spaces L2 and ℓ2 are both Hilbert spaces. In fact, by choosing a Hilbert basis (i.e., a maximal orthonormal subset of L2 or any Hilbert space), one sees that all Hilbert spaces are isometric to ℓ2(E), where E is a set with an appropriate cardinality.By contrast, if p > 2, the Fourier transform does not map into Lq.The Fourier transform for the real line (or, for periodic functions, see Fourier series), maps Lp(R) to Lq(R) (or Lp(T) to ℓq) respectively, where 1 ≤ p ≤ 2 and 1/p + 1/q = 1. This is a consequence of the Riesz–Thorin interpolation theorem, and is made precise with the Hausdorff–Young inequality.In penalized regression, 'L1 penalty' and 'L2 penalty' refer to penalizing either the L1 norm of a solution's vector of parameter values (i.e. the sum of its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector.In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as solutions to variational problems.In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910). Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces. Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, finance, engineering, and other disciplines.
Representation theory
One special case has had a significant impact on representation theory, namely the representation theory of quivers.[11] A quiver is simply a directed graph (with loops and multiple arrows allowed), but it can be made into a category (and also an algebra) by considering paths in the graph. Representations of such categories/algebras have illuminated several aspects of representation theory, for instance by allowing non-semisimple representation theory questions about a group to be reduced in some cases to semisimple representation theory questions about a quiver.More generally, one can relax the assumption that the category being represented has only one object. In full generality, this is simply the theory of functors between categories, and little can be said.Since groups are categories, one can also consider representation of other categories. The simplest generalization is to monoids, which are categories with one object. Groups are monoids for which every morphism is invertible. General monoids have representations in any category. In the category of sets, these are monoid actions, but monoid representations on vector spaces and other objects can be studied.Two types of representations closely related to linear representations are:For another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from G to the homeomorphism group of a topological space X.In the case where C is VectF, the category of vector spaces over a field F, this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of G in the category of sets.Every group G can be viewed as a category with a single object; morphisms in this category are just the elements of G. Given an arbitrary category C, a representation of G in C is a functor from G to C. Such a functor selects an object X in C and a group homomorphism from G to Aut(X), the automorphism group of X.This condition and the axioms for a group imply that ρ(g) is a bijection (or permutation) for all g in G. Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group SX of X.A set-theoretic representation (also known as a group action or permutation representation) of a group G on a set X is given by a function ρ from G to XX, the set of functions from X to X, such that for all g1, g2 in G and all x in X:The Hopf algebras associated to groups have a commutative algebra structure, and so general Hopf algebras are known as quantum groups, although this term is often restricted to certain Hopf algebras arising as deformations of groups or their universal enveloping algebras. The representation theory of quantum groups has added surprising insights to the representation theory of Lie groups and Lie algebras, for instance through the crystal basis of Kashiwara.Hopf algebras provide a way to improve the representation theory of associative algebras, while retaining the representation theory of groups and Lie algebras as special cases. In particular, the tensor product of two representations is a representation, as is the dual vector space.When considering representations of an associative algebra, one can forget the underlying field, and simply regard the associative algebra as a ring, and its representations as modules. This approach is surprisingly fruitful: many results in representation theory can be interpreted as special cases of results about modules over a ring.In one sense, associative algebra representations generalize both representations of groups and Lie algebras. A representation of a group induces a representation of a corresponding group ring or group algebra, while representations of a Lie algebra correspond bijectively to representations of its universal enveloping algebra. However, the representation theory of general associative algebras does not have all of the nice properties of the representation theory of groups and Lie algebras.Before the development of the general theory, many important special cases were worked out in detail, including the Hilbert modular forms and Siegel modular forms. Important results in the theory include the Selberg trace formula and the realization by Robert Langlands that the Riemann-Roch theorem could be applied to calculate the dimension of the space of automorphic forms. The subsequent notion of "automorphic representation" has proved of great technical value for dealing with the case that G is an algebraic group, treated as an adelic algebraic group. As a result, an entire philosophy, the Langlands program has developed around the relation between representation and number theoretic properties of automorphic forms.[40]Automorphic forms are a generalization of modular forms to more general analytic functions, perhaps of several complex variables, with similar transformation properties.[39] The generalization involves replacing the modular group PSL2 (R) and a chosen congruence subgroup by a semisimple Lie group G and a discrete subgroup Γ. Just as modular forms can be viewed as differential forms on a quotient of the upper half space H = PSL2 (R)/SO(2), automorphic forms can be viewed as differential forms (or similar objects) on Γ\G/K, where K is (typically) a maximal compact subgroup of G. Some care is required, however, as the quotient typically has singularities. The quotient of a semisimple Lie group by a compact subgroup is a symmetric space and so the theory of automorphic forms is intimately related to harmonic analysis on symmetric spaces.The representation theory of semisimple Lie groups has its roots in invariant theory[30] and the strong links between representation theory and algebraic geometry have many parallels in differential geometry, beginning with Felix Klein's Erlangen program and Élie Cartan's connections, which place groups and symmetry at the heart of geometry.[38] Modern developments link representation theory and invariant theory to areas as diverse as holonomy, differential operators and the theory of several complex variables.Invariant theory of infinite groups is inextricably linked with the development of linear algebra, especially, the theories of quadratic forms and determinants. Another subject with strong mutual influence is projective geometry, where invariant theory can be used to organize the subject, and during the 1960s, new life was breathed into the subject by David Mumford in the form of his geometric invariant theory.[37]Invariant theory studies actions on algebraic varieties from the point of view of their effect on functions, which form representations of the group. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. The modern approach analyses the decomposition of these representations into irreducibles.[36]Linear algebraic groups (or more generally, affine group schemes) are analogues in algebraic geometry of Lie groups, but over more general fields than just R or C. In particular, over finite fields, they give rise to finite groups of Lie type. Although linear algebraic groups have a classification that is very similar to that of Lie groups, their representation theory is rather different (and much less well understood) and requires different techniques, since the Zariski topology is relatively weak, and techniques from analysis are no longer available.[35]Lie superalgebras are generalizations of Lie algebras in which the underlying vector space has a Z2-grading, and skew-symmetry and Jacobi identity properties of the Lie bracket are modified by signs. Their representation theory is similar to the representation theory of Lie algebras.[34]Affine Lie algebras are a special case of Kac–Moody algebras, which have particular importance in mathematics and theoretical physics, especially conformal field theory and the theory of exactly solvable models. Kac discovered an elegant proof of certain combinatorial identities, Macdonald identities, which is based on the representation theory of affine Kac–Moody algebras.There are many classes of infinite-dimensional Lie algebras whose representations have been studied. Among these, an important class are the Kac–Moody algebras.[33] They are named after Victor Kac and Robert Moody, who independently discovered them. These algebras form a generalization of finite-dimensional semisimple Lie algebras, and share many of their combinatorial properties. This means that they have a class of representations that can be understood in the same way as representations of semisimple Lie algebras.Lie algebras, like Lie groups, have a Levi decomposition into semisimple and solvable parts, with the representation theory of solvable Lie algebras being intractable in general. In contrast, the finite-dimensional representations of semisimple Lie algebras are completely understood, after work of Élie Cartan. A representation of a semisimple Lie algebra g is analysed by choosing a Cartan subalgebra, which is essentially a generic maximal subalgebra h of g on which the Lie bracket is zero ("abelian"). The representation of g can be decomposed into weight spaces that are eigenspaces for the action of h and the infinitesimal analogue of characters. The structure of semisimple Lie algebras then reduces the analysis of representations to easily understood combinatorics of the possible weights that can occur.[31]A Lie algebra over a field F is a vector space over F equipped with a skew-symmetric bilinear operation called the Lie bracket, which satisfies the Jacobi identity. Lie algebras arise in particular as tangent spaces to Lie groups at the identity element, leading to their interpretation as "infinitesimal symmetries".[31] An important approach to the representation theory of Lie groups is to study the corresponding representation theory of Lie algebras, but representations of Lie algebras also have an intrinsic interest.[32]A general Lie group is a semidirect product of a solvable Lie group and a semisimple Lie group (the Levi decomposition).[31] The classification of representations of solvable Lie groups is intractable in general, but often easy in practical cases. Representations of semidirect products can then be analysed by means of general results called Mackey theory, which is a generalization of the methods used in Wigner's classification of representations of the Poincaré group.The representation theory of Lie groups can be developed first by considering the compact groups, to which results of compact representation theory apply.[26] This theory can be extended to finite-dimensional representations of semisimple Lie groups using Weyl's unitary trick: each semisimple real Lie group G has a complexification, which is a complex Lie group Gc, and this complex Lie group has a maximal compact subgroup K. The finite-dimensional representations of G closely correspond to those of K.A Lie group is a group that is also a smooth manifold. Many classical groups of matrices over the real or complex numbers are Lie groups.[30] Many of the groups important in physics and chemistry are Lie groups, and their representation theory is crucial to the application of group theory in those fields.[5]Harmonic analysis has also been extended from the analysis of functions on a group G to functions on homogeneous spaces for G. The theory is particularly well developed for symmetric spaces and provides a theory of automorphic forms (discussed below).If the group is neither abelian nor compact, no general theory is known with an analogue of the Plancherel theorem or Fourier inversion, although Alexander Grothendieck extended Tannaka–Krein duality to a relationship between linear algebraic groups and tannakian categories.Another approach involves considering all unitary representations, not just the irreducible ones. These form a category, and Tannaka–Krein duality provides a way to recover a compact group from its category of unitary representations.A major goal is to provide a general form of the Fourier transform and the Plancherel theorem. This is done by constructing a measure on the unitary dual and an isomorphism between the regular representation of G on the space L2(G) of square integrable functions on G and its representation on the space of L2 functions on the unitary dual. Pontrjagin duality and the Peter–Weyl theorem achieve this for abelian and compact G respectively.[27][29]The duality between the circle group S1 and the integers Z, or more generally, between a torus Tn and Zn is well known in analysis as the theory of Fourier series, and the Fourier transform similarly expresses the fact that the space of characters on a real vector space is the dual vector space. Thus unitary representation theory and harmonic analysis are intimately related, and abstract harmonic analysis exploits this relationship, by developing the analysis of functions on locally compact topological groups and related spaces.[7]For non-compact G, the question of which representations are unitary is a subtle one. Although irreducible unitary representations must be "admissible" (as Harish-Chandra modules) and it is easy to detect which admissible representations have a nondegenerate invariant sesquilinear form, it is hard to determine when this form is positive definite. An effective description of the unitary dual, even for relatively well-behaved groups such as real reductive Lie groups (discussed below), remains an important open problem in representation theory. It has been solved for many particular groups, such as SL(2,R) and the Lorentz group.[28]A major goal is to describe the "unitary dual", the space of irreducible unitary representations of G.[26] The theory is most well-developed in the case that G is a locally compact (Hausdorff) topological group and the representations are strongly continuous.[7] For G abelian, the unitary dual is just the space of characters, while for G compact, the Peter–Weyl theorem shows that the irreducible unitary representations are finite-dimensional and the unitary dual is discrete.[27] For example, if G is the circle group S1, then the characters are given by integers, and the unitary dual is Z.A unitary representation of a group G is a linear representation φ of G on a real or (usually) complex Hilbert space V such that φ(g) is a unitary operator for every g ∈ G. Such representations have been widely applied in quantum mechanics since the 1920s, thanks in particular to the influence of Hermann Weyl,[23] and this has inspired the development of the theory, most notably through the analysis of representations of the Poincaré group by Eugene Wigner.[24] One of the pioneers in constructing a general theory of unitary representations (for any group G rather than just for particular groups useful in applications) was George Mackey, and an extensive theory was developed by Harish-Chandra and others in the 1950s and 1960s.[25]As well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory, combinatorics and number theory.Modular representations of a finite group G are representations over a field whose characteristic is not coprime to |G|, so that Maschke's theorem no longer holds (because |G| is not invertible in F and so one cannot divide by it).[21] Nevertheless, Richard Brauer extended much of character theory to modular representations, and this theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were "too small".[22]Representations of a finite group G are also linked directly to algebra representations via the group algebra F[G], which is a vector space over F with the elements of G as a basis, equipped with the multiplication operation defined by the group operation, linearity, and the requirement that the group operation and scalar multiplication commute.Over arbitrary fields, another class of finite groups that have a good representation theory are the finite groups of Lie type. Important examples are linear algebraic groups over finite fields. The representation theory of linear algebraic groups and Lie groups extends these examples to infinite-dimensional groups, the latter being intimately related to Lie algebra representations. The importance of character theory for finite groups has an analogue in the theory of weights for representations of Lie groups and Lie algebras.Results such as Maschke's theorem and the unitary property that rely on averaging can be generalized to more general groups by replacing the average with an integral, provided that a suitable notion of integral can be defined. This can be done for compact topological groups (including compact Lie groups), using Haar measure, and the resulting theory is known as abstract harmonic analysis.Unitary representations are automatically semisimple, since Maschke's result can be proven by taking the orthogonal complement of a subrepresentation. When studying representations of groups that are not finite, the unitary representations provide a good generalization of the real and complex representations of a finite group.for all g in G and v, w in W. Hence any G-representation is unitary.Maschke's theorem holds more generally for fields of positive characteristic p, such as the finite fields, as long as the prime p is coprime to the order of G. When p and |G| have a common factor, there are G-representations that are not semisimple, which are studied in a subbranch called modular representation theory.The finite-dimensional G-representations can be understood using character theory: the character of a representation φ: G → GL(V) is the class function χφ: G → F defined byπG is equivariant, and its kernel is the required complement.Over a field of characteristic zero, the representation of a finite group G has a number of convenient properties. First, the representations of G are semisimple (completely reducible). This is a consequence of Maschke's theorem, which states that any subrepresentation V of a G-representation W has a G-invariant complement. One proof is to choose any projection π from W to V and replace it by its average πG defined byGroup representations are a very important tool in the study of finite groups.[19] They also arise in the applications of finite group theory to geometry and crystallography.[20] Representations of finite groups exhibit many of the features of the general theory and point the way to other branches and topics in representation theory.Representation theory is notable for the number of branches it has, and the diversity of the approaches to studying representations of groups and algebras. Although, all the theories have in common the basic concepts discussed already, they differ considerably in detail. The differences are at least 3-fold:In general, the tensor product of irreducible representations is not irreducible; the process of decomposing a tensor product as a direct sum of irreducible representations is known as Clebsch–Gordan theory.In cases where complete reducibility does not hold, one must understand how indecomposable representations can be built from irreducible representations as extensions of a quotient by a subrepresentation.In favorable circumstances, every finite-dimensional representation is a direct sum of irreducible representations: such representations are said to be semisimple. In this case, it suffices to understand only the irreducible representations. Examples where this "complete reducibility" phenomenon occur include finite and compact groups, and semisimple Lie algebras.The direct sum of two representations carries no more information about the group G than the two representations do individually. If a representation is the direct sum of two proper nontrivial subrepresentations, it is said to be decomposable. Otherwise, it is said to be indecomposable.If (V,φ) and (W,ψ) are representations of (say) a group G, then the direct sum of V and W is a representation, in a canonical way, via the equationIrreducible representations are the building blocks of representation theory: if a representation V is not irreducible then it is built from a subrepresentation and a quotient that are both "simpler" in some sense; for instance, if V is finite-dimensional, then both the subrepresentation and the quotient have smaller dimension.The definition of an irreducible representation implies Schur's lemma: an equivariant map α: V → W between irreducible representations is either the zero map or an isomorphism, since its kernel and image are subrepresentations. In particular, when V = W, this shows that the equivariant endomorphisms of V form an associative division algebra over the underlying field F. If F is algebraically closed, the only equivariant endomorphisms of an irreducible representation are the scalar multiples of the identity.If V has exactly two subrepresentations, namely the trivial subspace {0} and V itself, then the representation is said to be irreducible; if V has a proper nontrivial subrepresentation, the representation is said to be reducible.[15]If (V,ψ) is a representation of (say) a group G, and W is a linear subspace of V that is preserved by the action of G in the sense that g · w ∈ W for all w ∈ W (Serre [14] calls these W stable under G), then W is called a subrepresentation: by defining φ(g) to be the restriction of ψ(g) to W, (W, φ) is a representation of G and the inclusion of W into V is an equivariant map. The quotient space V/W can also be made into a representation of G.Isomorphic representations are, for practical purposes, "the same"; they provide the same information about the group or algebra being represented. Representation theory therefore seeks to classify representations up to isomorphism.Equivariant maps for representations of an associative or Lie algebra are defined similarly. If α is invertible, then it is said to be an isomorphism, in which case V and W (or, more precisely, φ and ψ) are isomorphic representations, also phrased as equivalent representations. An equivariant map is often called an intertwining map of representations. Also, in the case of a group G, it is on occasion called a G-map.for all g in G, i.e. the following diagram commutes:for all g in G and v in V. In terms of φ: G → GL(V) and ψ: G → GL(W), this meansIf V and W are vector spaces over F, equipped with representations φ and ψ of a group G, then an equivariant map from V to W is a linear map α: V → W such thatAn effective or faithful representation is a representation (V,φ) for which the homomorphism φ is injective.When V is of finite dimension n, one can choose a basis for V to identify V with Fn and hence recover a matrix representation with entries in the field F.The vector space V is called the representation space of φ and its dimension (if finite) is called the dimension of the representation (sometimes degree, as in [14]). It is also common practice to refer to V itself as the representation when the homomorphism φ is clear from the context; otherwise the notation (V,φ) can be used to denote a representation.and similarly in the other cases. This approach is both more concise and more abstract. From this point of view:The second way to define a representation focuses on the map φ sending g in G to a linear map φ(g): V → V, which satisfieswhere [x1, x2] is the Lie bracket, which generalizes the matrix commutator MN − NM.where e is the identity element of G and g1g2 is the product in G. The requirement for associative algebras is analogous, except that associative algebras do not always have an identity element, in which case equation (1) is ignored. Equation (2) is an abstract expression of the associativity of matrix multiplication. This doesn't hold for the matrix commutator and also there is no identity element for the commutator. Hence for Lie algebras, the only requirement is that for any x1, x2 in A and v in V:with two properties. First, for any g in G (or a in A), the mapThere are two ways to say what a representation is.[13] The first uses the idea of an action, generalizing the way that matrices act on column vectors by matrix multiplication. A representation of a group G or (associative or Lie) algebra A on a vector space V is a mapThis generalizes to any field F and any vector space V over F, with linear maps replacing matrices and composition replacing matrix multiplication: there is a group GL(V,F) of automorphisms of V, an associative algebra EndF(V) of all endomorphisms of V, and a corresponding Lie algebra gl(V,F).There are three main sorts of algebraic objects for which this can be done: groups, associative algebras and Lie algebras.[12]Let V be a vector space over a field F.[3] For instance, suppose V is Rn or Cn, the standard n-dimensional space of column vectors over the real or complex numbers respectively. In this case, the idea of representation theory is to do abstract algebra concretely by using n × n matrices of real or complex numbers.The success of representation theory has led to numerous generalizations. One of the most general is in category theory.[11] The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces. This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.Secondly, there are diverse approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.[10]Representation theory is pervasive across fields of mathematics, for two reasons. First, the applications of representation theory are diverse:[6] in addition to its impact on algebra, representation theory:Representation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood.[3] Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups.[4] Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.[5]Representation theory is a branch of mathematics that studies abstract algebraic structures by representing their elements as linear transformations of vector spaces, and studies modules over these abstract algebraic structures.[1] In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations in terms of matrix addition and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.[2]
Algebraic geometry
Algebraic geometry now finds applications in statistics,[8] control theory,[9][10] robotics,[11] error-correcting codes,[12] phylogenetics[13] and geometric modelling.[14] There are also connections to string theory,[15] game theory,[16] graph matchings,[17] solitons[18] and integer programming.[19]Modern analytic geometry is essentially equivalent to real and complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper GAGA, the name of which is French for Algebraic geometry and analytic geometry. Nevertheless, the two fields remain distinct, as the methods of proof are quite different and algebraic geometry includes also geometry in finite characteristic.An analytic variety is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the included concept of real or complex algebraic variety. Any complex manifold is an analytic variety. Since analytic varieties may have singular points, not all analytic varieties are manifolds.See also: derived algebraic geometry.In parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gröbner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.An important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's last theorem and are also used in elliptic curve cryptography.In the 1950s and 1960s Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely led by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities and moduli.B. L. van der Waerden, Oscar Zariski and André Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.In the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connexion between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.[7]The second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.It took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of homogeneous polynomial forms, and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.During the same period, Blaise Pascal and Gérard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek ruler and compass construction. Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the calculus of infinitesimals of Lagrange and Euler.Such techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccolò Fontana "Tartaglia" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry.[6] The French mathematicians Franciscus Vieta and later René Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of algebraic curves, such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).Some of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length x so that the cube of side x contained the same volume as the rectangular box a2b for given sides a and b. Menaechmus (circa 350 BC) considered the problem geometrically by intersecting the pair of plane conics ay = x2 and xy = ab.[1] The later work, in the 3rd century BC, of Archimedes and Apollonius studied more systematically problems on conic sections,[2] and also involved the use of coordinates.[1] The Arab mathematicians were able to solve by purely algebraic means certain cubic equations, and then to interpret the results geometrically. This was done, for instance, by Ibn al-Haytham in the 10th century AD.[3] Subsequently, Persian mathematician Omar Khayyám (born 1048 A.D.) discovered a method for solving cubic equations by intersecting a parabola with a circle.[4] A few years after Omar Khayyám, Sharaf al-Din al-Tusi's "Treatise on equations" has been described as inaugurating the beginning of algebraic geometry.[5]Algebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the derived algebraic geometry, introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand Toën, Gabrielle Vezzosi, Michel Vaquié and others; and developed further by Jacob Lurie, Bertrand Toën, and Gabrielle Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from early 1990s by Maxim Kontsevich and followers.The language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.Another formal generalization is possible to universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term variety of algebras should not be confused with algebraic variety.Sometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup.Most remarkably, in late 1950s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field k, and the category of finitely generated reduced k-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the étale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne-Mumford stacks, both often called algebraic stacks.The modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.Among these algorithms which solve a sub problem of the problems solved by Gröbner bases, one may cite testing if an affine variety is empty and solving nonhomogeneous polynomial systems which have a finite number of solutions. Such algorithms are rarely implemented because, on most entries Faugère's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity (probably because the evaluation of the complexity of Gröbner basis algorithms on a particular class of entries is a difficult task which has been done only in a few special cases).As an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand, CAD is yet, in practice, the best algorithm to count the number of connected components.Since 1973, most of the research on this subject is devoted either to improve CAD or to find alternate algorithms in special cases of general interest.While Gröbner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.The complexity of CAD is doubly exponential in the number of variables. This means that CAD allows, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.This theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators and (∧), or (∨), not (¬), for all (∀) and exists (∃). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (∀, ∃).CAD is an algorithm which was introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski–Seidenberg theorem on quantifier elimination over the real numbers.Gröbner bases are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faugère F5 algorithm realizes this complexity, as it may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow one to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gröbner basis is strongly related to the intrinsic difficulty of the problem.Gröbner basis computations do not allow one to compute directly the primary decomposition of I nor the prime ideals defining the irreducible components of V, but most algorithms for this involve Gröbner basis computation. The algorithms which are not based on Gröbner bases use regular chains but may need Gröbner bases in some exceptional situations.Given an ideal I defining an algebraic set V:A Gröbner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.A body of mathematical theory complementary to symbolic methods called numerical algebraic geometry has been developed over the last several decades. The main computational method is homotopy continuation. This supports, for example, a model of floating point computation for solving problems of algebraic geometry.Since then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.One may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France in June 1979. At this meeting,One of the challenging problems of real algebraic geometry is the unsolved Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8.Real algebraic geometry is the study of the real points of algebraic geometry.The only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand, the field of the rational functions or function field is a useful notion, which, similarly to the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.A polynomial in n + 1 variables vanishes at all points of a line passing through the origin if and only if it is homogeneous. In this case, one says that the polynomial vanishes at the corresponding point of Pn. This allows us to define a projective algebraic set in Pn as the set V(f1, ..., fk), where a finite set of homogeneous polynomials {f1, ..., fk} vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The projective varieties are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the projective coordinates ring being defined as the quotient of the graded ring or the polynomials in n + 1 variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.Nowadays, the projective space Pn of dimension n is usually defined as the set of the lines passing through a point, considered as the origin, in the affine space of dimension n + 1, or equivalently to the set of the vector lines in a vector space of dimension n + 1. When a coordinate system has been chosen in the space of dimension n + 1, all the points of a line have the same set of coordinates, up to the multiplication by an element of k. This defines the homogeneous coordinates of a point of Pn as a sequence of n + 1 elements of the base field k, defined up to the multiplication by a nonzero element of k (the same for the whole sequence).Thus many of the properties of algebraic varieties, including birational equivalence and all the topological properties, depend on the behavior "at infinity" and so it is natural to study the varieties in projective space. Furthermore, the introduction of projective techniques made many theorems in algebraic geometry simpler and sharper: For example, Bézout's theorem on the number of intersection points between two varieties can be stated in its sharpest form only in projective space. For these reasons, projective space plays a fundamental role in algebraic geometry.The consideration of the projective completion of the two curves, which is their prolongation "at infinity" in the projective plane, allows us to quantify this difference: the point at infinity of the parabola is a regular point, whose tangent is the line at infinity, while the point at infinity of the cubic curve is a cusp. Also, both curves are rational, as they are parameterized by x, and the Riemann-Roch theorem implies that the cubic curve must have a singularity, which must be at infinity, as all its points in the affine space are regular.Compare this to the variety V(y − x3). This is a cubic curve. As x goes to positive infinity, the slope of the line from the origin to the point (x, x3) goes to positive infinity just as before. But unlike before, as x goes to negative infinity, the slope of the same line goes to positive infinity as well; the exact opposite of the parabola. So the behavior "at infinity" of V(y − x3) is different from the behavior "at infinity" of V(y − x2).To see how this might come about, consider the variety V(y − x2). If we draw it, we get a parabola. As x goes to positive infinity, the slope of the line from the origin to the point (x, x2) also goes to positive infinity. As x goes to negative infinity, the slope of the same line goes to negative infinity.Just as the formulas for the roots of second, third, and fourth degree polynomials suggest extending real numbers to the more algebraically complete setting of the complex numbers, many properties of algebraic varieties suggest extending affine space to a more geometrically complete projective space. Whereas the complex numbers are obtained by adding the number i, a root of the polynomial x2 + 1, projective space is obtained by adding in appropriate points "at infinity", points where parallel lines may meet.The problem of resolution of singularities is to know if every algebraic variety is birationally equivalent to a variety whose projective completion is nonsingular (see also smooth completion). It was solved in the affirmative in characteristic 0 by Heisuke Hironaka in 1964 and is yet unsolved in finite characteristic.which may also be viewed as a rational map from the line to the circle.Two affine varieties are birationally equivalent if there are two rational functions between them which are inverse one to the other in the regions where both are defined. Equivalently, they are birationally equivalent if their function fields are isomorphic.As with regular maps, one may define a rational map from a variety V to a variety V'. As with the regular maps, the rational maps from V to V' may be identified to the field homomorphisms from k(V') to k(V).If V is an affine variety, its coordinate ring is an integral domain and has thus a field of fractions which is denoted k(V) and called the field of the rational functions on V or, shortly, the function field of V. Its elements are the restrictions to V of the rational functions over the affine space containing V. The domain of a rational function f is not V but the complement of the subvariety (a hypersurface) where the denominator of f vanishes.In contrast to the preceding sections, this section concerns only varieties and not algebraic sets. On the other hand, the definitions extend naturally to projective varieties (next section), as an affine variety and its projective completion have the same field of functions.Given a regular map g from V to V′ and a regular function f of k[V′], then f ∘ g ∈ k[V]. The map f → f ∘ g is a ring homomorphism from k[V′] to k[V]. Conversely, every ring homomorphism from k[V′] to k[V] defines a regular map from V to V′. This defines an equivalence of categories between the category of algebraic sets and the opposite category of the finitely generated reduced k-algebras. This equivalence is one of the starting points of scheme theory.The definition of the regular maps apply also to algebraic sets. The regular maps are also called morphisms, as they make the collection of all affine algebraic sets into a category, where the objects are the affine algebraic sets and the morphisms are the regular maps. The affine varieties is a subcategory of the category of the algebraic sets.If V′ is a variety contained in Am, we say that f is a regular map from V to V′ if the range of f is contained in V′.Using regular functions from an affine variety to A1, we can define regular maps from one affine variety to another. First we will define a regular map from a variety into affine space: Let V be a variety contained in An. Choose m regular functions on V, and call them f1, ..., fm. We define a regular map f from V to Am by letting f = (f1, ..., fm). In other words, each fi determines one coordinate of the range of f.Since regular functions on V come from regular functions on An, there is a relationship between the coordinate rings. Specifically, if a regular function on V is the restriction of two functions f and g in k[An], then f − g is a polynomial function which is null on V and thus belongs to I(V). Thus k[V] may be identified with k[An]/I(V).Just as with the regular functions on affine space, the regular functions on V form a ring, which we denote by k[V]. This ring is called the coordinate ring of V.It may seem unnaturally restrictive to require that a regular function always extend to the ambient space, but it is very similar to the situation in a normal topological space, where the Tietze extension theorem guarantees that a continuous function on a closed subset always extends to the ambient topological space.Just as continuous functions are the natural maps on topological spaces and smooth functions are the natural maps on differentiable manifolds, there is a natural class of functions on an algebraic set, called regular functions or polynomial functions. A regular function on an algebraic set V contained in An is the restriction to V of a regular function on An. For an algebraic set defined on the field of the complex numbers, the regular functions are smooth and even analytic.Some authors do not make a clear distinction between algebraic sets and varieties and use irreducible variety to make the distinction when needed.An algebraic set is called irreducible if it cannot be written as the union of two smaller algebraic sets. Any algebraic set is a finite union of irreducible algebraic sets and this decomposition is unique. Thus its elements are called the irreducible components of the algebraic set. An irreducible algebraic set is also called a variety. It turns out that an algebraic set is a variety if and only if it may be defined as the vanishing set of a prime ideal of the polynomial ring.For various reasons we may not always want to work with the entire ideal corresponding to an algebraic set U. Hilbert's basis theorem implies that ideals in k[An] are always finitely generated.The answer to the first question is provided by introducing the Zariski topology, a topology on An whose closed sets are the algebraic sets, and which directly reflects the algebraic structure of k[An]. Then U = V(I(U)) if and only if U is an algebraic set or equivalently a Zariski-closed set. The answer to the second question is given by Hilbert's Nullstellensatz. In one of its forms, it says that I(V(S)) is the radical of the ideal generated by S. In more abstract language, there is a Galois connection, giving rise to two closure operators; they can be identified, and naturally play a basic role in the theory; the example is elaborated at Galois connection.Two natural questions to ask are:Given a subset U of An, can one recover the set of polynomials which generate it? If U is any subset of An, define I(U) to be the set of all polynomials whose vanishing set contains U. The I stands for ideal: if two polynomials f and g both vanish on U, then f+g vanishes on U, and if h is any polynomial, then hf vanishes on U, so I(U) is always an ideal of the polynomial ring k[An].A subset of An which is V(S), for some S, is called an algebraic set. The V stands for variety (a specific type of algebraic set to be defined below).We say that a polynomial vanishes at a point if evaluating it at that point gives zero. Let S be a set of polynomials in k[An]. The vanishing set of S (or vanishing locus or zero set) is the set V(S) of all points in An where every polynomial in S vanishes. Symbolically,When a coordinate system is chosen, the regular functions on the affine n-space may be identified with the ring of polynomial functions in n variables over k. Therefore, the set of the regular functions on An is a ring, which is denoted k[An].A function f : An → A1 is said to be polynomial (or regular) if it can be written as a polynomial, that is, if there is a polynomial p in k[x1,...,xn] such that f(M) = p(t1,...,tn) for every point M with coordinates (t1,...,tn) in An. The property of a function to be polynomial (or regular) does not depend on the choice of a coordinate system in An.First we start with a field k. In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that k is algebraically closed. We consider the affine space of dimension n over k, denoted An(k) (or more simply An, when k is clear from the context). When one fixes a coordinate system, one may identify An(k) with kn. The purpose of not working with kn is to emphasize that one "forgets" the vector space structure that kn carries.A "slanted" circle in R3 can be defined as the set of all points (x,y,z) which satisfy the two polynomial equationsIn classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere of radius 1 in three-dimensional Euclidean space R3 could be defined as the set of all points (x,y,z) withMuch of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on "intrinsic" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles's proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.In the 20th century, algebraic geometry split into several subareas.Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving geometrical problems about these sets of zeros.
System of polynomial equations
While the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm[17] is based on the concept of a regular semi-algebraic system.The command RegularChains[RealTriangularize] is part of the Maple library RegularChains, written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command RegularChains[Triangularize] for computing the solutions whose coordinates are in the algebraic closure of the coefficient field. The underlying algorithms are based on the notion of a regular chain.The fourth solver is the Maple command RegularChains[RealTriangularize]. For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.The third solver is Bertini,[15][16] written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision. In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.The second solver is PHCpack,[11][14] written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.To extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.The rational univariate representation may be computed with Maple function Groebner[RationalUnivariateRepresentation].Internally, this solver, designed by F. Rouillier computes first a Gröbner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.The Maple function RootFinding[Isolate] takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.There are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.The roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.To deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.Then a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the systemThis method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore, it is computed by, at least, four different methods and the best value, say N, is kept.This is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades.[11]Nevertheless, two methods deserve to be mentioned here.The general numerical algorithms which are designed for any system of nonlinear equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow one to find all solutions. In particular, when a general method does not find any solution, this is usually not an indication that there is no solution.Contrarily to triangular decompositions and equiprojectable decompositions, the RUR is not defined in positive dimension.Moreover, the univariate polynomial h(x0) of the RUR may be factorized, and this gives a RUR for every irreducible factor. This provides the prime decomposition of the given ideal (that is the primary decomposition of the radical of the ideal). In practice, this provides an output with much smaller coefficients, especially in the case of systems with high multiplicities.For zero-dimensional systems, the RUR allows retrieval of the numeric values of the solutions by solving a single univariate polynomial and substituting them in rational functions. This allows production of certified approximations of the solutions to any given precision.The RUR is uniquely defined for a given separating variable, independently of any algorithm, and it preserves the multiplicities of the roots. This is a notable difference with triangular decompositions (even the equiprojectable decomposition), which, in general, do not preserve multiplicities. The RUR shares with equiprojectable decomposition the property of producing an output with coefficients of relatively small size.For example, for the system in the previous section, every linear combination of the variable, except the multiples of x, y and x + y, is a separating variable. If one chooses t = x – y/2 as a separating variable, then the RUR isGiven a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.where h is a univariate polynomial in x0 of degree D and g0, ..., gn are univariate polynomials in x0 of degree less than D.A RUR of a zero-dimensional system consists in a linear combination x0 of the variables, called separating variable, and a system of equations[9]The rational univariate representation or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by F. Rouillier.[8]The second issue is generally solved by outputting regular chains of a special form, sometimes called shape lemma, for which all di but the first one are equal to 1. For getting such regular chains, one may have to add a further variable, called separating variable, which is given the index 0. The rational univariate representation, described below, allows computing such a special regular chain, satisfying Dahan–Schost bound, by starting from either a regular chain or a Gröbner basis.The first issue has been solved by Dahan and Schost:[5][6] Among the sets of regular chains that represent a given set of solutions, there is a set for which the coefficients are explicitly bounded in terms of the size of the input system, with a nearly optimal bound. This set, called equiprojectable decomposition, depends only on the choice of the coordinates. This allows the use of modular methods for computing efficiently the equiprojectable decomposition.[7]This representation of the solutions are fully convenient for coefficients in a finite field. However, for rational coefficients, two aspects have to be taken care of:There is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gröbner basis for the graded reverse lexicographic order (grevlex), then deducing the lexicographical Gröbner basis by FGLM algorithm[3] and finally applying the Lextriangular algorithm.[4]There are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional)[2] into regular chains (or regular semi-algebraic systems).Every zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.The solutions of this system are obtained by solving the first univariate equation, substituting the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from fi has degree di and thus that the system has d1 ... dn solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).To such a regular chain is associated a triangular system of equationsThe usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists of a sequence of polynomials f1(x1), f2(x1, x2), ..., fn(x1, ..., xn) such that, for every i such that 1 ≤ i ≤ nThe other way to represent the solutions is said to be algebraic. It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field k of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation involving the solving of only one univariate polynomial for each solution is preferable: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.For zero-dimensional systems, solving consists of computing all the solutions. There are two different ways of outputting the solutions. The most common, possible for real or complex solutions, consists of outputting numeric approximations of the solutions. Such a solution is called numeric. A solution is certified if it is provided with a bound on the error of the approximations which separates the different solutions.A natural example of an open question about solving positive-dimensional systems is the following: decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them. The only published algorithm which allows one to solve this question is cylindrical algebraic decomposition, which is not efficient enough, in practice, to be used for this.If the system is positive-dimensional, it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean "finding a description of the solutions from which the relevant properties of the solutions are easy to extract". There is no commonly accepted such description. In fact there are many different "relevant properties", which involve almost every subfield of algebraic geometry.The first thing to do in solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gröbner basis of the left-hand sides of the equations. The system is inconsistent if this Gröbner basis is reduced to 1. The system is zero-dimensional if, for every variable there is a leading monomial of some element of the Gröbner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).This exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with Bézout's bound higher than, say, 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).A zero-dimensional system with as many equations as variables is said to be well-behaved.[1] Bézout's theorem asserts that a well-behaved system whose equations have degrees d1, ..., dn has at most d1...dn solutions. This bound is sharp. If all the degrees are equal to d, this bound becomes dn and is exponential in the number of variables.A system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension K of k. This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be positive-dimensional.A system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension K of k.A system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems, when constructed with random coefficients, are inconsistent. For example, the system  x3 − 1 = 0, x2 − 1 = 0 is overdetermined (having two equations but only one unknown), but it is not inconsistent since it has the solution x =1.In the case of a finite field, the same transformation allows always to suppose that the field k has a prime order.The elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.When solving a system over a finite field k with q elements, one is primarily interested in the solutions in k. As the elements of k are exactly the solutions of the equation xq − x = 0, it suffices, for restricting the solutions to k, to add the equation xiq − xi = 0 for each variable xi.is equivalent to the polynomial systemFor example, the equationA trigonometric equation is an equation g = 0 where g is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing sin(x) and cos(x) by two new variables s and c and adding the new equation s2 + c2 − 1 = 0.A solution is a set of the values for the xi which make all of the equations true and which belong to some algebraically closed field extension K of k. When k is the field of rational numbers, K is the field of complex numbers.Usually, the field k is either the field of rational numbers or a finite field, although most of the theory applies to any field.A system of polynomial equations is a set of simultaneous equations f1 = 0, ..., fh = 0 where the fi are polynomials in several variables, say x1, ..., xn, over some field k.
Linear equation
If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n – 1)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).In other words, if ai ≠ 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 ≠ 0, and rewrite the equationIf all the coefficients are zero, then either b ≠ 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewrittenAn everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point–slope form or slope–intercept form.where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.andA linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.Ergo,Thus,One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.andThese are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:andSince this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.becomes:Further, this representation extends to systems of linear equations.one can rewrite the equation in matrix form:Using the order of the standard formwhere a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.Using a determinant, one gets a determinant form, easy to remember:Expanding the products and regrouping the terms leads to the general form:Multiplying both sides of this equation by (x2 − x1) yields a form of the line generally referred to as the symmetric form:where (x1, y1) and (x2, y2) are two points on the line with x2 ≠ x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2 − y1)/(x2 − x1).The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y − y1) is proportional to the difference in the x coordinate (that is, x − x1). The proportionality constant is m (the slope of the line).where m is the slope of the line and (x1,y1) is any point on the line.where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.where A and B are not both equal to zero. The equation is usually written so that A ≥ 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is −A/B. The general form is sometimes written as:In the general (or standard[1]) form the linear equation is written as:Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and θ are variables; other letters represent constants (fixed numbers).Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.A common form of a linear equation in the two variables x and y isIf a = 0, then, if b = 0, every number is a solution of the equation, and, if b ≠ 0, there are no solutions (and the equation is said to be inconsistent).If a ≠ 0, there is a unique solutionA linear equation in one unknown x may always be rewrittenThis article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a ≠ 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a ≠ 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).
Linear equation over a ring

System of linear equations
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:where A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.A homogeneous system is equivalent to a matrix equation of the formA system of linear equations is homogeneous if all of the constant terms are zero:There is also a quantum algorithm for linear systems of equations.[3]A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.is given byCramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemThe last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsFor example, consider the following system:The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:Here x is the free variable, and y and z are dependent.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.The solution set to this system can be described by the following equations:For example, consider the following system:To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.There are several algorithms for solving a system of linear equations.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.For example, the equationsA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.For a more complicated example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For example, the equationsThe equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.The following pictures illustrate this trichotomy in the case of two variables:In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.A linear system may behave in any one of three possible ways:A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.The number of vectors in a basis for the span is now expressed as the rank of the matrix.where A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The vector equation is equivalent to a matrix equation of the formThis allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.A general system of m linear equations with n unknowns can be written asNow substitute this expression for x into the bottom equation:The simplest kind of linear system involves two equations and two variables:Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given byIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,
Gaussian elimination
Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]The Gaussian elimination can be performed over any field, not just the real numbers.One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 − 5n)/6 multiplications, and (2n3 + 3n2 − 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).All of this applies also to the reduced row echelon form, which is a particular row echelon form.One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:For example, consider the following matrixA variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.Computationally, for a n×n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = ∏diag(B) / d.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Suppose the goal is to find and describe the set of solutions to the following system of linear equations:A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.There are three types of elementary row operations which may be performed on the rows of a matrix:Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
Eigenvalues and eigenvectors
The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.leads to a so-called quadratic eigenvalue problem,orEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byPrincipal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.This matrix equation is equivalent to two linear equationsOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]A similar procedure is used for solving a differential equation of the formThe solution of this equation for x in terms of t is found by using its characteristic equationThe simplest difference equations have the formThe representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.So, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u+v,αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenBy definition of a linear transformation,which is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.Given an eigenvalue λ, consider the setThis equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such thatThe concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,The main eigenfunction article gives other examples.is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationOn the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 −1 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity γA is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.has a characteristic polynomial that is the product of its diagonal elements,As in the previous example, the lower triangular matrixrespectively, as well as scalar multiples of these vectors.These eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isConsider the lower triangular matrix,A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.respectively, as well as scalar multiples of these vectors.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isMatrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixandThenFor the complex conjugate pair of imaginary eigenvalues, note thatFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areConsider the cyclic permutation matrixThe characteristic polynomial of A isConsider the matrixThus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Any non-zero vector with v1 = v2 solves this equation. Therefore,For λ = 3, Equation (2) becomesis an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.Any non-zero vector with v1 = −v2 solves this equation. Therefore,For λ = 1, Equation (2) becomes,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.Taking the determinant to find characteristic polynomial of A,The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Consider the matrixA matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.or by instead left multiplying both sides by Q−1,Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenSince each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues λ1, λ2, ..., λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.where κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation isMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),Let A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenSuppose A has d ≤ n distinct eigenvalues λ1, λ2, ..., λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,The condition that γA(λ) ≤ μA(λ) can be proven by considering a particular eigenvalue ξ of A and diagonalizing the first γA(ξ) columns of A with respect to the eigenvectors of ξ, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix ξIγA(ξ). As a result, the characteristic polynomial of B will have a factor of (ξ − λ)γA(ξ). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of ξ as an eigenvalue of B is no less than the geometric multiplicity of ξ as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number, (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),If μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asSuppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofTaking the determinant of (M − λI), the characteristic polynomial of M isAs a brief example, which is described in more detail in the examples section later, consider the matrixwhere each λi may be real but in general is a complex number. The numbers λ1, λ2, ... λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equationwhere I is the n by n identity matrix.Equation (1) can be stated equivalently asthen v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.If it occurs that v and w are scalar multiples, that is ifwhere, for each row,orNow consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,In this case λ = −1/20.These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatConsider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationThe Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.referred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationwhere λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation
Fundamental matrix (computer vision)
The fundamental matrix is of rank 2. Its kernel defines the epipole.Fundamental matrix can be derived using the coplanarity condition. [2]The cameras then transform asThe fundamental matrix can be determined by a set of point correspondences. Additionally, these corresponding image points may be triangulated to world points with the help of camera matrices derived directly from this fundamental matrix. The scene composed of these world points is within a projective transformation of the true scene.[1]The fundamental matrix is a relationship between any two images of the same scene that constrains where the projection of points from the scene can occur in both images. Given the projection of a scene point into one of the images the corresponding point in the other image is constrained to a line, helping the search, and allowing for the detection of wrong correspondences. The relation between corresponding image points which the fundamental matrix represents is referred to as epipolar constraint, matching constraint, discrete matching constraint, or incidence relation.The term "fundamental matrix" was coined by QT Luong in his influential PhD thesis. It is sometimes also referred to as the "bifocal tensor". As a tensor it is a two-point tensor in that it is a bilinear form relating points in distinct coordinate systems.Being of rank two and determined only up to scale, the fundamental matrix can be estimated given at least seven point correspondences. Its seven parameters represent the only geometric information about cameras that can be obtained through point correspondences alone.
Computer vision
As of 2016, vision processing units are emerging as a new class of processor, to complement CPUs and graphics processing units (GPUs) in this role.[28]Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.[27]A few computer vision systems use image acquisition hardware with active illumination or something other than visible light or both. For example, a structured-light 3D scanner, a thermographic camera, a hyperspectral imager, radar imaging, a lidar scanner, a magnetic resonance image, a side-scan sonar, a synthetic aperture sonar, or etc. Such hardware captures "images" that are then processed often using the same computer vision algorithms used to process visible-light images.Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).There are many kinds of computer vision systems, nevertheless all of them contain these basic elements: a power source, at least one image acquisition device (i.e. camera, ccd, etc.), a processor as well as control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories like camera supports, cables and connectors.While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[26]The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.Image-understanding systems (IUS) include three levels of abstraction as follows: Low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are really topics for further research.The organization of a computer vision system is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems.An example in this field is inpainting.The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look like, a model which distinguishes them from the noise. By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models [17].Several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images . Examples of such tasks are:Several specialized tasks based on recognition exist, such as:Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans.[23] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature:[citation needed]Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.Other application areas include:One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer vision based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, i.e. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It can also be used for detecting certain task specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles, to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Mars Exploration Rover and ESA's ExoMars Rover.Military applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as "battlefield awareness", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.One of the most prominent application fields is medical computer vision or medical image processing. This area is characterized by the extraction of information from image data for the purpose of making a medical diagnosis of a patient. Generally, image data is in the form of microscopy images, X-ray images, angiography images, ultrasonic images, and tomography images. An example of information which can be extracted from such image data is detection of tumours, arteriosclerosis or other malign changes. It can also be measurements of organ dimensions, blood flow, etc. This application area also supports medical research by providing new information, e.g., about the structure of the brain, or about the quality of medical treatments. Applications of computer vision in the medical area also includes enhancement of images that are interpreted by humans, for example ultrasonic images or X-ray images, to reduce the influence of noise.Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.The following characterizations appear relevant but should not be taken as universally accepted:Computer graphics produces image data from 3D models, computer vision often produces 3D models from image data [17]. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.Beside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance.Yet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.Some strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.A third field which plays an important role is neurobiology, specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how "real" vision systems operate in order to solve certain vision related tasks. These results have led to a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in biology.Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infra-red light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.Artificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.Areas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.[15][16]The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[14] By the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11]What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11]In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it "describe what it saw".[12][13]Computer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[1][2][3] "Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding."[9] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[10] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, and image restoration.As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]Computer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[1][2][3]
Linear regression
Linear regression finds application in a wide range of environmental science applications. In Canada, the Environmental Effects Monitoring Program uses statistical analyses on fish and benthic surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem.[31]Linear regression is the predominant empirical tool in economics. For example, it is used to predict consumption spending,[27] fixed investment spending, inventory investment, purchases of a country's exports,[28] spending on imports,[28] the demand to hold liquid assets,[29] labor demand,[30] and labor supply.[30]The capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.Early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, suppose we have a regression model in which cigarette smoking is the independent variable of interest, and the dependent variable is lifespan measured in years. Researchers might include socio-economic status as an additional independent variable, to ensure that any observed effect of smoking on lifespan is not due to some effect of education or income. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.Trend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.A trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.Matrix calculations, like any others, are affected by rounding errors. An early summary of these effects, regarding the choice of computational methods for matrix inversion, was provided by Wilkinson.[26]Fitting of linear models by least squares often, but not always, arises in the context of statistical analysis. It can therefore be important that considerations of computational efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can beIn statistics and numerical analysis, the problem of numerical methods for linear least squares is an important one because linear regression models are one of the most important types of model, both as formal statistical models and for exploration of data sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to numerical precision.Some of the more common estimation techniques for linear regression are summarized below.A large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency.Errors-in-variables models (or "measurement error models") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.Hierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.Single index models[clarification needed] allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor β′x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant.[11]Some common examples of GLMs are:Generalized linear models (GLMs) are a framework for modeling a response variable y that is bounded or discrete. This is used, for example:Various models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances. For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.The general linear model considers the situation when the response variable Y is not a scalar but a vector. Conditional linearity of E(y|x) = Bx is still assumed, with a matrix B replacing the vector β of the classical linear regression model. Multivariate analogues of Ordinary Least-Squares (OLS) and Generalized Least-Squares (GLS) have been developed. "General linear models" are also called "multivariate linear models". These are not the same as multivariable linear models (also called "multiple linear models").The very simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression. The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression. Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model. Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression.Numerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.The notion of a "unique effect" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.[9] A commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.[10]The meaning of the expression "held fixed" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been "held fixed" by the experimenter. Alternatively, the expression "held fixed" can refer to a selection that takes place in the context of data analysis. In this case, we "hold a variable fixed" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of "held fixed" that can be used in an observational study.It is possible that the unique effect can be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj.Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to "hold ti fixed" and at the same time change the value of ti2).A fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are "held fixed". Specifically, the interpretation of βj is the expected change in y for a one-unit change in xj when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj.Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.where β1 determines the initial velocity of the ball, β2 is proportional to the standard gravity, and εi is due to measurement errors. Linear regression can be used to estimate the values of β1 and β2 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters β1 and β2; if we take regressors xi = (xi1, xi2)  = (ti, ti2), the model takes on the standard formExample. Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled asSome remarks on terminology and general use:whereOften these n equations are stacked together and written in vector form aswhere T denotes the transpose, so that xiTβ is the inner product between vectors xi and β.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.Linear regression has many practical uses. Most applications fall into one of the following two broad categories:Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[4] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.[3] Most commonly, the conditional mean of y given the value of X is assumed to be an affine function of X; less commonly, the median or some other quantile of the conditional distribution of y given X is expressed as a linear function of X. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of y given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis.In statistics, linear regression is a linear approach for modelling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.[1] (This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.)[2]
List of linear algebra topics

Numerical linear algebra
Common problems in numerical linear algebra include computing the following: LU decomposition, QR decomposition, singular value decomposition, eigenvalues.Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, fluid dynamics, and many other areas. Such software relies heavily on the development, analysis, and implementation of state-of-the-art algorithms for solving various numerical linear algebra problems, in large part because of the role of matrices in finite difference and finite element methods.
Simplex algorithm
Linear–fractional programming (LFP) is a generalization of linear programming (LP). In LP the objective function is a linear function, while the objective function of a linear–fractional program is a ratio of two linear functions. In other words, a linear program is a fractional–linear program in which the denominator is the constant function having the value one everywhere. A linear–fractional program can be solved by a variant of the simplex algorithm[40][41][42][43] or by the criss-cross algorithm.[44]Other algorithms for solving linear-programming problems are described in the linear-programming article. Another basis-exchange pivoting algorithm is the criss-cross algorithm.[38][39] There are polynomial-time algorithms for linear programming that use interior point methods: these include Khachiyan's ellipsoidal algorithm, Karmarkar's projective algorithm, and path-following algorithms.[13]Analyzing and quantifying the observation that the simplex algorithm is efficient in practice, even though it has exponential worst-case complexity, has led to the development of other measures of complexity. The simplex algorithm has polynomial-time average-case complexity under various probability distributions, with the precise average-case performance of the simplex algorithm depending on the choice of a probability distribution for the random matrices.[35][36] Another approach to studying "typical phenoma" uses Baire category theory from general topology, and to show that (topologically) "most" matrices can be solved by the simplex algorithm in a polynomial number of steps. Another method to analyze the performance of the simplex algorithm studies the behavior of worst-case scenarios under small perturbation – are worst-case scenarios stable under a small change (in the sense of structural stability), or do they become tractable? Formally, this method uses random problems to which is added a Gaussian random vector ("smoothed complexity").[37]The simplex method is remarkably efficient in practice and was a great improvement over earlier methods such as Fourier–Motzkin elimination. However, in 1972, Klee and Minty[33] gave an example, the Klee-Minty cube, showing that the worst-case complexity of simplex method as formulated by Dantzig is exponential time. Since then, for almost every variation on the method, it has been shown that there is a family of linear programs for which it performs badly. It is an open question if there is a variation with polynomial time, or even sub-exponential worst-case complexity.[34][35]History-based pivot rules such as Zadeh's Rule and Cunningham's Rule also try to circumvent the issue of stalling and cycling by keeping track how often particular variables are being used, and then favor such variables that have been used least often.If the values of all basic variables are strictly positive, then a pivot must result in an improvement in the objective value. When this is always the case no set of basic variables occurs twice and the simplex algorithm must terminate after a finite number of steps. Basic feasible solutions where at least one of the basic variables is zero are called degenerate and may result in pivots for which there is no improvement in the objective value. In this case there is no actual change in the solution but only a change in the set of basic variables. When several such pivots occur in succession, there is no improvement; in large industrial applications, degeneracy is common and such "stalling" is notable. Worse than stalling is the possibility the same set of basic variables occurs twice, in which case, the deterministic pivoting rules of the simplex algorithm will produce an infinite loop, or "cycle". While degeneracy is the rule in practice and stalling is common, cycling is rare in practice. A discussion of an example of practical cycling occurs in Padberg.[25] Bland's rule prevents cycling and thus guarantees that the simplex algorithm always terminates.[25][30][31] Another pivoting algorithm, the criss-cross algorithm never cycles on linear programs.[32]In large linear-programming problems A is typically a sparse matrix and, when the resulting sparsity of B is exploited when maintaining its invertible representation, the revised simplex algorithm is much more efficient than the standard simplex method. Commercial simplex solvers are based on the revised simplex algorithm.[25][26][27][28][29]In each simplex iteration, the only data required are the first row of the tableau, the (pivotal) column of the tableau corresponding to the entering variable and the right-hand-side. The latter can be updated using the pivotal column and the first row of the tableau can be updated using the (pivotal) row corresponding to the leaving variable. Both the pivotal column and pivotal row may be computed directly using the solutions of linear systems of equations involving the matrix B and a matrix-vector product using A. These observations motivate the "revised simplex algorithm", for which implementations are distinguished by their invertible representation of B.[26]The tableau form used above to describe the algorithm lends itself to an immediate implementation in which the tableau is maintained as a rectangular (m + 1)-by-(m + n + 1) array. It is straightforward to avoid storing the m explicit columns of the identity matrix that will occur within the tableau by virtue of B being a subset of the columns of [A, I]. This implementation is referred to as the "standard simplex algorithm". The storage and computation overhead are such that the standard simplex method is a prohibitively expensive approach to solving large linear programming problems.This is, fortuitously, already optimal and the optimum value for the original linear program is −130/7.The artificial variables are now 0 and they may be dropped giving a canonical tableau equivalent to the original problem:Now select column 3 as a pivot column, for which row 3 must be the pivot row, to getSelect column 5 as a pivot column, so the pivot row must be row 4, and the updated tableau isAfter pricing out this becomesNote that the equation defining the original objective function is retained in anticipation of Phase II.Introduce artificial variables u and v and objective function W = u + v, giving a new tableauThis is represented by the (non-canonical) tableauConsider the linear programThe simplex algorithm applied to the Phase I problem must terminate with a minimum value for the new objective function since, being the sum of nonnegative variables, its value is bounded below by 0. If the minimum is 0 then the artificial variables can be eliminated from the resulting canonical tableau producing a canonical tableau equivalent to the original problem. The simplex algorithm can then be applied to find the solution; this step is called Phase II. If the minimum is positive then there is no feasible solution for the Phase I problem where the artificial variables are all zero. This implies that the feasible region for the original problem is empty, and so the original problem has no solution.[11][12][25]In general, a linear program will not be given in canonical form and an equivalent canonical tableau must be found before the simplex algorithm can start. This can be accomplished by the introduction of artificial variables. Columns of the identity matrix are added as column vectors for these variables. If the b value for a constraint equation is negative, the equation is negated before adding the identity matrix columns. This does not change the set of feasible solutions or the optimal solution, and it ensures that the slack variables will constitute an initial feasible solution. The new tableau is in canonical form but it is not equivalent to the original problem. So a new objective function, equal to the sum of the artificial variables, is introduced and the simplex algorithm is applied to find the minimum; the modified linear program is called the Phase I problem.[24]so the minimum value of Z is −20.For the next step, there are no positive entries in the objective row and in factNow columns 4 and 5 represent the basic variables z and s and the corresponding basic feasible solution isColumns 2, 3, and 4 can be selected as pivot columns, for this example column 4 is selected. The values of z resulting from the choice of rows 2 and 3 as pivot rows are 10/1 = 10 and 15/3 = 5 respectively. Of these the minimum is 5, so row 3 must be the pivot row. Performing the pivot produceswhere columns 5 and 6 represent the basic variables s and t and the corresponding basic feasible solution isWith the addition of slack variables s and t, this is represented by the canonical tableauConsider the linear programis the minimum over all r so that arc > 0. This is called the minimum ratio test.[21] If there is more than one row for which the minimum is achieved then a dropping variable choice rule[23] can be used to make the determination.Next, the pivot row must be selected so that all the other basic variables remain positive. A calculation shows that this occurs when the resulting value of the entering variable is at a minimum. In other words, if the pivot column is c, then the pivot row r is chosen so thatOnce the pivot column has been selected, the choice of pivot row is largely determined by the requirement that the resulting solution be feasible. First, only positive entries in the pivot column are considered since this guarantees that the value of the entering variable will be nonnegative. If there are no positive entries in the pivot column then the entering variable can take any nonnegative value with the solution remaining feasible. In this case the objective function is unbounded below and there is no minimum.Note that by changing the entering variable choice rule so that it selects a column where the entry in the objective row is negative, the algorithm is changed so that it finds the maximum of the objective function rather than the minimum.If all the entries in the objective row are less than or equal to 0 then no choice of entering variable can be made and the solution is in fact optimal. It is easily seen to be optimal since the objective row now corresponds to an equation of the formIf there is more than one column so that the entry in the objective row is positive then the choice of which one to add to the set of basic variables is somewhat arbitrary and several entering variable choice rules[21] such as Devex algorithm[22] have been developed.Since the entering variable will, in general, increase from 0 to a positive number, the value of the objective function will decrease if the derivative of the objective function with respect to this variable is negative. Equivalently, the value of the objective function is decreased if the pivot column is selected so that the corresponding entry in the objective row of the tableau is positive.Let a linear program be given by a canonical tableau. The simplex algorithm proceeds by performing successive pivot operations each of which give an improved basic feasible solution; the choice of pivot element at each step is largely determined by the requirement that this pivot improves the solution.The geometrical operation of moving from a basic feasible solution to an adjacent basic feasible solution is implemented as a pivot operation. First, a nonzero pivot element is selected in a nonbasic column. The row containing this element is multiplied by its reciprocal to change this element to 1, and then multiples of the row are added to the other rows to change the other entries in the column to 0. The result is that, if the pivot element is in row r, then the column becomes the r-th column of the identity matrix. The variable for this column is now a basic variable, replacing the variable which corresponded to the r-th column of the identity matrix before the operation. In effect, the variable corresponding to the pivot column enters the set of basic variables and is called the entering variable, and the variable being replaced leaves the set of basic variables and is called the leaving variable. The tableau is still in canonical form but with the set of basic variables changed by one element.[11][12]where zB is the value of the objective function at the corresponding basic feasible solution. The updated coefficients, also known as relative cost coefficients, are the rates of change of the objective function with respect to the nonbasic variables.[12]be a tableau in canonical form. Additional row-addition transformations can be applied to remove the coefficients cT
B  from the objective function. This process is called pricing out and results in a canonical tableauLetConversely, given a basic feasible solution, the columns corresponding to the nonzero variables can be expanded to a nonsingular matrix. If the corresponding tableau is multiplied by the inverse of this matrix then the result is a tableau in canonical form.[20]A linear program in standard form can be represented as a tableau of the formWhen this process is complete the feasible region will be in the formIt is much easier to perform algebraic manipulation on inequalities in this form. In inequalities where ≥ appears such as the second one, some authors refer to the variable introduced as a surplus variable.are replaced withSecond, for each remaining inequality constraint, a new variable, called a slack variable, is introduced to change the constraint to an equality constraint. This variable represents the difference between the two sides of the inequality and is assumed to be non-negative. For example, the inequalitiesThe transformation of a linear program to one in standard form may be accomplished as follows.[17] First, for each variable with a lower bound other than 0, a new variable is introduced representing the difference between the variable and bound. The original variable can then be eliminated by substitution. For example, given the constraintAfter Dantzig included an objective function as part of his formulation during mid-1947, the problem was mathematically more tractable. Dantzig realized that one of the unsolved problems that he mistook as homework in his professor Jerzy Neyman's class (and actually later solved), was applicable to finding an algorithm for linear programs. This problem involved finding the existence of Lagrange multipliers for general linear programs over a continuum of variables, each bounded between zero and one, and satisfying linear constraints expressed in the form of Lebesgue integrals. Dantzig later published his "homework" as a thesis to earn his doctorate. The column geometry used in this thesis gave Dantzig insight that made him believe that the Simplex method would be very efficient.[16]George Dantzig worked on planning methods for the US Army Air Force during World War II using a desk calculator. During 1946 his colleague challenged him to mechanize the planning process to distract him from taking another job. Dantzig formulated the problem as linear inequalities inspired by the work of Wassily Leontief, however, at that time he didn't include an objective as part of his formulation. Without an objective, a vast number of solutions can be feasible, and therefore to find the "best" feasible solution, military-specified "ground rules" must be used that describe how goals can be achieved as opposed to specifying a goal itself. Dantzig's core insight was to realize that most such ground rules can be translated into a linear objective function that needs to be maximized.[14] Development of the simplex method was evolutionary and happened over a period of about a year.[15]The solution of a linear program is accomplished in two steps. In the first step, known as Phase I, a starting extreme point is found. Depending on the nature of the program this may be trivial, but in general it can be solved by applying the simplex algorithm to a modified version of the original program. The possible results of Phase I are either that a basic feasible solution is found or that the feasible region is empty. In the latter case the linear program is called infeasible. In the second step, Phase II, the simplex algorithm is applied using the basic feasible solution found in Phase I as a starting point. The possible results from Phase II are either an optimum basic feasible solution or an infinite edge on which the objective function is unbounded below.[11][12][13]It can also be shown that, if an extreme point is not a maximum point of the objective function, then there is an edge containing the point so that the objective function is strictly increasing on the edge moving away from the point.[10] If the edge is finite, then the edge connects to another extreme point where the objective function has a greater value, otherwise the objective function is unbounded above on the edge and the linear program has no solution. The simplex algorithm applies this insight by walking along edges of the polytope to extreme points with greater and greater objective values. This continues until the maximum value is reached, or an unbounded edge is visited (concluding that the problem has no solution). The algorithm always terminates because the number of vertices in the polytope is finite; moreover since we jump between vertices always in the same direction (that of the objective function), we hope that the number of vertices visited will be small.[10]It can be shown that for a linear program in standard form, if the objective function has a maximum value on the feasible region, then it has this value on (at least) one of the extreme points.[8] This in itself reduces the problem to a finite computation since there is a finite number of extreme points, but the number of extreme points is unmanageably large for all but the smallest linear programs.[9]The simplex algorithm operates on linear programs in standard form:The name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin.[2] Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones, and these become proper simplices with an additional constraint.[3][4][5][6] The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a polytope. The shape of this polytope is defined by the constraints applied to the objective function.In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming.[1]
Linear programming
Proprietary licenses:MINTO (Mixed Integer Optimizer, an integer programming solver which uses branch and bound algorithm) has publicly available source code[22] but is not open source.Copyleft (reciprocal) licenses:Permissive licenses:A bounded integral polyhedron is sometimes called a convex lattice polytope, particularly in two dimensions.One common way of proving that a polyhedron is integral is to show that it is totally unimodular. There are other general methods including the integer decomposition property and total dual integrality. Other specific well-known integral LPs include the matching polytope, lattice polyhedra, submodular flow polyhedra, and the intersection of 2 generalized polymatroids/g-polymatroids – e.g. see Schrijver 2003.Note that terminology is not consistent throughout the literature, so one should be careful to distinguish the following two concepts,Integral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.Such integer-programming algorithms are discussed by Padberg and in Beasley.Advanced algorithms for solving integer linear programs include:There are however some important subclasses of IP and MIP problems that are efficiently solvable, most notably problems where the constraint matrix is totally unimodular and the right-hand sides of the constraints are integers or – more general – where the system has the total dual integrality (TDI) property.If only some of the unknown variables are required to be integers, then the problem is called a mixed integer programming (MIP) problem. These are generally also NP-hard because they are even more general than ILP programs.If all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem. In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard. 0–1 integer programming or binary integer programming (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of Karp's 21 NP-complete problems.Simplex pivot methods preserve primal (or dual) feasibility. On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility—they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order. Pivot methods of this type have been studied since the 1970s. Essentially, these methods attempt to find the shortest pivot path on the arrangement polytope under the linear programming problem. In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.[11]The simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope. This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope. As a result, we are interested in knowing the maximum graph-theoretical diameter of polytopal graphs. It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.These questions relate to the performance analysis and development of simplex-like methods. The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time. It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.Although the Hirsch conjecture was recently disproved for higher dimensions, it still leaves the following questions open.This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century. In Smale's words, the third version of the problem "is the main unsolved problem of linear programming theory." While algorithms exist to solve linear programming in weakly polynomial time, such as the ellipsoid methods and interior-point techniques, no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables. The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.There are several open problems in the theory of linear programming, the solution of which would represent fundamental breakthroughs in mathematics and potentially major advances in our ability to solve large-scale linear programs.Covering and packing LPs can be solved approximately in nearly-linear time. That is, if matrix A is of dimension n×m and has N non-zero entries, then there exist algorithms that run in time O(N·(log N)O(1)/εO(1)) and produce O(1±ε) approximate solutions to given covering and packing LPs. The best known sequential algorithm of this kind runs in time O(N + (log N)·(n+m)/ε2),[19] and the best known parallel algorithm of this kind runs in O((log N)2/ε3) iterations, each requiring only a matrix-vector multiplication which is highly parallelizable.[20]The current opinion is that the efficiencies of good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming.[17] However, for specific types of LP problems, it may be that one type of solver is better than another (sometimes much better), and that the structure of the solutions generated by interior point methods versus simplex-based methods are significantly different with the support set of active variables being typically smaller for the later one.[18]For both theoretical and practical purposes, barrier function or path-following methods have been the most popular interior point methods since the 1990s.[17]Affine scaling is one of the oldest interior point methods to be developed. It was developed in the Soviet Union in the mid-1960s, but didn't receive much attention until the discovery of Karmarkar's algorithm, after which affine scaling was reinvented multiple times and presented as a simplified version of Karmarkar's. Affine scaling amounts to doing gradient descent steps within the feasible region, while rescaling the problem to make sure the steps move toward the optimum faster.[16]Khachiyan's algorithm was of landmark importance for establishing the polynomial-time solvability of linear programs. The algorithm was not a computational break-through, as the simplex method is more efficient for all but specially constructed families of linear programs.This is the first worst-case polynomial-time algorithm ever found for linear programming. To solve a problem which has n variables and can be encoded in L input bits, this algorithm uses O(n4L) pseudo-arithmetic operations on numbers with O(L) digits. Leonid Khachiyan solved this long-standing complexity issue in 1979 with the introduction of the ellipsoid method. The convergence analysis has (real-number) predecessors, notably the iterative methods developed by Naum Z. Shor and the approximation algorithms by Arkadi Nemirovski and D. Yudin.In contrast to the simplex algorithm, which finds an optimal solution by traversing the edges between vertices on a polyhedral set, interior-point methods move through the interior of the feasible region.Like the simplex algorithm of Dantzig, the criss-cross algorithm is a basis-exchange algorithm that pivots between bases. However, the criss-cross algorithm need not maintain feasibility, but can pivot rather from a feasible basis to an infeasible basis. The criss-cross algorithm does not have polynomial time-complexity for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee–Minty cube, in the worst case.[11][14]However, the simplex algorithm has poor worst-case behavior: Klee and Minty constructed a family of linear programming problems for which the simplex method takes a number of steps exponential in the problem size.[6][9][10] In fact, for some time it was not known whether the linear programming problem was solvable in polynomial time, i.e. of complexity class P.In practice, the simplex algorithm is quite efficient and can be guaranteed to find the global optimum if certain precautions against cycling are taken. The simplex algorithm has been proved to solve "random" problems efficiently, i.e. in a cubic number of steps,[12] which is similar to its behavior on practical problems.[6][13]The simplex algorithm, developed by George Dantzig in 1947, solves LP problems by constructing a feasible solution at a vertex of the polytope and then walking along a path on the edges of the polytope to vertices with non-decreasing values of the objective function until an optimum is reached for sure. In many practical problems, "stalling" occurs: Many pivots are made with no increase in the objective function.[6][7] In rare practical problems, the usual versions of the simplex algorithm may actually "cycle".[7] To avoid cycles, researchers developed new pivoting rules.[8][9][6][7][10][11]The vertices of the polytope are also called basic feasible solutions. The reason for this choice of name is as follows. Let d denote the number of variables. Then the fundamental theorem of linear inequalities implies (for feasible problems) that for every vertex x* of the LP feasible region, there exists a set of d (or fewer) inequality constraints from the LP such that, when we treat those d constraints as equalities, the unique solution is x*. Thereby we can study these vertices by means of looking at certain subsets of the set of all constraints (a discrete set), rather than the continuum of LP solutions. This principle underlies the simplex algorithm for solving linear programs.Otherwise, if a feasible solution exists and if the constraint set is bounded, then the optimum value is always attained on the boundary of the constraint set, by the maximum principle for convex functions (alternatively, by the minimum principle for concave functions) since linear functions are both convex and concave. However, some problems have distinct optimal solutions: For example, the problem of finding a feasible solution to a system of linear inequalities is a linear programming problem in which the objective function is the zero function (that is, the constant function taking the value zero everywhere): For this feasibility problem with the zero-function for its objective-function, if there are two distinct solutions, then every convex combination of the solutions is a solution.An optimal solution need not exist, for two reasons. First, if two constraints are inconsistent, then no feasible solution exists: For instance, the constraints x ≥ 2 and x ≤ 1 cannot be satisfied jointly; in this case, we say that the LP is infeasible. Second, when the polytope is unbounded in the direction of the gradient of the objective function (where the gradient of the objective function is the vector of the coefficients of the objective function), then no optimal value is attained because it is always possible to do better than any finite value of the objective function.Geometrically, the linear constraints define the feasible region, which is a convex polyhedron. A linear function is a convex function, which implies that every local minimum is a global minimum; similarly, a linear function is a concave function, which implies that every local maximum is a global maximum.This necessary condition for optimality conveys a fairly simple economic principle. In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are "leftovers"), then additional quantities of that resource must have no value. Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no "leftovers").So if the i-th slack variable of the primal is not zero, then the i-th variable of the dual is equal to zero. Likewise, if the j-th slack variable of the dual is not zero, then the j-th variable of the primal is equal to zero.Suppose that x = (x1, x2, ... , xn) is primal feasible and that y = (y1, y2, ... , ym) is dual feasible. Let (w1, w2, ..., wm) denote the corresponding primal slack variables, and let (z1, z2, ... , zn) denote the corresponding dual slack variables. Then x and y are optimal for their respective problems if and only ifIt is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:Finding a fractional coloring of a graph is another example of a covering LP. In this case, there is one constraint for each vertex of the graph and one variable for each independent set of the graph.Covering and packing LPs commonly arise as a linear programming relaxation of a combinatorial problem and are important in the study of approximation algorithms.[5] For example, the LP relaxations of the set packing problem, the independent set problem, and the matching problem are packing LPs. The LP relaxations of the set cover problem, the vertex cover problem, and the dominating set problem are also covering LPs.such that the matrix A and the vectors b and c are non-negative.The dual of a covering LP is a packing LP, a linear program of the form:such that the matrix A and the vectors b and c are non-negative.A covering LP is a linear program of the form:Note that we assume in our calculations steps that the program is in standard form. However, any linear program may be transformed to standard form and it is therefore not a limiting factor.Since this is a minimization problem, we would like to obtain a dual program that is a lower bound of the primal. In other words, we would like the sum of all right hand side of the constraints to be the maximal under the condition that for each primal variable the sum of its coefficients do not exceed its coefficient in the linear function. For example, x1 appears in n + 1 constraints. If we sum its constraints' coefficients we get a1,1y1 + a1,2y2 + ... + a1,nyn + f1s1. This sum must be at most c1. As a result, we get:We have m + n conditions and all variables are non-negative. We shall define m + n dual variables: yj and si. We get:Sometimes, one may find it more intuitive to obtain the dual program without looking at the program matrix. Consider the following linear program:Since each inequality can be replaced by an equality and a slack variable, this means each primal variable corresponds to a dual slack variable, and each dual variable corresponds to a primal slack variable. This relation allows us to speak about complementary slackness.Both the primal and the dual problems make use of the same matrix. In the primal space, this matrix expresses the consumption of physical quantities of inputs necessary to produce set quantities of outputs. In the dual space, it expresses the creation of the economic values associated with the outputs from set input unit prices.The coefficients that bound the inequalities in the primal space are used to compute the objective in the dual space, input quantities in this example. The coefficients used to compute the objective in the primal space bound the inequalities in the dual space, output unit prices in this example.To each variable in the primal space corresponds an inequality to satisfy in the dual space, both indexed by output type. To each inequality to satisfy in the primal space corresponds a variable in the dual space, both indexed by input type.The primal problem deals with physical quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce so as to maximize total revenue? The dual problem deals with economic values. With floor guarantees on all output unit prices, and assuming the available quantity of all inputs is known, what input unit pricing scheme to set so as to minimize total expenditure?In matrix form this becomes:Revisit the above example of the farmer who may grow wheat and barley with the set provision of some L land, F fertilizer and P pesticide. Assume now that y unit prices for each of these means of production (inputs) are set by a planning board. The planning board's job is to minimize the total cost of procuring the set amounts of inputs while providing the farmer with a floor on the unit price of each of his crops (outputs), S1 for wheat and S2 for barley. This corresponds to the following linear programming problem:A linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem. Likewise, if the dual is unbounded, then the primal must be infeasible. However, it is possible for both the dual and the primal to be infeasible. As an example, consider the linear program:There are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program. Additionally, every feasible solution for a linear program gives a bound on the optimal value of the objective function of its dual. The weak duality theorem states that the objective function value of the dual at any feasible solution is always greater than or equal to the objective function value of the primal at any feasible solution. The strong duality theorem states that if the primal has an optimal solution, x*, then the dual also has an optimal solution, y*, and cTx*=bTy*.An alternative primal formulation is:Every linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:In matrix form this becomes:The example above is converted into the following augmented form:Linear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The problems can then be written in the following block matrix form:In matrix form this becomes:Suppose that a farmer has a piece of farm land, say L km2, to be planted with either wheat or barley or some combination of the two. The farmer has a limited amount of fertilizer, F kilograms, and pesticide, P kilograms. Every square kilometer of wheat requires F1 kilograms of fertilizer and P1 kilograms of pesticide, while every square kilometer of barley requires F2 kilograms of fertilizer and P2 kilograms of pesticide. Let S1 be the selling price of wheat per square kilometer, and S2 be the selling price of barley. If we denote the area of land planted with wheat and barley by x1 and x2 respectively, then profit can be maximized by choosing optimal values for x1 and x2. This problem can be expressed with the following linear programming problem in the standard form:Other forms, such as minimization problems, problems with constraints on alternative forms, as well as problems involving negative variables can always be rewritten into an equivalent problem in standard form.The problem is usually expressed in matrix form, and then becomes:Standard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:Linear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems. Certain special cases of linear programming, such as network flow problems and multicommodity flow problems are considered important enough to have generated much research on specialized algorithms for their solution. A number of algorithms for other types of optimization problems work by solving LP problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as duality, decomposition, and the importance of convexity and its generalizations. Likewise, linear programming was heavily used in the early formation of microeconomics and is currently utilized in company management, such as planning, production, transportation, technology and other issues. Although the modern management issues are ever-changing, most companies would like to maximize profits or minimize costs with limited resources. Therefore, many issues can be characterized as linear programming problems.The linear programming problem was first shown to be solvable in polynomial time by Leonid Khachiyan in 1979, but a larger theoretical and practical breakthrough in the field came in 1984 when Narendra Karmarkar introduced a new interior-point method for solving linear-programming problems.Dantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.During 1946–1947, George B. Dantzig independently developed general linear programming formulation to use for planning problems in US Air Force. In 1947, Dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases. When Dantzig arranged a meeting with John von Neumann to discuss his simplex method, Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent. Dantzig provided formal proof in an unpublished report "A Theorem on Linear Inequalities" on January 5, 1948.[4] In the post-war years, many industries applied it in their daily planning.In 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the Soviet economist Leonid Kantorovich, who also proposed a method for solving it.[2] It is a way he developed, during World War II, to plan expenditures and returns in order to reduce costs of the army and to increase losses incurred to the enemy.[citation needed] Kantorovich's work was initially neglected in the USSR.[3] About the same time as Kantorovich, the Dutch-American economist T. C. Koopmans formulated classical economic problems as linear programs. Kantorovich and Koopmans later shared the 1975 Nobel prize in economics.[1] In 1941, Frank Lauren Hitchcock also formulated transportation problems as linear programs and gave a solution very similar to the later Simplex method;[2] Hitchcock had died in 1957 and the Nobel prize is not awarded posthumously.The problem of solving a system of linear inequalities dates back at least as far as Fourier, who in 1827 published a method for solving them,[1] and after whom the method of Fourier–Motzkin elimination is named.Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.Linear programs are problems that can be expressed in canonical form asMore formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).
Transformation matrix
More complicated perspective projections can be composed by combining this one with rotations, scales, translations, and shears to move the image plane and center of projection wherever they are desired.After carrying out the matrix multiplication, the homogeneous component wc will, in general, not be equal to 1. Therefore, to map back into the real plane we must perform the homogeneous divide or perspective divide by dividing each component by wc:Another type of transformation, of importance in 3D computer graphics, is the perspective projection. Whereas parallel projections are used to project points onto the image plane along parallel lines, the perspective projection projects points onto the image plane along lines that emanate from a single point, called the center of projection. This means that an object has a smaller projection when it is far away from the center of projection and a larger projection when it is closer.
When using affine transformations, the homogeneous component of a coordinate vector (normally called w) will never be altered. One can therefore safely assume that it is always 1 and ignore it. However, this is not true when using perspective projections.Using transformation matrices containing homogeneous coordinates, translations become linearly independent, and thus can be seamlessly intermixed with all other types of transformations. The reason is that the real plane is mapped to the w = 1 plane in real projective space, and so translation in real Euclidean space can be represented as a shear in real projective space. Although a translation is a non-linear transformation in a 2-D or 3-D Euclidean space described by Cartesian coordinates (i.e. it can't be combined with other transformations while preserving commutativity and other properties), it becomes, in a 3-D or 4-D projective space described by homogeneous coordinates, a simple linear transformation (a shear).All ordinary linear transformations are included in the set of affine transformations, and can be described as a simplified form of affine transformations. Therefore, any linear transformation can also be represented by a general transformation matrix. The latter is obtained by expanding the corresponding linear transformation matrix by one row and column, filling the extra space with zeros except for the lower-right corner, which must be set to 1. For example, the counter-clockwise rotation matrix from above becomes:A consequence of the ability to compose transformations by multiplying their matrices is that transformations can also be inverted by simply inverting their matrices. So, A−1 represents the transformation that "undoes" A.(This is called the associative property.) In other words, the matrix of the combined transformation A followed by B is simply the product of the individual matrices. Note that the multiplication is done in the opposite order from the English sentence: the matrix of "A followed by B" is BA, not AB.Composition is accomplished by matrix multiplication. If A and B are the matrices of two linear transformations, then the effect of applying first A and then B to a vector x is given by:One of the main motivations for using matrices to represent linear transformations is that transformations can then be easily composed (combined) and inverted.If the 4th component of the vector is 0 instead of 1, then only the vector's direction is reflected and its length remains unchanged, as if it were mirrored through a parallel plane that passes through the origin. This is a useful property as it allows the transformation of both positional vectors and normal vectors with the same matrix. See homogenous coordinates and affine transformations below for further explanation.Note that these are particular cases of a Householder reflection in two and three dimensions. A reflection about a line or plane that does not go through the origin is not a linear transformation — it is an affine transformation — as a 4x4 affine transformation matrix, it can be expressed as follows (assuming the normal is a unit vector):The matrix to rotate an angle θ about the axis defined by unit vector (l,m,n) is[5]Parallel projections are also linear transformations and can be represented simply by a matrix. However, perspective projections are not, and to represent these with a matrix, homogeneous coordinates can be used.As with reflections, the orthogonal projection onto a line that does not pass through the origin is an affine, not linear, transformation.For shear mapping (visually similar to slanting), there are two possibilities.These formulae assume that the x axis points right and the y axis points up. In formats such as SVG where the y axis points down, these matrices must be swapped.Similarly, a stretch by a factor k along the y-axis has the form x' = x; y' = ky, so the matrix associated with this transformation isThe matrix associated with a stretch by a factor k along the x-axis is given by:A stretch in the xy-plane is a linear transformation which enlarges all distances in a particular direction by a constant factor but does not affect distances in the perpendicular direction. We only consider stretches along the x-axis and y-axis. A stretch along the x-axis has the form x' = kx; y' = y for some positive constant k. (Note that if k is > 1, then this really is a “stretch”; if k is < 1, it is technically a “compression”, but we still call it a stretch. Also, if k=1, then the transformation is an identity, i.e. it has no effect.)Most common geometric transformations that keep the origin fixed are linear, including rotation, scaling, shearing, reflection, and orthogonal projection; if an affine transformation is not a pure translation it keeps some point fixed, and that point can be chosen as origin to make the transformation linear. In two dimensions, linear transformations can be represented using a 2×2 transformation matrix.With diagonalization, it is often possible to translate to and from eigenbases.It must be noted that the matrix representation of vectors and operators depends on the chosen basis; a similar matrix will result from an alternate basis. Nevertheless, the method to find the components remains the same.Put differently, a passive transformation refers to description of the same object as viewed from two different coordinate frames.In the physical sciences, an active transformation is one which actually changes the physical position of a system, and makes sense even in the absence of a coordinate system whereas a passive transformation is a change in the coordinate description of the physical system (change of basis). The distinction between active and passive transformations is important. By default, by transformation, mathematicians usually mean active transformations, while physicists could mean either.Linear transformations are not the only ones that can be represented by matrices. Some transformations that are non-linear on an n-dimensional Euclidean space Rn can be represented as linear transformations on the n+1-dimensional space Rn+1. These include both affine transformations (such as translation) and projective transformations. For this reason, 4×4 transformation matrices are widely used in 3D computer graphics. These n+1-dimensional transformation matrices are called, depending on their application, affine transformation matrices, projective transformation matrices, or more generally non-linear transformation matrices. With respect to an n-dimensional matrix, an n+1-dimensional matrix can be described as an augmented matrix.Matrices allow arbitrary linear transformations to be displayed in a consistent format, suitable for computation.[1] This also allows transformations to be concatenated easily (by multiplying their matrices).
International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Marie A. Vitulli

Alan Tucker
In 2009, Tucker was named a fellow of the American Association for the Advancement of Science.[6] He became one of the inaugural fellows of the American Mathematical Society in 2013.[7]Tucker is the son of mathematician Albert W. Tucker.[3] He earned a bachelor's degree from Harvard University in 1965 and then went on to graduate studies at Stanford University, earning a master's degree in 1967 and a doctorate in 1969.[4] His doctoral thesis, supervised by George Dantzig, concerned circular-arc graphs.[5] He then joined the Department of Applied Mathematics and Statistics at Stony Brook University in 1970, and has remained there for the rest of his career. Since 1989, he has been a S.U.N.Y. Distinguished Teaching Professor at Stony Brook. Since 2011, he has been editor in chief of the journal Applied Mathematics Letters.[4]Alan Curtiss Tucker is an American mathematician. He is a professor of applied mathematics at Stony Brook University, and the author of a widely used textbook on combinatorics;[1][2] he has also made research contributions to graph theory and coding theory. He also had four children, Katie, Lisa, Edward, and James.
Digital object identifier
The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox[permanent dead link], enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc….[21]In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]Major applications of the DOI system currently include:The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL — providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless.A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.
International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Digital object identifier
The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[38]DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[37]The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[34] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[35] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[36] The final standard was published on 23 April 2012.[1]Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[33]The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[32] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[31] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox[permanent dead link], enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[28][29] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[30] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[28][30]Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[27] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[26]A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[25]The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[22] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[23] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[24]The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.Other registries include Crossref and the multilingual European DOI Registration Agency.[20] Since 2015 RFCs can be referenced as doi:10.17487/rfc….[21]In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]Major applications of the DOI system currently include:The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL — providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI shall provide a more stable linking than simply using its URL. Every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. By failing to do so, the DOI resolves to a dead link leaving the DOI useless.A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.
Martha Siegel

Finitely generated module
Although coherence seems like a more cumbersome condition than finitely generated or finitely presented, it is nicer than them since the category of coherent modules is an abelian category, while, in general, neither finitely generated nor finitely presented modules form an abelian category.It is true also that the following conditions are equivalent for a ring R:Some crossover occurs for projective or flat modules. A finitely generated projective module is finitely presented, and a finitely related flat module is projective.Over any ring R, coherent modules are finitely presented, and finitely presented modules are both finitely generated and finitely related. For a Noetherian ring R, finitely generated, finitely presented, and coherent are equivalent conditions on a module.for a module M and free module F.Suppose now there is an epimorphism,Another formulation is this: a finitely generated module M is one for which there is an epimorphismFinitely cogenerated modules must have finite uniform dimension. This is easily seen by applying the characterization using the finitely generated essential socle. Somewhat asymmetrically, finitely generated modules do not necessarily have finite uniform dimension. For example, an infinite direct product of nonzero rings is a finitely generated (cyclic!) module over itself, however it clearly contains an infinite direct sum of nonzero submodules. Finitely generated modules do not necessarily have finite co-uniform dimension either: any ring R with unity such that R/J(R) is not a semisimple ring is a counterexample.Both f.g. modules and f.cog. modules have interesting relationships to Noetherian and Artinian modules, and the Jacobson radical J(M) and socle soc(M) of a module. The following facts illustrate the duality between the two conditions. For a module M:From these conditions it is easy to see that being finitely generated is a property preserved by Morita equivalence. The conditions are also convenient to define a dual notion of a finitely cogenerated module M. The following conditions are equivalent to a module being finitely cogenerated (f.cog.):The following conditions are equivalent to M being finitely generated (f.g.):By the same argument as above, a finitely generated module over a Dedekind domain A (or more generally a semi-hereditary ring) is torsion-free if and only if it is projective; consequently, a finitely generated module over A is a direct sum of a torsion module and a projective module. A finitely generated projective module over a Noetherian integral domain has constant rank and so the generic rank of a finitely generated module over A is the rank of its projective part.An example of a link between finite generation and integral elements can be found in commutative algebras. To say that a commutative algebra A is a finitely generated ring over R means that there exists a set of elements G = {x1, ..., xn} of A such that the smallest subring of A containing G and R is A itself. Because the ring product may be used to combine elements, more than just R-linear combinations of elements of G are generated. For example, a polynomial ring R[x] is finitely generated by {1,x} as a ring, but not as a module. If A is a commutative algebra (with unity) over R, then the following two statements are equivalent:[5]Any R-module is an inductive limit of finitely generated R-submodules. This is useful for weakening an assumption to the finite case (e.g., the characterization of flatness with the Tor functor.)For finitely generated modules over a commutative ring R, Nakayama's lemma is fundamental. Sometimes, the lemma allows one to prove finite dimensional vector spaces phenomena for finitely generated modules. For example, if f : M → M is a surjective R-endomorphism of a finitely generated module M, then f is also injective, and hence is an automorphism of M.[3] This says simply that M is a Hopfian module. Similarly, an Artinian module M is coHopfian: any injective endomorphism f is also a surjective endomorphism.[4]Let B be a ring and A its subring such that B is a faithfully flat right A-module. Then a left A-module F is finitely generated (resp. finitely presented) if and only if the B-module B ⊗A F is finitely generated (resp. finitely presented).[2]Let 0 → M′ → M → M′′ → 0 be an exact sequence of modules. Then M is finitely generated if M′, M′′ are finitely generated. There are some partial converses to this. If M is finitely generated and M'' is finitely presented (which is stronger than finitely generated; see below), then M′ is finitely generated. Also, M is Noetherian (resp. Artinian) if and only if M′, M′′ are Noetherian (resp. Artinian).More generally, an algebra (e.g., ring) that is a finitely generated module is a finitely generated algebra. Conversely, if a finitely generated algebra is integral (over the coefficient ring), then it is finitely generated module. (See integral element for more.)In general, a module is said to be Noetherian if every submodule is finitely generated. A finitely generated module over a Noetherian ring is a Noetherian module (and indeed this property characterizes Noetherian rings): A module over a Noetherian ring is finitely generated if and only if it is a Noetherian module. This resembles, but is not exactly Hilbert's basis theorem, which states that the polynomial ring R[X] over a Noetherian ring R is Noetherian. Both facts imply that a finitely generated commutative algebra over a Noetherian ring is again a Noetherian ring.Every homomorphic image of a finitely generated module is finitely generated. In general, submodules of finitely generated modules need not be finitely generated. As an example, consider the ring R = Z[X1, X2, ...] of all polynomials in countably many variables. R itself is a finitely generated R-module (with {1} as generating set). Consider the submodule K consisting of all those polynomials with zero constant term. Since every polynomial contains only finitely many terms whose coefficients are non-zero, the R-module K is not finitely generated.A module M is finitely generated if and only if any increasing chain Mi of submodules with union M stabilizes: i.e., there is some i such that Mi = M. If any increasing chain of submodules stabilizes (i.e., any submodule is finitely generated), then the module M is called a Noetherian module.Any module is the union of the directed set of its finitely generated submodules.In the case where the module M is a vector space over a field R, and the generating set is linearly independent, n is well-defined and is referred to as the dimension of M (well-defined means that any linearly independent generating set has n elements: this is the dimension theorem for vector spaces).If a set S generates a module that is finitely generated, then the finite generators of the module can be taken from S at the expense of possibly increasing the number of the generators (since only finitely many elements in S are needed to express the finite generators).for some n (M is a quotient of a free module of finite rank.)The set {a1, a2, ..., an} is referred to as a generating set for M in this case. The finite generators need not be a basis, since they need not be linearly independent over R. What is true is: M is finitely generated if and only if there is a surjective R-linear map:The left R-module M is finitely generated if there exist a1, a2, ..., an in M such that for any x in M, there exist r1, r2, ..., rn in R with x = r1a1 + r2a2 + ... + rnan.A finitely generated module over a field is simply a finite-dimensional vector space, and a finitely generated module over the integers is simply a finitely generated abelian group.Related concepts include finitely cogenerated modules, finitely presented modules, finitely related modules and coherent modules all of which are defined below. Over a Noetherian ring the concepts of finitely generated, finitely presented and coherent modules coincide.In mathematics, a finitely generated module is a module that has a finite generating set. A finitely generated R-module also may be called a finite R-module, finite over R,[1] or a module of finite type.
Well-order
A well-ordered set as topological space is a first-countable space if and only if it has order type less than or equal to ω1 (omega-one), that is, if and only if the set is countable or has the smallest uncountable order type.A subset is cofinal in the whole set if and only if it is unbounded in the whole set or it has a maximum which is also maximum of the whole set.For subsets we can distinguish:With respect to this topology there can be two kinds of elements:Every well-ordered set can be made into a topological space by endowing it with the order topology.If a set is totally ordered, then the following are equivalent to each other:Examples of well orders:An uncountable subset of the real numbers with the standard ordering ≤ cannot be a well order: Suppose X is a subset of R well ordered by ≤. For each x in X, let s(x) be the successor of x in ≤ ordering on X (unless x is the last element of X). Let A = { (x, s(x)) | x ∈ X } whose elements are nonempty and disjoint intervals. Each such interval contains at least one rational number, so there is an injective function from A to Q. There is an injection from X to A (except possibly for a last element of X which could be mapped to zero later). And it is well known that there is an injection from Q to the natural numbers (which could be chosen to avoid hitting zero). Thus there is an injection from X to the natural numbers which means that X is countable. On the other hand, a countably infinite subset of the reals may or may not be a well order with the standard "≤". For example,The standard ordering ≤ of any real interval is not a well ordering, since, for example, the open interval (0, 1) ⊆ [0,1] does not contain a least element. From the ZFC axioms of set theory (including the axiom of choice) one can show that there is a well order of the reals. Also Wacław Sierpiński proved that ZF + GCH (the generalized continuum hypothesis) imply the axiom of choice and hence a well order of the reals. Nonetheless, it is possible to show that the ZFC+GCH axioms alone are not sufficient to prove the existence of a definable (by a formula) well order of the reals.[1] However it is consistent with ZFC that a definable well ordering of the reals exists—for example, it is consistent with ZFC that V=L, and it follows from ZFC+V=L that a particular formula well orders the reals, or indeed any set.This has the order type ω.Another relation for well ordering the integers is the following definition: x ≤z y iff (|x| < |y| or (|x| = |y| and x ≤ y)). This well order can be visualized as follows:R is isomorphic to the ordinal number ω + ω.This relation R can be visualized as follows:The following relation R is an example of well ordering of the integers: x R y if and only if one of the following conditions holds:Unlike the standard ordering ≤ of the natural numbers, the standard ordering ≤ of the integers is not a well ordering, since, for example, the set of negative integers does not contain a least element.This is a well-ordered set of order type ω + ω. Every element has a successor (there is no largest element). Two elements lack a predecessor: 0 and 1.Another well ordering of the natural numbers is given by defining that all even numbers are less than all odd numbers, and the usual ordering applies within the evens and the odds:The standard ordering ≤ of the natural numbers is a well ordering and has the additional property that every non-zero natural number has a unique predecessor.For an infinite set the order type determines the cardinality, but not conversely: well-ordered sets of a particular cardinality can have many different order types. For a countably infinite set, the set of possible order types is even uncountable.Every well-ordered set is uniquely order isomorphic to a unique ordinal number, called the order type of the well-ordered set. The position of each element within the ordered set is also given by an ordinal number. In the case of a finite set, the basic operation of counting, to find the ordinal number of a particular object, or to find the object with a particular ordinal number, corresponds to assigning ordinal numbers one by one to the objects. The size (number of elements, cardinal number) of a finite set is equal to the order type. Counting in the everyday sense typically starts from one, so it assigns to each object the size of the initial segment with that object as last element. Note that these numbers are one more than the formal ordinal numbers according to the isomorphic order, because these are equal to the number of earlier objects (which corresponds to counting from zero). Thus for finite n, the expression "n-th element" of a well-ordered set requires context to know whether this counts from zero or one. In a notation "β-th element" where β can also be an infinite ordinal, it will typically count from zero.The observation that the natural numbers are well ordered by the usual less-than relation is commonly called the well-ordering principle (for natural numbers).Every well-ordered set is uniquely order isomorphic to a unique ordinal number, called the order type of the well-ordered set. The well-ordering theorem, which is equivalent to the axiom of choice, states that every set can be well ordered. If a set is well ordered (or even if it merely admits a well-founded relation), the proof technique of transfinite induction can be used to prove that a given statement is true for all elements of the set.If ≤ is a non-strict well ordering, then < is a strict well ordering. A relation is a strict well ordering if and only if it is a well-founded strict total order. The distinction between strict and non-strict well orders is often ignored since they are easily interconvertible.Every non-empty well-ordered set has a least element. Every element s of a well-ordered set, except a possible greatest element, has a unique successor (next element), namely the least element of the subset of all elements greater than s. There may be elements besides the least element which have no predecessor (see Natural numbers below for an example). In a well-ordered set S, every subset T which has an upper bound has a least upper bound, namely the least element of the subset of all upper bounds of T in S.In mathematics, a well-order (or well-ordering or well-order relation) on a set S is a total order on S with the property that every non-empty subset of S has a least element in this ordering. The set S together with the well-order relation is then called a well-ordered set. In some academic articles and textbooks these terms are instead written as wellorder, wellordered, and wellordering or well order, well ordered, and well ordering.
Dimension theorem for vector spaces

Logical equivalence
(Note that in this example classical logic is assumed. Some non-classical logics do not deem (1) and (2) logically equivalent.)Syntactically, (1) and (2) are derivable from each other via the rules of contraposition and double negation. Semantically, (1) and (2) are true in exactly the same models (interpretations, valuations); namely, those in which either Lisa is in France is false or Lisa is in Europe is true.The following statements are logically equivalent:Logical equivalences involving biconditionals：Logical equivalences involving conditional statements：
Axiom of choice

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Graduate Texts in Mathematics
The books in this series tend to be written at a more advanced level than the similar Undergraduate Texts in Mathematics series, although there is a fair amount of overlap between the two series in terms of material covered and difficulty level.Graduate Texts in Mathematics (GTM) (ISSN 0072-5285) is a series of graduate-level textbooks in mathematics published by Springer-Verlag. The books in this series, like the other Springer-Verlag mathematics series, are yellow books of a standard size (with variable numbers of pages). The GTM series is easily identified by a white band at the top of the book.
International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

James Demmel

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Felix Gantmacher

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Israel Gelfand
Israel Gelfand died at the Robert Wood Johnson University Hospital near his home in Highland Park, New Jersey. He was less than five weeks past his 96th birthday. His death was first reported on the blog of his former collaborator Andrei Zelevinsky[11] and confirmed a few hours later by an obituary in the Russian online newspaper Polit.ru.[12]In an October 2003 article in The New York Times, written on the occasion of his 90th birthday, Gelfand is described as a scholar who is considered "among the greatest mathematicians of the 20th century",[10] having exerted a tremendous influence on the field both through his own works and those of his students.Gelfand held several honorary degrees and was awarded the Order of Lenin three times for his research. In 1977 he was elected a Foreign Member of the Royal Society. He won the Wolf Prize in 1978, Kyoto Prize in 1989 and MacArthur Foundation Fellowship in 1994. He held the presidency of the Moscow Mathematical Society between 1968 and 1970, and was elected a foreign member of the U.S. National Academy of Science, the American Academy of Arts and Sciences, the Royal Irish Academy, the American Mathematical Society and the London Mathematical Society.Gelfand was married to Zorya Shapiro, and their two sons, Sergei and Vladimir both live in the United States. A third son, Aleksandr, died of leukemia. Following the divorce from his first wife, Gelfand married his second wife, Tatiana; together they had a daughter, Tatiana. The family also includes four grandchildren and three great-grandchildren.[7][8] The memories about I.Gelfand are collected at the special site[9] handled by his family.He worked extensively in mathematics education, particularly with correspondence education. In 1994, he was awarded a MacArthur Fellowship for this work.Gelfand also published works on biology and medicine.[4] For a long time he took an interest in cell biology and organized a research seminar on the subject.[5][6]The Gelfand–Tsetlin basis (also in the common spelling Zetlin) is a widely used tool in theoretical physics and the result of Gelfand's work on the representation theory of the unitary group and Lie groups in general.Gelfand is known for many developments including:A native of Kherson Governorate of the Russian Empire, Gelfand was born into a Jewish family in the small southern Ukrainian town of Okny. According to his own account, Gelfand was expelled from high school because his father had been a mill owner. Bypassing both high school and college, he proceeded to postgraduate study at Moscow State University, where his advisor was the preeminent mathematician Andrei Kolmogorov. He nevertheless managed to attend lectures at the University and began postgraduate study at the age of 19.[2]His legacy continues through his students, who include Endre Szemerédi, Alexandre Kirillov, Edward Frenkel,[1] Joseph Bernstein, as well as his own son, Sergei Gelfand.Israel Moiseevich Gelfand, also written Israïl Moyseyovich Gel'fand, or Izrail M. Gelfand (Yiddish: ישראל געלפֿאַנד‎, Russian: Изра́иль Моисе́евич Гельфа́нд; 2 September [O.S. 20 August] 1913 – 5 October 2009) was a prominent Soviet mathematician. He made significant contributions to many branches of mathematics, including group theory, representation theory and functional analysis. The recipient of many awards, including the Order of Lenin and the Wolf Prize, he was a Fellow of the Royal Society and professor at Moscow State University and, after immigrating to the United States shortly before his 76th birthday, at Rutgers University.
International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Ray Kunze

Mathematical Reviews
Current Mathematical Publications was a subject index in print format that published the newest and upcoming mathematical literature, chosen and indexed by Mathematical Reviews editors. It covered the period from 1965 until 2012, when it was discontinued.[8]The "All Journal MCQ" is computed by considering all the journals indexed by Mathematical Reviews as a single meta-journal, which makes it possible to determine if a particular journal has a higher or lower MCQ than average. The 2009 All Journal MCQ is 0.28.For the period 2004–2008, the top five journals in Mathematical Reviews by MCQ were:[7]Mathematical Reviews computes a "mathematical citation quotient" (MCQ) for each journal. Like the impact factor, this is a numerical statistic that measures the frequency of citations to a journal.[6] The MCQ is calculated by counting the total number of citations into the journal that have been indexed by Mathematical Reviews over a five-year period, and dividing this total by the total number of papers published by the journal during that five-year period.In 1980, all the contents of Mathematical Reviews since 1940 were integrated into an electronic searchable database. Eventually the contents became part of MathSciNet, which was officially launched in 1996.[2] MathSciNet also has extensive citation information.[5]Mathematical Reviews was founded by Otto E. Neugebauer in 1940[3] as an alternative to the German journal Zentralblatt für Mathematik,[4] which Neugebauer had also founded a decade earlier, but which under the Nazis had begun censoring reviews by and of Jewish mathematicians.[3] The goal of the new journal was to give reviews of every mathematical research publication. As of November 2007, the Mathematical Reviews database contained information on over 2.2 million articles. The authors of reviews are volunteers, usually chosen by the editors because of some expertise in the area of the article. It and Zentralblatt für Mathematik are the only comprehensive resources of this type. (The Mathematics section of Referativny Zhurnal is available only in Russian and is smaller in scale and difficult to access.) Often reviews give detailed summaries of the contents of the paper, sometimes with critical comments by the reviewer and references to related work. However, reviewers are not encouraged to criticize the paper, because the author does not have an opportunity to respond. The author's summary may be quoted when it is not possible to give an independent review, or when the summary is deemed adequate by the reviewer or the editors. Only bibliographic information may be given when a work is in an unusual language, when it is a brief paper in a conference volume, or when it is outside the primary scope of the Reviews. Originally the reviews were written in several languages, but later an "English only" policy was introduced. Selected reviews (called "featured reviews") were also published as a book by the AMS, but this program has been discontinued.Mathematical Reviews is a journal published by the American Mathematical Society (AMS) that contains brief synopses, and in some cases evaluations, of many articles in mathematics, statistics, and theoretical computer science.[1][2] The AMS also publishes an associated online bibliographic database called MathSciNet which contains an electronic version of Mathematical Reviews and additionally contains citation information for almost 3 million papers.
Paul Halmos
In 2005, Halmos and his wife Virginia funded the Euler Book Prize, an annual award given by the Mathematical Association of America for a book that is likely to improve the view of mathematics among the public. The first prize was given in 2007, the 300th anniversary of Leonhard Euler's birth, to John Derbyshire for his book about Bernhard Riemann and the Riemann hypothesis: Prime Obsession.[9]In these memoirs, Halmos claims to have invented the "iff" notation for the words "if and only if" and to have been the first to use the “tombstone” notation to signify the end of a proof,[7] and this is generally agreed to be the case. The tombstone symbol ∎ (Unicode U+220E) is sometimes called a halmos.[8]Halmos's 1985 "automathography" I Want to Be a Mathematician is an account of what it was like to be an academic mathematician in 20th century America. He called the book "automathography" rather than "autobiography", because its focus is almost entirely on his life as a mathematician, not his personal life. The book contains the following quote on Halmos' view of what doing mathematics means:In the American Scientist 56(4): 375–389, Halmos argued that mathematics is a creative art, and that mathematicians should be seen as artists, not number crunchers. He discussed the division of the field into mathology and mathophysics, further arguing that mathematicians and painters think and work in related ways.In addition to his original contributions to mathematics, Halmos was an unusually clear and engaging expositor of university mathematics. He won the Lester R. Ford Award in 1971[5] and again in 1977 (shared with W. P. Ziemer, W. H. Wheeler, S. H. Moolgavkar, J. H. Ewing and W. H. Gustafson).[6] Halmos chaired the American Mathematical Society committee that wrote the AMS style guide for academic mathematics, published in 1973. In 1983, he received the AMS's Steele Prize for exposition.In a series of papers reprinted in his 1962 Algebraic Logic, Halmos devised polyadic algebras, an algebraic version of first-order logic differing from the better known cylindric algebras of Alfred Tarski and his students. An elementary version of polyadic algebra is described in monadic Boolean algebra.Halmos taught at Syracuse University, the University of Chicago (1946–60), the University of Michigan (~1961–67), the University of California at Santa Barbara (1976–78), the University of Hawaii, and Indiana University. From his 1985 retirement from Indiana until his death, he was affiliated with the Mathematics department at Santa Clara University.Shortly after his graduation, Halmos left for the Institute for Advanced Study, lacking both job and grant money. Six months later, he was working under John von Neumann, which proved a decisive experience. While at the Institute, Halmos wrote his first book, Finite Dimensional Vector Spaces, which immediately established his reputation as a fine expositor of mathematics.[4]Halmos arrived in the U.S. at 13 years of age. He obtained his B.A. from the University of Illinois, majoring in mathematics, but fulfilling the requirements for both a math and philosophy degree. He took only three years to obtain the degree, and was only 19 when he graduated. He then began a Ph.D. in philosophy, still at the Champaign-Urbana campus; but, after failing his masters' oral exams,[2] he shifted to mathematics, graduating in 1938. Joseph L. Doob supervised his dissertation, titled Invariants of Certain Stochastic Transformations: The Mathematical Theory of Gambling Systems.[3]Paul Richard Halmos (Hungarian: Halmos Pál; March 3, 1916 – October 2, 2006) was a Hungarian-Jewish-born American mathematician who made fundamental advances in the areas of mathematical logic, probability theory, statistics, operator theory, ergodic theory, and functional analysis (in particular, Hilbert spaces). He was also recognized as a great mathematical expositor. According to György Marx, Halmos was one of The Martians.[1]
Undergraduate Texts in Mathematics

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Leon Mirsky

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Igor Shafarevich

Springer Science+Business Media
For some of its journals, Springer does not require its authors to transfer their copyrights, and allows them to decide whether their articles are published under an open-access licence or in the traditional restricted licence model.[13] While open-access publishing typically requires the author to pay a fee for copyright retention, this fee is sometimes covered by a third party. For example, a national institution in Poland allows authors to publish in open-access journals without incurring any personal cost - but using public funds.[14] Springer is a member of the Open Access Scholarly Publishers Association.AuthorMapper is a free online tool for visualizing scientific research that enables document discovery based on author locations and geographic maps. The tool helps users explore patterns in scientific research, identify literature trends, discover collaborative relationships, and locate experts in several scientific/medical fields.SpringerMaterials was launched in 2009 and is a platform for accessing the Landolt-Börnstein database of research and information on materials and their properties.SpringerImages was launched in 2008 and offers a collection of currently 1.8 million images spanning science, technology, and medicine.SpringerProtocols is home to a collection of protocols, recipes which provide step-by-step instructions for conducting experiments in research labs.Springer provides its electronic book and journal content on its SpringerLink site, which launched in 1996.In 2015, Springer retracted 64 of the papers it had published after it was found that they had gone through a fraudulent peer review process.[12]In 2014, it was revealed that Springer had published 16 fake papers in its journals that had been computer-generated using SCIgen. Springer subsequently removed all the papers from these journals. IEEE had also done the same thing by removing more than 100 fake papers from its conference proceedings.[11]In 2013, the London-based private equity firm BC Partners acquired a majority stake in Springer from EQT and GIC for $4.4 billion.[10]In 2011, Springer acquired Pharma Marketing and Publishing Services from Wolters Kluwer.[9]The closing of the sale was confirmed in February 2010 after the competition authorities in the USA and in Europe approved the transfer.In 2009, Cinven and Candover sold Springer to two private equity firms, EQT Partners and Government of Singapore Investment Corporation.Springer acquired the open-access publisher BioMed Central in October 2008 for an undisclosed amount.The academic publishing company BertelsmannSpringer was formed after Bertelsmann bought a majority stake in Springer-Verlag in 1999.[5][7] The British investment groups Cinven and Candover bought BertelsmannSpringer from Bertelsmann in 2003.[7] They merged the company in 2004 with the Dutch publisher Kluwer Academic Publishers which they bought from Wolters Kluwer in 2002,[8] to form Springer Science+Business Media.Julius Springer founded Springer-Verlag in Berlin in 1842 and his son Ferdinand Springer grew it from a small firm of 4 employees into Germany's second largest academic publisher with 65 staff in just 30 years.[5][6] In 1964, Springer expanded its business internationally, opening an office in New York City. Offices in Tokyo, Paris, Milan, Hong Kong, and Delhi soon followed.On 15 January 2015, Holtzbrinck Publishing Group / Nature Publishing Group and Springer Science+Business Media announced a merger.[3] The transaction was concluded in May 2015 and a new joint venture company, Springer Nature, was formed, with Holtzbrinck having the majority 53% share and BC Partners retaining 47% interest in the company.[4]Springer Science+Business Media or Springer, part of Springer Nature since 2015, is a global publishing company that publishes books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing.[1] Springer also hosts a number of scientific databases, including SpringerLink, Springer Protocols, and SpringerImages. Book publications include major reference works, textbooks, monographs and book series; more than 168,000 titles are available as e-books in 24 subject collections.[2] Springer has major offices in Berlin, Heidelberg, Dordrecht, and New York City.
International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Georgiy Shilov

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

Michiel Hazewinkel

Encyclopedia of Mathematics
A new dynamic version of the encyclopedia is now available as a public wiki online.[2] This new wiki is a collaboration between Springer and the European Mathematical Society. This new version of the encyclopedia includes the entire contents of the previous online version, but all entries can now be publicly updated to include the newest advancements in mathematics. All entries will be monitored for content accuracy by members of an editorial board[3] selected by the European Mathematical Society.Until November 29, 2011, a static version of the encyclopedia could be browsed online free of charge online.[1] This URL now redirects to the new wiki incarnation of the EOM.The encyclopedia has been translated from the Soviet Matematicheskaya entsiklopediya (1977) originally edited by Ivan Matveevich Vinogradov and extended with comments and three supplements adding several thousand articles.The 2002 version contains more than 8,000 entries covering most areas of mathematics at a graduate level, and the presentation is technical in nature. The encyclopedia is edited by Michiel Hazewinkel and was published by Kluwer Academic Publishers until 2003, when Kluwer became part of Springer. The CD-ROM contains animations and three-dimensional objects.The Encyclopedia of Mathematics (also EOM and formerly Encyclopaedia of Mathematics) is a large reference work in mathematics. It is available in book form and on CD-ROM.
International Standard Book Number
Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[48]Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[47] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[46] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[45]Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[44] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[43] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.ThenLetIn general, the ISBN-13 check digit is calculated as follows.Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.Formally, using modular arithmetic, we can say:The 2005 edition of the International ISBN Agency's official manual[42] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:Thus the check digit is 2.For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[41]Formally, we can say:It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:Formally, using modular arithmetic, we can say:For example, for an ISBN-10 of 0-306-40615-2:The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[40] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[39]By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[38] Here are some sample ISBN-10 codes, illustrating block length variations.Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[37] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[12] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[36] The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[12] Registration group identifiers have primarily been allocated within the 978 prefix element.[33] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[34] Books published in rare languages typically have longer group identifiers.[35]A full directory of ISBN agencies is available on the International ISBN Agency website.[16] Partial listing:ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[15]A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[14]An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[11] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[10]An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[4][5] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[9]The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[3] for the booksellers and stationers WHSmith and others in 1965.[4] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[5] (regarded as the "Father of the ISBN"[6]) and in 1968 in the US by Emery Koltay[5] (who later became director of the U.S. ISBN agency R.R. Bowker).[6][7][8]Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.Privately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.[2]The initial ISBN configuration of recognition[clarification needed] was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.[1]
Book sources

Edit this page
You can also convert between 10 and 13 digit ISBN numbers with these tools:
You can look up ISBNs for different editions of the same book, hardback or paperback, first print or a reprint, even re-editions where the title has changed using xISBN. xISBN's linkages are determined algorithmically, based on the concepts of the Functional Requirements for Bibliographic Records.
These links produce citations in various referencing styles.
If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language.


Google Books and Amazon.com may be particularly helpful if you want to verify citations in Wikipedia articles, because they often enable you to search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available).
This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). 

MathWorld
This case made a wave of headlines in online publishing circles. The PlanetMath project was a result of MathWorld's being unavailable.[7]The case was later settled out of court, with WRI paying an unspecified amount and complying with other stipulations. Among these stipulations is the inclusion of a copyright notice at the bottom of the website and broad rights for the CRC Press to produce MathWorld in printed book form. The site then became once again available free to the public.In 2000, CRC Press sued Wolfram Research Inc. (WRI), WRI president Stephen Wolfram, and author Eric Weisstein, due to what they considered a breach of contract: that the MathWorld content was to remain in print only. The site was taken down by a court injunction.[6]Eric W. Weisstein, the creator of the site, was a physics and astronomy student who got into the habit of writing notes on his mathematical readings. In 1995 he put his notes online and called it "Eric's Treasure Trove of Mathematics." It contained hundreds of pages/articles, covering a wide range of mathematical topics. The site became popular as an extensive single resource on mathematics on the web. Weisstein continuously improved the notes and accepted corrections and comments from online readers. In 1998, he made a contract with CRC Press and the contents of the site were published in print and CD-ROM form, titled "CRC Concise Encyclopedia of Mathematics." The free online version became only partially accessible to the public. In 1999 Weisstein went to work for Wolfram Research, Inc. (WRI), and WRI renamed the Math Treasure Trove to MathWorld and hosted it on the company's website[5] without access restrictions.MathWorld is an online mathematics reference work, created and largely written by Eric W. Weisstein. It is sponsored by and licensed to Wolfram Research, Inc. and was partially funded by the National Science Foundation's National Science Digital Library grant to the University of Illinois at Urbana–Champaign.
Template:Linear algebra

Template talk:Linear algebra

Scalar (mathematics)
Operations that apply to a single value at a time.The scalar multiplication of vector spaces and modules is a special case of scaling, a kind of linear transformation.In this case the "scalars" may be complicated objects. For instance, if R is a ring, the vectors of the product space Rn can be made into a module with the n×n matrices with entries from R as the scalars. Another example comes from manifold theory, where the space of sections of the tangent bundle forms a module over the algebra of real functions on the manifold.When the requirement that the set of scalars form a field is relaxed so that it need only form a ring (so that, for example, the division of scalars need not be defined, or the scalars need not be commutative), the resulting more general algebraic structure is called a module.The norm is usually defined to be an element of V's scalar field K, which restricts the latter to fields that support the notion of sign. Moreover, if V has dimension 2 or more, K must be closed under square root, as well as the four arithmetic operations; thus the rational numbers Q are excluded, but the surd field is acceptable. For this reason, not every scalar product space is a normed vector space.Alternatively, a vector space V can be equipped with a norm function that assigns to every vector v in V a scalar ||v||. By definition, multiplying v by a scalar k also multiplies its norm by |k|. If ||v|| is interpreted as the length of v, this operation can be described as scaling the length of v by k. A vector space equipped with a norm is called a normed vector space (or normed linear space).According to a fundamental theorem of linear algebra, every vector space has a basis. It follows that every vector space over a scalar field K is isomorphic to a coordinate vector space where the coordinates are elements of K. For example, every real vector space of dimension n is isomorphic to n-dimensional real space Rn.The scalars can be taken from any field, including the rational, algebraic, real, and complex numbers, as well as finite fields.According to a citation in the Oxford English Dictionary the first recorded usage of the term "scalar" in English came with W. R. Hamilton in 1846, referring to the real part of a quaternion:The word scalar derives from the Latin word scalaris, an adjectival form of scala (Latin for "ladder"), from which the English word scale also comes. The first recorded usage of the word "scalar" in mathematics occurs in François Viète's Analytic Art (In artem analyticem isagoge) (1591):[5][page needed][6]The term scalar matrix is used to denote a matrix of the form kI where k is a scalar and I is the identity matrix.The term is also sometimes used informally to mean a vector, matrix, tensor, or other usually "compound" value that is actually reduced to a single component. Thus, for example, the product of a 1×n matrix and an n×1 matrix, which is formally a 1×1 matrix, is often said to be a scalar.The real component of a quaternion is also called its scalar part.A scalar product operation – not to be confused with scalar multiplication – may be defined on a vector space, allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an inner product space.In linear algebra, real numbers or other elements of a field are called scalars and relate to vectors in a vector space through the operation of scalar multiplication, in which a vector can be multiplied by a number to produce another vector.[2][3][4] More generally, a vector space may be defined by using any field instead of real numbers, such as complex numbers. Then the scalars of that vector space will be the elements of the associated field.A scalar is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector.[1]
Euclidean vector
This distinction between vectors and pseudovectors is often ignored, but it becomes important in studying symmetry properties. See parity (physics).One example of a pseudovector is angular velocity. Driving in a car, and looking forward, each of the wheels has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the reflection of this angular velocity vector points to the right, but the actual angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include magnetic field, torque, or more generally any cross product of two (true) vectors.Some vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip and gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the orientation of space. A vector which gains a minus sign when the orientation of space changes is called a pseudovector or an axial vector. Ordinary vectors are sometimes called true vectors or polar vectors to distinguish them from pseudovectors. Pseudovectors occur most frequently as the cross product of two ordinary vectors.In the language of differential geometry, the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a contravariant vector to be a tensor of contravariant rank one. Alternatively, a contravariant vector is defined to be a tangent vector, and the rules for transforming a contravariant vector follow from the chain rule.Therefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely asWork is the dot product of force and displacementForce is a vector with dimensions of mass×length/time2 and Newton's second law is the scalar multiplicationAcceleration a of a point is vector which is the time derivative of velocity. Its dimensions are length/time2.where x0 is the position at time t=0. Velocity is the time derivative of position. Its dimensions are length/time.The velocity v of a point or particle is a vector, its length gives the speed. For constant velocity the position at time t will bewhich specifies the position of y relative to x. The length of this vector gives the straight-line distance from x to y. Displacement has the dimensions of length.Given two points x = (x1, x2, x3), y = (y1, y2, y3) their displacement is a vectorThe position vector has dimensions of length.The position of a point x = (x1, x2, x3) in three-dimensional space can be represented as a position vector whose base point is the originOften in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter t. For instance, if r represents the position vector of a particle, then r(t) gives a parametric representation of the trajectory of the particle. Vector-valued functions can be differentiated and integrated by differentiating or integrating the components of the vector, and many of the familiar rules from calculus continue to hold for the derivative and integral of vector-valued functions.In abstract vector spaces, the length of the arrow depends on a dimensionless scale. If it represents, for example, a force, the "scale" is of physical dimension length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if "1 newton" and "5 m" are both represented with an arrow of 2 cm, the scales are 1:250 and 1 m:50 N respectively. Equal length of vectors of different dimension has no particular significance unless there is some proportionality constant inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.Vectors have many uses in physics and other sciences.By applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.[13]The advantage of this method is that a direction cosine matrix can usually be obtained independently by using Euler angles or a quaternion to relate the two vector bases, so the basis conversions can be performed directly, without having to work out all the dot products described above.The properties of a direction cosine matrix, C are [[14]]:By referring collectively to e1, e2, e3 as the e basis and to n1, n2, n3 as the n basis, the matrix containing all the cjk is known as the "transformation matrix from e to n", or the "rotation matrix from e to n" (because it can be imagined as the "rotation" of a vector from one basis to another), or the "direction cosine matrix from e to n"[13] (because it contains direction cosines). The properties of a rotation matrix are such that its inverse is equal to its transpose. This means that the "rotation matrix from e to n" is the transpose of "rotation matrix from n to e".This matrix equation relates the scalar components of a in the n basis (u,v, and w) with those in the e basis (p, q, and r). Each matrix element cjk is the direction cosine relating nj to ek.[13] The term direction cosine refers to the cosine of the angle between two unit vectors, which is also equal to their dot product.[13] Therefore,and these equations can be expressed as the single matrix equationReplacing each dot product with a unique scalar givesDistributing the dot-multiplication givesThe values of p, q, r, and u, v, w relate to the unit vectors in such a way that the resulting vector sum is exactly the same physical vector a in both cases. It is common to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle). In such a case it is necessary to develop a method to convert between bases so the basic vector operations such as addition and subtraction can be performed. One way to express u, v, w in terms of p, q, r is to use column matrices along with a direction cosine matrix containing the information that relates the two bases. Such an expression can be formed by substitution of the above equations to formand the scalar components in the n basis are, by definition,In another orthnormal basis n = {n1, n2, n3} that is not necessarily aligned with e, the vector a is expressed asThe scalar components in the e basis are, by definition,All examples thus far have dealt with vectors expressed in terms of the same basis, namely, the e basis {e1, e2, e3}. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector. In the e basis, a vector a is expressed, by definition, asThe scalar triple product is linear in all three entries and anti-symmetric in the following sense:In components (with respect to a right-handed orthonormal basis), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the determinant of the 3-by-3 matrix having the three vectors as rowsIt has three primary uses. First, the absolute value of the box product is the volume of the parallelepiped which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are linearly dependent, which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors a, b and c are right-handed.The scalar triple product (also called the box product or mixed triple product) is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by (a b c) and defined as:For arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector (see below).The cross product can be written asThe length of a × b can be interpreted as the area of the parallelogram having a and b as sides.The cross product a × b is defined so that a, b, and a × b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.where θ is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist two unit vectors that are perpendicular to both a and b, namely, n and (–n).The cross product (also called the vector product or outer product) is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a × b, is a vector perpendicular to both a and b and is defined asThe dot product can also be defined as the sum of the products of the components of each vector aswhere θ is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of the component of b that points in the same direction as a.The dot product of two vectors a and b (sometimes called the inner product, or, since its result is a scalar, the scalar product) is denoted by a ∙ b and is defined as:If r is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples (r = −1 and r = 2) are given below:Intuitively, multiplying by a scalar r stretches a vector out by a factor of r. Geometrically, this can be visualized (at least in the case when r is an integer) as placing r copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.A vector may also be multiplied, or re-scaled, by a real number r. In the context of conventional vector algebra, these real numbers are often called scalars (from scale) to distinguish them from vectors. The operation of multiplying a vector by a scalar is called scalar multiplication. The resulting vector isSubtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a − b, as illustrated below:The difference of a and b isThe addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b isTo normalize a vector a = [a1, a2, a3], scale the vector by the reciprocal of its length ‖a‖. That is:A unit vector is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as normalizing a vector. A unit vector is often indicated with a hat as in â.This happens to be equal to the square root of the dot product, discussed below, of the vector with itself:which is a consequence of the Pythagorean theorem since the basis vectors e1, e2, e3 are orthogonal unit vectors.The length of the vector a can be computed with the Euclidean normThe length or magnitude or norm of the vector a is denoted by ‖a‖ or, less commonly, |a|, which is not to be confused with the absolute value (a scalar "norm").Two vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.are opposite ifandTwo vectors are opposite if they have the same magnitude but opposite direction. So two vectorsare equal ifandTwo vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectorsand assumes that all vectors have the origin as a common base point. A vector a will be written asThe following section uses the Cartesian coordinate system with basis vectorsIn these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a global coordinate system, or inertial reference frame).A vector can also be broken up with respect to "non-fixed" basis vectors that change their orientation as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively normal, and tangent to a surface (see figure). Moreover, the radial and tangential components of a vector relate to the radius of rotation of an object. The former is parallel to the radius and the latter is orthogonal to it.[12]The choice of a basis doesn't affect the properties of a vector or its behaviour under transformations.The decomposition or resolution[11] of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.As explained above a vector is often described by a set of vector components that add up to form the given vector. Typically, these components are the projections of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be decomposed or resolved with respect to that set.The notation ei is compatible with the index notation and the summation convention commonly used in higher level mathematics, physics, and engineering.where a1, a2, a3 are called the vector components (or vector projections) of a on the basis vectors or, equivalently, on the corresponding Cartesian axes x, y, and z (see figure), while a1, a2, a3 are the respective scalar components (or scalar projections).orThese have the intuitive interpretation as vectors of unit length pointing up the x, y, and z axis of a Cartesian coordinate system, respectively. In terms of these, any vector a in R3 can be expressed in the form:Another way to represent a vector in n-dimensions is to introduce the standard basis vectors. For instance, in three dimensions, there are three of them:These numbers are often arranged into a column vector or row vector, particularly when dealing with matrices, as follows:This can be generalised to n-dimensional Euclidean space (or Rn).In three dimensional Euclidean space (or R3), vectors are identified with triples of scalar components:As an example in two dimensions (see figure), the vector from the origin O = (0,0) to the point A = (2,3) is simply written asIn order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an n-dimensional Euclidean space can be represented as coordinate vectors in a Cartesian coordinate system. The endpoint of a vector can be identified with an ordered list of n real numbers (n-tuple). These numbers are the coordinates of the endpoint of the vector, with respect to a given Cartesian coordinate system, and are typically called the scalar components (or scalar projections) of the vector on the axes of the coordinate system.On a two-dimensional diagram, sometimes a vector perpendicular to the plane of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 ⊙) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 ⊗) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an arrow head on and viewing the flights of an arrow from the back.Vectors are usually shown in graphs or other diagrams as arrows (directed line segments), as illustrated in the figure. Here the point A is called the origin, tail, base, or initial point; point B is called the head, tip, endpoint, terminal point or final point. The length of the arrow is proportional to the vector's magnitude, while the direction in which the arrow points indicates the vector's direction.In pure mathematics, a vector is any element of a vector space over some field and is often represented as a coordinate vector. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has "magnitude and direction".In physics, as well as mathematics, a vector is often identified with a tuple of components, or list of numbers, that act as scalar coefficients for a set of basis vectors. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called covariant or contravariant depending on how the transformation of the vector's components is related to the transformation of the basis. In general, contravariant vectors are "regular vectors" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as gradient. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1 m becomes 1000 mm–a contravariant change in numerical value. In contrast, a gradient of 1 K/m becomes 0.001 K/mm–a covariant change in value. See covariance and contravariance of vectors. Tensors are another type of quantity that behave in this way; a vector is one type of tensor.However, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of vector spaces (for free vectors) and affine spaces (for bound vectors, as each represented by an ordered pair of "points"). An important example is Minkowski space that is important to our understanding of special relativity, where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from thermodynamics, where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.[10]In the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a length or magnitude and a direction to vectors. In addition, the notion of direction is strictly associated with the notion of an angle between two vectors. If the dot product of two vectors is defined — a scalar-valued product of two vectors —, then it's also possible to define a length; the dot product gives a convenient algebraic characterization of both angle (a function of the dot product between any two non-zero vectors) and length (the square root of the dot product of a vector by itself). In three dimensions, it is further possible to define the cross product, which supplies an algebraic characterization of the area and orientation in space of the parallelogram defined by two vectors (used as sides of the parallelogram). In any dimension (and, in particular, higher dimensions), it's possible to define the exterior product, which (among other things) supplies an algebraic characterization of the area and orientation in space of the n-dimensional parallelotope defined by n vectors.This coordinate representation of free vectors allows their algebraic features to be expressed in a convenient numerical fashion. For example, the sum of the two (free) vectors (1,2,3) and (−2,0,4) is the (free) vectorIn Cartesian coordinates a free vector may be thought of in terms of a corresponding bound vector, in this sense, whose initial point has the coordinates of the origin O = (0,0,0). It is then determined by the coordinates of that bound vector's terminal point. Thus the free vector represented by (1,0,0) is a vector of unit length pointing along the direction of the positive x-axis.Vectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is velocity, the magnitude of which is speed. For example, the velocity 5 meters per second upward could be represented by the vector (0,5) (in 2 dimensions with the positive y axis as 'up'). Another quantity represented by a vector is force, since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, displacement, linear acceleration, angular acceleration, linear momentum, and angular momentum. Other physical vectors, such as the electric and magnetic field, are represented as a system of vectors at each point of a physical space; that is, a vector field. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition: Angular displacement and electric current. Consequently, these are not vectors.Since the physicist's concept of force has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force F of 15 newtons. If the positive axis is also directed rightward, then F is represented by the vector 15 N, and if positive points leftward, then the vector for F is −15 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement Δs of 4 meters would be 4 m or −4 m, depending on its direction, and its magnitude would be 4 m regardless.The term vector also has generalizations to higher dimensions and to more formal approaches with much wider applications.This article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as geometric, spatial, or Euclidean vectors.In physics and engineering, a vector is typically regarded as a geometric entity characterized by a magnitude and a direction. It is formally defined as a directed line segment, or arrow, in a Euclidean space.[8] In pure mathematics, a vector is defined more generally as any element of a vector space. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called Euclidean space.Josiah Willard Gibbs, who was exposed to quaternions through James Clerk Maxwell's Treatise on Electricity and Magnetism, separated off their vector part for independent treatment. The first half of Gibbs's Elements of Vector Analysis, published in 1881, presents what is essentially the modern system of vector analysis.[6] In 1901 Edwin Bidwell Wilson published Vector Analysis, adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus.In 1878 Elements of Dynamic was published by William Kingdon Clifford. Clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.Peter Guthrie Tait carried the quaternion standard after Hamilton. His 1867 Elementary Treatise of Quaternions included extensive treatment of the nabla or del operator ∇.Several other mathematicians developed vector-like systems in the middle of the nineteenth century, including Augustin Cauchy, Hermann Grassmann, August Möbius, Comte de Saint-Venant, and Matthew O'Brien. Grassmann's 1840 work Theorie der Ebbe und Flut (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.[6]The term vector was introduced by William Rowan Hamilton as part of a quaternion, which is a sum q = s + v of a Real number s (also called scalar) and a 3-dimensional vector. Like Bellavitis, Hamilton viewed vectors as representative of classes of equipollent directed segments. As complex numbers use an imaginary unit to complement the real line, Hamilton considered the vector v to be the imaginary part of a quaternion:Giusto Bellavitis abstracted the basic idea in 1835 when he established the concept of equipollence. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an equivalence relation on the pairs of points (bipoints) in the plane and thus erected the first space of vectors in the plane.[6]:52–4The concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.[6]Vectors play an important role in physics: the velocity and acceleration of a moving object and the forces acting on it can all be described with vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can still be represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.A vector is what is needed to "carry" the point A to the point B; the Latin word vector means "carrier".[4] It was first used by 18th century astronomers investigating planet rotation around the Sun.[5] The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from A to B. Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.
Vector space
The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.generalizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x ∈ W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v ∈ V. An important example is the space of solutions of a system of inhomogeneous linear equationsRoughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapModules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[103] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 × R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.such that for every x in X, the fiber π−1(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X × V → X makes the product X × V into a "trivial" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of π to π−1(U) is isomorphic[nb 16] to the trivial bundle U × V → U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X × V). For example, the Möbius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 × R, because the latter is orientable whereas the former is not.[99]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous mapRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).[92][93]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2π), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isWhen Ω = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function f its value at the p: δ(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).[77]A distribution (or generalized function) is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain Ω:Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]When a field, F is explicitly stated, a common term used is F-algebra.The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 ⊗ v2 and v2 ⊗ v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 ⊗ v2 = − v2 ⊗ v1 yields the exterior algebra.[73]The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsExamples include the vector space of n-by-n matrices, with [x, y] = xy − yx, the commutator of two matrices, and R3, endowed with the cross product.Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone–Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61] The Hilbert space L2(Ω), with inner product given byImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]there exists a function f(x) belonging to the vector space Lp(Ω) such thatThe space of integrable functions on a given domain Ω (for example an interval) satisfying |f|p < ∞, and equipped with this norm are called Lebesgue spaces, denoted Lp(Ω).[nb 10] These spaces are complete.[59] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < ∞, satisfying the conditionMore generally than sequences of real numbers, functions f: Ω → R are endowed with a norm that replaces the above sum by the Lebesgue integralis finite. The topologies on the infinite-dimensional space ℓ p are inequivalent for different p. E.g. the sequence of vectors xn = (2−n, 2−n, ..., 2−n, 0, 0, ...), i.e. the first 2n components are 2−n, the following ones are 0, converges to the zero vector for p = ∞, but does not for p = 1:Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] A first example is the vector space ℓ p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 ≤ p ≤ ∞) given byFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V → W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V∗ consists of continuous functionals V → R (or to C). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and ∞-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifdenotes the limit of the corresponding finite partial sums of the sequence (fi)i∈N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.In such topological vector spaces one can consider series of vectors. The infinite sumConvergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Coordinate space Fn can be equipped with the standard dot product:where f+ denotes the positive part of f and f− the negative part.[48]A vector space may be given a partial order ≤, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionsFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.These rules ensure that the map f from the V × W to V ⊗ W that maps a tuple (v, w) to v ⊗ w is bilinear. The universality states that given any vector space X and any bilinear map g : V × W → X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v ⊗ w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.subject to the rulesThe tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorsThe tensor product V ⊗F W, or simply V ⊗ W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g : V × W → X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v ↦ g(v, w) is linear in the sense above and likewise for fixed v.The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.the derivatives of the function f appear linearly (as opposed to f′′(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)′ = f′ + g ′ and (c·f)′ = c·f′ for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).In the corresponding mapAn important example is the kernel of a linear map x ↦ Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.The kernel ker(f) of a linear map f : V → W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v) : v ∈ V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W ⊂ V, the quotient space V/W ("V modulo W") is defined as follows: as a set, it consists of v + W = {v + w : w ∈ W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a · (v + W) = (a · v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space "forgets" information that is contained in the subspace W.A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n – 1.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in λ, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.Endomorphisms, linear maps f : V → V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying λv = f(v), where λ is a scalar, is called an eigenvector of f with eigenvalue λ.[nb 5][35] Equivalently, v is an element of the kernel of the difference f − λ · Id (where Id is the identity map V → V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue λ is equivalent toThe determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Moreover, after choosing bases of V and W, any linear map f : V → W is uniquely represented by a matrix via this assignment.[33]or, using the matrix multiplication of the matrix A with the coordinate vector x:Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingOnce a basis of V is chosen, linear maps f : V → W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no "canonical" or preferred isomorphism; actually an isomorphism φ : Fn → V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via φ. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.Linear maps V → W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V∗.[28] Via the injective natural map V → V∗∗, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.An isomorphism is a linear map f : V → W such that there exists an inverse map g : W → V, which is a map such that the two possible compositions f ∘ g : W → W and g ∘ f : V → V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(α) over Q depends on α. If α satisfies some polynomial equation The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e−x and xe−x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i ∈ I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:yields f(x) = a e−x + bx e−x, where a and b are arbitrary constants, and ex is the natural exponential function.are given by triples with arbitrary a, b = a/2, and c = −5a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelySystems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions ofand similarly for multiplication. Such function spaces occur in many geometric situations, when Ω is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Functions from any fixed set Ω to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byIn fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c ⋅ (x + iy) = (c ⋅ x) + i(c ⋅ y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such asAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by Möbius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse −v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.Subtraction of two vectors and division by a (non-zero) scalar can be defined asLikewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:To qualify as a vector space, the set V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.Elements of V are commonly called vectors. Elements of F are commonly called scalars.A vector space over a field F is a set V together with two operations that satisfy the eight axioms listed below.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.andA second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (−1)v = −v has the opposite direction and the same length as v (blue vector pointing down in the right image).The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The concept of vector space will first be explained by describing two particular examples:Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.
Scalar multiplication
where i, j, k are the quaternion units. The non-commutativity of quaternion multiplication prevents the transition of changing ij = +k to ji = −k.For quaternion scalars and matrices:For a real scalar and matrix:When the underlying ring is commutative, for example, the real or complex number field, these two multiplications are the same, and are simply called scalar multiplication. However, for matrices over a more general ring that are not commutative, such as the quaternions, they may not be equal.explicitly:Similarly, the right scalar multiplication of a matrix A with a scalar λ is defined to beexplicitly:The left scalar multiplication of a matrix A with a scalar λ gives another matrix λA of the same size as A. The entries of λA are defined byThe same idea applies if K is a commutative ring and V is a module over K. K can even be a rig, but then there is no additive inverse. If K is not commutative, the distinct operations left scalar multiplication cv and right scalar multiplication vc may be defined.When V is Kn, scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such.As a special case, V may be taken to be K itself and scalar multiplication may then be taken to be simply the multiplication in the field.Scalar multiplication may be viewed as an external binary operation or as an action of the field on the vector space. A geometric interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor.Here + is addition either in the field or in the vector space, as appropriate; and 0 is the additive identity in either. Juxtaposition indicates either scalar multiplication or the multiplication operation in the field.Scalar multiplication obeys the following rules (vector in boldface):In general, if K is a field and V is a vector space over K, then scalar multiplication is a function from K × V to V. The result of applying this function to c in K and v in V is denoted cv.In mathematics, scalar multiplication is one of the basic operations defining a vector space in linear algebra[1][2][3] (or more generally, a module in abstract algebra[4][5]). In common geometrical contexts, scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction. The term "scalar" itself derives from this usage: a scalar is that which scales vectors. Scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and must be distinguished from inner product of two vectors (where the product is a scalar).
Vector projection
In geometric algebra, they can be further generalized to the notions of projection and rejection of a general multivector onto/from any invertible k-blade.Similarly, for inner product spaces with more than three dimensions, the notions of projection onto a vector and rejection from a vector can be generalized to the notions of projection onto a hyperplane, and rejection from a hyperplane.For a three-dimensional inner product space, the notions of projection of a vector onto another and rejection of a vector from another can be generalized to the notions of projection of a vector onto a plane, and rejection of a vector from a plane.[4] The projection of a vector on a plane is its orthogonal projection on that plane. The rejection of a vector from a plane is its orthogonal projection on a straight line which is orthogonal to that plane. Both are vectors. The first is parallel to the plane, the second is orthogonal. For a given vector and plane, the sum of projection and rejection is equal to the original vector.Since the notions of vector length and angle between vectors can be generalized to any n-dimensional inner product space, this is also true for the notions of orthogonal projection of a vector, projection of a vector onto another, and rejection of a vector from another. In some cases, the inner product coincides with the dot product. Whenever they don't coincide, the inner product is used instead of the dot product in the formal definitions of projection and rejection.The vector projection is an important operation in the Gram–Schmidt orthonormalization of vector space bases. It is also used in the Separating axis theorem to detect whether two convex shapes intersect.The orthogonal projection can be represented by a projection matrix. To project a vector onto the unit vector a = (ax, ay, az), it would need to be multiplied with this projection matrix:The vector rejection of a on b is a vector a2 which is either null or orthogonal to b. More exactly:The vector projection of a on b is a vector a1 which is either null or parallel to b. More exactly:The scalar projection a on b is a scalar which has a negative sign if 90 < θ ≤ 180 degrees. It coincides with the length |c| of the vector projection if the angle is smaller than 90°. More exactly:Hence,By definition,The latter formula is computationally more efficient than the former. Both require two dot products and eventually the multiplication of a scalar by a vector, but the former additionally requires a square root and the division of a vector by a scalar,[3] while the latter additionally requires only the division of a scalar by a scalar.or[2]which is equivalent to eitherSimilarly, the definition of the vector projection of a onto b becomesBy the above-mentioned property of the dot product, the definition of the scalar projection becomesWhen θ is not known, the cosine of θ can be computed in terms of a and b, by the following property of the dot product a · b:Hence,By definition, the vector rejection of a on b isThe vector projection of a on b is a vector whose magnitude is the scalar projection of a on b and whose angle against b is either 0 or 180 degrees. Namely, it is defined asA scalar projection can be used as a scale factor to compute the corresponding vector projection.where θ is the angle between a and b.The scalar projection of a on b is a scalar equal toThe vector projection of a on b and the corresponding rejection are sometimes denoted by a∥b and a⊥b, respectively.The vector component or vector resolute of a perpendicular to b, sometimes also called the vector rejection of a from b,[1] is the orthogonal projection of a onto the plane (or, in general, hyperplane) orthogonal to b. Both the projection a1 and rejection a2 of a vector a are vectors, and their sum is equal to a, which implies that the rejection is given bywhere the operator · denotes a dot product, |a| is the length of a, and θ is the angle between a and b. The scalar projection is equal to the length of the vector projection, with a minus sign if the direction of the projection is opposite to the direction of b.The vector projection of a vector a on (or onto) a nonzero vector b (also known as the vector component or vector resolution of a in the direction of b) is the orthogonal projection of a onto a straight line parallel to b. It is a vector parallel to b, defined as
Linear span
(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)Let X be a normed space and let E be any non-empty subset of X. ThenClosed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma).The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.The closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.One mathematical formulation of this isIn functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,…,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,…,an of A, then the sum of cyclic modules,Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].This also indicates that a basis is a minimal spanning set when V is finite-dimensional.Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.This theorem is so well known that at times it is referred to as the definition of span of a set.Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.The set of functions xn where n is a non-negative integer spans the space of polynomials.The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.Another spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all subspaces containing that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.
Linear map
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank–nullity theorem:If f : V → W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n × n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol ∗ denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f : V → W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.
Projection (linear algebra)
As stated above, projections are a special case of idempotents. Analytically, orthogonal projections are non-commutative generalizations of characteristic functions. Idempotents are used in classifying, for instance, semisimple algebras, while measure theory begins with considering characteristic functions of measurable sets. Therefore, as one can imagine, projections are very often encountered in the context operator algebras. In particular, a von Neumann algebra is generated by its complete lattice of projections.Projections (orthogonal and otherwise) play a major role in algorithms for certain linear algebra problems:The above argument makes use of the assumption that both U and V are closed. In general, given a closed subspace U, there need not exist a complementary closed subspace V, although for Hilbert spaces this can always be done by taking the orthogonal complement. For Banach spaces, a one-dimensional subspace always has a closed complementary subspace. This is an immediate consequence of Hahn–Banach theorem. Let U be the linear span of u. By Hahn–Banach, there exists a bounded linear functional φ such that φ(u) = 1. The operator P(x) = φ(x)u satisfies P2 = P, i.e. it is a projection. Boundedness of φ implies continuity of P and therefore ker(P) = ran(I − P) is a closed complementary subspace of U.The converse holds also, with an additional assumption. Suppose U is a closed subspace of X. If there exists a closed subspace V such that X = U ⊕ V, then the projection P with range U and kernel V is continuous. This follows from the closed graph theorem. Suppose xn → x and Pxn → y. One needs to show that Px = y. Since U is closed and {Pxn} ⊂ U, y lies in U, i.e. Py = y. Also, xn − Pxn = (I − P)xn → x − y. Because V is closed and {(I − P)xn} ⊂ V, we have x − y ∈ V, i.e. P(x − y) = Px − Py = Px − y = 0, which proves the claim.where r is the rank of P. Here Ir is the identity matrix of size r, and 0d−r is the zero matrix of size d − r. If the vector space is complex and equipped with an inner product, then there is an orthonormal basis in which the matrix of P is[11]Any projection P = P2 on a vector space of dimension d over a field is a diagonalizable matrix, since its minimal polynomial divides x2 − x, which splits into distinct linear factors. Thus there exists a basis in which P has the formThis expression generalizes the formula for orthogonal projections given above.[9][10]Projections are defined by their null space and the basis vectors used to characterize their range (which is the complement of the null space). When these basis vectors are orthogonal to the null space, then the projection is an orthogonal projection. When these basis vectors are not orthogonal to the null space, the projection is an oblique projection. Let the vectors u1, ..., uk form a basis for the range of the projection, and assemble these vectors in the n-by-k matrix A. The range and the null space are complementary spaces, so the null space has dimension n − k. It follows that the orthogonal complement of the null space has dimension k. Let v1, ..., vk form a basis for the orthogonal complement of the null space of the projection, and assemble these vectors in the matrix B. Then the projection is defined byThe term oblique projections is sometimes used to refer to non-orthogonal projections. These projections are also used to represent spatial figures in two-dimensional drawings (see oblique projection), though not as frequently as orthogonal projections. Whereas calculating the fitted value of an ordinary least squares regression requires an orthogonal projection, the calculating the fitted value of an instrumental variables regression requires an oblique projection.All these formulas also hold for complex inner product spaces, provided that the conjugate transpose is used instead of the transpose. Further details on sums of projectors can be found in Banerjee and Roy (2014).[8]If the orthogonal condition is enhanced to AT W B = AT WT B = 0 with W non-singular, the following holds:The orthonormality condition can also be dropped. If u1, ..., uk is a (not necessarily orthonormal) basis, and A is the matrix with these vectors as columns, then the projection is:[5][6]The matrix AT is the partial isometry that vanishes on the orthogonal complement of U and A is the isometry that embeds U into the underlying vector space. The range of PA is therefore the final space of A. It is also clear that A·AT is the identity operator on U.which can be rewritten asThis formula can be generalized to orthogonal projections on a subspace of arbitrary dimension. Let u1, ..., uk be an orthonormal basis of the subspace U, and let A denote the n-by-k matrix whose columns are u1, ..., uk. Then the projection is given by:[4]by the properties of the dot product of parallel and perpendicular vectors.A simple case occurs when the orthogonal projection is onto a line. If u is a unit vector on the line, then the projection is given by the outer productAn orthogonal projection is a bounded operator. This is because for every v in the vector space we have, by Cauchy–Schwarz inequality:for every x and y in W; thus P = P*.A projection is orthogonal if and only if it is self-adjoint. Using the self-adjoint and idempotent properties of P, for any x and y in W we have Px ∈ U, y − Py ∈ V, andThe product of projections is not, in general, a projection, even if they are orthogonal. If projections commute, then their product is a projection.In infinite dimensional vector spaces, the spectrum of a projection is contained in {0, 1} asLet W be a finite dimensional vector space and P be a projection on W. Suppose the subspaces U and V are the range and kernel of P respectively. Then P has the following properties:The projection P is orthogonal if and only if α = 0.proving that P is indeed a projection.Via matrix multiplication, one sees thatA simple example of a non-orthogonal (oblique) projection (for definition see below) isTo see that P is indeed a projection, i.e., P = P2, we computeThe action of this matrix on an arbitrary vector isFor example, the function which maps the point (x, y, z) in three-dimensional space R3 to the point (x, y, 0) is an orthogonal projection onto the x–y plane. This function is represented by the matrixIn linear algebra and functional analysis, a projection is a linear transformation P from a vector space to itself such that P 2 = P. That is, whenever P is applied twice to any value, it gives the same result as if it were applied once (idempotent). It leaves its image unchanged.[1] Though abstract, this definition of "projection" formalizes and generalizes the idea of graphical projection. One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object.
Linear independence
If such a linear dependence exists, then the n vectors are linearly dependent. It makes sense to identify two linear dependencies if one arises as a non-zero multiple of the other, because in this case the two describe the same linear relationship among the vectors. Under this identification, the set of all linear dependencies among v1, ...., vn is a projective space.A linear dependence among vectors v1, ..., vn is a tuple (a1, ..., an) with n scalar components, not all zero, such thatTake the first derivative of the above equation such thatthen ai = 0 for all i in {1, ..., n}.SinceSuppose that a1, a2, ..., an are elements of R such thatThen e1, e2, ..., en are linearly independent.Let V = Rn and consider the following elements in V, known as the natural basis vectors:If there are more vectors than dimensions, the vectors are linearly dependent. This is illustrated in the example above of three vectors in R2.for all possible lists of m rows. (In case m = n, this requires only one determinant, as above. If m > n, then it is a theorem that the vectors must be linearly dependent.) This fact is valuable for theory; in practical calculations more efficient methods are available.Furthermore, the reverse is true. That is, we can test whether the m vectors are linearly dependent by testing whetherOtherwise, suppose we have m vectors of n coordinates, with m < n. Then A is an n×m matrix and Λ is a column vector with m entries, and we are again interested in AΛ = 0. As we saw previously, this is equivalent to a list of n equations. Consider the first m rows of A, the first m equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈i1,...,im〉 is any list of m rows, then the equation must be true for those rows.Since the determinant is non-zero, the vectors (1, 1) and (−3, 2) are linearly independent.We are interested in whether AΛ = 0 for some nonzero vector Λ. This depends on the determinant of A, which isWe may write a linear combination of the columns asIn this case, the matrix formed by the vectors iswhere a3 can be chosen arbitrarily. Thus, the vectors v1, v2 and v3 are linearly dependent.This equation is easily solved to define non-zero ai,Rearrange to solve for v3 and obtain,Row reduce this equation to obtain,are linearly dependent, form the matrix equation,In order to determine if the three vectors in R4,This shows that ai = 0, which means that the vectors v1 = (1, 1) and v2 = (−3, 2) are linearly independent.The same row reduction presented above yields,orTwo vectors: Now consider the linear dependence of the two vectors v1 = (1, 1), v2 = (−3, 2), and check,which shows that non-zero ai exist such that v3 = (2, 4) can be defined in terms of v1 = (1, 1), v2 = (−3, 2). Thus, the three vectors are linearly dependent.We can now rearrange this equation to obtainContinue the row reduction by (i) dividing the second row by 5, and then (ii) multiplying by 3 and adding to the first row, that isRow reduce this matrix equation by subtracting the first row from the second to obtain,orThree vectors: Consider the set of vectors v1 = (1, 1), v2 = (−3, 2) and v3 = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such thatAlso note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set. In general, n linearly independent vectors are required to describe any location in n-dimensional space.In this example the "3 miles north" vector and the "4 miles east" vector are linearly independent. That is to say, the north vector cannot be described in terms of the east vector, and vice versa. The third "5 miles northeast" vector is a linear combination of the other two vectors, and it makes the set of vectors linearly dependent, that is, one of the three vectors is unnecessary.A geographic example may help to clarify the concept of linear independence. A person describing the location of a certain place might say, "It is 3 miles north and 4 miles east of here." This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth's surface). The person might add, "The place is 5 miles northeast of here." Although this last statement is true, it is not necessary.A set of vectors which is linearly independent and spans some vector space, forms a basis for that vector space. For example, the vector space of all polynomials in x over the reals has the (infinite) subset {1, x, x2, ...} as a basis.A set X of elements of V is linearly independent if the corresponding family {x}x∈X is linearly independent. Equivalently, a family is dependent if a member is in the linear span of the rest of the family, i.e., a member is a linear combination of the rest of the family. The trivial case of the empty family must be regarded as linearly independent for theorems to apply.where the index set J is a nonempty, finite subset of I.In order to allow the number of linearly independent vectors in a vector space to be countably infinite, it is useful to define linear dependence as follows. More generally, let V be a vector space over a field K, and let {vi | i∈I} be a family of elements of V. The family is linearly dependent over K if there exists a family {aj | j∈J} of elements of K, not all zero, such thatA vector space can be of finite-dimension or infinite-dimension depending on the number of linearly independent basis vectors. The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a basis for a vector space.In the theory of vector spaces, a set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent. These concepts are central to the definition of dimension.[1]
Linear combination

Basis (linear algebra)
This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent.Hence Lmax is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.If Lmax would not span V, there would exist some vector w of V that cannot be expressed as a linear combination of elements of Lmax (with coefficients in the field F). In particular, w cannot be an element of Lmax. Let Lw = Lmax ∪ {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of Lmax, and Lmax is independent). As Lmax ⊆ Lw, and Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax), this contradicts the maximality of Lmax. Thus this shows that Lmax spans V.It remains to prove that Lmax is a basis of V. Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.As X is nonempty, and every totally ordered subset of (X, ⊆) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ⊆ L for some element L of X, then L = Lmax.Since (Y, ⊆) is totally ordered, every finite subset of LY is a subset of an element of Y, which is a linearly independent subset of V, and hence every finite subset of LY is linearly independent. Thus LY is linearly independent, so LY is an element of X. Therefore, LY is an upper bound for Y in (X, ⊆): it is an element of X, that contains every element Y.Let Y be a subset of X that is totally ordered by ⊆, and let LY be the union of all the elements of Y (which are themselves certain subsets of V).The set X is nonempty since the empty set is an independent subset of V, and it is partially ordered by inclusion, which is denoted, as usual, by ⊆.Let V be any vector space over some field F. Let X be the set of all linearly independent subsets of V.The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the n-dimensional cube [−1,1]n as a function of dimension, n. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within π/2 ±0.037π/2 then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within π/2 ±0.037π/2 then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each n, 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in n-dimensional ball. Choose N independent random vectors from a ball (they are independent and identically distributed). Let θ be a small positive number. Then forFor a probability distribution in Rn with a probability density function, such as the equidistribution in a n-dimensional ball with respect to Lebesgue measure, it can be shown that randomly and independently chosen n-vectors will form a basis with probability one, which is due to the fact that n linearly dependent vectors x1,..., xn in Rn should satisfy the equation det[x1,..., xn]=0 (zero determinant of the matrix with columns xi), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.[5][6]for suitable (real or complex) coefficients ak, bk. But many[2] square-integrable functions cannot be represented as finite linear combinations of these basis functions, which therefore do not comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.The functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are linearly independent, and every function f that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense thatIn the study of Fourier series, one learns that the functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions f satisfyingThe common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fréchet spaces.The maps sending a vector v to the components aj(v) are linear maps from V to F, because of φ−1 is linear. Hence they are linear functionals. They form a basis for the dual space of V, called the dual basis.The inverse of the linear isomorphism φ determined by an ordered basis {vi} equips V with coordinates: if, for a vector v ∈ V, φ−1(v) = (a1, a2,...,an) ∈ Fn, then the components aj = aj(v) are the coordinates of v in the sense that v = a1(v) v1 + a2(v) v2 + ... + an(v) vn.These two constructions are clearly inverse to each other. Thus ordered bases for V are in 1-1 correspondence with linear isomorphisms Fn → V.where x = x1e1 + x2e2 + ... + xnen is an element of Fn. It is not hard to check that φ is a linear isomorphism.Conversely, given an ordered basis, consider the map defined bywhere {ei} is the standard basis for Fn.is a linear isomorphism. Define an ordered basis {vi} for V bySuppose first thatProof. The proof makes use of the fact that the standard basis of Fn is an ordered basis.Suppose V is an n-dimensional vector space over a field F. A choice of an ordered basis for V is equivalent to a choice of a linear isomorphism φ from the coordinate space Fn to V.A basis is just a linearly independent set of vectors with or without a given ordering. For many purposes it is convenient to work with an ordered basis. For example, when working with a coordinate representation of a vector it is customary to speak of the "first" or "second" coordinate, which makes sense only if an ordering is specified for the basis. For finite-dimensional vector spaces one typically indexes a basis {vi} by the first n integers. An ordered basis is also called a frame.Since the above matrix has a nonzero determinant, its columns form a basis of R2. See: invertible matrix.Simply compute the determinantSince (−1,2) is clearly not a multiple of (1,1) and since (1,1) is not the zero vector, these two vectors are linearly independent. Since the dimension of R2 is 2, the two vectors already form a basis of R2 without needing any extension.Subtracting the first equation from the second, we get:Then we have to solve the equations:Part II: To prove that these two vectors generate R2, we have to let (a, b) be an arbitrary element of R2, and show that there exist numbers r, s ∈ R such that:Hence we have linear independence.Adding this equation to the first equation then:Subtracting the first equation from the second, we obtain:(i.e., they are linearly dependent). Then:To prove that they are linearly independent, suppose that there are numbers a, b such that:We have to prove that these two vectors are linearly independent and that they generate R2.Often, a mathematical result can be proven in more than one way. Here, using three different proofs, we show that the vectors (1,1) and (−1,2) form a basis for R2.A similar question is when does a subset S contain a basis. This occurs if and only if S spans V. In this case, S will usually contain several different bases.Let S be a subset of a vector space V. To extend S to a basis means to find a basis B that contains S as a subset. This can be done if and only if S is linearly independent. Almost always, there is more than one such B, except in rather special circumstances (i.e. S is already a basis, or S is empty and V has two elements).A set of vectors can be represented by a matrix of which each column consists of the components of the corresponding vector of the set. As a basis is a set of vectors, a basis can be given by a matrix of this kind. The change of basis of any object of the space is related to this matrix. For example, coordinate tuples change with its inverse.Given a vector space V over a field F and suppose that {v1, ..., vn} and {α1, ..., αn} are two bases for V. By definition, if ξ is a vector in V then ξ = x1α1 + ... + xnαn for a unique choice of scalars x1, ..., xn in F called the coordinates of ξ relative to the ordered basis {α1, ..., αn}. The vector x = (x1, ..., xn) in Fn is called the coordinate tuple of ξ (relative to this basis). The unique linear map φ : Fn → V with φ(vj) = αj for j = 1, ..., n is called the coordinate isomorphism for V and the basis {α1, ..., αn}. Thus φ(x) = ξ if and only if ξ = x1α1 + ... + xnαn.In M22, {M1,1, M1,2, M2,1, M2,2}, where M22 is the set of all 2×2 matrices. and Mm,n is the 2×2 matrix with a 1 in the m,n position and zeros everywhere else.In P2, where P2 is the set of all polynomials of degree at most 2, {1, x, x2} is the standard basis.In Rn, {e1, ..., en}, where ei is the ith column of the identity matrix.Standard bases for example:Also many vector sets can be attributed a standard basis which comprises both spanning and linearly independent vectors.Every vector space has a basis. The proof of this requires the axiom of choice. All bases of a vector space have the same cardinality (number of elements), called the dimension of the vector space. This result is known as the dimension theorem, and requires the ultrafilter lemma, a strictly weaker form of the axiom of choice.Again, B denotes a subset of a vector space V. Then, B is a basis if and only if any of the following equivalent conditions are met:It is often convenient to list the basis vectors in a specific order, for example, when considering the transformation matrix of a linear map with respect to a basis. We then speak of an ordered basis, which we define to be a sequence (rather than a set) of linearly independent vectors that span V: see Ordered bases and coordinates below.The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors. Settings that permit infinite linear combinations allow alternative definitions of the basis concept: see Related notions below.A vector space that has a finite basis is called finite-dimensional. To deal with infinite-dimensional spaces, we must generalize the above definition to include infinite basis sets. We therefore say that a set (finite or infinite) B ⊂ V is a basis, ifThe numbers ai are called the coordinates of the vector x with respect to the basis B, and by the first property they are uniquely determined.In more detail, suppose that B = { v1, …, vn } is a finite subset of a vector space V over a field F (such as the real or complex numbers R or C). Then B is a basis if it satisfies the following conditions:A basis B of a vector space V over a field F is a linearly independent subset of V that spans V.Given a basis of a vector space V, every element of V can be expressed uniquely as a linear combination of basis vectors, whose coefficients are referred to as vector coordinates or components. A vector space can have several distinct sets of basis vectors; however each such set has the same number of elements, with this number being the dimension of the vector space.In mathematics, a set of elements (vectors) in a vector space V is called a basis, or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set.[1] In more general terms, a basis is a linearly independent spanning set.
Row and column spaces
When V is not an inner product space, the coimage of T can be defined as the quotient space V / ker(T).If V is an inner product space, then the orthogonal complement to the kernel can be thought of as a generalization of the row space. This is sometimes called the coimage of T. The transformation T is one-to-one on its coimage, and the coimage maps isomorphically onto the image of T.If V and W are vector spaces, then the kernel of a linear transformation T: V → W is the set of vectors v ∈ V for which T(v) = 0. The kernel of a linear transformation is analogous to the null space of a matrix.The row space and null space are two of the four fundamental subspaces associated with a matrix A (the other two being the column space and left null space).It follows that the null space of A is the orthogonal complement to the row space. For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin. This provides a proof of the rank–nullity theorem (see dimension above).where r1, ... , rm are the row vectors of A. Thus Ax = 0 if and only if x is orthogonal (perpendicular) to each of the row vectors of A.The null space of matrix A is the set of all vectors x for which Ax = 0. The product of the matrix A and the vector x can be written in terms of the dot product of vectors:where n is the number of columns of the matrix A. The equation above is known as the rank–nullity theorem.The rank of a matrix is also equal to the dimension of the column space. The dimension of the null space is called the nullity of the matrix, and is related to the rank by the following equation:The dimension of the row space is called the rank of the matrix. This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots. For example, the 3 × 3 matrix in the example above has rank two.[5]The pivots indicate that the first two columns of AT form a basis of the column space of AT. Therefore, the first two rows of A (before any row reductions) also form a basis of the row space of A.It is sometimes convenient to find a basis for the row space from among the rows of the original matrix instead (for example, this result is useful in giving an elementary proof that the determinantal rank of a matrix is equal to its rank). Since row operations can affect linear dependence relations of the row vectors, such a basis is instead found indirectly using the fact that the column space of AT is equal to the row space of A. Using the example matrix A above, find AT and reduce it to row echelon form:This algorithm can be used in general to find a basis for the span of a set of vectors. If the matrix is further simplified to reduced row echelon form, then the resulting basis is uniquely determined by the row space.Once the matrix is in echelon form, the nonzero rows are a basis for the row space. In this case, the basis is { (1, 3, 2), (0, 1, 0) }. Another possible basis { (1, 0, 2), (0, 1, 0) } comes from a further reduction.[5]r1, r2, r3 represents the rows.The rows of this matrix span the row space, but they may not be linearly independent, in which case the rows will not be a basis. To find a basis, we reduce A to row echelon form:For example, consider the matrixThe row space is not affected by elementary row operations. This makes it possible to use row reduction to find a basis for the row space.The column space of A is equal to the row space of AT.For a matrix that represents a homogeneous system of linear equations, the row space consists of all linear equations that follow from those in the system.The set of all such vectors is the row space of A. In this case, the row space is precisely the set of vectors (x, y, z) ∈ K3 satisfying the equation z = 2x (using Cartesian coordinates, this set is a plane through the origin in three-dimensional space).then the row vectors are r1 = (1, 0, 2) and r2 = (0, 1, 0). A linear combination of r1 and r2 is any vector of the formFor example, ifwhere c1, c2, ... , cm are scalars. The set of all possible linear combinations of r1, ... , rm is called the row space of A. That is, the row space of A is the span of the vectors r1, ... , rm.Let K be a field of scalars. Let A be an m × n matrix, with row vectors r1, r2, ... , rm. A linear combination of these vectors is any vector of the formfor any c1, ..., cn, with replacement of the vector m-space with "right free module", which changes the order of scalar multiplication of the vector vk to the scalar ck such that it is written in an unusual order vector–scalar.[4]Similarly the column space (sometimes disambiguated as right column space) can be defined for matrices over a ring K asFor a matrix A, the column space, row space, null space, and left null space are sometimes referred to as the four fundamental subspaces.It follows that the left null space (the null space of AT) is the orthogonal complement to the column space of A.because row vectors of AT are transposes of column vectors vk of A. Thus ATx = 0 if and only if x is orthogonal (perpendicular) to each of the column vectors of A.The left null space of A is the set of all vectors x such that xTA = 0T. It is the same as the null space of the transpose of A. The product of the matrix AT and the vector x can be written in terms of the dot product of vectors:This is known as the rank–nullity theorem.The nullity of a matrix is the dimension of the null space, and is equal to the number of columns in the reduced row echelon form that do not have pivots.[3] The rank and nullity of a matrix A with n columns are related by the equation:Because the column space is the image of the corresponding matrix transformation, the rank of a matrix is the same as the dimension of the image. For example, the transformation R4 → R4 described by the matrix above maps all of R4 to some three-dimensional subspace.The dimension of the column space is called the rank of the matrix. The rank is equal to the number of pivots in the reduced row echelon form, and is the maximum number of linearly independent columns that can be chosen from the matrix. For example, the 4 × 4 matrix in the example above has rank three.The above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set. A different algorithm for finding a basis from a spanning set is given in the row space article; finding a basis for the column space of A is equivalent to finding a basis for the row space of the transpose matrix AT.Note that the independent columns of the reduced row echelon form are precisely the columns with pivots. This makes it possible to determine which columns are linearly independent by reducing only to echelon form.At this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two. (Specifically, v3 = –2v1 + v2.) Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space:The columns of this matrix span the column space, but they may not be linearly independent, in which case some subset of them will form a basis. To find this basis, we reduce A to reduced row echelon form:For example, consider the matrixThe columns of A span the column space, but they may not form a basis if the column vectors are not linearly independent. Fortunately, elementary row operations do not affect the dependence relations between the column vectors. This makes it possible to use row reduction to find a basis for the column space.Therefore, the column space of A consists of all possible products Ax, for x ∈ Cn. This is the same as the image (or range) of the corresponding matrix transformation.Any linear combination of the column vectors of a matrix A can be written as the product of A with a column vector:where c1, c2, ..., cn are scalars. The set of all possible linear combinations of v1, ... ,vn is called the column space of A. That is, the column space of A is the span of the vectors v1, ... , vn.Let K be a field of scalars. Let A be an m × n matrix, with column vectors v1, v2, ..., vn. A linear combination of these vectors is any vector of the formthe rows are r1 = (2,4,1,3,2), r2 = (−1,−2,1,0,5), r3 = (1,6,2,2,2), r4 = (3,6,2,5,1). Consequently, the row space of J is the subspace of R5 spanned by { r1, r2, r3, r4 }. Since these four row vectors are linearly independent, the row space is 4-dimensional. Moreover, in this case it can be seen that they are all orthogonal to the vector n = (6,−1,4,−4,0), so it can be deduced that the row space consists of all vectors in R5 that are orthogonal to n.Given a matrix J:Intuitively, given a matrix A, the action of the matrix A on a vector x will return a linear combination of the columns of A weighted by the coordinates of x as coefficients. Another way to look at this is that it will (1) first project x into the row space of A, (2) perform an invertible transformation, and (3) place the resulting vector y in the column space of A. Thus the result y = A x must reside in the column space of A. See singular value decomposition for more details on this second interpretation.[clarification needed]The concept of row space generalizes to matrices over C, the field of complex numbers, or over any field.The column space of a matrix A is the set of all linear combinations of the columns in A. If A = [a1, ...., an], then colsp(A) = span {a1, ...., an}.If one considers the matrix as a linear transformation from Rn to Rm, then the column space of the matrix equals the image of this linear transformation.Let A be an m-by-n matrix. ThenThis article considers matrices of real numbers. The row and column spaces are subspaces of the real spaces Rn and Rm respectively.The row space is defined similarly.In linear algebra, the column space (also called the range or image) of a matrix A is the span (set of all possible linear combinations) of its column vectors. The column space of a matrix is the image or range of the corresponding matrix transformation.
Row and column spaces
When V is not an inner product space, the coimage of T can be defined as the quotient space V / ker(T).If V is an inner product space, then the orthogonal complement to the kernel can be thought of as a generalization of the row space. This is sometimes called the coimage of T. The transformation T is one-to-one on its coimage, and the coimage maps isomorphically onto the image of T.If V and W are vector spaces, then the kernel of a linear transformation T: V → W is the set of vectors v ∈ V for which T(v) = 0. The kernel of a linear transformation is analogous to the null space of a matrix.The row space and null space are two of the four fundamental subspaces associated with a matrix A (the other two being the column space and left null space).It follows that the null space of A is the orthogonal complement to the row space. For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin. This provides a proof of the rank–nullity theorem (see dimension above).where r1, ... , rm are the row vectors of A. Thus Ax = 0 if and only if x is orthogonal (perpendicular) to each of the row vectors of A.The null space of matrix A is the set of all vectors x for which Ax = 0. The product of the matrix A and the vector x can be written in terms of the dot product of vectors:where n is the number of columns of the matrix A. The equation above is known as the rank–nullity theorem.The rank of a matrix is also equal to the dimension of the column space. The dimension of the null space is called the nullity of the matrix, and is related to the rank by the following equation:The dimension of the row space is called the rank of the matrix. This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots. For example, the 3 × 3 matrix in the example above has rank two.[5]The pivots indicate that the first two columns of AT form a basis of the column space of AT. Therefore, the first two rows of A (before any row reductions) also form a basis of the row space of A.It is sometimes convenient to find a basis for the row space from among the rows of the original matrix instead (for example, this result is useful in giving an elementary proof that the determinantal rank of a matrix is equal to its rank). Since row operations can affect linear dependence relations of the row vectors, such a basis is instead found indirectly using the fact that the column space of AT is equal to the row space of A. Using the example matrix A above, find AT and reduce it to row echelon form:This algorithm can be used in general to find a basis for the span of a set of vectors. If the matrix is further simplified to reduced row echelon form, then the resulting basis is uniquely determined by the row space.Once the matrix is in echelon form, the nonzero rows are a basis for the row space. In this case, the basis is { (1, 3, 2), (0, 1, 0) }. Another possible basis { (1, 0, 2), (0, 1, 0) } comes from a further reduction.[5]r1, r2, r3 represents the rows.The rows of this matrix span the row space, but they may not be linearly independent, in which case the rows will not be a basis. To find a basis, we reduce A to row echelon form:For example, consider the matrixThe row space is not affected by elementary row operations. This makes it possible to use row reduction to find a basis for the row space.The column space of A is equal to the row space of AT.For a matrix that represents a homogeneous system of linear equations, the row space consists of all linear equations that follow from those in the system.The set of all such vectors is the row space of A. In this case, the row space is precisely the set of vectors (x, y, z) ∈ K3 satisfying the equation z = 2x (using Cartesian coordinates, this set is a plane through the origin in three-dimensional space).then the row vectors are r1 = (1, 0, 2) and r2 = (0, 1, 0). A linear combination of r1 and r2 is any vector of the formFor example, ifwhere c1, c2, ... , cm are scalars. The set of all possible linear combinations of r1, ... , rm is called the row space of A. That is, the row space of A is the span of the vectors r1, ... , rm.Let K be a field of scalars. Let A be an m × n matrix, with row vectors r1, r2, ... , rm. A linear combination of these vectors is any vector of the formfor any c1, ..., cn, with replacement of the vector m-space with "right free module", which changes the order of scalar multiplication of the vector vk to the scalar ck such that it is written in an unusual order vector–scalar.[4]Similarly the column space (sometimes disambiguated as right column space) can be defined for matrices over a ring K asFor a matrix A, the column space, row space, null space, and left null space are sometimes referred to as the four fundamental subspaces.It follows that the left null space (the null space of AT) is the orthogonal complement to the column space of A.because row vectors of AT are transposes of column vectors vk of A. Thus ATx = 0 if and only if x is orthogonal (perpendicular) to each of the column vectors of A.The left null space of A is the set of all vectors x such that xTA = 0T. It is the same as the null space of the transpose of A. The product of the matrix AT and the vector x can be written in terms of the dot product of vectors:This is known as the rank–nullity theorem.The nullity of a matrix is the dimension of the null space, and is equal to the number of columns in the reduced row echelon form that do not have pivots.[3] The rank and nullity of a matrix A with n columns are related by the equation:Because the column space is the image of the corresponding matrix transformation, the rank of a matrix is the same as the dimension of the image. For example, the transformation R4 → R4 described by the matrix above maps all of R4 to some three-dimensional subspace.The dimension of the column space is called the rank of the matrix. The rank is equal to the number of pivots in the reduced row echelon form, and is the maximum number of linearly independent columns that can be chosen from the matrix. For example, the 4 × 4 matrix in the example above has rank three.The above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set. A different algorithm for finding a basis from a spanning set is given in the row space article; finding a basis for the column space of A is equivalent to finding a basis for the row space of the transpose matrix AT.Note that the independent columns of the reduced row echelon form are precisely the columns with pivots. This makes it possible to determine which columns are linearly independent by reducing only to echelon form.At this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two. (Specifically, v3 = –2v1 + v2.) Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space:The columns of this matrix span the column space, but they may not be linearly independent, in which case some subset of them will form a basis. To find this basis, we reduce A to reduced row echelon form:For example, consider the matrixThe columns of A span the column space, but they may not form a basis if the column vectors are not linearly independent. Fortunately, elementary row operations do not affect the dependence relations between the column vectors. This makes it possible to use row reduction to find a basis for the column space.Therefore, the column space of A consists of all possible products Ax, for x ∈ Cn. This is the same as the image (or range) of the corresponding matrix transformation.Any linear combination of the column vectors of a matrix A can be written as the product of A with a column vector:where c1, c2, ..., cn are scalars. The set of all possible linear combinations of v1, ... ,vn is called the column space of A. That is, the column space of A is the span of the vectors v1, ... , vn.Let K be a field of scalars. Let A be an m × n matrix, with column vectors v1, v2, ..., vn. A linear combination of these vectors is any vector of the formthe rows are r1 = (2,4,1,3,2), r2 = (−1,−2,1,0,5), r3 = (1,6,2,2,2), r4 = (3,6,2,5,1). Consequently, the row space of J is the subspace of R5 spanned by { r1, r2, r3, r4 }. Since these four row vectors are linearly independent, the row space is 4-dimensional. Moreover, in this case it can be seen that they are all orthogonal to the vector n = (6,−1,4,−4,0), so it can be deduced that the row space consists of all vectors in R5 that are orthogonal to n.Given a matrix J:Intuitively, given a matrix A, the action of the matrix A on a vector x will return a linear combination of the columns of A weighted by the coordinates of x as coefficients. Another way to look at this is that it will (1) first project x into the row space of A, (2) perform an invertible transformation, and (3) place the resulting vector y in the column space of A. Thus the result y = A x must reside in the column space of A. See singular value decomposition for more details on this second interpretation.[clarification needed]The concept of row space generalizes to matrices over C, the field of complex numbers, or over any field.The column space of a matrix A is the set of all linear combinations of the columns in A. If A = [a1, ...., an], then colsp(A) = span {a1, ...., an}.If one considers the matrix as a linear transformation from Rn to Rm, then the column space of the matrix equals the image of this linear transformation.Let A be an m-by-n matrix. ThenThis article considers matrices of real numbers. The row and column spaces are subspaces of the real spaces Rn and Rm respectively.The row space is defined similarly.In linear algebra, the column space (also called the range or image) of a matrix A is the span (set of all possible linear combinations) of its column vectors. The column space of a matrix is the image or range of the corresponding matrix transformation.
Orthogonality
Stereo vinyl records encode both the left and right stereo channels in a single groove. The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal. The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side.[17] A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.In board games such as chess which feature a grid of squares, 'orthogonal' is used to mean "in the same row/'rank' or column/'file'". This is the counterpart to squares which are "diagonally adjacent".[16] In the ancient Chinese board game Go a player can capture the stones of an opponent by occupying all orthogonally-adjacent points.In neuroscience, a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.In analytical chemistry, analyses are "orthogonal" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement. This is often required as a part of a new drug application.In synthetic organic chemistry orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, DNA has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively.[15] Bioorthogonal chemistry refers to chemical reactions occurring inside living systems without reacting with naturally present cellular components. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent, interactions being compatible; reversibly forming without interference from the other.In combinatorics, two n×n Latin squares are said to be orthogonal if their superimposition yields all possible n2 combinations of entries.[14]In taxonomy, an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.When performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated,[13] since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions). One econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments, relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.In OFDM, the subcarrier frequencies are chosen so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.Another scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other. Well known examples include (a, g, and n) versions of 802.11 Wi-Fi; WiMAX; ITU-T G.hn, DVB-T, the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL.In communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions. One such scheme is TDMA, where the orthogonal basis functions are nonoverlapping rectangular pulses ("time slots").An instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task)[12] and is designed such that instructions can use any register in any addressing mode. This terminology results from considering an instruction as a vector whose components are the instruction fields. One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes.[citation needed]Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation, and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results.[10] This usage was introduced by Van Wijngaarden in the design of Algol 68:The term "orthogonal line" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of "orthogonal lines" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the Web site of the Thyssen-Bornemisza Museum states that "Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours." [1]In art, the perspective (imaginary) lines pointing to the vanishing point are referred to as "orthogonal lines".is the Kronecker delta. In other words, every pair of them (excluding pairing of a function with itself) is orthogonal, and the norm of each is 1. See in particular the orthogonal polynomials.whereThe members of such a set of functions are orthonormal with respect to w on the interval [a, b] ifThe members of a set of functions {fi : i = 1, 2, 3, ...} are orthogonal with respect to w on the interval [a, b] ifWe write the norm with respect to this inner product asOrthogonality of two functions with respect to one inner product does not imply orthogonality with respect to another inner product.We say that functions f and g are orthogonal if their inner product (equivalently, the value of this integral) is zero:In simple cases, w(x) = 1.By using integral calculus, it is common to use the following to define the inner product of two functions f and g with respect to a nonnegative weight function w over an interval [a, b]:In four-dimensional Euclidean space, the orthogonal complement of a line is a hyperplane and vice versa, and that of a plane is a plane.[9]Note that the geometric concept two planes being perpendicular does not correspond to the orthogonal complement, since in three dimensions a pair of vectors, one from each of a pair of perpendicular planes, might meet at any angle.The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace. In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa.[9]In Euclidean space, two vectors are orthogonal if and only if their dot product is zero, i.e. they make an angle of 90° (π/2 radians), or one of the vectors is zero.[8] Hence orthogonality of vectors is an extension of the concept of perpendicular vectors to spaces of any dimension.A vector space with a bilinear form generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are orthogonal. The case of a pseudo-Euclidean plane uses the term hyperbolic orthogonality. In the diagram, axes x′ and t′ are hyperbolic-orthogonal for any given ϕ.In certain cases, the word normal is used to mean orthogonal, particularly in the geometric sense as in the normal to a surface. For example, the y-axis is normal to the curve y = x2 at the origin. However, normal may also refer to the magnitude of a vector. In particular, a set is called orthonormal (orthogonal plus normal) if it is an orthogonal set of unit vectors. As a result, use of the term normal to mean "orthogonal" is often avoided. The word "normal" also has a different meaning in probability and statistics.A set of vectors in an inner product space is called pairwise orthogonal if each pairing of them is orthogonal. Such a set is called an orthogonal set.The word comes from the Greek ὀρθός (orthos), meaning "upright", and γωνία (gonia), meaning "angle". The ancient Greek ὀρθογώνιον orthogōnion (< ὀρθός orthos 'upright'[1] + γωνία gōnia 'angle'[2]) and classical Latin orthogonium originally denoted a rectangle.[3] Later, they came to mean a right triangle. In the 12th century, the post-classical Latin word orthogonalis came to mean a right angle or something related to a right angle.[4]By extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.In mathematics, orthogonality is the generalization of the notion of perpendicularity to the linear algebra of bilinear forms. Two elements u and v of a vector space with bilinear form B are orthogonal when B(u, v) = 0. Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of function spaces, families of orthogonal functions are used to form a basis.
Kernel (linear algebra)
Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]The problem of computing the kernel on a computer depends on the nature of the coefficients.are a basis of the kernel of A.The last three columns of B are zero columns. Therefore, the three last vectors of C,Putting the upper part in column echelon form by column operations on the whole matrix givesThenFor example, suppose thatIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.A basis of the kernel of a matrix may be computed by Gaussian elimination.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.Note also that the following dot products are zero:The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.Since c is a free variable, this can be expressed equally well as,for c a scalar.Now we can express an element of the kernel:Rewriting yields:Gauss–Jordan elimination reduces this to:which can be written in matrix form as:which can be expressed as a homogeneous system of linear equations involving x, y, and z:The kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichConsider the matrixWe give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Geometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax = b isThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.If u and v are two possible solutions to the above equation, thenThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The product Ax can be written in terms of the dot product of vectors as follows:The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The matrix equation is equivalent to a homogeneous system of linear equations:Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.This implies the rank–nullity theorem:It follows that the image of L is isomorphic to the quotient of V by the kernel:The kernel of L is a linear subspace of the domain V.[1] In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,
Eigenvalues and eigenvectors
The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.leads to a so-called quadratic eigenvalue problem,orEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byPrincipal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.This matrix equation is equivalent to two linear equationsOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]A similar procedure is used for solving a differential equation of the formThe solution of this equation for x in terms of t is found by using its characteristic equationThe simplest difference equations have the formThe representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.So, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u+v,αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenBy definition of a linear transformation,which is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.Given an eigenvalue λ, consider the setThis equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such thatThe concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,The main eigenfunction article gives other examples.is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationOn the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 −1 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity γA is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.has a characteristic polynomial that is the product of its diagonal elements,As in the previous example, the lower triangular matrixrespectively, as well as scalar multiples of these vectors.These eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isConsider the lower triangular matrix,A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.respectively, as well as scalar multiples of these vectors.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,which has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.The characteristic polynomial of A isMatrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixandThenFor the complex conjugate pair of imaginary eigenvalues, note thatFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areConsider the cyclic permutation matrixThe characteristic polynomial of A isConsider the matrixThus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Any non-zero vector with v1 = v2 solves this equation. Therefore,For λ = 3, Equation (2) becomesis an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.Any non-zero vector with v1 = −v2 solves this equation. Therefore,For λ = 1, Equation (2) becomes,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.Taking the determinant to find characteristic polynomial of A,The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Consider the matrixA matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.or by instead left multiplying both sides by Q−1,Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenSince each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues λ1, λ2, ..., λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.where κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation isMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),Let A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenSuppose A has d ≤ n distinct eigenvalues λ1, λ2, ..., λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,The condition that γA(λ) ≤ μA(λ) can be proven by considering a particular eigenvalue ξ of A and diagonalizing the first γA(ξ) columns of A with respect to the eigenvectors of ξ, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix ξIγA(ξ). As a result, the characteristic polynomial of B will have a factor of (ξ − λ)γA(ξ). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of ξ as an eigenvalue of B is no less than the geometric multiplicity of ξ as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number, (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),If μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asSuppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofTaking the determinant of (M − λI), the characteristic polynomial of M isAs a brief example, which is described in more detail in the examples section later, consider the matrixwhere each λi may be real but in general is a complex number. The numbers λ1, λ2, ... λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equationwhere I is the n by n identity matrix.Equation (1) can be stated equivalently asthen v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.If it occurs that v and w are scalar multiples, that is ifwhere, for each row,orNow consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,In this case λ = −1/20.These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatConsider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationThe Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.referred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationwhere λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation
Outer product
The outer product is useful in computing physical quantities (e.g., the tensor of inertia), and performing transform operations in digital signal processing and digital image processing. It is also useful in statistical analysis for computing the covariance and auto-covariance matrices for two vector-valued random variables.In some programming languages, given a two-argument function f (or a binary operator), the outer product of f and two one-dimensional arrays A and B is a two-dimensional array C such that C[i,j] = f(A[i],B[j]). This is syntactically represented in various ways: in APL, as the infix binary operator °.f; in R, as the function outer(A, B, f);[5] in Mathematica, as Outer[f,A,B]. In MATLAB, the function kron(A,B) is used for this product. These often generalize to multi-dimensional arguments, and more than two arguments.If W = V, then one can pair the covector w ∈ V∗ with the vector v ∈ V via the map (w, v) ↦ w(v), which is the duality pairing between V and its dual.If V and W are finite-dimensional, then the space of all linear transformations from W to V, denoted Hom(W, V), is generated by such outer products; in fact, the rank of a matrix is the minimal number of such outer products needed to express it as a sum (this is the tensor rank of a matrix). In this case Hom(W, V) is isomorphic to W∗ ⊗ V.Here y(w) denotes the value of the linear functional y (which is an element of the dual space of W) when evaluated at the element w ∈ W. This scalar in turn is multiplied by x to give as the final result an element of the space V.Let V and W be two vector spaces, and let W∗ be the dual space of W. Given vectors x ∈ V and y ∈ W∗, then the tensor product y ⊗ x corresponds to the map A : W → V given byTo understand the matrix definition of outer product in terms of the definition of tensor product:For example, if A is of order 3 with dimensions (3, 5, 7) and B is of order 2 with dimensions (10, 100), their outer product c is of order 5 with dimensions (3, 5, 7, 10, 100). If A has a component A[2, 2, 4] = 11 and B has a component B[8, 88] = 13, then the component of C formed by the outer product is C[2, 2, 4, 8, 88] = 143.similarly for higher order tensors:The outer product on tensors is typically referred to as the tensor product. Given a tensor a of order q with dimensions (i1, ..., iq), and a tensor b of order r with dimensions (j1, ..., jr), their outer product c is of order q + r with dimensions (k1, ..., kq+r) which are the i  dimensions followed by the j  dimensions. It is denoted in coordinate-free notation using ⊗ and components are defined in index notation by:[4]For complex vectors, outer product can be defined as above, or with the complex conjugate of v (denoted v∗ or v̅). Namely, matrix A is obtained by multiplying each element of u by the complex conjugate of each element of v.their outer product u ⊗ v is defined as the m × n matrix A obtained by multiplying each element of u by each element of v:[2][3]Given the vectors("Matrix rank" should not be confused with "tensor order", or "tensor degree", which is sometimes referred to as "rank".)which is just a scalar vTx multiplied by a vector u.If u and v are both nonzero then the outer product matrix uvT always has matrix rank 1, as can be easily seen by multiplying it with a vector x:which is the standard inner product for Euclidean vector spaces, better known as the dot product. The inner product is the trace of the outer product.If m = n, then one can take the matrix product the other way, yielding a scalar (or 1 × 1 matrix):For complex vectors, it is customary to use the conjugate transpose of v (denoted vH):Or in index notation:The outer product u ⊗ v is equivalent to a matrix multiplication uvT, provided that u is represented as a m × 1 column vector and v as a n × 1 column vector (which makes vT a row vector).[1] For instance, if m = 4 and n = 3, thenThe outer product is also a related function in some computer programming languages.The outer product contrasts with the dot product, which takes as input a pair of coordinate vectors and produces a scalar.
Inner product space
As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the "outer (alternatively, wedge) product". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.More abstractly, the outer product is the bilinear map W × V∗ → Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V∗ × V → F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V → V∗) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 × n covector with an n × 1 vector, yielding a 1 × 1 matrix (a scalar), while the outer product is the product of an m × 1 vector with a 1 × n covector, yielding an m × n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V → V∗) and thus hold more generally.Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that ⟨x,y⟩ ≠ 0, though y need not equal x; in other words, the induced map to the dual space V → V∗ is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "−" to them differs depending on conventions).This construction is used in numerous contexts. The Gelfand–Naimark–Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x : ||x|| = 0}. The sesquilinear form ⟨·,·⟩ factors through W.If V is a vector space and ⟨·,···⟩ a semi-definite sesquilinear form, then the function:Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.Several types of linear maps A from an inner product space V to an inner product space W are of relevance:Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [−π,π] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.Orthogonality of the sequence {ek}k follows immediately from the fact that if k ≠ j, thenis an isometric linear map with dense image.is an orthonormal basis of the space C[−π,π] with the L2 inner product. The mappingTheorem. Let V be the inner product space C[−π,π]. Then the sequence (indexed on set of all integers) of continuous functionsThis theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:is an isometric linear map V → l2 with a dense image.Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of V. Then the mapParseval's identity leads immediately to the following theorem:The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]Theorem. Any complete inner product space V has an orthonormal basis.Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show thatTheorem. Any separable inner product space V has an orthonormal basis.Using an infinite-dimensional analog of the Gram-Schmidt process one may show:if α ≠ β and ⟨eα,eα⟩ = ||eα|| = 1 for all α, β ∈ A.is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis andThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collectionLet V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram–Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if ⟨ei,ej⟩ = 0 for every i ≠ j and ⟨ei,ei⟩ = ||ei|| = 1 for each i.This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]A linear space with a norm such as:is an inner product.For real matrices of the same size, ⟨A,B⟩ := tr(ABT) with transpose as conjugationis an inner product.[6][7][8] In this case, ⟨X,X⟩ = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.For real random variables X and Y, the expected value of their productThis sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.This space is not complete; consider for example, for the interval [−1,1] the sequence of continuous "step" functions, { fk}k, defined by:The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product iswhere M is any Hermitian positive-definite matrix and y† is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.The general form of an inner product on Cn is known as the Hermitian form and is given bywhere xT is the transpose of x.More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.A simple example is the real numbers with the standard multiplication as the inner productis also known as additivity.The property of an inner product space V thatAssuming the underlying field to be R, the inner product becomes symmetric, and we obtainCombining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:From the linearity property it is derived that x = 0 implies ⟨x,x⟩ = 0. while from the positive-definiteness axiom we obtain the converse, ⟨x,x⟩ = 0 implies x = 0. Combining these two, we have the property that ⟨x,x⟩ = 0 if and only if x = 0.In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.Conjugate symmetry and linearity in the first variable givesMoreover, sesquilinearity (see below) implies thatNotice that conjugate symmetry implies that ⟨x,x⟩ is real for all x, since we have:When F = R, conjugate symmetry reduces to symmetry. That is, ⟨x,y⟩ = ⟨y,x⟩ for F = R; while for F = C, ⟨x,y⟩ is equal to the complex conjugate.In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that ⟨x,x⟩ is only required to be non-negative. We show how to treat these below.There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product ⟨x,y⟩ as ⟨y|x⟩ (the bra–ket notation of quantum mechanics), respectively y†x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V∗, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking ⟨x,y⟩ to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both ⟨·,·⟩ and ⟨·|·⟩ as distinct notations differing only in which argument is conjugate linear.that satisfies the following three axioms for all vectors x, y, z ∈ V and all scalars a ∈ F:[2][3]Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a mapIn this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]
Dot product
A dot product function is included in BLAS level 1.The straightforward algorithm for calculating a floating-point dot product of vectors can suffer from catastrophic cancellation. To avoid this, approaches such as the Kahan summation algorithm are used.The inner product between a tensor of order n and a tensor of order m is a tensor of order n + m − 2, see tensor contraction for details.Dyadics have a dot product and "double" dot product defined on them, see Dyadics (Product of dyadic and dyadic) for their definitions.Matrices have the Frobenius inner product, which is analogous to the vector inner product. It is defined as the sum of the products of the corresponding components of two matrices A and B having the same size:Inner products can have a weight function, i.e. a function which weights each term of the inner product with a value.Generalized further to complex functions ψ(x) and χ(x), by analogy with the complex inner product above, gives[1]This notion can be generalized to continuous functions: just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some interval a ≤ x ≤ b (also denoted [a, b]):[1]The dot product is defined for vectors that have a finite number of entries. Thus these vectors can be regarded as discrete functions: a length-n vector u is, then, a function with domain {k ∈ ℕ ∣ 1 ≤ k ≤ n}, and ui is a notation for the image of i by the function/vector u.The inner product of two vectors over the field of complex numbers is, in general, a complex number, and is sesquilinear instead of bilinear. An inner product space is a normed vector space, and the inner product of a vector with itself is real and positive-definite.This type of scalar product is nevertheless useful, and leads to the notions of Hermitian form and of general inner product spaces.The angle between two complex vectors is then given bywhere bi is the complex conjugate of bi. Then the scalar product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However this scalar product is thus sesquilinear rather than bilinear: it is conjugate linear and not linear in b, and the scalar product is not symmetric, sinceFor vectors with complex entries, using the given definition of the dot product would lead to quite different properties. For instance the dot product of a vector with itself would be an arbitrary complex number, and could be zero without the vector being the zero vector (such vectors are called isotropic); this in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the scalar product, through the alternative definition[1]In physics, vector magnitude is a scalar in the physical sense, i.e. a physical quantity independent of the coordinate system, expressed as the product of a numerical value and a physical unit, not just a number. The dot product is also a scalar in this sense, given by the formula, independent of the coordinate system. Examples include:[8][9]This identity, also known as Lagrange's formula may be remembered as "BAC minus CAB", keeping in mind which vectors are dotted together. This formula finds application in simplifying vector calculations in physics.The vector triple product is defined by[1][2]Its value is the determinant of the matrix whose columns are the Cartesian coordinates of the three vectors. It is the signed volume of the parallelogram defined by the three vectors.The scalar triple product of three vectors is defined asThere are two ternary operations involving dot product and cross product.which is the law of cosines.Given two vectors a and b separated by angle θ (see image right), they form a triangle with a third side c = a − b. The dot product of this with itself is:The dot product fulfills the following properties if a, b, and c are real vectors and r is a scalar.[1][2]which is precisely the algebraic definition of the dot product. So the geometric dot product equals the algebraic dot product.Now applying the distributivity of the geometric version of the dot product giveswhere ai is the component of vector a in the direction of ei.Also, by the geometric definition, for any vector ei and a vector a, we noteWhere δ ij is the Kronecker delta.Thus in general we can say that:and since they form right angles with each other, if i ≠ j,The vectors ei are an orthonormal basis, which means that they have unit length and are at right angles to each other. Hence since these vectors have unit lengthIf e1, ..., en are the standard basis vectors in Rn, then we may writeIt also satisfies a distributive law, meaning thatThe dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar α,The dot product is thus characterized geometrically by[4]In terms of the geometric definition of the dot product, this can be rewrittenwhere θ is the angle between a and b.The scalar projection (or scalar component) of a Euclidean vector a in the direction of a Euclidean vector b is given bythe formula for the Euclidean length of the vector.which givesThis implies that the dot product of a vector a with itself isAt the other extreme, if they are codirectional, then the angle between them is 0° andIn particular, if a and b are orthogonal, then the angle between them is 90° andwhere θ is the angle between a and b.Using the above example, a 1 × 3 matrix (row vector) is multiplied by a 3 × 1 matrix (column vector) to get the result (1 × 1 matrix is obtained by matrix multiplication, which is a scalar):The dot product can also be written as:where Σ denotes summation and n is the dimension of the vector space. For instance, in three-dimensional space, the dot product of vectors [1, 3, −5] and [4, −2, −1] is:The dot product of two vectors a = [a1, a2, ..., an] and b = [b1, b2, ..., bn] is defined as:[1]In modern presentations of Euclidean geometry, the points of space are defined in terms of their Cartesian coordinates, and Euclidean space itself is commonly identified with the real coordinate space Rn. In such a presentation, the notions of length and angles are defined by means of the dot product. The length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle of two vectors of length one is defined as their dot product. So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.The dot product may be defined algebraically or geometrically. The geometric definition is based on the notions of angle and distance (magnitude of vectors). The equivalence of these two definitions relies on having a Cartesian coordinate system for Euclidean space.The name "dot product" is derived from the centered dot " · " that is often used to designate this operation; the alternative name "scalar product" emphasizes that the result is a scalar, rather than a vector, which is the case for the vector product in three-dimensional space.Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern geometry, Euclidean spaces are often defined by using vector spaces. In this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle of two vectors is the quotient of their dot product by the product of their lengths).In mathematics, the dot product or scalar product[note 1] is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used and often called inner product (or rarely projection product); see also inner product space.
Transpose
Ideally, one might hope to transpose a matrix with minimal additional storage. This leads to the problem of transposing an n × m matrix in-place, with O(1) additional storage or at most storage much less than mn. For n ≠ m, this involves a complicated permutation of the data elements that is non-trivial to implement in-place. Therefore, efficient in-place matrix transposition has been the subject of numerous research publications in computer science, starting in the late 1950s, and several algorithms have been developed.However, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering. For example, with a matrix stored in row-major order, the rows of the matrix are contiguous in memory and the columns are discontiguous. If repeated operations need to be performed on the columns, for example in a fast Fourier transform algorithm, transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing memory locality.On a computer, one can often avoid explicitly transposing a matrix in memory by simply accessing the same data in a different order. For example, software libraries for linear algebra, such as BLAS, typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid the necessity of data movement.Over a complex vector space, one often works with sesquilinear forms (conjugate-linear in one argument) instead of bilinear forms. The Hermitian adjoint of a map between such spaces is defined similarly, and the matrix of the Hermitian adjoint is given by the conjugate transpose matrix if the bases are orthonormal.The adjoint allows us to consider whether g : W → V is equal to f −1 : W → V. In particular, this allows the orthogonal group over a vector space V with a quadratic form to be defined without reference to matrices (nor the components thereof) as the set of all linear maps V → V for which the adjoint equals the inverse.These bilinear forms define an isomorphism between V and V∗, and between W and W∗, resulting in an isomorphism between the transpose and adjoint of f. The matrix of the adjoint of a map is the transposed matrix only if the bases are orthonormal with respect to their bilinear forms. In this context, many authors use the term transpose to refer to the adjoint as defined here.If f : V → W is a linear map between vector spaces V and W, we define g as the adjoint of f if g : W → V satisfiesIf the vector spaces V and W have respectively nondegenerate bilinear forms BV and BW, a concept known as the adjoint, which is closely related to the transpose, may be defined:Every linear map to the dual space f : V → V∗ defines a bilinear form B : V × V → F, with the relation B(v, w) = f(v)(w). By defining the transpose of this bilinear form as the bilinear form tB defined by the transpose tf : V∗∗ → V∗ i.e. tB(w, v) = tf(Ψ(w))(v), we find that B(v, w) = tB(w, v). Here, Ψ is the natural homomorphism V → V∗∗ into the double dual.If the matrix A describes a linear map with respect to bases of V and W, then the matrix AT describes the transpose of that linear map with respect to the dual bases.The definition of the transpose may be seen to be independent of any bilinear form on the vector spaces, unlike the adjoint (below).where ⟨·,·⟩ is the natural pairing of each dual space with its respective vector space. This definition also applies unchanged to left modules and to vector spaces.[2]Equivalently, the transpose tf is defined by the relationIf f : V → W is a linear map between right R-modules V and W with respective dual modules V∗ and W∗, the transpose of f is the linear mapThe transpose may be defined more generally:A square complex matrix whose transpose is equal to its conjugate inverse is called a unitary matrix; that is, A is unitary ifA square matrix whose transpose is equal to its inverse is called an orthogonal matrix; that is, A is orthogonal ifA square complex matrix whose transpose is equal to the negation of its complex conjugate is called a skew-Hermitian matrix; that is, A is skew-Hermitian ifA square complex matrix whose transpose is equal to the matrix with every entry replaced by its complex conjugate (denoted here with an overline) is called a Hermitian matrix (equivalent to the matrix being equal to its conjugate transpose); that is, A is Hermitian ifA square matrix whose transpose is equal to its negative is called a skew-symmetric matrix; that is, A is skew-symmetric ifA square matrix whose transpose is equal to itself is called a symmetric matrix; that is, A is symmetric ifFor matrices A, B and scalar c we have the following properties of transpose:The transpose of a matrix was introduced in 1858 by the British mathematician Arthur Cayley.[1]If A is an m × n matrix then AT is an n × m matrix.Formally, the i th row, j th column element of AT is the j th row, i th column element of A:In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted as AT (also written A′, Atr, tA or At). It is achieved by any one of the following equivalent actions:
Gram–Schmidt process

System of linear equations
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:where A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.A homogeneous system is equivalent to a matrix equation of the formA system of linear equations is homogeneous if all of the constant terms are zero:There is also a quantum algorithm for linear systems of equations.[3]A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.is given byCramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemThe last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsFor example, consider the following system:The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:Here x is the free variable, and y and z are dependent.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.The solution set to this system can be described by the following equations:For example, consider the following system:To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.There are several algorithms for solving a system of linear equations.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.For example, the equationsA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.For a more complicated example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For example, the equationsThe equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.The following pictures illustrate this trichotomy in the case of two variables:In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.A linear system may behave in any one of three possible ways:A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.The number of vectors in a basis for the span is now expressed as the rank of the matrix.where A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The vector equation is equivalent to a matrix equation of the formThis allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.A general system of m linear equations with n unknowns can be written asNow substitute this expression for x into the bottom equation:The simplest kind of linear system involves two equations and two variables:Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given byIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,
Euclidean space
Another line of generalization is to consider other number fields than one of real numbers. Over complex numbers, a Hilbert space can be seen as a generalization of Euclidean dot product structure, although the definition of the inner product becomes a sesquilinear form for compatibility with metric structure.If one replaces the inner product of a Euclidean space with an indefinite quadratic form, the result is a pseudo-Euclidean space. Smooth manifolds built from such spaces are called pseudo-Riemannian manifolds. Perhaps their most famous application is the theory of relativity, where flat spacetime is a pseudo-Euclidean space called Minkowski space, where rotations correspond to motions of hyperbolic spaces mentioned above. Further generalization to curved spacetimes form pseudo-Riemannian manifolds, such as in general relativity.Also, the concept of a Riemannian manifold permits an expression of the Euclidean structure in any smooth coordinate system, via metric tensor. From this tensor one can compute the Riemann curvature tensor. Where the latter equals to zero, the metric structure is locally Euclidean (it means that at least some open set in the coordinate space is isometric to a piece of Euclidean space), no matter whether coordinates are affine or curvilinear.A smooth manifold is a Hausdorff topological space that is locally diffeomorphic to Euclidean space. Diffeomorphism does not respect distance and angle, but if one additionally prescribes a smoothly varying inner product on the manifold's tangent spaces, then the result is what is called a Riemannian manifold. Put differently, a Riemannian manifold is a space constructed by deforming and patching together Euclidean spaces. Such a space enjoys notions of distance and angle, but they behave in a curved, non-Euclidean manner. The simplest Riemannian manifold, consisting of Rn with a constant inner product, is essentially identical to Euclidean n-space itself. Less trivial examples are n-sphere and hyperbolic spaces. Discovery of the latter in the 19th century was branded as the non-Euclidean geometry.Although Euclidean spaces are no longer considered to be the only possible setting for a geometry, they act as prototypes for other geometric objects. Ideas and terminology from Euclidean geometry (both traditional and analytic) are pervasive in modern mathematics, where other geometric objects share many similarities with Euclidean spaces, share part of their structure, or embed Euclidean spaces.Topographical maps and technical drawings are planar Euclidean. An idea behind them is the scale invariance of Euclidean geometry, that permits to represent large objects in a small sheet of paper, or a screen.Aside from countless uses in fundamental mathematics, a Euclidean model of the physical space can be used to solve many practical problems with sufficient precision. Two usual approaches are a fixed, or stationary reference frame (i.e. the description of a motion of objects as their positions that change continuously with time), and the use of Galilean space-time symmetry (such as in Newtonian mechanics). To both of them the modern Euclidean geometry provides a convenient formalism; for example, the space of Galilean velocities is itself a Euclidean space (see relative velocity for details).Since Euclidean space is a metric space, it is also a topological space with the natural topology induced by the metric. The metric topology on En is called the Euclidean topology, and it is identical to the standard topology on Rn. A set is open if and only if it contains an open ball around each of its points; in other words, open balls form a base of the topology. The topological dimension of the Euclidean n-space equals n, which implies that spaces of different dimension are not homeomorphic. A finer result is the invariance of domain, which proves that any subset of n-space, that is (with its subspace topology) homeomorphic to an open subset of n-space, is itself open.Root systems are special sets of Euclidean vectors. A root system is often identical to the set of vertices of a regular polytope.The concept of a polytope belongs to affine geometry, which is more general than Euclidean. But Euclidean geometry distinguish regular polytopes. For example, affine geometry does not see the difference between an equilateral triangle and a right triangle, but in Euclidean space the former is regular and the latter is not.Polytope is a concept that generalizes polygons on a plane and polyhedra in 3-dimensional space (which are among the earliest studied geometrical objects). A simplex is a generalization of a line segment (1-simplex) and a triangle (2-simplex). A tetrahedron is a 3-simplex.A triangle can be thought of as a 3-gon on a plane, a special (and the first meaningful in Euclidean geometry) case of a polygon.A (non-degenerate) triangle is defined by three points not lying on the same line. Any triangle lies on one plane. The concept of triangle is not specific to Euclidean spaces, but Euclidean triangles have numerous special properties and define many derived objects.This is not only a line which a pair (A, B) of distinct points defines. Points on the line which lie between A and B, together with A and B themselves, constitute a line segment A B. Any line segment has the length, which equals to distance between A and B. If A = B, then the segment is degenerate and its length equals to 0, otherwise the length is positive.Any two distinct points lie on exactly one line. Any line and a point outside it lie on exactly one plane. More generally, the properties of flats and their incidence of Euclidean space are shared with affine geometry, whereas the affine geometry is devoid of distances and angles.The simplest (after points) objects in Euclidean space are flats, or Euclidean subspaces of lesser dimension. Points are 0-dimensional flats, 1-dimensional flats are called (straight) lines, and 2-dimensional flats are planes. (n − 1)-dimensional flats are called hyperplanes.See below about expression of the Euclidean structure in curvilinear coordinates.Another approach, which goes in line with ideas of differential geometry and conformal geometry, is orthogonal coordinates, where coordinate hypersurfaces of different coordinates are orthogonal, although curved. Examples include the polar coordinate system on Euclidean plane, the second important plane coordinate system.Cartesian coordinates are arguably the standard, but not the only possible option for a Euclidean space. Skew coordinates are compatible with the affine structure of En, but make formulae for angles and distances more complicated.The group structure determines which conditions a metric space needs to satisfy to be a Euclidean space:Along with translations, rotations, reflections, as well as the identity transformation, Euclidean motions comprise also glide reflections (for n ≥ 2), screw operations and rotoreflections (for n ≥ 3), and even more complex combinations of primitive transformations for n ≥ 4.The structure of Euclidean spaces – distances, lines, vectors, angles (up to sign), and so on – is invariant under the transformations of their associated Euclidean group. For instance, translations form a commutative subgroup that acts freely and transitively on En, while the stabilizer of any point there is the aforementioned O(n).As the group of all isometries, ISO(n), the Euclidean group is important because it makes Euclidean geometry a case of Klein geometry, a theoretical framework including many alternative geometries.The Euclidean group E(n), also referred to as the group of all isometries ISO(n), treats translations, rotations, and reflections in a uniform way, considering them as group actions in the context of group theory, and especially in Lie group theory. These group actions preserve the Euclidean structure.Among linear transforms in O(n) which reverse the orientation are hyperplane reflections. This is the only possible case for n ≤ 2, but starting from three dimensions, such isometry in the general position is a rotoreflection.Groups SO(n) are well-studied for n ≤ 4. There are no non-trivial rotations in 0- and 1-spaces. Rotations of a Euclidean plane (n = 2) are parametrized by the angle (modulo 1 turn). Rotations of a 3-space are parametrized with axis and angle, whereas a rotation of a 4-space is a superposition of two 2-dimensional rotations around perpendicular planes.But a Euclidean space is orientable.[footnote 2] Each of these transformations either preserves or reverses orientation depending on whether its determinant is +1 or −1 respectively. Only transformations which preserve orientation, which form the special orthogonal group SO(n), are considered (proper) rotations. This group has, as a Lie group, the same dimension n(n − 1) /2 and is the identity component of O(n).where QT is the transpose of Q and I is the identity matrix.Such transforms constitute a group called the orthogonal group O(n). Its elements Q are exactly solutions of a matrix equationSymmetries of a Euclidean space are transformations which preserve the Euclidean metric (called isometries). Although aforementioned translations are most obvious of them, they have the same structure for any affine space and do not show a distinctive character of Euclidean geometry. Another family of symmetries leave one point fixed, which may be seen as the origin without loss of generality. All transformations, which preserves the origin and the Euclidean metric, are linear maps. Such transformations Q must, for any x and y, satisfy:Unlike the aforementioned situation with distance, the scale of angles is the same in pure mathematics, physics, and computing. It does not depend on the scale of distances; all distances may be multiplied by some fixed factor, and all angles will be preserved. Usually, the angle is considered a dimensionless quantity, but there are different units of measurement, such as radian (preferred in pure mathematics and theoretical physics) and degree (°) (preferred in most applications).The angle does not change if vectors x and y are multiplied by positive numbers.where arccos is the arccosine function. It is useful only for n > 1,[footnote 1] and the case n = 2 is somewhat special. Namely, on an oriented Euclidean plane one can define an angle between two vectors as a number defined modulo 1 turn (usually denoted as either 2π or 360°), such that ∠y x = −∠x y. This oriented angle is equal either to the angle θ from the formula above or to −θ. If one non-zero vector is fixed (such as the first basis vector), then each non-zero vector is uniquely defined by its magnitude and angle.The (non-reflex) angle θ (0° ≤ θ ≤ 180°) between vectors x and y is then given byThe metric space structure is the main reason behind the use of real numbers R, not some other ordered field, as the mathematical foundation of Euclidean (and many other) spaces. Euclidean space is a complete metric space, a property which is impossible to achieve operating over rational numbers, for example.This distance function (which makes a metric space) is sufficient to define all Euclidean geometry, including the dot product. Thus, a real coordinate space together with this Euclidean structure is called Euclidean space. Its vectors form an inner product space (in fact a Hilbert space), and a normed vector space.This distance function is called the Euclidean metric. This formula expresses a special case of the Pythagorean theorem.Finally, one can use the norm to define a metric (or distance function) on Rn byThis length function satisfies the required properties of a norm and is called the Euclidean norm on Rn.The inner product of x with itself is always non-negative. This product allows us to define the "length" of a vector x through square root:where xi and yi are ith coordinates of vectors x and y respectively. The result is always a real number.These are distances between points and the angles between lines or vectors, which satisfy certain conditions (see below), which makes a set of points a Euclidean space. The natural way to obtain these quantities is by introducing and using the standard inner product (also known as the dot product) on Rn.[3] The inner product of any two real n-vectors x and y is defined byA Euclidean space is not technically a vector space but rather an affine space, on which a vector space acts by translations, or, conversely, a Euclidean vector is the difference (displacement) in an ordered pair of points, not a single point. Intuitively, the distinction says merely that there is no canonical choice of where the origin should go in the space, because it can be translated anywhere. When a certain point is chosen, it can be declared the origin and subsequent calculations may ignore the difference between a point and its coordinate vector, as said above. See point–vector distinction for details.Once the Euclidean plane has been described in this language, it is actually a simple matter to extend its concept to arbitrary dimensions. For the most part, the vocabulary, formulae, and calculations are not made any more difficult by the presence of more dimensions. (However, rotations are more subtle in high dimensions, and visualizing high-dimensional spaces remains difficult, even for experienced mathematicians.)In order to make all of this mathematically precise, the theory must clearly define the notions of distance, angle, translation, and rotation for a mathematically described space. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a "mathematical" space is a number, not something expressed in inches or metres. The standard way to define such space, as carried out in the remainder of this article, is to define the Euclidean plane as a two-dimensional real vector space equipped with an inner product.[3] The reason for working with arbitrary vector spaces instead of Rn is that it is often preferable to work in a coordinate-free manner (that is, without choosing a preferred basis). For then:One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation about a fixed point in the plane, in which every point in the plane turns about that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).From the modern viewpoint, there is essentially only one Euclidean space of each dimension. While Euclidean space is defined by a set of axioms, these axioms do not specify how the points are to be represented.[2] Euclidean space can, as one possible choice of representation, be modeled using Cartesian coordinates. In this case, the Euclidean space is then modeled by the real coordinate space (Rn) of the same dimension. In one dimension, this is the real line; in two dimensions, it is the Cartesian plane; and in higher dimensions it is a coordinate space with three or more real number coordinates. Mathematicians denote the n-dimensional Euclidean space by En if they wish to emphasize its Euclidean nature, but Rn is used as well since the latter is assumed to have the standard Euclidean structure, and these two structures are not always distinguished. Euclidean spaces have finite dimension.[3]Classical Greek geometry defined the Euclidean plane and Euclidean three-dimensional space using certain postulates, while the other properties of these spaces were deduced as theorems. Geometric constructions are also used to define rational numbers. When algebra and mathematical analysis became developed enough, this relation reversed and now it is more common to define Euclidean space using Cartesian coordinates and the ideas of analytic geometry. It means that points of the space are specified with collections of real numbers, and geometric shapes are defined as equations and inequalities. This approach brings the tools of algebra and calculus to bear on questions of geometry and has the advantage that it generalizes easily to Euclidean spaces of more than three dimensions.In geometry, Euclidean space encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces. It is named after the Ancient Greek mathematician Euclid of Alexandria.[1] The term "Euclidean" distinguishes these spaces from other types of spaces considered in modern geometry. Euclidean spaces also generalize to higher dimensions.
Vector algebra
A seven-dimensional cross product is similar to the cross product in that its result is a vector orthogonal to the two arguments; there is however no natural way of selecting one of the possible such products.The cross product does not readily generalise to other dimensions, though the closely related exterior product does, whose result is a bivector. In two dimensions this is simply a pseudoscalarFor arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector.The cross product can be written asThe length of a × b can be interpreted as the area of the parallelogram having a and b as sides.The cross product a × b is defined so that a, b, and a × b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.where θ is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist two unit vectors that are perpendicular to both a and b, namely, n and (–n).The cross product (also called the vector product or outer product) is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a × b, is a vector perpendicular to both a and b and is defined asThe dot product can also be defined as the sum of the products of the components of each vector aswhere θ is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of that component of b that points in the same direction as a.The dot product of two vectors a and b (sometimes called the inner product, or, since its result is a scalar, the scalar product) is denoted by a ∙ b and is defined as:Scalar multiplication is distributive over vector addition in the following sense: r(a + b) = ra + rb for all vectors a and b and all scalars r. One can also show that a − b = a + (−1)b.If r is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples (r = −1 and r = 2) are given below:Intuitively, multiplying by a scalar r stretches a vector out by a factor of r. Geometrically, this can be visualized (at least in the case when r is an integer) as placing r copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.A vector may also be multiplied, or re-scaled, by a real number r. In the context of conventional vector algebra, real numbers are often called scalars (from scale) to distinguish them from vectors. The operation of multiplying a vector by a scalar is called scalar multiplication. The resulting vector isSubtraction of two vectors may also be performed by adding the opposite of the second vector to the first vector, that is, a − b = a + (−b).Subtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a − b, as illustrated below:The difference of a and b isThis addition method is sometimes called the parallelogram rule because a and b form the sides of a parallelogram and a + b is one of the diagonals. If a and b are bound vectors that have the same base point, this point will also be the base point of a + b. One can check geometrically that a + b = b + a and (a + b) + c = a + (b + c).The addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b is defined as:In mathematics and linear algebra, vector algebra refers to algebraic operations in vector spaces. Most commonly, it refers to operations on Euclidean vectors.
Cross product
Several kinds of triple products and products of more than three vectors were also examined. The above-mentioned triple product expansion was also included.Two main kinds of vector multiplications were defined, and they were called as follows:The cross notation and the name "cross product" began with Gibbs. Originally they appeared in privately published notes for his students in 1881 as Elements of Vector Analysis. The utility for mechanics was noted by Aleksandr Kotelnikov. Gibbs's notation and the name "cross product" later reached a wide audience through Vector Analysis, a textbook by Edwin Bidwell Wilson, a former student. Wilson rearranged material from Gibbs's lectures, together with material from publications by Heaviside, Föpps, and Hamilton. He divided vector analysis into three parts:Largely independent of this development, and largely unappreciated at the time, Hermann Grassmann created a geometric algebra not tied to dimension two or three, with the exterior product playing a central role. In 1853 Augustin-Louis Cauchy, a contemporary of Grassmann, published a paper on algebraic keys which were used to solve equations and had the same multiplication properties as the cross product.[22][23] William Kingdon Clifford combined the algebras of Hamilton and Grassmann to produce Clifford algebra, where in the case of three-dimensional vectors the bivector produced from two vectors dualizes to a vector, thus reproducing the cross product.Oliver Heaviside in England and Josiah Willard Gibbs, a professor at Yale University in Connecticut, also felt that quaternion methods were too cumbersome, often requiring the scalar or vector part of a result to be extracted. Thus, about forty years after the quaternion product, the dot product and cross product were introduced—to heated opposition. Pivotal to (eventual) acceptance was the efficiency of the new approach, allowing Heaviside to reduce the equations of electromagnetism from Maxwell's original 20 to the four commonly seen today.[21]In 1878 William Kingdon Clifford published his Elements of Dynamic which was an advanced text for its time. He defined the product of two vectors[20] to have magnitude equal to the area of the parallelogram of which they are two sides, and direction perpendicular to their plane.In 1773, the Italian mathematician Joseph Louis Lagrange, (born Giuseppe Luigi Lagrangia), introduced the component form of both the dot and cross products in order to study the tetrahedron in three dimensions.[19] In 1843 the Irish mathematical physicist Sir William Rowan Hamilton introduced the quaternion product, and with it the terms "vector" and "scalar". Given two quaternions [0, u] and [0, v], where u and v are vectors in R3, their quaternion product can be summarized as [−u ⋅ v, u × v]. James Clerk Maxwell used Hamilton's quaternion tools to develop his famous electromagnetism equations, and for this and other reasons quaternions for a time were an essential part of physics education.See § Alternative ways to compute the cross product for numerical details.If the cross product is defined as a binary operation, it takes as input exactly two vectors. If its output is not required to be a vector or a pseudovector but instead a matrix, then it can be generalized in an arbitrary number of dimensions.[16][17][18]These products are all multilinear and skew-symmetric, and can be defined in terms of the determinant and parity.In the context of multilinear algebra, the cross product can be seen as the (1,2)-tensor (a mixed tensor, specifically a bilinear map) obtained from the 3-dimensional volume form,[note 2] a (0,3)-tensor, by raising an index.As mentioned above, the cross product can be interpreted in three dimensions as the Hodge dual of the exterior product. In any finite n dimensions, the Hodge dual of the exterior product of n-1 vectors is a vector. So, instead of a binary operation, in arbitrary finite dimensions, the cross product is generalized as the Hodge dual of the exterior product of some given n-1 vectors. This generalization is called external product[15]The exterior product and dot product can be combined (through summation) to form the geometric product.In general dimension, there is no direct analogue of the binary cross product that yields specifically a vector. There is however the exterior product, which has similar properties, except that the exterior product of two vectors is now a 2-vector instead of an ordinary vector. As mentioned above, the cross product can be interpreted as the exterior product in three dimensions by using the Hodge dual to map 2-vectors to vectors. The Hodge dual of the exterior product yields an (n − 2)-vector, which is a natural generalization of the cross product in any number of dimensions.A cross product for 7-dimensional vectors can be obtained in the same way by using the octonions instead of the quaternions. The nonexistence of nontrivial vector-valued cross products of two vectors in other dimensions is related to the result from Hurwitz's theorem that the only normed division algebras are the ones with dimension 1, 2, 4, and 8.Alternatively, using the above identification of the 'purely imaginary' quaternions with R3, the cross product may be thought of as half of the commutator of two quaternions.For instance, the above given cross product relations among i, j, and k agree with the multiplicative relations among the quaternions i, j, and k. In general, if a vector [a1, a2, a3] is represented as the quaternion a1i + a2j + a3k, the cross product of two vectors can be obtained by taking their product as quaternions and deleting the real part of the result. The real part will be the negative of the dot product of the two vectors.The cross product can also be described in terms of quaternions, and this is why the letters i, j, k are a convention for the standard basis on R3. The unit vectors i, j, k correspond to "binary" (180 deg) rotations about their respective axes (Altmann, S. L., 1986, Ch. 12), said rotations being represented by "pure" quaternions (zero real part) with unit norms.The cross product can be seen as one of the simplest Lie products, and is thus generalized by Lie algebras, which are axiomatized as binary products satisfying the axioms of multilinearity, skew-symmetry, and the Jacobi identity. Many Lie algebras exist, and their study is a major field of mathematics, called Lie theory.There are several ways to generalize the cross product to the higher dimensions.A handedness-free approach is possible using exterior algebra.Because the cross product may also be a (true) vector, it may not change direction with a mirror image transformation. This happens, according to the above relationships, if one of the operands is a (true) vector and the other one is a pseudovector (e.g., the cross product of two vectors). For instance, a vector triple product involving three (true) vectors is a (true) vector.So by the above relationships, the unit basis vectors i, j and k of an orthonormal, right-handed (Cartesian) coordinate frame must all be pseudovectors (if a basis of mixed vector types is disallowed, as it normally is) since i × j = k, j × k = i and k × i = j.More generally, the result of a cross product may be either a vector or a pseudovector, depending on the type of its operands (vectors or pseudovectors). Namely, vectors and pseudovectors are interrelated in the following ways under application of the cross product:When measurable quantities involve cross products, the handedness of the coordinate systems used cannot be arbitrary. However, when physics laws are written as equations, it should be possible to make an arbitrary choice of the coordinate system (including handedness). To avoid problems, one should be careful to never write down an equation where the two sides do not behave equally under all transformations that need to be considered. For example, if one side of the equation is a cross product of two vectors, one must take into account that when the handedness of the coordinate system is not fixed a priori, the result is not a (true) vector but a pseudovector. Therefore, for consistency, the other side must also be a pseudovector.[citation needed]This can be thought of as the oriented multi-dimensional element "perpendicular" to the bivector. Only in three dimensions is the result an oriented line element – a vector – whereas, for example, in 4 dimensions the Hodge dual of a bivector is two-dimensional – another oriented plane element. So, only in three dimensions is the cross product of a and b the vector dual to the bivector a ∧ b: it is perpendicular to the bivector, with orientation dependent on the coordinate system's handedness, and has the same magnitude relative to the unit normal vector as a ∧ b has relative to the unit bivector; precisely the properties described above.The cross product can be viewed in terms of the exterior product. In this context, it is an external product.[14] This view allows for a natural geometric interpretation of the cross product. In exterior algebra the exterior product (or wedge product) of two vectors is a bivector. A bivector is an oriented plane element, in much the same way that a vector is an oriented line element. Given two vectors a and b, one can view the bivector a ∧ b as the oriented parallelogram spanned by a and b. The cross product is then obtained by taking the Hodge dual of the bivector a ∧ b, mapping 2-vectors to vectors:The trick of rewriting a cross product in terms of a matrix multiplication appears frequently in epipolar and multi-view geometry, in particular when deriving matching constraints.In vector calculus, the cross product is used to define the formula for the vector operator curl.The cross product frequently appears in the description of rigid motions. Two points P and Q on a rigid body can be related by:The cross product is used in calculating the volume of a polyhedron such as a tetrahedron or parallelepiped.The cross product can be used to calculate the normal for a triangle or polygon, an operation frequently performed in computer graphics. For example, the winding of a polygon (clockwise or anticlockwise) about a point within the polygon can be calculated by triangulating the polygon (like spoking a wheel) and summing the angles (between the spokes) using the cross product to keep track of the sign of each angle.The cross product appears in the calculation of the distance of two skew lines (lines not in the same plane) from each other in three-dimensional space.The cross product has applications in various contexts: e.g. it is used in computational geometry, physics and engineering. A non-exhaustive list of examples follows.then:IfSimilarly to the mnemonic device above, a "cross" or X can be visualized between the two vectors in the equation. This may be helpful for remembering the correct cross product formula.Since the first diagonal in Sarrus's scheme is just the main diagonal of the above-mentioned 3×3 matrix, the first three letters of the word xyzzy can be very easily remembered.The second and third equations can be obtained from the first by simply vertically rotating the subscripts, x → y → z → x. The problem, of course, is how to remember the first equation, and two options are available for this purpose: either to remember the relevant two diagonals of Sarrus's scheme (those containing i), or to remember the xyzzy sequence.then:where:IfThe word "xyzzy" can be used to remember the definition of the cross product.In classical mechanics: representing the cross product by using the Levi-Civita symbol can cause mechanical symmetries to be obvious when physical systems are isotropic. (An example: consider a particle in a Hooke's Law potential in three-space, free to oscillate in three dimensions; none of these dimensions are "special" in any sense, so symmetries lie in the cross-product-represented angular momentum, which are made clear by the abovementioned Levi-Civita representation).[citation needed]in which repeated indices are summed over the values 1 to 3. Note that this representation is another form of the skew-symmetric representation of the cross product:The cross product can alternatively be defined in terms of the Levi-Civita symbol εijk and a dot product ηmi (= δmi for an orthonormal basis), which are useful in converting vector notation for tensor applications:As mentioned above, the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3), whose elements can be identified with the 3×3 skew-symmetric matrices. The map a → [a]× provides an isomorphism between R3 and so(3). Under this map, the cross product of 3-vectors corresponds to the commutator of 3x3 skew-symmetric matrices.The above-mentioned triple product expansion (bac–cab rule) can be easily proven using this notation.and from fact that [a]× is skew-symmetric it follows thatFrom the general properties of the cross product follows immediately thatThis notation is also often much easier to work with, for example, in epipolar geometry.This result can be generalized to higher dimensions using geometric algebra. In particular in any dimension bivectors can be identified with skew-symmetric matrices, so the product between a skew-symmetric matrix and vector is equivalent to the grade-1 part of the product of a bivector and vector.[13] In three dimensions bivectors are dual to vectors so the product is equivalent to the cross product, with the bivector instead of its vector dual. In higher dimensions the product can still be calculated but bivectors have more degrees of freedom and are not equivalent to vectors.[13]thenAlso, if a is itself expressed as a cross product:The columns [a]×,i of the skew-symmetric matrix for a vector a can be also obtained by calculating the cross product with unit vectors, i.e.:where superscript T refers to the transpose operation, and [a]× is defined by:The vector cross product also can be expressed as the product of a skew-symmetric matrix and a vector:[11]for every vector x in R3. The cross product with n therefore describes the infinitesimal generator of the rotations about n. These infinitesimal generators form the Lie algebra so(3) of the rotation group SO(3), and we obtain the result that the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3).The cross product conveniently describes the infinitesimal generators of rotations in R3. Specifically, if n is a unit vector in R3 and R(φ, n) denotes a rotation about the axis through the origin specified by n, with angle φ (measured in radians, counterclockwise when viewed from the tip of n), thenIf a = c and b = d this simplifies to the formula above.It is a special case of another formula, also sometimes called Lagrange's identity, which is the three dimensional case of the Binet-Cauchy identity:[11][12]In R3, Lagrange's equation is a special case of the multiplicativity |vw| = |v||w| of the norm in the quaternion algebra.The same result is found directly using the components of the cross product found from:where a and b may be n-dimensional vectors. This also shows that the Riemannian volume form for surfaces is exactly the surface element from vector calculus. In the case where n = 3, combining these two equations results in the expression for the magnitude of the cross product in terms of its components:[10]can be compared with another relation involving the right-hand side, namely Lagrange's identity expressed as:[9]The relation:The combination of this requirement and the property that the cross product be orthogonal to its constituents a and b provides an alternative definition of the cross product.[8]which is the magnitude of the cross product expressed in terms of θ, equal to the area of the parallelogram defined by a and b (see definition above).Invoking the Pythagorean trigonometric identity one obtains:the above given relationship can be rewritten as follows:The right-hand side is the Gram determinant of a and b, the square of the area of the parallelogram defined by the vectors. This condition determines the magnitude of the cross product. Namely, since the dot product is defined, in terms of the angle θ between the two vectors, as:The cross product and the dot product are related by:where I is the identity matrix.Other identities relate the cross product to the scalar triple product:where ∇2 is the vector Laplacian operator.The mnemonic "BAC minus CAB" is used to remember the order of the vectors in the right hand member. This formula is used in physics to simplify vector calculations. A special case, regarding gradients and useful in vector calculus, isThe vector triple product is the cross product of a vector with the result of another cross product, and is related to the dot product by the following formulaIt is the signed volume of the parallelepiped with edges a, b and c and as such the vectors can be used in any order that's an even permutation of the above ordering. The following therefore are equal:The cross product is used in both forms of the triple product. The scalar triple product of three vectors is defined aswhere a and b are vectors that depend on the real variable t.The product rule of differential calculus applies to any bilinear operation, and therefore also to the cross product:For the sum of two cross products, the following identity holds:The cross product of two vectors lies in the null space of the 2 × 3 matrix with the vectors as rows:More generally, the cross product obeys the following identity under matrix transformations:From the geometrical definition, the cross product is invariant under proper rotations about the axis defined by a × b. In formulae:As b − c cannot be simultaneously parallel (for the cross product to be 0) and perpendicular (for the dot product to be 0) to a, it must be the case that b and c cancel: b = c.If, in addition to a × b = a × c and a ≠ 0 as above, it is the case that a ⋅ b = a ⋅ c thenfor some scalar t.This can be the case where b and c cancel, but additionally where a and b − c are parallel; that is, they are related by a scale factor t, leading to:Distributivity, linearity and Jacobi identity show that the R3 vector space together with vector addition and the cross product forms a Lie algebra, the Lie algebra of the real orthogonal group in 3 dimensions, SO(3). The cross product does not obey the cancellation law: that is, a × b = a × c with a ≠ 0 does not imply b = c, but only that:It is not associative, but satisfies the Jacobi identity:and compatible with scalar multiplication so thatdistributive over addition,The cross product is anticommutative,The self cross product of a vector is the zero vector:If the cross product of two vectors is the zero vector (i.e. a × b = 0), then either one or both of the inputs is the zero vector, (a = 0 or b = 0) or else they are parallel or antiparallel (a ∥ b) so that the sine of the angle between them is zero (θ = 0° or θ = 180° and sinθ = 0).Unit vectors enable two convenient identities: the dot product of two unit vectors yields the cosine (which may be positive or negative) of the angle between the two unit vectors. The magnitude of the cross product of the two unit vectors yields the sine (which will always be positive).Because the magnitude of the cross product goes by the sine of the angle between its arguments, the cross product can be thought of as a measure of perpendicularity in the same way that the dot product is a measure of parallelism. Given two unit vectors, their cross product has a magnitude of 1 if the two are perpendicular and a magnitude of zero if the two are parallel. The dot product of two unit vectors behaves just oppositely: it is zero when the unit vectors are perpendicular and 1 if the unit vectors are parallel.Since the result of the scalar triple product may be negative, the volume of the parallelepiped is given by its absolute value. For instance,Indeed, one can also compute the volume V of a parallelepiped having a, b and c as edges by using a combination of a cross product and a dot product, called scalar triple product (see Figure 2):The magnitude of the cross product can be interpreted as the positive area of the parallelogram having a and b as sides (see Figure 1):which gives the components of the resulting vector directly.Using cofactor expansion along the first row instead, it expands to[6]This determinant can be computed using Sarrus's rule or cofactor expansion. Using Sarrus's rule, it expands toThe cross product can also be expressed as the formal[note 1] determinant:Using column vectors, we can represent the same result as follows:meaning that the three scalar components of the resulting vector s = s1i + s2j + s3k = u × v areThis can be interpreted as the decomposition of u × v into the sum of nine simpler cross products involving vectors aligned with i, j, or k. Each one of these nine cross products operates on two vectors that are easy to handle as they are either parallel or orthogonal to each other. From this decomposition, by using the above-mentioned equalities and collecting similar terms, we obtain:Their cross product u × v can be expanded using distributivity:These equalities, together with the distributivity and linearity of the cross product (but both do not follow easily from the definition given above), are sufficient to determine the cross product of any two vectors u and v. Each vector can be defined as the sum of three orthogonal components parallel to the standard basis vectors:The definition of the cross product also implies thatwhich imply, by the anticommutativity of the cross product, thatThe standard basis vectors i, j, and k satisfy the following equalities in a right hand coordinate system:Both the cross notation (a × b) and the name cross product were possibly inspired by the fact that each scalar component of a × b is computed by multiplying non-corresponding components of a and b. Conversely, a dot product a ⋅ b involves multiplications between corresponding components of a and b. As explained below, the cross product can be expressed in the form of a determinant of a special 3 × 3 matrix. According to Sarrus's rule, this involves multiplications between matrix elements identified by crossed diagonals.In 1877, to emphasize the fact that the result of a dot product is a scalar while the result of a cross product is a vector, William Kingdon Clifford coined the alternative names scalar product and vector product for the two operations.[5] These alternative names are still widely used in the literature.In 1881, Josiah Willard Gibbs, and independently Oliver Heaviside, introduced both the dot product and the cross product using a period (a . b) and an "x" (a x b), respectively, to denote them.[5]This, however, creates a problem because transforming from one arbitrary reference system to another (e.g., a mirror image transformation from a right-handed to a left-handed coordinate system), should not change the direction of n. The problem is clarified by realizing that the cross product of two vectors is not a (true) vector, but rather a pseudovector. See cross product and handedness for more detail.Using the cross product requires the handedness of the coordinate system to be taken into account (as explicit in the definition above). If a left-handed coordinate system is used, the direction of the vector n is given by the left-hand rule and points in the opposite direction.By convention, the direction of the vector n is given by the right-hand rule, where one simply points the forefinger of the right hand in the direction of a and the middle finger in the direction of b. Then, the vector n is coming out of the thumb (see the picture on the right). Using this rule implies that the cross product is anti-commutative, i.e., b × a = −(a × b). By pointing the forefinger toward b first, and then pointing the middle finger toward a, the thumb will be forced in the opposite direction, reversing the sign of the product vector.where θ is the angle between a and b in the plane containing them (hence, it is between 0° and 180°), ‖a‖ and ‖b‖ are the magnitudes of vectors a and b, and n is a unit vector perpendicular to the plane containing a and b in the direction given by the right-hand rule (illustrated). If the vectors a and b are parallel (i.e., the angle θ between them is either 0° or 180°), by the above formula, the cross product of a and b is the zero vector 0.The cross product is defined by the formula[3][4]The cross product a × b is defined as a vector c that is perpendicular (orthogonal) to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span.The cross product of two vectors a and b is defined only in three-dimensional space and is denoted by a × b. In physics, sometimes the notation a ∧ b is used,[2] though this is avoided in mathematics to avoid confusion with the exterior product.Like the dot product, it depends on the metric of Euclidean space, but unlike the dot product, it also depends on a choice of orientation or "handedness". The product can be generalized in various ways; it can be made independent of orientation by changing the result to pseudovector, or in arbitrary dimensions the exterior product of vectors can be used with a bivector or two-form result. Also, using the orientation and metric structure just as for the traditional 3-dimensional cross product, one can in n dimensions take the product of n − 1 vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions.[1] If one adds the further requirement that the product be uniquely defined, then only the 3-dimensional cross product qualifies. (See § Generalizations, below, for other dimensions.)If two vectors have the same direction (or have the exact opposite direction from one another, i.e. are not linearly independent) or if either one has zero length, then their cross product is zero. More generally, the magnitude of the product equals the area of a parallelogram with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths. The cross product is anticommutative (i.e., a × b = −(b × a)) and is distributive over addition (i.e., a × (b + c) = a × b + a × c). The space R3 together with the cross product is an algebra over the real numbers, which is neither commutative nor associative, but is a Lie algebra with the cross product being the Lie bracket.In mathematics and vector algebra, the cross product or vector product (occasionally directed area product to emphasize the geometric significance) is a binary operation on two vectors in three-dimensional space (R3) and is denoted by the symbol ×. Given two linearly independent vectors a and b, the cross product, a × b, is a vector that is perpendicular to both a and b and thus normal to the plane containing them. It has many applications in mathematics, physics, engineering, and computer programming. It should not be confused with dot product (projection product).
Triple product

Seven-dimensional cross product
There are also trivial products. As noted already, a binary product only exists in 7, 3, 1 and 0 dimensions, the last two being identically zero. A further trivial 'product' arises in even dimensions, which takes a single vector and produces a vector of the same magnitude orthogonal to it through the left contraction with a suitable bivector. In two dimensions this is a rotation through a right angle.One version of the product of three vectors in eight dimensions is given byWith these conditions a non-trivial cross product only exists:The Gram determinant is the squared volume of the parallelotope with a1, ..., ak as edges.Nonzero binary cross products exist only in three and seven dimensions. Further products are possible when lifting the restriction that it must be a binary product.[19][20] We require the product to be multi-linear, alternating, vector-valued, and orthogonal to each of the input vectors. The orthogonality requirement implies that in n dimensions, no more than n − 1 vectors can be used. The magnitude of the product should equal the volume of the parallelotope with the vectors as edges, which can be calculated using the Gram determinant. The conditions areIn three dimensions the cross product is invariant under the action of the rotation group, SO(3), so the cross product of x and y after they are rotated is the image of x × y under the rotation. But this invariance is not true in seven dimensions; that is, the cross product is not invariant under the group of rotations in seven dimensions, SO(7). Instead it is invariant under the exceptional Lie group G2, a subgroup of SO(7).[8][16]where [x, y, z] is the associator.The failure of the 7-dimension cross product to satisfy the Jacobi identity is due to the nonassociativity of the octonions. In fact,The cross product only exists in three and seven dimensions as one can always define a multiplication on a space of one higher dimension as above, and this space can be shown to be a normed division algebra. By Hurwitz's theorem such algebras only exist in one, two, four, and eight dimensions, so the cross product must be in zero, one, three or seven dimensions. The products in zero and one dimensions are trivial, so non-trivial cross products only exist in three and seven dimensions.[17][18]The space ℝ⊕V with this multiplication is then isomorphic to the octonions.[16]Conversely, suppose V is a 7-dimensional Euclidean space with a given cross product. Then one can define a bilinear multiplication on ℝ⊕V as follows:Just as the 3-dimensional cross product can be expressed in terms of the quaternions, the 7-dimensional cross product can be expressed in terms of the octonions. After identifying ℝ7 with the imaginary octonions (the orthogonal complement of the real line in O), the cross product is given in terms of octonion multiplication byThis is combined with the exterior product to give the cross productA similar calculation is done is seven dimensions, except as trivectors form a 35-dimensional space there are many trivectors that could be used, though not just any trivector will do. The trivector that gives the same product as the above coordinate transform isThis is bilinear, alternate, has the desired magnitude, but is not vector valued. The vector, and so the cross product, comes from the product of this bivector with a trivector. In three dimensions up to a scale factor there is only one trivector, the pseudoscalar of the space, and a product of the above bivector and one of the two unit trivectors gives the vector result, the dual of the bivector.The product can also be calculated using geometric algebra. The product starts with the exterior product, a bivector valued product of two vectors:It can be seen that both multiplication rules follow from the same Fano diagram by simply renaming the unit vectors, and changing the sense of the center unit vector. Considering all possible permutations of the basis there are 480 multiplication tables and so 480 cross products like this.[14]also obtained directly from the diagram with the rule that any two unit vectors on a straight line are connected by multiplication to the third unit vector on that straight line with signs according to the arrows (sign of the permutation that orders the unit vectors).orTwo different multiplication tables have been used in this article, and there are more.[5][12] These multiplication tables are characterized by the Fano plane,[13][14] and these are shown in the figure for the two tables used here: at top, the one described by Sabinin, Sbitneva, and Shestakov, and at bottom that described by Lounesto. The numbers under the Fano diagrams (the set of lines in the diagram) indicate a set of indices for seven independent products in each case, interpreted as ijk → ei × ej = ek. The multiplication table is recovered from the Fano diagram by following either the straight line connecting any three points, or the circle in the center, with a sign as given by the arrows. For example, the first row of multiplications resulting in e1 in the above listing is obtained by following the three paths connected to e1 in the lower Fano diagram: the circular path e2 × e4, the diagonal path e3 × e7, and the edge path e6 × e1 = e5 rearranged using one of the above identities as:The cross product is then given byAs the cross product is bilinear the operator x×– can be written as a matrix, which takes the form[citation needed]The ej component of cross product x × y is given by selecting all occurrences of ej in the table and collecting the corresponding components of x from the left column and of y from the top row. The result is:which produces diagonals further out, and so on.with i = 1...7 modulo 7 and the indices i, i + 1 and i + 3 allowed to permute evenly. Together with anticommutativity this generates the product. This rule directly produces the two diagonals immediately adjacent to the diagonal of zeros in the table. Also, from an identity in the subsection on consequences,More compactly this rule can be written asUsing e1 to e7 for the basis vectors a different multiplication table from the one in the Introduction, leading to a different cross product, is given with anticommutativity by[8]Once we have established a multiplication table, it is then applied to general vectors x and y by expressing x and y in terms of the basis and expanding x × y through bilinearity.To define a particular cross product, an orthonormal basis {ej} may be selected and a multiplication table provided that determines all the products {ei × ej}. One possible multiplication table is described in the Example section, but it is not unique.[5] Unlike three dimensions, there are many tables because every pair of unit vectors is perpendicular to five other unit vectors, allowing many choices for each cross product.Other properties follow only in the three-dimensional case, and are not satisfied by the seven-dimensional cross product, notably,Further properties follow from the definition, including the following identities:In contrast the three-dimensional cross product, which is unique (apart from sign), there are many possible binary cross products in seven dimensions. One way to see this is to note that given any pair of vectors x and y ∈ ℝ7 and any vector v of magnitude |v| = |x||y| sin θ in the five-dimensional space perpendicular to the plane spanned by x and y, it is possible to find a cross product with a multiplication table (and an associated set of basis vectors) such that x × y = v. Unlike in three dimensions, x × y = a × b does not imply that a and b lie in the same plane as x and y.[8]The restriction to 0, 1, 3 and 7 dimensions is related to Hurwitz's theorem, that normed division algebras are only possible in 1, 2, 4 and 8 dimensions. The cross product is formed from the product of the normed division algebra by restricting it to the 0, 1, 3, or 7 imaginary dimensions of the algebra, giving nonzero products in only three and seven dimensions.[11]Given the properties of bilinearity, orthogonality and magnitude, a nonzero cross product exists only in three and seven dimensions.[2][8][10] This can be shown by postulating the properties required for the cross product, then deducing an equation which is only satisfied when the dimension is 0, 1, 3 or 7. In zero dimensions there is only the zero vector, while in one dimension all vectors are parallel, so in both these cases the product must be identically zero.if x × x = 0 is assumed as a separate axiom.[10]which is the area of the parallelogram in the plane of x and y with the two vectors as sides.[9] A third statement of the magnitude condition iswhere (x·y) is the Euclidean dot product and |x| is the Euclidean norm. The first property states that the product is perpendicular to its arguments, while the second property gives the magnitude of the product. An equivalent expression in terms of the angle θ between the vectors[7] is[8]The cross product on a Euclidean space V is a bilinear map from V × V to V, mapping vectors x and y in V to another vector x × y also in V, where x × y has the properties[1][6]The top left 3 × 3 corner of this table gives the cross product in three dimensions.There are 480 such tables, one for each of the products satisfying the definition.[5] This table can be summarized by the relation[4]This can be repeated for the other six components.The table can be used to calculate the product of any two vectors. For example, to calculate the e1 component of x × y the basis vectors that multiply to produce e1 can be picked out to giveThe product can be given by a multiplication table, such as the one here. This table, due to Cayley,[3][4] gives the product of basis vectors ei and ej for each i, j from 1 to 7. For example, from the tableThe seven-dimensional cross product is one way of generalising the cross product to other than three dimensions, and it is the only other non-trivial bilinear product of two vectors that is vector-valued, anticommutative and orthogonal.[2] In other dimensions there are vector-valued products of three or more vectors that satisfy these conditions, and binary products with bivector results.In mathematics, the seven-dimensional cross product is a bilinear operation on vectors in seven-dimensional Euclidean space. It assigns to any two vectors a, b in R7 a vector a × b also in R7.[1] Like the cross product in three dimensions, the seven-dimensional product is anticommutative and a × b is orthogonal both to a and to b. Unlike in three dimensions, it does not satisfy the Jacobi identity, and while the three-dimensional cross product is unique up to a sign, there are many seven-dimensional cross products. The seven-dimensional cross product has the same relationship to the octonions as the three-dimensional product does to the quaternions.
Multilinear algebra

Geometric algebra
For programmers, this is a code generator with support for C, C++, C# and Java.Software allowing script creation and including sample visualizations, manual and GA introduction.The link provides a manual, introduction to GA and sample material as well as the software.The following is a list of freely available software that does not require ownership of commercial software or purchase of any commercial products for this purpose:GA is a very application-oriented subject. There is a reasonably steep initial learning curve associated with it, but this can be eased somewhat by the use of applicable software.In computer graphics and robotics, geometric algebras have been revived in order to efficiently represent rotations and other transformations. For applications of GA in robotics (screw theory, kinematics and dynamics using versors), computer vision, control and neural computing (geometric learning) see Bayro (2010).Progress on the study of Clifford algebras quietly advanced through the twentieth century, although largely due to the work of abstract algebraists such as Hermann Weyl and Claude Chevalley. The geometrical approach to geometric algebras has seen a number of 20th-century revivals. In mathematics, Emil Artin's Geometric Algebra[31] discusses the algebra associated with each of a number of geometries, including affine geometry, projective geometry, symplectic geometry, and orthogonal geometry. In physics, geometric algebras have been revived as a "new" way to do classical mechanics and electromagnetism, together with more advanced topics such as quantum mechanics and gauge theory.[3] David Hestenes reinterpreted the Pauli and Dirac matrices as vectors in ordinary space and spacetime, respectively, and has been a primary contemporary advocate for the use of geometric algebra.Nevertheless, another revolutionary development of the 19th-century would completely overshadow the geometric algebras: that of vector analysis, developed independently by Josiah Willard Gibbs and Oliver Heaviside. Vector analysis was motivated by James Clerk Maxwell's studies of electromagnetism, and specifically the need to express and manipulate conveniently certain differential equations. Vector analysis had a certain intuitive appeal compared to the rigors of the new algebras. Physicists and mathematicians alike readily adopted it as their geometrical toolkit of choice, particularly following the influential 1901 textbook Vector Analysis by Edwin Bidwell Wilson, following lectures of Gibbs.Also developed are the concept of vector manifold and geometric integration theory (which generalizes Cartan's differential forms).or the fundamental theorem of integral calculus.reduces toas a geometric product, effectively generalizing Stokes' theorem (including the differential form version of it).and then one can writeEssentially, the vector derivative is defined so that the GA version of Green's theorem is true,Geometric calculus extends the formalism to include differentiation and integration including differential geometry and differential forms.[29]and the derivative with respect to angle isthis path vector can be conveniently written in complex exponential formBy designating the unit bivector of this plane as the imaginary numberFor example, torque is generally defined as the magnitude of the perpendicular force component times distance, or work per unit angle.The cross product can be viewed in terms of the exterior product allowing a more natural geometric interpretation of the cross product as a bivector using the dual relationshipThe mathematical description of rotational forces such as torque and angular momentum often makes use of the cross product of vector calculus in three dimensions with a convention of orientation (handedness).andsoThenSpinors are defined as elements of the even subalgebra of a real GA; an analysis of the GA approach to spinors is given by Francis and Kosowsky .[28]Lundholm defines the Pin, Spin, and Spin+ subgroups, generated by unit vectors, and in the case of Spin and Spin+, only an even number of such vector factors can be present .[27]By the Cartan–Dieudonné theorem we have that every isometry can be given as reflections in hyperplanes and since composed reflections provide rotations then we have that orthogonal transformations are versors.Since both operators and operand are versors there is potential for alternative examples such as rotating a rotor or reflecting a spinor always provided that some geometrical or physical significance can be attached to such operations.Some authors use the term “versor product” to refer to the frequently occurring case where an operand is "sandwiched" between operators. The descriptions for rotations and reflections, including their outermorphisms, are examples of such sandwiching. The outermorphisms have a particularly simple algebraic form. Specifically, a mapping of vectors of the formand for the product of an even number of vectors thatwhereSimple reflections in a hyperplane are readily expressed in the algebra through conjugation with a single vector. These serve to generate the group of general rotoreflections and rotations.with the rejection being defined asA fast changing and fluid area of GA, CGA is also being investigated for applications to relativistic physics.This procedure has some similarities to the procedure for working with homogeneous coordinates in projective geometry and in this case allows the modeling of Euclidean transformations as orthogonal transformations.A compact description of the current state of the art is provided by Bayro-Corrochano & Scheuermann (2010), which also includes further references, in particular to Dorst, Fontijne & Mann (2007). Other useful references are Li (2008) and Bayro-Corrochano (2010).In a recent paper, [19] Dorst seeks a solution for expressing projective transformations in GA (usually handled via 4 by 4 coordinate matrices). In a postscript to that paper, reference is made to a further paper [20] that Dorst describes as resolving most of the weaknesses in this (research) area.Dotting the "Pauli vector" (a dyad):Although a lot of attention has been placed on CGA in recent years, it is to be noted that GA is not just one algebra, it is one of a family of algebras with the same essential structure.[16]for a blade, extended to the whole algebra through linearity.Dorst (2002) makes an argument for the use of contractions in preference to Hestenes's inner product; they are algebraically more regular and have cleaner geometric interpretations. A number of identities incorporating the contractions are valid without restriction of their inputs. For example,Among these several different generalizations of the inner product on vectors are:The inner product on vectors can also be generalized, but in more than one non-equivalent way. The paper (Dorst 2002) gives a full treatment of several different inner products developed for geometric algebras and their interrelationships, and the notation is taken from there. Many authors use the same symbol as for the inner product of vectors for their chosen extension (e.g. Hestenes and Perwass). No consistent notation has emerged.This generalization is consistent with the above definition involving antisymmetrization. Another generalization related to the exterior product is the commutator product:It is common practice to extend the exterior product on vectors to the entire algebra. This may be done through the use of the grade projection operator:The exterior product is naturally extended as an associative bilinear binary operator between any two elements of the algebra, satisfying the identitiesMost instances of geometric algebras of interest have a nondegenerate quadratic form. If the quadratic form is fully degenerate, the inner product of any two vectors is always zero, and the geometric algebra is then simply an exterior algebra. Unless otherwise stated, this article will treat only nondegenerate geometric algebras.Then by simple addition:so that the symmetric product can be written asThus we can define the inner product[b] of vectors asThe geometric product was first briefly mentioned by Hermann Grassmann,[6] who was chiefly interested in developing the closely related exterior algebra. In 1878, William Kingdon Clifford greatly expanded on Grassmann's work to form what are now usually called Clifford algebras in his honor (although Clifford himself chose to call them "geometric algebras"). For several decades, geometric algebras went somewhat ignored, greatly eclipsed by the vector calculus then newly developed to describe electromagnetism. The term "geometric algebra" was repopularized by Hestenes in the 1960s, who advocated its importance to relativistic physics.[7]Specific examples of geometric algebras applied in physics include the spacetime algebra (or the less common alternative formulation, the algebra of physical space) and the conformal geometric algebra. Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis, differential geometry, e.g. by using the Clifford algebra instead of differential forms. Geometric algebra has been advocated, most notably by David Hestenes[2] and Chris Doran,[3] as the preferred mathematical framework for physics. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity.[4] GA has also found use as a computational tool in computer graphics[5] and robotics.
Exterior algebra
A short while later, Alfred North Whitehead, borrowing from the ideas of Peano and Grassmann, introduced his universal algebra. This then paved the way for the 20th century developments of abstract algebra by placing the axiomatic notion of an algebraic system on a firm logical footing.The import of this new theory of vectors and multivectors was lost to mid 19th century mathematicians,[25] until being thoroughly vetted by Giuseppe Peano in 1888. Peano's work also remained somewhat obscure until the turn of the century, when the subject was unified by members of the French geometry school (notably Henri Poincaré, Élie Cartan, and Gaston Darboux) who applied Grassmann's ideas to the calculus of differential forms.The algebra itself was built from a set of rules, or axioms, capturing the formal aspects of Cayley and Sylvester's theory of multivectors. It was thus a calculus, much like the propositional calculus, except focused exclusively on the task of formal reasoning in geometrical terms.[24] In particular, this new development allowed for an axiomatic characterization of dimension, a property that had previously only been examined from the coordinate point of view.The exterior algebra was first introduced by Hermann Grassmann in 1844 under the blanket term of Ausdehnungslehre, or Theory of Extension.[22] This referred more generally to an algebraic (or axiomatic) theory of extended quantities and was one of the early precursors to the modern notion of a vector space. Saint-Venant also published similar ideas of exterior calculus for which he claimed priority over Grassmann.[23]The exterior algebra is the main ingredient in the construction of the Koszul complex, a fundamental object in homological algebra.The Jacobi identity holds if and only if ∂∂ = 0, and so this is a necessary and sufficient condition for an anticommutative nonassociative algebra L to be a Lie algebra. Moreover, in that case ΛL is a chain complex with boundary operator ∂. The homology associated to this complex is the Lie algebra homology.defined on decomposable elements byLet L be a Lie algebra over a field K, then it is possible to define the structure of a chain complex on the exterior algebra of L. This is a K-linear mappingThe exterior algebra over the complex numbers is the archetypal example of a superalgebra, which plays a fundamental role in physical theories pertaining to fermions and supersymmetry. A single element of the exterior algebra is called a supernumber[21] or Grassmann number. The exterior algebra itself is then just a one-dimensional superspace: it is just the set of all of the points in the exterior algebra. The topology on this space is essentially the weak topology, the open sets being the cylinder sets. An n-dimensional superspace is just the n-fold product of exterior algebras.In representation theory, the exterior algebra is one of the two fundamental Schur functors on the category of vector spaces, the other being the symmetric algebra. Together, these constructions are used to generate the irreducible representations of the general linear group; see fundamental representation.In particular, the exterior derivative gives the exterior algebra of differential forms on a manifold the structure of a differential algebra. The exterior derivative commutes with pullback along smooth mappings between manifolds, and it is therefore a natural differential operator. The exterior algebra of differential forms, equipped with the exterior derivative, is a cochain complex whose cohomology is called the de Rham cohomology of the underlying manifold and plays a vital role in the algebraic topology of differentiable manifolds.The exterior algebra has notable applications in differential geometry, where it is used to define differential forms. A differential form at a point of a differentiable manifold is an alternating multilinear form on the tangent space at the point. Equivalently, a differential form of degree k is a linear functional on the k-th exterior power of the tangent space. As a consequence, the exterior product of multilinear forms defines a natural exterior product for differential forms. Differential forms play a major role in diverse areas of differential geometry.Decomposable k-vectors in ΛkV correspond to weighted k-dimensional linear subspaces of V. In particular, the Grassmannian of k-dimensional subspaces of V, denoted Grk(V), can be naturally identified with an algebraic subvariety of the projective space P(ΛkV). This is called the Plücker embedding.The decomposable k-vectors have geometric interpretations: the bivector u ∧ v represents the plane spanned by the vectors, "weighted" with a number, given by the area of the oriented parallelogram with sides u and v. Analogously, the 3-vector u ∧ v ∧ w represents the spanned 3-space weighted by the volume of the oriented parallelepiped with edges u, v, and w.In physics, many quantities are naturally represented by alternating operators. For example, if the motion of a charged particle is described by velocity and acceleration vectors in four-dimensional spacetime, then normalization of the velocity vector requires that the electromagnetic force must be an alternating operator on the velocity. Its six degrees of freedom are identified with the electric and magnetic fields.All results obtained from other definitions of the determinant, trace and adjoint can be obtained from this definition (since these definitions are equivalent). Here are some basic properties related to these new definitions:In applications to linear algebra, the exterior product provides an abstract algebraic manner for describing the determinant and the minors of a matrix. For instance, it is well known that the magnitude of the determinant of a square matrix is equal to the volume of the parallelotope whose sides are the columns of the matrix. This suggests that the determinant can be defined in terms of the exterior product of the column vectors. Likewise, the k × k minors of a matrix can be defined by looking at the exterior products of column vectors chosen k at a time. These ideas can be extended not just to matrices but to linear transformations as well: the magnitude of the determinant of a linear transformation is the factor by which it scales the volume of any given reference parallelotope. So the determinant of a linear transformation can be defined in terms of what the transformation does to the top exterior power. The action of a transformation on the lesser exterior powers gives a basis-independent way to talk about the minors of the transformation.where n is the dimension of V.The components of this tensor are precisely the skew part of the components of the tensor product s ⊗ t, denoted by square brackets on the indices:The exterior product of two alternating tensors t and s of ranks r and p is given bywhere ti1⋅⋅⋅ir is completely antisymmetric in its indices.Suppose that V has finite dimension n, and that a basis e1, ..., en of V is given. then any alternating tensor t ∈ Ar(V) ⊂ Tr(V) can be written in index notation asAlthough this product differs from the tensor product, the kernel of Alt is precisely the ideal I (again, assuming that K has characteristic 0), and there is a canonical isomorphismThe antisymmetrization (or sometimes the skew-symmetrization) of a decomposable tensor is defined byLet Tr(V) be the space of homogeneous tensors of degree r. This is spanned by decomposable tensorsIf K is a field of characteristic 0,[18] then the exterior algebra of a vector space V can be canonically identified with the vector subspace of T(V) consisting of antisymmetric tensors. Recall that the exterior algebra is the quotient of T(V) by the ideal I generated by x ⊗ x.is exact.[17]is exact, and if W is 1-dimensional thenis a short exact sequence of vector spaces then Λk(V) has a filtrationSlightly more generally, ifThis is a graded isomorphism; i.e.,In particular, the exterior algebra of a direct sum is isomorphic to the tensor product of the exterior algebras:is an exact sequence of graded vector spaces[15] as isis a short exact sequence of vector spaces, thenIfThe components of the transformation Λ(k) relative to a basis of V and W is the matrix of k × k minors of f. In particular, if V = W and V is of finite dimension n, then Λn(f) is a mapping of a one-dimensional vector space Λn to itself, and is therefore given by a scalar: the determinant of f.LetIn particular, Λ(f) preserves homogeneous degree. The k-graded components of Λ(f) are given on decomposable elements bysuch thatSuppose that V and W are a pair of vector spaces and f : V → W is a linear transformation. Then, by the universal construction, there exists a unique homomorphism of graded algebrasfor all y ∈ Λl(V).where now x♭ ∈ Λl(V∗) ≃ (Λl(V))∗ is the dual l-vector defined byIndeed, more generally for v ∈ Λk−l(V), w ∈ Λk(V), and x ∈ Λl(V), iteration of the above adjoint properties givesfor all y ∈ V. This property completely characterizes the inner product on the exterior algebra.where x♭ ∈ V∗ is the linear functional defined byWith respect to the inner product, exterior multiplication and the interior product are mutually adjoint. Specifically, for v ∈ Λk−1(V), w ∈ Λk(V), and x ∈ V,constitute an orthonormal basis for Λk(V).the determinant of the matrix of inner products. In the special case vi = wi, the inner product is the square norm of the k-vector, given by the determinant of the Gramian matrix (⟨vi, vj⟩). This is then extended bilinearly (or sesquilinearly in the complex case) to a non-degenerate inner product on ΛkV. If ei, i = 1, 2, ..., n, form an orthonormal basis of V, then the vectors of the formFor V a finite-dimensional space, an inner product on V defines an isomorphism of V with V∗, and so also an isomorphism of ΛkV with (ΛkV)∗. The pairing between these two spaces also takes the form of an inner product. On decomposable k-vectors,where id is the identity mapping, and the inner product has metric signature (p, q) — p pluses and q minuses.If, in addition to a volume form, the vector space V is equipped with an inner product identifying V with V∗, then the resulting isomorphism is called the Hodge dual (or more commonly the Hodge star operator)In the geometrical setting, a non-zero element of the top exterior power Λn(V) (which is a one-dimensional vector space) is sometimes called a volume form (or orientation form, although this term may sometimes lead to ambiguity). Relative to a given volume form σ, the isomorphism is given explicitly byby the recursive definitionSuppose that V has finite dimension n. Then the interior product induces a canonical isomorphism of vector spacesFurther properties of the interior product include:These three properties are sufficient to characterize the interior product as well as define it in the general infinite-dimensional case.The interior product satisfies the following properties:Additionally, let iαf = 0 whenever f is a pure scalar (i.e., belonging to Λ0V).Suppose that w ∈ ΛkV. Then w is a multilinear mapping of V∗ to K, so it is defined by its values on the k-fold Cartesian product V∗ × V∗ × ... × V∗. If u1, u2, ..., uk−1 are k − 1 elements of V∗, then defineThis derivation is called the interior product with α, or sometimes the insertion operator, or contraction by α.Suppose that V is finite-dimensional. If V∗ denotes the dual space to the vector space V, then for each α ∈ V∗, it is possible to define an antiderivation on the algebra Λ(V),The counit is the homomorphism ε : Λ(V) → K that returns the 0-graded component of its argument. The coproduct and counit, along with the exterior product, define the structure of a bialgebra on the exterior algebra.where the tensor product on the right-hand side is of multilinear linear maps (extended by zero on elements of incompatible homogeneous degree: more precisely, α ∧ β = ε ∘ (α ⊗ β) ∘ Δ, where ε is the counit, as defined presently).In terms of the coproduct, the exterior product on the dual space is just the graded dual of the coproduct:The tensor symbol ⊗ used in this section should be understood with some caution: it is not the same tensor symbol as the one being used in the definition of the alternating product. Intuitively, it is perhaps easiest to think it as just another, but different, tensor product: it is still (bi-)linear, as tensor products should be, but it is the product that is appropriate for the definition of a bialgebra, that is, for creating the object Λ(V) ⊗ Λ(V). Any lingering doubt can be shaken by pondering the equalities (1 ⊗ v) ∧ (1 ⊗ w) = 1 ⊗ (v ∧ w) and (v ⊗ 1) ∧ (1 ⊗ w) = v ⊗ w, which follow from the definition of the coalgebra, as opposed to naive manipulations involving the tensor and wedge symbols. This distinction is developed in greater detail in the article on tensor algebras. Here, there is much less of a problem, in that the alternating product Λ clearly corresponds to multiplication in the bialgebra, leaving the symbol ⊗ free for use in the definition of the bialgebra. In practice, this presents no particular problem, as long as one avoids the fatal trap of replacing alternating sums of ⊗ by the wedge symbol, with one exception. One can construct an alternating product from ⊗, with the understanding that it works in a different space. Immediately below, an example is given: the alternating product for the dual space can be given in terms of the coproduct. The construction of the bialgebra here parallels the construction in the tensor algebra article almost exactly, except for the need to correctly track the alternating signs for the exterior algebra.Observe that the coproduct preserves the grading of the algebra. That is, one has thatExpanding this out in detail, one obtains the following expression on decomposable elements:Extending to the full space Λ(V), one has, in general,The correct form of this homomorphism is not what one might naively write, but has to be the one carefully defined in the coalgebra article. In this case, one obtainson elements v∈V. The symbol 1 stands for the unit element of the field K. Recall that K ⊂ Λ(V), so that the above really does lie in Λ(V) ⊗ Λ(V). This definition of the coproduct is extended to the full space Λ(V) by (linear) homomorphism. That is, for v,w∈V, one has, by definition, the homomorphismThe exterior product of multilinear forms defined above is dual to a coproduct defined on Λ(V), giving the structure of a coalgebra. The coproduct is a linear function Δ : Λ(V) → Λ(V) ⊗ Λ(V) which is given byThere is a correspondence between the graded dual of the graded algebra Λ(V) and alternating multilinear forms on V. The exterior algebra (as well as the symmetric algebra) inherits a bialgebra structure, and, indeed, a Hopf algebra structure, from the tensor algebra. See the article on tensor algebras for a detailed treatment of the topic.where here Shk,m ⊂ Sk+m is the subset of (k,m) shuffles: permutations σ of the set {1, 2, ..., k + m} such that σ(1) < σ(2) < ... < σ(k), and σ(k + 1) < σ(k + 2) < ... < σ(k + m).This definition of the exterior product is well-defined even if the field K has finite characteristic, if one considers an equivalent version of the above that does not use factorials or any constants:where the alternation Alt of a multilinear map is defined to be the average of the sign-adjusted values over all the permutations of its variables:Under this identification, the exterior product takes a concrete form: it produces a new anti-symmetric map from two given ones. Suppose ω : Vk → K and η : Vm → K are two anti-symmetric maps. As in the case of tensor products of multilinear maps, the number of variables of their exterior product is the sum of the numbers of their variables. It is defined as follows:[13]is called an alternating multilinear form. The set of all alternating multilinear forms is a vector space, as the sum of two such maps, or the product of such a map with a scalar, is again alternating. By the universal property of the exterior power, the space of alternating forms of degree k on V is naturally isomorphic with the dual vector space (ΛkV)∗. If V is finite-dimensional, then the latter is naturally isomorphic to Λk(V∗). In particular, the dimension of the space of alternating maps from Vk to K is n choose k.The above discussion specializes to the case when X = K, the base field. In this case an alternating multilinear functionwhich associates to k vectors from V their exterior product, i.e. their corresponding k-vector, is also alternating. In fact, this map is the "most general" alternating operator defined on Vk: given any other alternating operator f : Vk → X, there exists a unique linear map φ : Λk(V) → X with f = φ ∘ w. This universal property characterizes the space Λk(V) and can serve as its definition.The mapsuch that whenever v1, ..., vk are linearly dependent vectors in V, thenGiven two vector spaces V and X, an alternating operator from Vk to X is a multilinear mapExterior algebras of vector bundles are frequently considered in geometry and topology. There are no essential differences between the algebraic properties of the exterior algebra of finite-dimensional vector bundles and those of the exterior algebra of finitely generated projective modules, by the Serre–Swan theorem. More general exterior algebras can be defined for sheaves of modules.Given a commutative ring R and an R-module M, we can define the exterior algebra Λ(M) just as above, as a suitable quotient of the tensor algebra T(M). It will satisfy the analogous universal property. Many of the properties of Λ(M) also require that M be a projective module. Where finite dimensionality is used, the properties further require that M be finitely generated and projective. Generalizations to the most common situations can be found in (Bourbaki 1989).Rather than defining Λ(V) first and then identifying the exterior powers Λk(V) as certain subspaces, one may alternatively define the spaces Λk(V) first and then combine them to form the algebra Λ(V). This approach is often used in differential geometry and is described in the next section.As a consequence of this construction, the operation of assigning to a vector space V its exterior algebra Λ(V) is a functor from the category of vector spaces to the category of algebras.(and use ∧ as the symbol for multiplication in Λ(V)). It is then straightforward to show that Λ(V) contains V and satisfies the above universal property.To construct the most general algebra that contains V and whose multiplication is alternating on V, it is natural to start with the most general associative algebra that contains V, the tensor algebra T(V), and then enforce the alternating property by taking a suitable quotient. We thus take the two-sided ideal I in T(V) generated by all elements of the form v ⊗ v for v in V, and define Λ(V) as the quotientLet V be a vector space over the field K. Informally, multiplication in Λ(V) is performed by manipulating symbols and imposing a distributive law, an associative law, and using the identity v ∧ v = 0 for v ∈ V. Formally, Λ(V) is the "most general" algebra in which these rules hold for the multiplication, in the sense that any unital associative K-algebra containing V with alternating multiplication on V must contain a homomorphic image of Λ(V). In other words, the exterior algebra has the following universal property:[10]In addition to studying the graded structure on the exterior algebra, Bourbaki (1989) studies additional graded structures on exterior algebras, such as those on the exterior algebra of a graded module (a module that already carries its own gradation).The exterior product is graded anticommutative, meaning that if α ∈ Λk(V) and β ∈ Λp(V), thenandMoreover, if K is the basis field, we havegives the exterior algebra the additional structure of a graded algebra, that isThe exterior product of a k-vector with a p-vector is a (k + p)-vector, once again invoking bilinearity. As a consequence, the direct sum decomposition of the preceding sectionandIn characteristic 0, the 2-vector α has rank p if and only ifwhere aij = −aji (the matrix of coefficients is skew-symmetric). The rank of the matrix aij is therefore even, and is twice the rank of the form α.Rank is particularly important in the study of 2-vectors (Sternberg 1974, §III.6) (Bryant et al. 1991). The rank of a 2-vector α can be identified with half the rank of the matrix of coefficients of α in a basis. Thus if ei is a basis for V, then α can be expressed uniquely asThe rank of the k-vector α is the minimal number of decomposable k-vectors in such an expansion of α. This is similar to the notion of tensor rank.where each α(i) is decomposable, sayIf α ∈ Λk(V), then it is possible to express α as a linear combination of decomposable k-vectors:(where by convention Λ0(V) = K and Λ1(V) = V), and therefore its dimension is equal to the sum of the binomial coefficients, which is 2n.Any element of the exterior algebra can be written as a sum of k-vectors. Hence, as a vector space the exterior algebra is a direct sumIn particular, Λk(V) = {0} for k > n.By counting the basis elements, the dimension of Λk(V) is equal to a binomial coefficient:every vector vj can be written as a linear combination of the basis vectors ei; using the bilinearity of the exterior product, this can be expanded to a linear combination of exterior products of those basis vectors. Any exterior product in which the same basis vector appears more than once is zero; any exterior product in which the basis vectors do not appear in the proper order can be reordered, changing the sign whenever two basis vectors change places. In general, the resulting coefficients of the basis k-vectors can be computed as the minors of the matrix that describes the vectors vj in terms of the basis ei.is a basis for Λk(V). The reason is the following: given any exterior product of the formIf the dimension of V is n and {e1, ..., en} is a basis of V, then the set(This is a symplectic form, since α ∧ α ≠ 0.[9])If α ∈ Λk(V), then α is said to be a k-vector. If, furthermore, α can be expressed as an exterior product of k elements of V, then α is said to be decomposable. Although decomposable k-vectors span Λk(V), not every element of Λk(V) is decomposable. For example, in R4, the following 2-vector is not decomposable:The kth exterior power of V, denoted Λk(V), is the vector subspace of Λ(V) spanned by elements of the formIn particular, if xi = xj for some i ≠ j, then the following generalization of the alternating property also holds:where sgn(σ) is the signature of the permutation σ.[8]More generally, if σ is a permutation of the integers [1, ..., k], and x1, x2, ..., xk are elements of V, it follows thathenceThe exterior product is alternating on elements of V, which means that x ∧ x = 0 for all x ∈ V, by the above construction. It follows that the product is also anticommutative on elements of V, for supposing that x, y ∈ V,where the + I means that we derive the tensor product in the usual way and find the coset (or equivalence class) in the quotient with respect to the ideal I. Equivalently, any two tensors that differ by only an element of the ideal are considered to be the same element in the exterior algebra.The exterior product ∧ of two elements of Λ(V) is defined byThe exterior algebra Λ(V) over a vector space V over a field K is defined as the quotient algebra of the tensor algebra T(V) by the two-sided ideal I generated by all elements of the form x ⊗ x for x ∈ V (i.e. all tensors that can be expressed as the tensor product of any vector in V by itself).[7] Symbolically,The cross product and triple product in three dimensions each admit both geometric and algebraic interpretations. The cross product u × v can be interpreted as a vector which is perpendicular to both u and v and whose magnitude is equal to the area of the parallelogram determined by the two vectors. It can also be interpreted as the vector consisting of the minors of the matrix with columns u and v. The triple product of u, v, and w is geometrically a (signed) volume. Algebraically, it is the determinant of the matrix with columns u, v, and w. The exterior product in three dimensions allows for similar interpretations: it, too, can be identified with oriented areas or volumes spanned by the two or three vectors involved. In fact, in the presence of a positively oriented orthonormal basis, the exterior product generalizes these geometric notions to higher dimensions.where e1 ∧ e2 ∧ e3 is the basis vector for the one-dimensional space Λ3(R3). The scalar coefficient is the triple product of the three vectors.the exterior product of three vectors isBringing in a third vectorwhere {e1 ∧ e2, e3 ∧ e1, e2 ∧ e3} is the basis for the three-dimensional space Λ2(R3). The coefficients above are the same as those in the usual definition of the cross product of vectors in three dimensions, the only difference being that the exterior product is not an ordinary vector, but instead is a 2-vector.isandFor vectors in R3, the exterior algebra is closely related to the cross product and triple product. Using the standard basis {e1, e2, e3}, the exterior product of a pair of vectorsWith the exception of the last property, the exterior product of two vectors satisfies the same properties as the area. In a certain sense, the exterior product generalizes the final property by allowing the area of a parallelogram to be compared to that of any "standard" chosen parallelogram in a parallel plane (here, the one with sides e1 and e2). In other words, the exterior product provides a basis-independent formulation of area.[6]The fact that this coefficient is the signed area is not an accident. In fact, it is relatively easy to see that the exterior product should be related to the signed area if one tries to axiomatize this area as an algebraic construct. In detail, if A(v, w) denotes the signed area of the parallelogram of which the pair of vectors v and w form two adjacent sides, then A must satisfy the following properties:Consider now the exterior product of v and w:are a pair of given vectors in R2, written in components. There is a unique parallelogram having v and w as two of its sides. The area of this parallelogram is given by the standard determinant formula:Suppose thatThe Cartesian plane R2 is a vector space equipped with a basis consisting of a pair of unit vectorsThe definition of the exterior algebra makes sense for spaces not just of geometric vectors, but of other vector-like objects such as vector fields or functions. In full generality, the exterior algebra can be defined for modules over a commutative ring, and for other structures of interest in abstract algebra. It is one of these more general constructions where the exterior algebra finds one of its most important applications, where it appears as the algebra of differential forms that is fundamental in areas that use differential geometry. Differential forms are mathematical objects that represent infinitesimal areas of infinitesimal parallelograms (and higher-dimensional bodies), and so can be integrated over surfaces and higher dimensional manifolds in a way that generalizes the line integrals from calculus. The exterior algebra also has many algebraic properties that make it a convenient tool in algebra itself. The association of the exterior algebra to a vector space is a type of functor on vector spaces, which means that it is compatible in a certain way with linear transformations of vector spaces. The exterior algebra is one example of a bialgebra, meaning that its dual space also possesses a product, and this dual product is compatible with the exterior product. This dual algebra is precisely the algebra of alternating multilinear forms, and the pairing between the exterior algebra and its dual is given by the interior product.The exterior algebra, or Grassmann algebra after Hermann Grassmann,[4] is the algebraic system whose product is the exterior product. The exterior algebra provides an algebraic setting in which to answer geometric questions. For instance, blades have a concrete geometric interpretation, and objects in the exterior algebra can be manipulated according to a set of unambiguous rules. The exterior algebra contains objects that are not only k-blades, but sums of k-blades; such a sum is called a k-vector.[5] The k-blades, because they are simple products of vectors, are called the simple elements of the algebra. The rank of any k-vector is defined to be the smallest number of simple elements of which it is a sum. The exterior product extends to the full exterior algebra, so that it makes sense to multiply any two elements of the algebra. Equipped with this product, the exterior algebra is an associative algebra, which means that α ∧ (β ∧ γ) = (α ∧ β) ∧ γ for any elements α, β, γ. The k-vectors have degree k, meaning that they are sums of products of k vectors. When elements of different degrees are multiplied, the degrees add like multiplication of polynomials. This means that the exterior algebra is a graded algebra.When regarded in this manner, the exterior product of two vectors is called a 2-blade. More generally, the exterior product of any number k of vectors can be defined and is sometimes called a k-blade. It lives in a space known as the kth exterior power. The magnitude of the resulting k-blade is the volume of the k-dimensional parallelotope whose edges are the given vectors, just as the magnitude of the scalar triple product of vectors in three dimensions gives the volume of the parallelepiped generated by those vectors.In mathematics, the exterior product or wedge product of vectors is an algebraic construction used in geometry to study areas, volumes, and their higher-dimensional analogs. The exterior product of two vectors u and v, denoted by u ∧ v, is called a bivector and lives in a space called the exterior square, a vector space that is distinct from the original space of vectors. The magnitude[3] of u ∧ v can be interpreted as the area of the parallelogram with sides u and v, which in three dimensions can also be computed using the cross product of the two vectors. Like the cross product, the exterior product is anticommutative, meaning that u ∧ v = −(v ∧ u) for all vectors u and v. One way to visualize a bivector is as a family of parallelograms all lying in the same plane, having the same area, and with the same orientation—a choice of clockwise or counterclockwise.
Bivector
More generally every real geometric algebra is isomorphic to a matrix algebra. These contain bivectors as a subspace, though often in a way which is not especially useful. These matrices are mainly of interest as a way of classifying Clifford algebras.[25]Real bivectors in Λ2ℝn are isomorphic to n×n skew-symmetric matrices, or alternately to antisymmetric tensors of order 2 on ℝn. While bivectors are isomorphic to vectors (via the dual) in three dimensions they can be represented by skew-symmetric matrices in any dimension. This is useful for relating bivectors to problems described by matrices, so they can be re-cast in terms of bivectors, given a geometric interpretation, then often solved more easily or related geometrically to other bivector problems.[24]As noted above a bivector can be written as a skew-symmetric matrix, which through the exponential map generates a rotation matrix that describes the same rotation as the rotor, also generated by the exponential map but applied to the vector. But it is also used with other bivectors such as the angular velocity tensor and the electromagnetic tensor, respectively a 3×3 and 4×4 skew-symmetric matrix or tensor.where the angle brackets denote the scalar part of the geometric product. In the same way all projective space operations can be written in terms of geometric algebra, with bivectors representing general lines in projective space, so the whole geometry can be developed using geometric algebra.[14]which in Cℓ3(ℝ) and ℝℙ2 simplifies toSo the condition for the lines given by A, B and C to be collinear isThe operation "⋁" is the meet, which can be defined as above in terms of the join, J = A ∧ B [clarification needed] for non-zero A ∧ B. Using these operations projective geometry can be formulated in terms of geometric algebra. For example, given a third (non-zero) bivector C the point p lies on the line given by C if and only ifA description of the projective geometry can be constructed in the geometric algebra using basic operations. For example, given two distinct points in ℝℙn − 1 represented by vectors a and b the line between them is given by a ∧ b (or b ∧ a). Two lines intersect in a point if A ∧ B = 0 for their bivectors A and B. This point is given by the vectorGeometric algebra can be applied to projective geometry in a straightforward way. The geometric algebra used is Cℓn(ℝ), n ≥ 3, the algebra of the real vector space ℝn. This is used to describe objects in the real projective space ℝℙn − 1. The non-zero vectors in Cℓn(ℝ) or ℝn are associated with points in the projective space so vectors that differ only by a scale factor, so their exterior product is zero, map to the same point. Non-zero simple bivectors in Λ2ℝn represent lines in ℝℙn − 1, with bivectors differing only by a (positive or negative) scale factor representing the same line.Bivectors are also related to the rotation matrix in n dimensions. As in three dimensions the characteristic equation of the matrix can be solved to find the eigenvalues. In odd dimensions this has one real root, with eigenvector the fixed axis, and in even dimensions it has no real roots, so either all or all but one of the roots are complex conjugate pairs. Each pair is associated with a simple component of the bivector associated with the rotation. In particular the log of each pair is ± the magnitude, while eigenvectors generated from the roots are parallel to and so can be used to generate the bivector. In general the eigenvalues and bivectors are unique, and the set of eigenvalues gives the full decomposition into simple bivectors; if roots are repeated then the decomposition of the bivector into simple bivectors is not unique.One notable feature, related to the number of simple bivectors and so rotation planes, is that in odd dimensions every rotation has a fixed axis – it is misleading to call it an axis of rotation as in higher dimensions rotations are taking place in multiple planes orthogonal to it. This is related to bivectors, as bivectors in odd dimensions decompose into the same number of bivectors as the even dimension below, so have the same number of planes, but one extra dimension. As each plane generates rotations in two dimensions in odd dimensions there must be one dimension, that is an axis, that is not being rotated.[23]is the rotor generated by bivector B. Simple rotations, that take place in a plane of rotation around a fixed blade of dimension (n − 2) are generated by simple bivectors, while other bivectors generate more complex rotations which can be described in terms of the simple bivectors they are sums of, each related to a plane of rotation. All bivectors can be expressed as the sum of orthogonal and commutative simple bivectors, so rotations can always be decomposed into a set of commutative rotations about the planes associated with these bivectors. The group of the rotors in n dimensions is the spin group, Spin(n).As in three and four dimensions rotors are generated by the exponential map, sois the sum of three simple bivectors but no less. As in four dimensions it is always possible to find orthogonal simple bivectors for this sum.The number of simple bivectors needed to form a general bivector rises with the dimension, so for n odd it is (n − 1) / 2, for n even it is n / 2. So for four and five dimensions only two simple bivectors are needed but three are required for six and seven dimensions. For example, in six dimensions with standard basis (e1, e2, e3, e4, e5, e6) the bivectorAs has been suggested in earlier sections much of geometric algebra generalises well into higher dimensions. The geometric algebra for the real space ℝn is Cℓn(ℝ), and the subspace of bivectors is Λ2ℝn.using the same differential operator ∂.[22]where A is the vector magnetic potential and V is the electric potential. It is related to the electromagnetic bivector as followsThis when decomposed according to geometric algebra, using geometric products which have both grade raising and grade lowering effects, is equivalent to Maxwell's four equations. This is the form in a vacuum, but the general form is only a little more complex. It is also related to the electromagnetic four-potential, a vector A given byTogether these can be used to give a particularly compact form for Maxwell's equations in a vacuum:The operator ∇ is a differential operator in geometric algebra, acting on the space dimensions and given by ∇M = ∇·M + ∇∧M. When applied to vectors ∇·M is the divergence and ∇∧M is the curl but with a bivector rather than vector result, that is dual in three dimensions to the curl. For general quantity M they act as grade lowering and raising differential operators. In particular if M is a scalar then this operator is just the gradient, and it can be thought of as a geometric algebraic del operator.where j is current density and ρ is charge density. They are related by a differential operator ∂, which iswhere e4 is again the basis vector for the time-like dimension and c is the speed of light. The product Be123 yields the bivector that is Hodge dual to B in three dimensions, as discussed above, while Ee4 as a product of orthogonal vectors is also bivector valued. As a whole it is the electromagnetic tensor expressed more compactly as a bivector, and is used as follows. First it is related to the 4-current J, a vector quantity given byMaxwell's equations are used in physics to describe the relationship between electric and magnetic fields. Normally given as four differential equations they have a particularly compact form when the fields are expressed as a spacetime bivector from Λ2ℝ3,1. If the electric and magnetic fields in ℝ3 are E and B then the electromagnetic bivector is(Note: in this section traditional 3-vectors are indicated by lines over the symbols and spacetime vector and bivectors by bold symbols, with the vectors J and A exceptionally in uppercase)The set of all rotations in spacetime form the Lorentz group, and from them most of the consequences of special relativity can be deduced. More generally this show how transformations in Euclidean space and spacetime can all be described using the same kind of algebra.In general all spacetime rotations are generated from bivectors through the exponential map, that is, a general rotor generated by bivector A is of the formwhere Ω is the bivector (e14, etc.), identified via the metric with an antisymmetric linear transformation of ℝ3,1. These are Lorentz boosts, expressed in a particularly compact way, using the same kind of algebra as in ℝ3 and ℝ4.The simple bivectors e14, e24 and e34 have positive squares and as planes span a space dimension and the time dimension. These also generate rotations through the exponential map, but instead of trigonometric functions, hyperbolic functions are needed, which generates a rotor as follows:The simple bivectors are of two types. The simple bivectors e23, e31 and e12 have negative squares and span the bivectors of the three-dimensional subspace corresponding to Euclidean space, ℝ3. These bivectors generate ordinary rotations in ℝ3.(Note the order and indices above are not universal – here e4 is the time-like dimension). The geometric algebra is Cℓ3,1(ℝ), and the subspace of bivectors is Λ2ℝ3,1.Spacetime is a mathematical model for our universe used in special relativity. It consists of three space dimensions and one time dimension combined into a single four-dimensional space. It is naturally described using geometric algebra and bivectors, with the Euclidean metric replaced by a Minkowski metric. That algebra is identical to that of Euclidean space, except the signature is changed, soIt is always possible to do this as all bivectors can be expressed as sums of orthogonal bivectors.Bivectors in general do not commute, but one exception is orthogonal bivectors and exponents of them. So if the bivector B = B1 + B2, where B1 and B2 are orthogonal simple bivectors, is used to generate a rotation it decomposes into two simple rotations that commute as follows:These are generated by bivectors in a straightforward way. Simple rotations are generated by simple bivectors, with the fixed plane the dual or orthogonal to the plane of the bivector. The rotation can be said to take place about that plane, in the plane of the bivector. All other bivectors generate double rotations, with the two angles of the rotation equalling the magnitudes of the two simple bivectors the non-simple bivector is composed of. Isoclinic rotations arise when these magnitudes are equal, in which case the decomposition into two simple bivectors is not unique.[21]The rotations generated are more complex though. They can be categorised as follows:As in three dimensions bivectors in four dimension generate rotations through the exponential map, and all rotations can be generated this way. As in three dimensions if B is a bivector then the rotor R is eB/2 and rotations are generated in the same way:Similarly, every bivector can be written as the sum of two simple bivectors. It is useful to choose two orthogonal bivectors for this, and this is always possible to do. Moreover, for a generic bivector the choice of simple bivectors is unique, that is, there is only one way to decompose into orthogonal bivectors; the only exception is when the two orthogonal bivectors have equal magnitudes (as in the above example): in this case the decomposition is not unique.[1] The decomposition is always unique in the case of simple bivectors, with the added bonus that one of the orthogonal parts is zero.All bivectors in four dimensions can be generated using at most two exterior products and four vectors. The above bivector can be written asThe element e1234 is the pseudoscalar in Cℓ4, distinct from the scalar, so the square is non-scalar.In four dimensions bivectors are generated by the exterior product of vectors in ℝ4, but with one important difference from ℝ3 and ℝ2. In four dimensions not all bivectors are simple. There are bivectors such as e12 + e34 that cannot be generated by the exterior product of two vectors. This also means they do not have a real, that is scalar, square. In this caseIn four dimensions the Hodge dual of a bivector is a bivector, and the space Λ2ℝ4 is dual to itself. Normal vectors are not unique, instead every plane is orthogonal to all the vectors in its Hodge dual space. This can be used to partition the bivectors into two 'halves', in the following way. We have three pairs of orthogonal bivectors: (e12, e34), (e13, e24) and (e14, e23). There are four distinct ways of picking one bivector from each of the first two pairs, and once these first two are picked their sum yields the third bivector from the other pair. For example, (e12, e13, e14) and (e23, e24, e34).In four dimensions the basis elements for the space Λ2ℝ4 of bivectors are (e12, e13, e14, e23, e24, e34), so a general bivector is of the formThis only works in three dimensions as it is the only dimension where a vector parallel to both bivectors must exist. In higher dimensions bivectors generally are not associated with a single plane, or if they are (simple bivectors) two bivectors may have no vector in common, and so sum to a non-simple bivector.This can be interpreted geometrically as seen in the diagram: the two areas sum to give a third, with the three areas forming faces of a prism with a, b, c and b + c as edges. This corresponds to the two ways of calculating the area using the distributivity of the exterior product:Bivectors can be added together as areas. Given two non-zero bivectors B and C in three dimensions it is always possible to find a vector that is contained in both, a say, so the bivectors can be written as exterior products involving a:Like vectors these have magnitudes |A · B| = |A||B| cos θ and |A × B| = |A||B| sin θ, where θ is the angle between the planes. In three dimensions it is the same as the angle between the normal vectors dual to the planes, and it generalises to some extent in higher dimensions.The product of two bivectors has a geometric interpretation. For non-zero bivectors A and B the product can be split into symmetric and antisymmetric parts as follows:This relates the cross product to the exterior product. It can also be used to represent physical quantities, like torque and angular momentum. In vector algebra they are usually represented by vectors, perpendicular to the plane of the force, linear momentum or displacement that they are calculated from. But if a bivector is used instead the plane is the plane of the bivector, so is a more natural way to represent the quantities and the way they act. It also unlike the vector representation generalises into other dimensions.Bivectors and axial vectors are related by Hodge dual. In a real vector space the Hodge dual relates a subspace to its orthogonal complement, so if a bivector is represented by a plane then the axial vector associated with it is simply the plane's surface normal. The plane has two normals, one on each side, giving the two possible orientations for the plane and bivector.where θ is the angle between the vectors. This is the area of the parallelogram with edges a and b, as shown in the diagram. One interpretation is that the area is swept out by b as it moves along a. The exterior product is antisymmetric, so reversing the order of a and b to make a move along b results in a bivector with the opposite direction that is the negative of the first. The plane of bivector a ∧ b contains both a and b so they are both parallel to the plane.In three dimensions all bivectors can be generated by the exterior product of two vectors. If the bivector B = a ∧ b then the magnitude of B isAll bivectors can be interpreted as planes, or more precisely as directed plane segments. In three dimensions there are three properties of a bivector that can be interpreted geometrically:As suggested by their name and that of the algebra, one of the attractions of bivectors is that they have a natural geometric interpretation. This can be described in any dimension but is best done in three where parallels can be drawn with more familiar objects, before being applied to higher dimensions. In two dimensions the geometric interpretation is trivial, as the space is two-dimensional so has only one plane, and all bivectors are associated with it differing only by a scale factor.Bivectors have a number of advantages over axial vectors. They better disambiguate axial and polar vectors, that is the quantities represented by them, so it is clearer which operations are allowed and what their results are. For example, the inner product of a polar vector and an axial vector resulting from the cross product in the triple product should result in a pseudoscalar, a result which is more obvious if the calculation is framed as the exterior product of a vector and bivector. They generalises to other dimensions; in particular bivectors can be used to describe quantities like torque and angular momentum in two as well as three dimensions. Also, they closely match geometric intuition in a number of ways, as seen in the next section.[19]so are related by the Hodge dual:This relationship extends to operations like the vector valued cross product and bivector valued exterior product, as when written as determinants they are calculated in the same way:This is easier to use as the product is just the geometric product. But it is antisymmetric because (as in two dimensions) the unit pseudoscalar i squares to −1, so a negative is needed in one of the products.where ∗ indicates the Hodge dual. Note that if the underlying orientation is reversed by inversion through the origin, both the identification of the axial vectors with the usual vectors and the Hodge dual change sign, but the bivectors don't budge. Alternately, using the unit pseudoscalar in Cℓ3(ℝ), i = e1e2e3 givesThe rotation vector is an example of an axial vector. Axial vectors, or pseudovectors, are vectors with the special feature that their coordinates undergo a sign change relative to the usual vectors (also called "polar vectors") under inversion through the origin, reflection in a plane, or other orientation-reversing linear transformation.[17] Examples include quantities like torque, angular momentum and vector magnetic fields. Quantities that would use axial vectors in vector algebra are properly represented by bivectors in geometric algebra.[18] More precisely, if an underlying orientation is chosen, the axial vectors are naturally identified with the usual vectors; the Hodge dual then gives the isomorphism between axial vectors and bivectors, so each axial vector is associated with a bivector and vice versa; that isBivectors are related to the eigenvalues of a rotation matrix. Given a rotation matrix M the eigenvalues can calculated by solving the characteristic equation for that matrix 0 = det(M − λI). By the fundamental theorem of algebra this has three roots, but only one real root as there is only one eigenvector, the axis of rotation. The other roots must be a complex conjugate pair. They have unit magnitude so purely imaginary logarithms, equal to the magnitude of the bivector associated with the rotation, which is also the angle of rotation. The eigenvectors associated with the complex eigenvalues are in the plane of the bivector, so the exterior product of two non-parallel eigenvectors result in the bivector, or at least a multiple of it.and the matrix MR can be also calculated directly from rotor R:The rotation described by MR is the same as that described by the rotor R given bySkew symmetric matrices generate orthogonal matrices with determinant 1 through the exponential map. In particular the exponent of a bivector associated with a rotation is a rotation matrix, that is the rotation matrix MR given by the above skew-symmetric matrix isThis multiplied by vectors on both sides gives the same vector as the product of a vector and bivector minus the outerproduct; an example is the angular velocity tensor.Bivectors are isomorphic to skew-symmetric matrices; the general bivector B23e23 + B31e31 + B12e12 maps to the matrixThis is identical to two dimensions, except here rotors are four-dimensional objects isomorphic to the quaternions. This can be generalised to all dimensions, with rotors, elements of the even subalgebra with unit magnitude, being generated by the exponential map from bivectors. They form a double cover over the rotation group, so the rotors R and −R represent the same rotation.As to two dimensions the quantity eΩθ is called a rotor and written R. The quantity e−Ωθ is then R−1, and they generate rotations as followsThe bivector Ωθ generates a rotation through the exponential map. The even elements generated rotate a general vector in three dimensions in the same way as quaternions:So rotations can be represented by bivectors. Just as quaternions are elements of the geometric algebra, they are related by the exponential map in that algebra.The exponential can be defined in terms of its power series, and easily evaluated using the fact that Ω squared is -1.In geometric algebra the rotation is represented by a bivector. This can be seen in its relation to quaternions. Let Ω be a unit bivector in the plane of rotation, and let θ be the angle of rotation. Then the rotation bivector is Ωθ. The quaternion closely corresponds to the exponential of half of the bivector Ωθ. That is, the components of the quaternion correspond to the scalar and bivector parts of the following expression:The quaternion associated with the rotation isThe rotation vector, from the axis-angle representation of rotations, is a compact way of representing rotations in three dimensions. In its most compact form, it consists of a vector, the product of a unit vector ω that is the axis of rotation with the (signed) angle of rotation θ, so that the magnitude of the overall rotation vector θω equals the (unsigned) rotation angle.This suggests that the usual split of a quaternion into scalar and vector parts would be better represented as a split into scalar and bivector parts; if this is done the quaternion product is merely the geometric product. It also relates quaternions in three dimensions to complex numbers in two, as each is isomorphic to the even subalgebra for the dimension, a relationship that generalises to higher dimensions.The even subalgebra, that is the algebra consisting of scalars and bivectors, is isomorphic to the quaternions, ℍ. This can be seen by comparing the basis to the quaternion basis, or from the above product which is identical to the quaternion product, except for a change of sign which relates to the negative products in the bivector interior product A · B. Other quaternion properties can be similarly related to or derived from geometric algebra.Bivectors are not closed under the geometric product, but the even subalgebra is. In three dimensions it consists of all scalar and bivector elements of the geometric algebra, so a general element can be written for example a + A, where a is the scalar part and A is the bivector part. It is written Cℓ +
3  and has basis (1, e23, e31, e12). The product of two general elements of the even subalgebra isThe full geometric algebra in three dimensions, Cℓ3(ℝ), has basis (1, e1, e2, e3, e23, e31, e12, e123). The element e123 is a trivector and the pseudoscalar for the geometry. Bivectors in three dimensions are sometimes identified with pseudovectors[16] to which they are related, as discussed below.This is another version of Euler's formula, but with a general bivector in three dimensions. Unlike in two dimensions bivectors are not commutative so properties that depend on commutativity do not apply in three dimensions. For example, in general eA + B ≠ eAeB in three (or more) dimensions.A bivector B can be written as the product of its magnitude and a unit bivector, so writing β for |B| and using the Taylor series for the exponential map it can be shown thatThe exterior product of two bivectors in three dimensions is zero.which can be split into symmetric scalar and antisymmetric bivector parts as followswhile when multiplied they produce the followingthey can be added like vectorsIn three dimensions all bivectors are simple and so the result of an exterior product. The unit bivectors e23, e31 and e12 form a basis for the space of bivectors Λ2ℝ3, which is itself a three-dimensional linear space. So if a general bivector is:This can be split into the symmetric, scalar valued, interior product and the antisymmetric, bivector valued, exterior product:In three dimensions the geometric product of two vectors isand the rotation it generates is[15]Of these the last product is the one that generalises into higher dimensions. The quantity needed is called a rotor and is given the symbol R, so in two dimensions a rotor that rotates through angle θ can be writtenThe product of a vector with a bivector in two dimensions is anticommutative, so the following products all generate the same rotationwhich when multiplied by any vector rotates it through an angle θ about the origin:where θ is a real number. Putting this into the Taylor series for the exponential map and using the property e122 = −1 results in a bivector version of Euler's formula,Second a general bivector can be writtenAll the properties of complex numbers can be derived from bivectors, but two are of particular interest. First as with complex numbers products of bivectors and so the even subalgebra are commutative. This is only true in two dimensions, so properties of the bivector in two dimensions that depend on commutativity do not usually generalise to higher dimensions.The complex numbers are usually identified with the coordinate axes and two-dimensional vectors, which would mean associating them with the vector elements of the geometric algebra. There is no contradiction in this, as to get from a general vector to a complex number an axis needs to be identified as the real axis, e1 say. This multiplies by all vectors to generate the elements of even subalgebra.With the properties of negative square and unit magnitude, the unit bivector can be identified with the imaginary unit from complex numbers. The bivectors and scalars together form the even subalgebra of the geometric algebra, which is isomorphic to the complex numbers ℂ. The even subalgebra has basis (1, e12), the whole algebra has basis (1, e1, e2, e12).so it is called the unit bivector. The term unit bivector can be used in other dimensions but it is only uniquely defined (up to a sign) in two dimensions and all bivectors are multiples of e12. As the highest grade element of the algebra e12 is also the pseudoscalar which is given the symbol i.All bivectors in two dimensions are of this form, that is multiples of the bivector e1e2, written e12 to emphasise it is a bivector rather than a vector. The magnitude of e12 is 1, withThis can be split into the symmetric, scalar valued, interior product and an antisymmetric, bivector valued exterior product:A vector in real two-dimensional space ℝ2 can be written a = a1e1 + a2e2, where a1 and a2 are real numbers, e1 and e2 are orthonormal basis vectors. The geometric product of two such vectors isWhen working with coordinates in geometric algebra it is usual to write the basis vectors as (e1, e2, ...), a convention that will be used here.If the bivector is simple the last term is zero and the product is the scalar valued A · A, which can be used as a check for simplicity. In particular the exterior product of bivectors only exists in four or more dimensions, so all bivectors in two and three dimensions are simple.[1]Of particular interest is the product of a bivector with itself. As the commutator product is antisymmetric the product simplifies toThe space of bivectors Λ2ℝn are a Lie algebra over ℝ, with the commutator product as the Lie bracket. The full geometric product of bivectors generates the even subalgebra.The quantity A · B is the scalar valued interior product, while A ∧ B is the grade 4 exterior product that arises in four or more dimensions. The quantity A × B is the bivector valued commutator product, given byThe geometric product of two bivectors, A and B, iscannot be written as the exterior product of two vectors. A bivector that can be written as the exterior product of two vectors is simple. In two and three dimensions all bivectors are simple, but not in four or more dimensions; in four dimensions every bivector is the sum of at most two exterior products. A bivector has a real square if and only if it is simple, and only simple bivectors can be represented geometrically by an oriented plane area.[1]The exterior product of two vectors is a bivector, but not all bivectors are exterior products of two vectors. For example, in four dimensions the bivectorOf particular interest are the unit bivectors formed from the products of the standard basis. If ei and ej are distinct basis vectors then the product ei ∧ ej is a bivector. As the vectors are orthogonal this is just eiej, written eij, with unit magnitude as the vectors are unit vectors. The set of all such bivectors form a basis for Λ2ℝn. For instance in four dimensions the basis for Λ2ℝ4 is (e1e2, e1e3, e1e4, e2e3, e2e4, e3e4) or (e12, e13, e14, e23, e24, e34).[13]A unit bivector is one with unit magnitude. It can be derived from any non-zero bivector by dividing the bivector by its magnitude, that isFor general bivectors the magnitude can be calculated by taking the norm of the bivector considered as a vector in the space Λ2ℝn. If the magnitude is zero then all the bivector's components are zero, and the bivector is the zero bivector which as an element of the geometric algebra equals the scalar zero.As noted in the previous section the magnitude of a simple bivector, that is one that is the exterior product of two vectors a and b, is |a||b|sin θ, where θ is the angle between the vectors. It is written |B|, where B is the bivector.The subalgebra generated by the bivectors is the even subalgebra of the geometric algebra, written Cℓ +
n . This algebra results from considering all products of scalars and bivectors generated by the geometric product. It has dimension 2n−1, and contains Λ2ℝn as a linear subspace with dimension 1/2n(n − 1) (a triangular number). In two and three dimensions the even subalgebra contains only scalars and bivectors, and each is of particular interest. In two dimensions the even subalgebra is isomorphic to the complex numbers, ℂ, while in three it is isomorphic to the quaternions, ℍ. More generally the even subalgebra can be used to generate rotations in any dimension, and can be generated by bivectors in the algebra.although other conventions are used, in particular as vectors and bivectors are both elements of the geometric algebra.To distinguish them from vectors, bivectors are written here with bold capitals, for example:With a negative square it cannot be a scalar or vector quantity, so it is a new sort of object, a bivector. It has magnitude |a| |b| |sinθ|, where θ is the angle between the vectors, and so is zero for parallel vectors.which using the Pythagorean trigonometric identity gives the value of (a ∧ b)2To examine the nature of a ∧ b, consider the formulaThat is, the geometric product is the sum of the symmetric interior product and antisymmetric exterior product.and by addition:It is antisymmetric in a and bJust as the interior product can be formulated as the symmetric part of the geometric product another quantity, the exterior product can be formulated as its antisymmetric part:It is symmetric, scalar valued, and can be used to determine the angle between two vectors: in particular if a and b are orthogonal the product is zero.is a sum of scalars and so a scalar. From the law of cosines on the triangle formed by the vectors its value is |a||b|cosθ, where θ is the angle between the vectors. It is therefore identical to the interior product between two vectors, and is written the same way,From associativity a(ab) = a2b, a scalar times b. When b is not parallel to and hence not a scalar multiple of a, ab cannot be a scalar. ButThe bivector arises from the definition of the geometric product over a vector space. For vectors a, b and c, the geometric product on vectors is defined as follows:For this article the bivector will be considered only in real geometric algebras. This in practice is not much of a restriction, as all useful applications are drawn from such algebras. Also unless otherwise stated, all examples have a Euclidean metric and so a positive-definite quadratic form.Around this time Josiah Willard Gibbs and Oliver Heaviside developed vector calculus, which included separate cross product and dot products that were derived from quaternion multiplication.[5][6][7] The success of vector calculus, and of the book Vector Analysis by Gibbs and Wilson, had the effect that the insights of Hamilton and Clifford were overlooked for a long time, since much of 20th century mathematics and physics was formulated in vector terms. Gibbs used vectors to fill the role of bivectors in three dimensions, and used "bivector" to describe an unrelated quantity, a use that has sometimes been copied.[8][9][10] Today the bivector is largely studied as a topic in geometric algebra, a Clifford algebra over real or complex vector spaces with a nondegenerate quadratic form. Its resurgence was led by David Hestenes who, along with others, applied geometric algebra to a range of new applications in physics.[11]The bivector was first defined in 1844 by German mathematician Hermann Grassmann in exterior algebra as the result of the exterior product of two vectors. Just the previous year, in Ireland, William Rowan Hamilton had discovered quaternions. It was not until English mathematician William Kingdon Clifford in 1888 added the geometric product to Grassmann's algebra, incorporating the ideas of both Hamilton and Grassmann, and founded Clifford algebra, that the bivector as it is known today was fully understood.In layman terms, any surface is the same bivector, if it has the same area, same orientation, and is parallel to the same plane (see figure).Geometrically, a simple bivector can be interpreted as an oriented plane segment, much as vectors can be thought of as directed line segments.[3] The bivector a ∧ b has a magnitude equal to the area of the parallelogram with edges a and b, has the attitude of the plane spanned by a and b, and has orientation being the sense of the rotation that would align a with b.[3][4]Bivectors are generated by the exterior product on vectors: given two vectors a and b, their exterior product a ∧ b is a bivector, as is the sum of any bivectors. Not all bivectors can be generated as a single exterior product. More precisely, a bivector that can be expressed as an exterior product is called simple; in up to three dimensions all bivectors are simple, but in higher dimensions this is not the case.[1] The exterior product of two vectors is anticommutative and alternating, so b ∧ a is the negation of the bivector a ∧ b, producing the opposite orientation, and a ∧ a is the zero bivector.In mathematics, a bivector or 2-vector is a quantity in exterior algebra or geometric algebra that extends the idea of scalars and vectors. If a scalar is considered an order zero quantity, and a vector is an order one quantity, then a bivector can be thought of as being of order two. Bivectors have applications in many areas of mathematics and physics. They are related to complex numbers in two dimensions and to both pseudovectors and quaternions in three dimensions. They can be used to generate rotations in any number of dimensions, and are a useful tool for classifying such rotations. They also are used in physics, tying together a number of otherwise unrelated quantities.
Multivector
Bivectors play many important roles in physics, for example, in the classification of electromagnetic fields.Now define a map G : Λ2V × Λ2V → R by insisting thatwhere the Einstein summation convention is being used.As bivectors are elements of a vector space Λ2V (where V is a finite-dimensional vector space with dim V = n), it makes sense to define an inner product on this vector space as follows. First, write any element F ∈ Λ2V in terms of a basis (ei ∧ ej)1 ≤ i < j ≤ n of Λ2V asBivectors are connected to pseudovectors, and are used to represent rotations in geometric algebra.In geometric algebra, also, a bivector is a grade 2 element (a 2-vector) resulting from the wedge product of two vectors, and so it is geometrically an oriented area, in the same way a vector is an oriented line segment. If a and b are two vectors, the bivector a ∧ b hasA bivector is therefore an element of the antisymmetric tensor product of a tangent space with itself.In the algebra of physical space (the geometric algebra of Euclidean 3-space, used as a model of (3+1)-spacetime), a sum of a scalar and a vector is called a paravector, and represents a point in spacetime (the vector the space, the scalar the time).In the presence of a volume form (such as given an inner product and an orientation), pseudovectors and pseudoscalars can be identified with vectors and scalars, which is routine in vector calculus, but without a volume form this cannot be done without a choice.If a given element is homogeneous of a grade k, then it is a k-vector, but not necessarily a k-blade. Such an element is a k-blade when it can be expressed as the wedge product of k vectors. A geometric algebra generated by a 4-dimensional Euclidean vector space illustrates the point with an example: The sum of any two blades with one taken from the XY-plane and the other taken from the ZW-plane will form a 2-vector that is not a 2-blade. In a geometric algebra generated by a Euclidean vector space of dimension 2 or 3, all sums of 2-blades may be written as a single 2-blade.The highest grade element in a space is called a pseudoscalar.In geometric algebra, a multivector is defined to be the sum of different-grade k-blades, such as the summation of a scalar, a vector, and a 2-vector.[16] A sum of only k-grade components is called a k-vector,[17] or a homogeneous multivector.[18]Multivectors play a central role in the mathematical formulation of physics known as geometric algebra. The term geometric algebra was used by E. Artin for matrix methods in projective geometry.[12] It was D. Hestenes who used geometric algebra to describe the application of Clifford algebras to classical mechanics,[13] This formulation was expanded to geometric calculus by D. Hestenes and G. Sobczyk,[14] who provided new terminology for a variety of features in this application of Clifford algebra to physics. C. Doran and A. Lasenby show that Hestene's geometric algebra provides a convenient formulation for modern physics.[15]The set of multivectors constructed using Clifford's product yields an associative algebra known as a Clifford algebra. Inner products with different properties can be used to construct different Clifford algebras.[10][11]which yieldsIn contrast to the wedge product, the Clifford product of a vector with itself is no longer zero. To see this compute the product,therefore the basis vectors are alternating,Clifford's relation preserves the alternating property for the product of vectors that are perpendicular. This can be seen for the orthogonal unit vectors ei, i = 1, ..., n in Rn. Clifford's relation yieldsThe Clifford product between two vectors u and v is linear and associative like the wedge product, and has the additional property that the multivector uv is coupled to the inner product u · v by Clifford's relation,W. K. Clifford combined multivectors with the inner product defined on the vector space, in order to obtain a general construction for hypercomplex numbers that includes the usual complex numbers and Hamilton's quaternions.[8][9]Because the six homogeneous coordinates of a line can be obtained from the join of two points or the intersection of two planes, the line is said to be self dual in projective space.So, the Plücker coordinates of the line μ are given bythenIn order to obtain the Plucker coordinates of the line μ, map the multivectors π and ρ to their dual point coordinates using the Hodge star operator,[4]A line as the intersection of two planes: A line μ in projective space can also be defined as the set of points x that form the intersection of two planes π and ρ defined by grade three multivectors, so the points x are the solutions to the linear equationsThese are known as the Plücker coordinates of the line, though they are also an example of Grassmann coordinates.A line as the join of two points: In projective space the line λ through two points p and q can be viewed as the intersection of the affine space H: w = 1 with the plane x = αp + βq in R4. The multivector p ∧ q provides homogeneous coordinates for the lineThe four components of p ∧ q ∧ r that define the plane λ are called the Grassmann coordinates of the plane. Because four homogeneous coordinates define both a point and a plane in projective space, the geometry of points is dual to the geometry of planes.This equation is satisfied by points x = αp + βq + γr for real values of α, β and γ.which simplifies to the equation of a planeA plane in the affine component H: w = 1 is the set of points x = (x, y, z, 1) in the intersection of H with the 3-space defined by p ∧ q ∧ r. These points satisfy x ∧ p ∧ q ∧ r = 0, that is,Notice that substitution of αp + βq + γr for p multiplies this multivector by a constant. Therefore, the components of p ∧ q ∧ r are homogeneous coordinates for the 3-space through the origin of R4.Three dimensional projective space, P3 consists of all lines through the origin of R4. Let the three dimensional hyperplane, H: w = 1, be the affine component of projective space defined by the points x = (x, y, z, 1). The multivector p ∧ q ∧ r defines a parallelepiped in R4 given byThe three components of p ∧ q that define the line λ are called the Grassmann coordinates of the line. Because three homogeneous coordinates define both a point and a line, the geometry of points is said to be dual to the geometry of lines in the projective plane. This is called the principle of duality.This equation is satisfied by points x = αp + βq for real values of α and β.which simplifies to the equation of a lineThe set of points x = (x, y, 1) on the line through p and q is the intersection of the plane defined by p ∧ q with the plane E: z = 1. These points satisfy x ∧ p ∧ q = 0, that is,Notice that substitution of αp + βq for p multiplies this multivector by a constant. Therefore, the components of p ∧ q are homogeneous coordinates for the plane through the origin of R3.Points in the affine component E: z = 1 of the projective plane have coordinates x = (x, y, 1). A linear combination of two points p = (p1, p2, 1) and q = (q1, q2, 1) defines a plane in R3 that intersects E in the line joining p and q. The multivector p ∧ q defines a parallelogram in R3 given byA convenient way to view a multivector on Pn is to examine it in an affine component of Pn, which is the intersection of the lines through the origin of Rn+1 with a selected hyperplane, such as H: xn+1 = 1. Lines through the origin of R3 intersect the plane E: z = 1 to define an affine version of the projective plane that only lacks the points z = 0, called the points at infinity.Points in a real projective space Pn are defined to be lines through the origin of the vector space Rn+1. For example, the projective plane P2 is the set of lines through the origin of R3. Thus, multivectors defined on Rn+1 can be viewed as multivectors on Pn.In this section, we consider multivectors on a projective space Pn, which provide a convenient set of coordinates for lines, planes and hyperplanes that have properties similar to the homogeneous coordinates of points, called Grassmann coordinates.[7]In higher-dimensional spaces, the component three-vectors are projections of the volume of a parallelepiped onto the coordinate three-spaces, and the magnitude of the three-vector is the volume of the parallelepiped as it sits in the higher-dimensional space.This shows that the magnitude of the three-vector u ∧ v ∧ w is the volume of the parallelepiped spanned by the three vectors u, v and w.Notice that because V has dimension three, there is one basis three-vector in ΛV. Compute the three-vectorThis shows that the magnitude of the bivector u ∧ v is the area of the parallelogram spanned by the vectors u and v as it lies in the three-dimensional space V. The components of the bivector are the projected areas of the parallelogram on each of the three coordinate planes.The components of this bivector are the same as the components of the cross product. The magnitude of this bivector is the square root of the sum of the squares of its components.and the bivector u ∧ v is computed to beMore features of multivectors can be seen by considering the three dimensional vector space V = R3. In this case, let the basis vectors be e1, e2, and e3, so u, v and w are given byThe relationship between the magnitude of a multivector and the area or volume spanned by the vectors is an important feature in all dimensions. Furthermore, the linear functional version of a multivector that computes this volume is known as a differential form.The vertical bars denote the determinant of the matrix, which is the area of the parallelogram spanned by the vectors u and v. The magnitude of u ∧ v is the area of this parallelogram. Notice that because V has dimension two the basis bivector e1 ∧ e2 is the only multivector in ΛV.and the multivector u ∧ v, also called a bivector, is computed to beProperties of multivectors can be seen by considering the two dimensional vector space V = R2. Let the basis vectors be e1 and e2, so u and v are given byIt is easy to check that the magnitude of a three-vector in four dimensions measures the volume of the parallelepiped spanned by these vectors.The following examples show that a bivector in two dimensions measures the area of a parallelogram, and the magnitude of a bivector in three dimensions also measures the area of a parallelogram. Similarly, a three-vector in three dimensions measures the volume of a parallelepiped.The p-vector obtained from the wedge product of p separate vectors in an n-dimensional space has components that define the projected (p − 1)-volumes of the p-parallelotope spanned by the vectors. The square root of the sum of the squares of these components defines the volume of the p-parallelotope.[4][6]The linearity of the wedge product allows a multivector to be defined as the linear combination of basis multivectors. There are (n
p) basis p-vectors in an n-dimensional vector space.[4]The product of p vectors is called a grade p multivector, or a p-vector. The maximum grade of a multivector is the dimension of the vector space V.The wedge product operation used to construct multivectors is linear, associative and alternating, which reflect the properties of the determinant. This means for vectors u, v and w in a vector space V and for scalars α, β, the wedge product has the properties,The product of a p-vector and a k-vector is a (k + p)-vector so the set of linear combinations of all multivectors on V is an associative algebra, which is closed with respect to the wedge product. This algebra, denoted by Λ(V), is called the exterior algebra of V.[5]The set of multivectors on a vector space V is graded by the number of basis vectors that form a basis multivector. A multivector that can be expressed as a linear combination of products, each product being of p vectors, is called a grade p multivector, or a p-vector. The space of p-vectors forms the pth exterior power, denoted Λp(V). The dimension of the vector space V determines the maximum grade of a multivector.A multivector is the result of a product defined for elements in a vector space V. A vector space with a linear product operation between elements of the space is called an algebra; examples are matrix algebra and vector algebra.[1][2][3] The algebra of multivectors is constructed using the wedge product ∧ and is related to the exterior algebra of differential forms.[4]
Matrix (mathematics)
Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toA number λ and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n×n-matrix A is called positive-definite if for all nonzero vectors x ∈ Rn the associated quadratic form given bywhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2×2 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generallywhere 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:
Block matrix

Matrix decomposition
These factorizations are based on early work by Fredholm (1903), Hilbert (1904) and Schmidt (1907). For an account, and a translation to English of the seminal papers, see Stewart (2011).There exist analogues of the SVD, QR, LU and Cholesky factorizations for quasimatrices and cmatrices or continuous matrices.[11] A ‘quasimatrix’ is, like a matrix, a rectangular scheme whose elements are indexed, but one discrete index is replaced by a continuous index. Likewise, a ‘cmatrix’, is continuous in both indices. As an example of a cmatrix, one can think of the kernel of an integral operator.The Jordan normal form and the Jordan–Chevalley decompositionSimilarly, the QR decomposition expresses A as QR with Q an orthogonal matrix and R an upper triangular matrix. The system Q(Rx) = b is solved by Rx = QTb = c, and the system Rx = c is solved by 'back substitution'. The number of additions and multiplications required is about twice that of using the LU solver, but no more digits are required in inexact arithmetic because the QR decomposition is numerically stable.In numerical analysis, different decompositions are used to implement efficient matrix algorithms.In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.
Invertible matrix
Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any m-by-n matrix.Therefore,then,More generally, ifSuppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given byIf it is also the case that A − X has rank 1 then this simplifies tothen A is nonsingular and its inverse isMore generally, if A is "near" the invertible matrix X in the sense thatTruncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfiesthen A is nonsingular and its inverse may be expressed by a Neumann series:[11]If a matrix A has the property thatSince a blockwise inversion of an n × n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is Ω(n2 log n).[10]where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.Equating Equations (1) and (2) leads toThe inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A − BD−1C are nonsingular,[8] the result isThe nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D − CA−1B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D − CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.Matrices can also be inverted blockwise by using the following analytic inversion formula:With increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley–Hamilton method leads to an expression that is still tractable:The Cayley–Hamilton decomposition givesThe determinant of A can be computed by applying the rule of Sarrus as follows:(where the scalar A is not to be confused with the matrix A). If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given byA computationally efficient 3 × 3 matrix inversion is given byThe Cayley–Hamilton method givesThis is possible because 1/(ad − bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.The cofactor equation listed above yields the following result for 2 × 2 matrices. Inversion of these matrices can be done as follows:[6]where |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.so thatWriting the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:where L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.If matrix A is positive definite, then its inverse can be obtained asIf matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given byThe formula can be rewritten in terms of complete Bell polynomials of arguments tl = - (l - 1)! tr(Al) aswhere n is dimension of A, and tr(A) is the trace of matrix A given by the sum of the main diagonal. The sum is taken over s and the sets of all kl ≥ 0 satisfying the linear Diophantine equationThe Cayley–Hamilton theorem allows the inverse of A to be expressed in terms of det(A), traces and powers of A [5]Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:Gauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.As an example of a non-invertible, or singular, matrix, consider the matrixConsider the following 2-by-2 matrix:In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices. Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.Over the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn×n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.for finite square matrices A and B, then alsoIt follows from the theory of matrices that ifA matrix that is its own inverse, i.e. such that A = A−1 and A2 = I, is called an involutory matrix.Furthermore, the following properties hold for an invertible matrix A:Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:The set of n × n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined. The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse. If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In. If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.where In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that
Minor (linear algebra)
Keep in mind that adjunct is not adjugate or adjoint. In modern terminology, the "adjoint" of a matrix most often refers to the corresponding adjoint operator.Using this notation the inverse matrix is written this way:In some books instead of cofactor the term adjunct is used.[6] Moreover, it is denoted as Aij and defined in the same way as cofactor:where the coefficients agree with the minors computed earlier.we can simplify this expression toandwhere the two expressions correspond to the two columns of our matrix. Using the properties of the wedge product, namely that it is bilinear andare −13 (from the first two rows), −7 (from the first and last row), and 5 (from the last two rows). Now consider the wedge productIf the columns of a matrix are wedged together k at a time, the k × k minors appear as the components of the resulting k-vectors. For example, the 2 × 2 minors of the matrixA more systematic, algebraic treatment of the minor concept is given in multilinear algebra, using the wedge product: the k-minors of a matrix are the entries in the kth exterior power map.where the sum extends over all subsets K of {1,...,n} with k elements. This formula is a straightforward extension of the Cauchy–Binet formula.Both the formula for ordinary matrix multiplication and the Cauchy–Binet formula for the determinant of the product of two matrices are special cases of the following general statement about the minors of a product of two matrices. Suppose that A is an m × n matrix, B is an n × p matrix, I is a subset of {1,...,m} with k elements and J is a subset of {1,...,p} with k elements. ThenWe will use the following notation for minors: if A is an m × n matrix, I is a subset of {1,...,m} with k elements and J is a subset of {1,...,n} with k elements, then we write [A]I,J for the k × k minor of A that corresponds to the rows with index in I and the columns with index in J.Given an m × n matrix with real entries (or entries from any other field) and rank r, then there exists at least one non-zero r × r minor, while all larger minors are zero.The transpose of the cofactor matrix is called the adjugate matrix (also called the classical adjoint) of A.Then the inverse of A is the transpose of the cofactor matrix times the reciprocal of the determinant of A:One can write down the inverse of an invertible matrix by computing its cofactors by using Cramer's rule, as follows. The matrix formed by all of the cofactors of a square matrix A is called the cofactor matrix (also called the matrix of cofactors or comatrix):The cofactor expansion along the ith row gives:The complement, Bijk...,pqr..., of a minor, Mijk...,pqr..., of a square matrix, A, is formed by the determinant of the matrix A from which all the rows (ijk...) and columns (pqr...) associated with Mijk...,pqr... have been removed. The complement of the first minor of an element aij is merely that element.[5]So the cofactor of the (2,3) entry isTo compute the minor M2,3 and the cofactor C2,3, we find the determinant of the above matrix with row 2 and column 3 removed.To illustrate these definitions, consider the following 3 by 3 matrix,In linear algebra, a minor of a matrix A is the determinant of some smaller square matrix, cut down from A by removing one or more of its rows or columns. Minors obtained by removing just one row and one column from square matrices (first minors) are required for calculating matrix cofactors, which in turn are useful for computing both the determinant and inverse of square matrices.
Matrix multiplication

Rank (linear algebra)
Note that the tensor rank of a matrix can also mean the minimum number of simple tensors necessary to express the matrix as a linear combination, and that this definition does agree with matrix rank as here discussed.Matrix rank should not be confused with tensor order, which is called tensor rank. Tensor order is the number of indices required to write a tensor, and thus matrices all have tensor order 2. More precisely, matrices are tensors of type (1,1), having one row index and one column index, also called covariant order 1 and contravariant order 1; see Tensor (intrinsic definition) for details.There is a notion of rank for smooth maps between smooth manifolds. It is equal to the linear rank of the derivative.Thinking of matrices as tensors, the tensor rank generalizes to arbitrary tensors; note that for tensors of order greater than 2 (matrices are order 2 tensors), rank is very hard to compute, unlike for matrices.There are different generalisations of the concept of rank to matrices over arbitrary rings. In those generalisations, column rank, row rank, dimension of column space and dimension of row space of a matrix may be different from the others or may not exist.In the field of communication complexity, the rank of the communication matrix of a function gives bounds on the amount of communication needed for two parties to compute the function.In control theory, the rank of a matrix can be used to determine whether a linear system is controllable, or observable.One useful application of calculating the rank of a matrix is the computation of the number of solutions of a system of linear equations. According to the Rouché–Capelli theorem, the system is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, then the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank. In this case (and assuming the system of equations is in the real or complex numbers) the system of equations has infinitely many solutions.We assume that A is an m × n matrix, and we define the linear map f by f(x) = Ax as above.A non-vanishing p-minor (p × p submatrix with non-zero determinant) shows that the rows and columns of that submatrix are linearly independent, and thus those rows and columns of the full matrix are linearly independent (in the full matrix), so the row and column rank are at least as large as the determinantal rank; however, the converse is less straightforward. The equivalence of determinantal rank and column rank is a strengthening of the statement that if the span of n vectors has dimension p, then p of those vectors span the space (equivalently, that one can choose a spanning set that is a subset of the vectors): the equivalence implies that a subset of the rows and a subset of the columns simultaneously define an invertible submatrix (equivalently, if the span of n vectors has dimension p, then p of these vectors span the space and there is a set of p coordinates on which they are linearly independent).The rank of A is the largest order of any non-zero minor in A. (The order of a minor is the side-length of the square sub-matrix of which it is the determinant.) Like the decomposition rank characterization, this does not give an efficient way of computing the rank, but it is useful theoretically: a single non-zero minor witnesses a lower bound (namely its order) for the rank of the matrix, which can be useful (for example) to prove that certain operations do not lower the rank of a matrix.As in the case of the "dimension of image" characterization, this can be generalized to a definition of the rank of any linear map: the rank of a linear map f : V → W is the minimal dimension k of an intermediate space X such that f can be written as the composition of a map V → X and a map X → W. Unfortunately, this definition does not suggest an efficient manner to compute the rank (for which it is better to use one of the alternative definitions). See rank factorization for details.The rank of A is the maximal number of linearly independent rows of A; this is the dimension of the row space of A.Given the same linear mapping f as above, the rank is n minus the dimension of the kernel of f. The rank–nullity theorem states that this definition is equivalent to the preceding one.The rank of A is the dimension of the image of f. This definition has the advantage that it can be applied to any linear map without need for a specific matrix.defined byGiven the matrix A, there is an associated linear mappingIn all the definitions in this section, the matrix A is taken to be an m × n matrix over an arbitrary field F.Let A be a matrix of size m × n (with m rows and n columns). Let the column rank of A be r and let c1,...,cr be any basis for the column space of A. Place these as the columns of an m × r matrix C. Every column of A can be expressed as a linear combination of the r columns in C. This means that there is an r × n matrix R such that A = CR. R is the matrix whose i-th column is formed from the coefficients giving the i-th column of A as a linear combination of the r columns of C. Now, each row of A is given by a linear combination of the r rows of R. Therefore, the rows of R form a spanning set of the row space of A and, by the Steinitz exchange lemma, the row rank of A cannot exceed r. This proves that the row rank of A is less than or equal to the column rank of A. This result can be applied to any matrix, so apply the result to the transpose of A. Since the row rank of the transpose of A is the column rank of A and the column rank of the transpose of A is the row rank of A, this establishes the reverse inequality and we obtain the equality of the row rank and the column rank of A. (Also see rank factorization.)The fact that the column and row ranks of any matrix are equal forms an important part of the fundamental theorem of linear algebra. We present two proofs of this result. The first is short, uses only basic properties of linear combinations of vectors, and is valid over any field. The proof is based upon Wardlaw (2005).[3] The second is an elegant argument using orthogonality and is valid for matrices over the real numbers; it is based upon Mackiw (1995).[2] Both proofs can be found in the book by Banerjee and Roy (2014) [4]When applied to floating point computations on computers, basic Gaussian elimination (LU decomposition) can be unreliable, and a rank-revealing decomposition should be used instead. An effective alternative is the singular value decomposition (SVD), but there are other less expensive choices, such as QR decomposition with pivoting (so-called rank-revealing QR factorization), which are still more numerically robust than Gaussian elimination. Numerical determination of rank requires a criterion for deciding when a value, such as a singular value from the SVD, should be treated as zero, a practical choice which depends on both the matrix and the application.The final matrix (in row echelon form) has two non-zero rows and thus the rank of matrix A is 2.can be put in reduced row-echelon form by using the following elementary row operations:For example, the matrix A given byA common approach to finding the rank of a matrix is to reduce it to a simpler form, generally row echelon form, by elementary row operations. Row operations do not change the row space (hence do not change the row rank), and, being invertible, map the column space to an isomorphic space (hence do not change the column rank). Once in row echelon form, the rank is clearly the same for both row rank and column rank, and equals the number of pivots (or basic columns) and also the number of non-zero rows.of A has rank 1. Indeed, since the column vectors of A are the row vectors of the transpose of A, the statement that the column rank of a matrix equals its row rank is equivalent to the statement that the rank of a matrix is equal to the rank of its transpose, i.e., rk(A) = rk(AT).has rank 1: there are nonzero columns, so the rank is positive, but any pair of columns is linearly dependent. Similarly, the transposeThe matrixhas rank 2: the first two rows are linearly independent, so the rank is at least 2, but all three rows are linearly dependent (the third is equal to the second subtracted from the first) so the rank must be less than 3.The matrixThe rank is also the dimension of the image of the linear transformation that is given by multiplication by A. More generally, if a linear operator on a vector space (possibly infinite-dimensional) has finite-dimensional image (e.g., a finite-rank operator), then the rank of the operator is defined as the dimension of the image.A matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be rank deficient if it does not have full rank.A fundamental result in linear algebra is that the column rank and the row rank are always equal. (Two proofs of this result are given in Proofs that column rank = row rank below.) This number (i.e., the number of linearly independent rows or columns) is simply called the rank of A.The column rank of A is the dimension of the column space of A, while the row rank of A is the dimension of the row space of A.In this section we give some definitions of the rank of a matrix. Many definitions are possible; see Alternative definitions for several of these.The rank is commonly denoted rank(A) or rk(A); sometimes the parentheses are not written, as in rank A.In linear algebra, the rank of a matrix A is the dimension of the vector space generated (or spanned) by its columns.[1] This corresponds to the maximal number of linearly independent columns of A. This, in turn, is identical to the dimension of the space spanned by its rows.[2] Rank is thus a measure of the "nondegenerateness" of the system of linear equations and linear transformation encoded by A. There are multiple equivalent definitions of rank. A matrix's rank is one of its most fundamental characteristics.
Transformation matrix
More complicated perspective projections can be composed by combining this one with rotations, scales, translations, and shears to move the image plane and center of projection wherever they are desired.After carrying out the matrix multiplication, the homogeneous component wc will, in general, not be equal to 1. Therefore, to map back into the real plane we must perform the homogeneous divide or perspective divide by dividing each component by wc:Another type of transformation, of importance in 3D computer graphics, is the perspective projection. Whereas parallel projections are used to project points onto the image plane along parallel lines, the perspective projection projects points onto the image plane along lines that emanate from a single point, called the center of projection. This means that an object has a smaller projection when it is far away from the center of projection and a larger projection when it is closer.
When using affine transformations, the homogeneous component of a coordinate vector (normally called w) will never be altered. One can therefore safely assume that it is always 1 and ignore it. However, this is not true when using perspective projections.Using transformation matrices containing homogeneous coordinates, translations become linearly independent, and thus can be seamlessly intermixed with all other types of transformations. The reason is that the real plane is mapped to the w = 1 plane in real projective space, and so translation in real Euclidean space can be represented as a shear in real projective space. Although a translation is a non-linear transformation in a 2-D or 3-D Euclidean space described by Cartesian coordinates (i.e. it can't be combined with other transformations while preserving commutativity and other properties), it becomes, in a 3-D or 4-D projective space described by homogeneous coordinates, a simple linear transformation (a shear).All ordinary linear transformations are included in the set of affine transformations, and can be described as a simplified form of affine transformations. Therefore, any linear transformation can also be represented by a general transformation matrix. The latter is obtained by expanding the corresponding linear transformation matrix by one row and column, filling the extra space with zeros except for the lower-right corner, which must be set to 1. For example, the counter-clockwise rotation matrix from above becomes:A consequence of the ability to compose transformations by multiplying their matrices is that transformations can also be inverted by simply inverting their matrices. So, A−1 represents the transformation that "undoes" A.(This is called the associative property.) In other words, the matrix of the combined transformation A followed by B is simply the product of the individual matrices. Note that the multiplication is done in the opposite order from the English sentence: the matrix of "A followed by B" is BA, not AB.Composition is accomplished by matrix multiplication. If A and B are the matrices of two linear transformations, then the effect of applying first A and then B to a vector x is given by:One of the main motivations for using matrices to represent linear transformations is that transformations can then be easily composed (combined) and inverted.If the 4th component of the vector is 0 instead of 1, then only the vector's direction is reflected and its length remains unchanged, as if it were mirrored through a parallel plane that passes through the origin. This is a useful property as it allows the transformation of both positional vectors and normal vectors with the same matrix. See homogenous coordinates and affine transformations below for further explanation.Note that these are particular cases of a Householder reflection in two and three dimensions. A reflection about a line or plane that does not go through the origin is not a linear transformation — it is an affine transformation — as a 4x4 affine transformation matrix, it can be expressed as follows (assuming the normal is a unit vector):The matrix to rotate an angle θ about the axis defined by unit vector (l,m,n) is[5]Parallel projections are also linear transformations and can be represented simply by a matrix. However, perspective projections are not, and to represent these with a matrix, homogeneous coordinates can be used.As with reflections, the orthogonal projection onto a line that does not pass through the origin is an affine, not linear, transformation.For shear mapping (visually similar to slanting), there are two possibilities.These formulae assume that the x axis points right and the y axis points up. In formats such as SVG where the y axis points down, these matrices must be swapped.Similarly, a stretch by a factor k along the y-axis has the form x' = x; y' = ky, so the matrix associated with this transformation isThe matrix associated with a stretch by a factor k along the x-axis is given by:A stretch in the xy-plane is a linear transformation which enlarges all distances in a particular direction by a constant factor but does not affect distances in the perpendicular direction. We only consider stretches along the x-axis and y-axis. A stretch along the x-axis has the form x' = kx; y' = y for some positive constant k. (Note that if k is > 1, then this really is a “stretch”; if k is < 1, it is technically a “compression”, but we still call it a stretch. Also, if k=1, then the transformation is an identity, i.e. it has no effect.)Most common geometric transformations that keep the origin fixed are linear, including rotation, scaling, shearing, reflection, and orthogonal projection; if an affine transformation is not a pure translation it keeps some point fixed, and that point can be chosen as origin to make the transformation linear. In two dimensions, linear transformations can be represented using a 2×2 transformation matrix.With diagonalization, it is often possible to translate to and from eigenbases.It must be noted that the matrix representation of vectors and operators depends on the chosen basis; a similar matrix will result from an alternate basis. Nevertheless, the method to find the components remains the same.Put differently, a passive transformation refers to description of the same object as viewed from two different coordinate frames.In the physical sciences, an active transformation is one which actually changes the physical position of a system, and makes sense even in the absence of a coordinate system whereas a passive transformation is a change in the coordinate description of the physical system (change of basis). The distinction between active and passive transformations is important. By default, by transformation, mathematicians usually mean active transformations, while physicists could mean either.Linear transformations are not the only ones that can be represented by matrices. Some transformations that are non-linear on an n-dimensional Euclidean space Rn can be represented as linear transformations on the n+1-dimensional space Rn+1. These include both affine transformations (such as translation) and projective transformations. For this reason, 4×4 transformation matrices are widely used in 3D computer graphics. These n+1-dimensional transformation matrices are called, depending on their application, affine transformation matrices, projective transformation matrices, or more generally non-linear transformation matrices. With respect to an n-dimensional matrix, an n+1-dimensional matrix can be described as an augmented matrix.Matrices allow arbitrary linear transformations to be displayed in a consistent format, suitable for computation.[1] This also allows transformations to be concatenated easily (by multiplying their matrices).
Cramer's rule

Gaussian elimination
Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]The Gaussian elimination can be performed over any field, not just the real numbers.One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 − 5n)/6 multiplications, and (2n3 + 3n2 − 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).All of this applies also to the reduced row echelon form, which is a particular row echelon form.One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:For example, consider the following matrixA variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.Computationally, for a n×n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = ∏diag(B) / d.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Suppose the goal is to find and describe the set of solutions to the following system of linear equations:A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.There are three types of elementary row operations which may be performed on the rows of a matrix:Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
Abstract algebra
In physics, groups are used to represent symmetry operations, and the usage of group theory could simplify differential equations. In gauge theory, the requirement of local symmetry can be used to deduce the equations describing a system. The groups that describe those symmetries are Lie groups, and the study of Lie groups and Lie algebras reveals much about the physical system; for instance, the number of force carriers in a theory is equal to the dimension of the Lie algebra, and these bosons interact with the force they mediate if the Lie algebra is nonabelian.[2]Because of its generality, abstract algebra is used in many fields of mathematics and science. For instance, algebraic topology uses algebraic objects to study topologies. The Poincaré conjecture, proved in 2003, asserts that the fundamental group of a manifold, which encodes information about connectedness, can be used to determine whether a manifold is a sphere or not. Algebraic number theory studies various number rings that generalize the set of integers. Using tools of algebraic number theory, Andrew Wiles proved Fermat's Last Theorem.Examples involving several operations include:Examples of algebraic structures with a single binary operation are:By abstracting away various amounts of detail, mathematicians have defined various algebraic structures that are used in many areas of mathematics. For instance, almost all systems studied are sets, to which the theorems of set theory apply. Those sets that have a certain binary operation defined on them form magmas, to which the concepts concerning magmas, as well those concerning sets, apply. We can add additional constraints on the algebraic structure, such as associativity (to form semigroups); identity, and inverses (to form groups); and other more complex structures. With additional structure, more theorems could be proved, but the generality is reduced. The "hierarchy" of algebraic objects (in terms of generality) creates a hierarchy of the corresponding theories: for instance, the theorems of group theory may be used when studying rings (algebraic objects that have two binary operations with certain axioms) since a ring is a group over one of its operations. In general there is a balance between the amount of generality and the richness of the theory: more general structures have usually fewer nontrivial theorems and fewer applications.These processes were occurring throughout all of mathematics, but became especially pronounced in algebra. Formal definition through primitive operations and axioms were proposed for many basic algebraic structures, such as groups, rings, and fields. Hence such things as group theory and ring theory took their places in pure mathematics. The algebraic investigations of general fields by Ernst Steinitz and of commutative and then general rings by David Hilbert, Emil Artin and Emmy Noether, building up on the work of Ernst Kummer, Leopold Kronecker and Richard Dedekind, who had considered ideals in commutative rings, and of Georg Frobenius and Issai Schur, concerning representation theory of groups, came to define abstract algebra. These developments of the last quarter of the 19th century and the first quarter of 20th century were systematically exposed in Bartel van der Waerden's Moderne algebra, the two-volume monograph published in 1930–1931 that forever changed for the mathematical world the meaning of the word algebra from the theory of equations to the theory of algebraic structures.The end of the 19th and the beginning of the 20th century saw a tremendous shift in the methodology of mathematics. Abstract algebra emerged around the start of the 20th century, under the name modern algebra. Its study was part of the drive for more intellectual rigor in mathematics. Initially, the assumptions in classical algebra, on which the whole of mathematics (and major parts of the natural sciences) depend, took the form of axiomatic systems. No longer satisfied with establishing properties of concrete objects, mathematicians started to turn their attention to general theory. Formal definitions of certain algebraic structures began to emerge in the 19th century. For example, results about various groups of permutations came to be seen as instances of general theorems that concern a general notion of an abstract group. Questions of structure and classification of various mathematical objects came to forefront.The abstract notion of a group appeared for the first time in Arthur Cayley's papers in 1854. Cayley realized that a group need not be a permutation group (or even finite), and may instead consist of matrices, whose algebraic properties, such as multiplication and inverses, he systematically investigated in succeeding years. Much later Cayley would revisit the question whether abstract groups were more general than permutation groups, and establish that, in fact, any group is isomorphic to a group of permutations.The theory of permutation groups received further far-reaching development in the hands of Augustin Cauchy and Camille Jordan, both through introduction of new concepts and, primarily, a great wealth of results about special classes of permutation groups and even some general theorems. Among other things, Jordan defined a notion of isomorphism, still in the context of permutation groups and, incidentally, it was he who put the term group in wide use.Note, however, that he got by without formalizing the concept of a group, or even of a permutation group. The next step was taken by Évariste Galois in 1832, although his work remained unpublished until 1846, when he considered for the first time what is now called the closure property of a group of permutations, which he expressed asPaolo Ruffini was the first person to develop the theory of permutation groups, and like his predecessors, also in the context of solving algebraic equations. His goal was to establish the impossibility of an algebraic solution to a general algebraic equation of degree greater than four. En route to this goal he introduced the notion of the order of an element of a group, conjugacy, the cycle decomposition of elements of permutation groups and the notions of primitive and imprimitive and proved some important theorems relating these concepts, such asPermutations were studied by Joseph-Louis Lagrange in his 1770 paper Réflexions sur la résolution algébrique des équations (Thoughts on the algebraic solution of equations) devoted to solutions of algebraic equations, in which he introduced Lagrange resolvents. Lagrange's goal was to understand why equations of third and fourth degree admit formulae for solutions, and he identified as key objects permutations of the roots. An important novel step taken by Lagrange in this paper was the abstract view of the roots, i.e. as symbols and not as numbers. However, he did not consider composition of permutations. Serendipitously, the first edition of Edward Waring's Meditationes Algebraicae (Meditations on Algebra) appeared in the same year, with an expanded version published in 1782. Waring proved the main theorem on symmetric functions, and specially considered the relation between the roots of a quartic equation and its resolvent cubic. Mémoire sur la résolution des équations (Memoire on the Solving of Equations) of Alexandre Vandermonde (1771) developed the theory of symmetric functions from a slightly different angle, but like Lagrange, with the goal of understanding solvability of algebraic equations.Leonhard Euler considered algebraic operations on numbers modulo an integer, modular arithmetic, in his generalization of Fermat's little theorem. These investigations were taken much further by Carl Friedrich Gauss, who considered the structure of multiplicative groups of residues mod n and established many properties of cyclic and more general abelian groups that arise in this way. In his investigations of composition of binary quadratic forms, Gauss explicitly stated the associative law for the composition of forms, but like Euler before him, he seems to have been more interested in concrete results than in general theory. In 1870, Leopold Kronecker gave a definition of an abelian group in the context of ideal class groups of a number field, generalizing Gauss's work; but it appears he did not tie his definition with previous work on groups, particularly permutation groups. In 1882, considering the same question, Heinrich M. Weber realized the connection and gave a similar definition that involved the cancellation property but omitted the existence of the inverse element, which was sufficient in his context (finite groups).There were several threads in the early development of group theory, in modern language loosely corresponding to number theory, theory of equations, and geometry.Numerous textbooks in abstract algebra start with axiomatic definitions of various algebraic structures and then proceed to establish their properties. This creates a false impression that in algebra axioms had come first and then served as a motivation and as a basis of further study. The true order of historical development was almost exactly the opposite. For example, the hypercomplex numbers of the nineteenth century had kinematic and physical motivations but challenged comprehension. Most theories that are now recognized as parts of algebra started as collections of disparate facts from various branches of mathematics, acquired a common theme that served as a core around which various results were grouped, and finally became unified on a basis of a common set of concepts. An archetypical example of this progressive synthesis can be seen in the history of group theory.As in other parts of mathematics, concrete problems and examples have played important roles in the development of abstract algebra. Through the end of the nineteenth century, many – perhaps most – of these problems were in some way related to the theory of algebraic equations. Major themes include:Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called variety of groups.Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory is a formalism that allows a unified way for expressing properties and constructions that are similar for various structures.In algebra, which is a broad division of mathematics, abstract algebra (occasionally called modern algebra) is the study of algebraic structures. Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras. The term abstract algebra was coined in the early 20th century to distinguish this area of study from the other parts of algebra.
Dual space
be continuous for the chosen topology on V′. Further, there is still a choice of a topology on V′′, and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V′, so that the continuous double dual V′′ is not uniquely defined as a set. Saying that Ψ maps from V to V′′, or in other words, that Ψ(x) is continuous on V′ for every x ∈ V, is a reasonable minimal requirement on the topology of V′, namely that the evaluation mappingsWhen V is a topological vector space, one can still define Ψ(x) by the same formula, for every x ∈ V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial. However, if V is Hausdorff and locally convex, the map Ψ is injective from V to the algebraic dual V′∗ of the continuous dual, again as a consequence of the Hahn–Banach theorem.[15]As a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning ‖ Ψ(x) ‖ = ‖ x ‖ for all x in V. Normed spaces for which the map Ψ is a bijection are called reflexive.In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator Ψ : V → V′′ from a normed space V into its continuous double dual V′′, defined byThe topology of V and the topology of real or complex numbers can be used to induce on V′ a dual space topology.If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space ℓ 1 is separable, but its dual ℓ ∞ is not.and it follows from the Hahn–Banach theorem that j′ induces an isometric isomorphism V′ / W⊥ → W′.Then, the dual of the quotient V / W  can be identified with W⊥, and the dual of W can be identified with the quotient V′ / W⊥.[14] Indeed, let P denote the canonical surjection from V onto the quotient V / W ; then, the transpose P′ is an isometric isomorphism from (V / W )′ into V′, with range equal to W⊥. If j denotes the injection map from W into V, then the kernel of the transpose j′ is the annihilator of W:Assume that W is a closed linear subspace of a normed space V, and consider the annihilator of W in V′,When T is a continuous linear map between two topological vector spaces V and W, then the transpose T′ is continuous when W′ and V′ are equipped with"compatible" topologies: for example when, for X = V and X = W, both duals X′ have the strong topology β(X′, X) of uniform convergence on bounded sets of X, or both have the weak-∗ topology σ(X′, X) of pointwise convergence on X. The transpose T′ is continuous from β(W′, W) to β(V′, V), or from σ(W′, W) to σ(V′, V).When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V′. For every bounded linear map T on V, the transpose and the adjoint operators are linked byWhen T is a compact linear map between two Banach spaces V and W, then the transpose T′ is compact. This can be proved using the Arzelà–Ascoli theorem.When V and W are normed spaces, the norm of the transpose in L(W′, V′) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T′ is injective.The resulting functional T′(φ) is in V′. The assignment T → T′ produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W′ to V′. When T and U are composable continuous linear maps, thenIf T : V → W is a continuous linear map between two topological vector spaces, then the (continuous) transpose T′ : W′ → V′ is defined by the same formula as before:By the Riesz–Markov–Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.In a similar manner, the continuous dual of ℓ 1 is naturally identified with ℓ ∞ (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with ℓ 1.is finite. Define the number q by 1/p + 1/q = 1. Then the continuous dual of ℓ p is naturally identified with ℓ q: given an element φ ∈ (ℓ p)′, the corresponding element of ℓ q is the sequence (φ(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) ∈ ℓ q, the corresponding continuous linear functional φ on ℓ p is defined by φ(b) = ∑n anbn for all b = (bn) ∈ ℓ p (see Hölder's inequality).Let 1 < p < ∞ be a real number and consider the Banach space ℓ p of all sequences a = (an) for whichHere are the three most important special cases.form its local base.As a particular consequence, if V is a direct sum of two subspaces A and B, then V∗ is a direct sum of A0 and B0.If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional f : V → F factors through V/W if and only if W is in the kernel of f. There is thus an isomorphismafter identifying W with its image in the second dual space under the double duality isomorphism V ≈ V∗∗. Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.If V is finite-dimensional, and W is a vector subspace, thenIn particular if A and B are subspaces of V, it follows thatand equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, thenMoreover, if A and B are two subsets of V, thenThe annihilator of a subset is itself a vector space. In particular, ∅0 = V∗ is all of V∗ (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S ⊂ T ⊂ V, thenLet S be a subset of V. The annihilator of S in V∗, denoted here S0, is the collection of linear functionals f ∈ V∗ such that [f, s] = 0 for all s ∈ S. That is, S0 consists of all linear functionals f : V → F such that the restriction to S vanishes: f|S = 0.If the linear map f is represented by the matrix A with respect to two bases of V and W, then f∗ is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f∗ is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.The assignment f ↦ f∗ produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)∗ = g∗f∗. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify (f∗)∗ with f using the natural injection into the double dual.where the bracket [·,·] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual. This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.The following identity holds for all φ ∈ W∗ and v ∈ V:for every φ ∈ W∗. The resulting functional f∗(φ) in V∗ is called the pullback of φ along f.If f : V → W is a linear map, then the transpose (or dual) f∗ : W∗ → V∗ is defined byThe conjugate space V∗ can be identified with the set of all additive complex-valued functionals f: V → C such thatIf the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form ⟨·,·⟩ determines an isomorphism of V with the complex conjugate of the dual spaceThus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V∗ and nondegenerate bilinear forms on V.defined bywhere the right hand side is defined as the functional on V taking each w ∈ V to ⟨v,w⟩. In other words, the bilinear form determines a linear mappingIf V is finite-dimensional, then V is isomorphic to V∗. But there is in general no natural isomorphism between these two spaces.[7] Any bilinear form ⟨·,·⟩ on V gives a mapping of V into its dual space viaThus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.is a special case of a general result relating direct sums (of modules) to direct products.On the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identificationNote that (FA)0 may be identified (essentially by definition) with the direct sum of infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphismsAgain the sum is finite because fα is nonzero for only finitely many α.The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values θα = T(eα) it takes on the basis of V, and any function θ : A → F (with θ(α) = θα) defines a linear functional T on V byin V (the sum is finite by the assumption on f, and any v ∈ V may be written in this way by the definition of the basis).This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {eα : α ∈ A} identifies V with the space (FA)0 of functions f : A → F such that fα = f(α) is nonzero for only finitely many α ∈ A, where such a function f is identified with the vectorConsider, for instance, the space R∞, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i ∈ N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R∞ is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R∞ to give the number ∑anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R∞ is countably infinite, whereas RN does not have a countable basis.If V is not finite-dimensional but has a basis[6] eα indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements eα (α ∈ A) of the dual space, but they will not form a basis.If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V∗ form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V∗ are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]In particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n × 1 matrix and a 1 × 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 × n matrix, i.e., M must be a row vector.for any choice of coefficients ci ∈ F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equationsIf V is finite-dimensional, then V∗ has the same dimension as V. Given a basis {e1, ..., en} in V, it is possible to construct a specific basis in V∗, called the dual basis. This dual basis is a set {e1, ..., en} of linear functionals on V, defined by the relationThe pairing of a functional φ in the dual space V∗ and an element x of V is sometimes denoted by a bracket: φ(x) = [x,φ] [2] or φ(x) = ⟨φ,x⟩.[3] This pairing defines a nondegenerate bilinear mapping[4] ⟨·,·⟩ : V∗ × V → F called the natural pairing.for all φ and ψ ∈ V∗, x ∈ V, and a ∈ F. Elements of the algebraic dual space V∗ are sometimes called covectors or one-forms.Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space. When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.
Direct sum of modules
Every Hilbert space is isomorphic to a direct sum of sufficiently many copies of the base field (either R or C). This is equivalent to the assertion that every Hilbert space has an orthonormal basis. More generally, every closed subspace of a Hilbert space is complemented: it admits an orthogonal complement. Conversely, the Lindenstrauss–Tzafriri theorem asserts that if every closed subspace of a Banach space is complemented, then the Banach space is isomorphic (topologically) to a Hilbert space.This space is complete and we get a Hilbert space.The inner product of two such function α and β is then defined as:Alternatively and equivalently, one can define the direct sum of the Hilbert spaces Hi as the space of all functions α with domain I, such that α(i) is an element of Hi for every i in I and:If infinitely many Hilbert spaces Hi for i in I are given, we can carry out the same construction; notice that when defining the inner product, only finitely many summands will be non-zero. However, the result will only be an inner product space and it will not necessarily be complete. We then define the direct sum of the Hilbert spaces Hi to be the completion of this inner product space.The resulting direct sum is a Hilbert space which contains the given Hilbert spaces as mutually orthogonal subspaces.If finitely many Hilbert spaces H1,...,Hn are given, one can construct their orthogonal direct sum as above (since they are vector spaces), defining the inner product as:in which the summation makes sense even for infinite index sets I because only finitely many of the terms are non-zero.Let {(Mi,bi : i ∈ I} be a family indexed by I of modules equipped with bilinear forms. The orthogonal direct sum is the module direct sum with bilinear form B defined by[1]For example, if we take the index set I = N and Xi = R, then the direct sum ⨁i∈NXi is the space l1, which consists of all the sequences (ai) of reals with finite norm ||a|| = ∑i |ai|.The norm is given by the sum above. The direct sum with this norm is again a Banach space.Generally, if Xi is a collection of Banach spaces, where i traverses the index set I, then the direct sum ⨁i∈I Xi is a module consisting of all functions x defined over I such that x(i) ∈ Xi for all i ∈ I andThe direct sum of two Banach spaces X and Y is the direct sum of X and Y considered as vector spaces, with the norm ||(x,y)|| = ||x||X + ||y||Y for all x in X and y in Y.Max Zorn realized that the classical Cayley–Dickson construction missed constructing some composition algebras that arise as real subalgebras in the (ℂ, z2) series, in particular the split-octonions. A modified Cayley–Dickson construction, still based on use of the direct sum A ⊕ A of a base algebra A, has since been used to exhibit the series ℝ, split-complex numbers, split-quaternions, and split-octonions.Leonard Dickson developed this construction doubling quaternions for Cayley numbers, and the doubling method involving the direct sum A ⊕ A is called the Cayley–Dickson construction. In the instance beginning with K = ℝ, the series generates complex numbers, quaternions, octonions, and sedenions. Beginning with K = ℂ and the norm n(z) = z2, the series continues with bicomplex numbers, biquaternions, and bioctonions.The construction described above, as well as Wedderburn's use of the terms direct sum and direct product follow a different convention from the one in category theory. In categorical terms, Wedderburn's direct sum is a categorical product, whilst Wedderburn's direct product is a coproduct (or categorical sum), which (for commutative algebras) actually corresponds to the tensor product of algebras.Consider these classical examples:A direct sum of algebras X and Y is the direct sum as vector spaces, with productIn some classical texts, the notion of direct sum of algebras over a field is also introduced. This construction, however, does not provide a coproduct in the category of algebras, but a direct product (see note below and the remark on direct sums of rings).If the modules we are considering carry some additional structure (e.g. a norm or an inner product), then the direct sum of the modules can often be made to carry this additional structure, as well. In this case, we obtain the coproduct in the appropriate category of all objects carrying the additional structure. Two prominent examples occur for Banach spaces and Hilbert spaces.The direct sum gives a collection of objects the structure of a commutative monoid, in that the addition of objects is defined, but not subtraction. In fact, subtraction can be defined, and every commutative monoid can be extended to an abelian group. This extension is known as the Grothendieck group. The extension is done by defining equivalence classes of pairs of objects, which allows certain pairs to be treated as inverses. The construction, detailed in the article on the Grothendieck group, is "universal", in that it has the universal property of being unique, and homomorphic to any other embedding of an abelian monoid in an abelian group.Dually, the direct product is the product.such that f o ji = fi for all i.which sends the elements of Mi to those functions which are zero for all arguments but i. If fi : Mi → M are arbitrary R-linear maps for every i, then there exists precisely one R-linear mapIn the language of category theory, the direct sum is a coproduct and hence a colimit in the category of left R-modules, which means that it is characterized by the following universal property. For every i in I, consider the natural embeddingA submodule N of M is a direct summand of M if there exists some other submodule N′ of M such that M is the internal direct sum of N and N′. In this case, N and N′ are complementary submodules.Suppose M is some R-module, and Mi is a submodule of M for every i in I. If every x in M can be written in one and only one way as a sum of finitely many elements of the Mi, then we say that M is the internal direct sum of the submodules Mi (Halmos 1974, §18). In this case, M is naturally isomorphic to the (external) direct sum of the Mi as defined above (Adamson 1972, p.61).One should notice a clear similarity between the definitions of the direct sum of two vector spaces and of two abelian groups. In fact, each is a special case of the construction of the direct sum of two modules. Additionally, by modifying the definition one can accommodate the direct sum of an infinite family of modules. The precise definition is as follows (Bourbaki 1989, §II.1.6).This construction readily generalises to any finite number of abelian groups.The subgroup G × {0} of G ⊕ H is isomorphic to G and is often identified with G; similarly for {0} × H and H. (See internal direct sum below.) With this identification, it is true that every element of G ⊕ H can be written in one and only one way as the sum of an element of G and an element of H. The rank of G ⊕ H is equal to the sum of the ranks of G and H.It is customary to write the elements of an ordered sum not as ordered pairs (g, h), but as a sum g + h.The resulting abelian group is called the direct sum of G and H and is usually denoted by a plus symbol inside a circle:for g in G, h in H, and n an integer. This parallels the extension of the scalar product of vector spaces to the direct sum above.Integral multiples are similarly defined componentwise byfor g1, g2 in G, and h1, h2 in H.For abelian groups G and H which are written additively, the direct product of G and H is also called a direct sum (Mac Lane & Birkhoff 1999, §V.6). Thus the cartesian product G × H is equipped with the structure of an abelian group by defining the operations componentwise:This construction readily generalises to any finite number of vector spaces.The subspace V × {0} of V ⊕ W is isomorphic to V and is often identified with V; similarly for {0} × W and W. (See internal direct sum below.) With this identification, every element of V ⊕ W can be written in one and only one way as the sum of an element of V and an element of W. The dimension of V ⊕ W is equal to the sum of the dimensions of V and W. One elementary use is the reconstruction of a finite vector space from any subspace W and its orthogonal complement:It is customary to write the elements of an ordered sum not as ordered pairs (v, w), but as a sum v + w.The resulting vector space is called the direct sum of V and W and is usually denoted by a plus symbol inside a circle:for v, v1, v2 ∈ V, w, w1, w2 ∈ W, and α ∈ K.Suppose V and W are vector spaces over the field K. The cartesian product V × W can be given the structure of a vector space over K (Halmos 1974, §18) by defining the operations componentwise:We give the construction first in these two cases, under the assumption that we have only two objects. Then we generalise to an arbitrary family of arbitrary modules. The key elements of the general construction are more clearly identified by considering these two cases in depth.The most familiar examples of this construction occur when considering vector spaces (modules over a field) and abelian groups (modules over the ring Z of integers). The construction may also be extended to cover Banach spaces and Hilbert spaces.In abstract algebra, the direct sum is a construction which combines several modules into a new, larger module. The direct sum of modules is the smallest module which contains the given modules as submodules with no "unnecessary" constraints, making it an example of a coproduct. Contrast with the direct product, which is the dual notion.
Function space
Functional analysis is organized around adequate techniques to bring function spaces as topological vector spaces within reach of the ideas that would apply to normed spaces of finite dimension.Function spaces appear in various areas of mathematics:When the domain X has additional structure, one might consider instead the subset (or subspace) of all such functions which respect that structure. For example, if X is also vector space over F, the set of linear maps X → V form a vector space over F with pointwise operations (often denoted Hom(X,V)). One such space is the dual space of V: the set of linear functionals V → F with addition and scalar multiplication defined pointwise.Let V be a vector space over a field F and let X be any set. The functions X → V can be given the structure of a vector space over F where the operations are defined pointwise, that is, for any f, g : X → V, any x in X, and any c in F, defineIn mathematics, a function space is a set of functions between two fixed sets. Often, the domain and/or codomain will have additional structure which is inherited by the function space. For example, the set of functions from any set X into a vector space have a natural vector space structure given by pointwise addition and scalar multiplication. In other scenarios, the function space might inherit a topological or metric structure, hence the name function space.
Quotient space (linear algebra)
If, furthermore, X is metrizable, then so is X/M. If X is a Fréchet space, then so is X/M (Dieudonné 1970, 12.11.3).Then X/M is a locally convex space, and the topology on it is the quotient topology.The quotient of a locally convex space by a closed subspace is again locally convex (Dieudonné 1970, 12.14.8). Indeed, suppose that X is locally convex so that the topology on X is generated by a family of seminorms {pα | α ∈ A} where A is an index set. Let M be a closed subspace, and define seminorms qα by on X/MIf X is a Hilbert space, then the quotient space X/M is isomorphic to the orthogonal complement of M.Let C[0,1] denote the Banach space of continuous real-valued functions on the interval [0,1] with the sup norm. Denote the subspace of all functions f ∈ C[0,1] with f(0) = 0 by M. Then the equivalence class of some function g is determined by its value at 0, and the quotient space C[0,1] / M is isomorphic to R.The quotient space X/M is complete with respect to the norm, so it is a Banach space.If X is a Banach space and M is a closed subspace of X, then the quotient X/M is again a Banach space. The quotient space is already endowed with a vector space structure by the construction of the previous section. We define a norm on X/M byThe cokernel of a linear operator T : V → W is defined to be the quotient space W/im(T).Let T : V → W be a linear operator. The kernel of T, denoted ker(T), is the set of all x ∈ V such that Tx = 0. The kernel is a subspace of V. The first isomorphism theorem of linear algebra says that the quotient space V/ker(T) is isomorphic to the image of V in W. An immediate corollary, for finite-dimensional spaces, is the rank–nullity theorem: the dimension of V is equal to the dimension of the kernel (the nullity of T) plus the dimension of the image (the rank of T).If U is a subspace of V, the dimension of V/U is called the codimension of U in V. Since a basis of V may be constructed from a basis A of U and a basis B of V/U by adding a representative of each element of B to A, the dimension of V is the sum of the dimensions of U and V/U. If V is finite-dimensional, it follows that the codimension of U in V is the difference between the dimensions of V and U (Halmos 1974, Theorem 22.2):There is a natural epimorphism from V to the quotient space V/U given by sending x to its equivalence class [x]. The kernel (or nullspace) of this epimorphism is the subspace U. This relationship is neatly summarized by the short exact sequenceAn important example of a functional quotient space is a Lp space.then the quotient space V/U is naturally isomorphic to W (Halmos 1974, Theorem 22.1).More generally, if V is an (internal) direct sum of subspaces U and W,Another example is the quotient of Rn by the subspace spanned by the first m standard basis vectors. The space Rn consists of all n-tuples of real numbers (x1,…,xn). The subspace, identified with Rm, consists of all n-tuples such that the last n-m entries are zero: (x1,…,xm,0,0,…,0). Two vectors of Rn are in the same congruence class modulo the subspace if and only if they are identical in the last n−m coordinates. The quotient space Rn/ Rm is isomorphic to Rn−m in an obvious manner.Let X = R2 be the standard Cartesian plane, and let Y be a line through the origin in X. Then the quotient space X/Y can be identified with the space of all lines in X which are parallel to Y. That is to say that, the elements of the set X/Y are lines in X parallel to Y. This gives one way in which to visualize quotient spaces geometrically.The mapping that associates to v ∈ V the equivalence class [v] is known as the quotient map.It is not hard to check that these operations are well-defined (i.e. do not depend on the choice of representative). These operations turn the quotient space V/N into a vector space over K with N being the zero class, [0].The quotient space V/N is then defined as V/~, the set of all equivalence classes over V by ~. Scalar multiplication and addition are defined on the equivalence classes bysince it is given byThe equivalence class (or, in this case, the coset) of x is often denotedFormally, the construction is as follows (Halmos 1974, §21-22). Let V be a vector space over a field K, and let N be a subspace of V. We define an equivalence relation ~ on V by stating that x ~ y if x − y ∈ N. That is, x is related to y if one can be obtained from the other by adding an element of N. From this definition, one can deduce that any element of N is related to the zero vector; more precisely all the vectors in N get mapped into the equivalence class of the zero vector.In linear algebra, the quotient of a vector space V by a subspace N is a vector space obtained by "collapsing" N to zero. The space obtained is called a quotient space and is denoted V/N (read V mod N or V by N).
Linear subspace
See the article on null space for an example.If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.See the article on column space for an example.If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.See the article on row space for an example.Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N ∩ N⊥ = N ≠ {0}. The same case presents the ⊥ operation in symplectic vector spaces.If V is an inner product space, then the orthogonal complement ⊥ of any subspace of V is again a subspace. This operation, understood as negation (¬), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.Here the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:For example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequalityIf U and W are subspaces, their sum is the subspaceFor every vector space V, the set {0} and V itself are subspaces of V.[7]Proof:Given subspaces U and W of a vector space V, then their intersection U ∩ W := {v ∈ V : v is an element of both U and W} is also a subspace of V.[6]A subspace cannot lie in any subspace of lesser dimension. If dim U = k, a finite number, and U ⊂ W, then dim W = k if and only if U = W.The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).for (t1, t2, ... , tk) ≠ (u1, u2, ... , uk).[5] If v1, ..., vk are linearly independent, then the coordinates t1, ..., tk for a vector in the span are uniquely determined.In general, vectors v1, ... , vk are called linearly independent ifIn general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1, 0, 0), (0, 0, 1), and (2, 0, 3) is just the xz-plane, with each point on the plane described by infinitely many different values of t1, t2, t3.The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:If the vectors v1, ... , vk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1, ... , vk.The set of all possible linear combinations is called the span:In general, a linear combination of vectors v1, v2, ... , vk is any vector of the formThe expression on the right is called a linear combination of the vectors (2, 5, −1) and (3, −4, 2). These two vectors are said to span the resulting subspace.In linear algebra, the system of parametric equations can be written as a single vector equation:is a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]For example, the set of all vectors (x, y, z) parameterized by the equationsThe subset of Kn described by a system of homogeneous linear parametric equations is a subspace:Every subspace of Kn can be described as the null space of some matrix (see algorithms, below).The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrixIn a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:is a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.For example (over real or rational numbers), the set of all vectors (x, y, z) satisfying the equationsThe solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F = 0 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.A way to characterize subspaces is that they are closed under linear combinations. That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W. Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.Examples that extend these themes are common in functional analysis.Example IV: Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions. The same sort of argument as before shows that this is a subspace too.Proof:Example III: Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R. Let C(R) be the subset consisting of continuous functions. Then C(R) is a subspace of RR.In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace. (The equation in example I was z = 0, and the equation in example II was x = y.) Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.Proof:Example II: Let the field be R again, but now let the vector space be the Cartesian plane R2. Take W to be the set of points (x, y) of R2 such that x = y. Then W is a subspace of R2.Proof:Example I: Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3. Take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V. Then W is a subspace if:In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspaces[disambiguation needed].
Tensor product
However, these kinds of notation are not universally present in array languages. Other array languages may require explicit treatment of indices (for example, MATLAB), and/or may not support higher-order functions such as the Jacobian derivative (for example, Fortran/APL).Note that J's treatment also allows the representation of some tensor fields, as a and b may be functions instead of constants. This product of two functions is a derived function, and if a and b are differentiable, then a */ b is differentiable.Array programming languages may have this pattern built in. For example, in APL the tensor product is expressed as ○.× (for example A ○.× B or A ○.× B ○.× C). In J the tensor product is the dyadic form of */ (for example a */ b or a */ b */ c).That is, in the symmetric algebra two adjacent vectors (and therefore all of them) can be interchanged. The resulting objects are called symmetric tensors.The symmetric algebra is constructed in a similar manner:Note that when the underlying field of V does not have characteristic 2, then this definition is equivalent tois defined asTwo notable constructions in linear algebra can be constructed as quotients of the tensor product: the exterior algebra and the symmetric algebra. For example, given a vector space V, the exterior productA general context for tensor product is that of a monoidal category.It should be mentioned that, though called "tensor product", this is not a tensor product of graphs in the above sense; actually it is the category-theoretic product in the category of graphs and graph homomorphisms. However it is actually the Kronecker tensor product of the adjacency matrices of the graphs. Compare also the section Tensor product of linear maps above.This is a special case of the product of tensors if they are seen as multilinear maps (see also tensors as multilinear maps). Thus the components of the tensor product of multilinear forms can be computed by the Kronecker product.is isomorphic (as an A-algebra) to the Adeg(f).where now f is interpreted as the same polynomial, but with its coefficients regarded as elements of B. In the larger field B, the polynomial may become reducible, which brings in Galois theory. For example, if A = B is a Galois extension of R, thenA particular example is when A and B are fields containing a common subfield R. The tensor product of fields is closely related to Galois theory: if, say, A = R[x] / f(x), where f is some irreducible polynomial with coefficients in R, the tensor product can be calculated asFor example,Let R be a commutative ring. The tensor product of R-modules applies, in particular, if A and B are R-algebras. In this case, the tensor product A ⊗R B is an R-algebra itself by puttingis not usually injective. For example, tensoring the (injective) map given by multiplication with n, n : Z → Z with Z/nZ yields the zero map 0 : Z/nZ → Z/nZ, which is not injective. Higher Tor functors measure the defect of the tensor product being not left exact. All higher Tor functors are assembled in the derived tensor product.Here NJ := ⨁j ∈ J N and the map is determined by sending some n ∈ N in the jth copy of NJ to ajin (in NI). Colloquially, this may be rephrased by saying that a presentation of M gives rise to a presentation of M ⊗R N. This is referred to by saying that the tensor product is a right exact functor. It is not in general left exact, that is, given an injective map of R-modules M1 → M2, the tensor productFor vector spaces, the tensor product V ⊗ W is quickly computed since bases of V of W immediately determine a basis of V ⊗ W, as was mentioned above. For modules over a general (commutative) ring, not every module is free. For example, Z/nZ is not a free abelian group (= Z-module). The tensor product with Z/nZ is given byLet A be a right R-module and B be a left R-module B. Then the tensor product of A and B is an abelian group defined byThe universal property also carries over, slightly modified: the map φ : A × B → A ⊗R B defined by (a, b) ↦ a ⊗ b is a middle linear map (referred to as "the canonical middle linear map".[13]); that is,[14] it satisfies:is imposed. If R is non-commutative, this is no longer an R-module, but just an abelian group.More generally, the tensor product can be defined even if the ring is non-commutative (ab ≠ ba). In this case A has to be a right-R-module and B is a left-R-module, and instead of the last two relations above, the relationwhere now F(A × B) is the free R-module generated by the cartesian product and G is the R-module generated by the same relations as above.The tensor product of two modules A and B over a commutative ring R is defined in exactly the same way as the tensor product of vector spaces over a field:where u∗ in End(V∗) is the transpose of u, that is, in terms of the obvious pairing on V ⊗ V∗,Here Hom(-,-) denotes the K-vector space of all linear maps. This is an example of adjoint functors: the tensor product is "left adjoint" to Hom.Furthermore, given three vector spaces U, V, W the tensor product is linked to the vector space of all linear maps, as follows:This result impliesGiven two finite dimensional vector spaces U, V, denote the dual space of U as U*, we have the following relation:The interplay of evaluation and coevaluation map can be used to characterize finite-dimensional vector spaces without referring to bases.[12]where v1, ..., vn is any basis of V, and vi∗ is its dual basis. Surprisingly, this map does not depend on our choice of basis.[11]On the other hand, if V is finite-dimensional, there is a canonical map in the other direction (called the coevaluation map)is called tensor contraction (for r, s > 0).The resulting mapwhich on elementary tensors is defined byA particular example is the tensor product of some vector space V with its dual vector space V∗ (which consists of all linear maps f from V to the ground field K). In this case, there is a canonical evaluation mapand[10] Thus, the components of the tensor product of two tensors are the ordinary product of the components of each tensor. Another example: let U be a tensor of type (1, 1) with components Uαβ, and let V be a tensor of type (1, 0) with components V γ. ThenPicking a basis of V and the corresponding dual basis of V∗ naturally induces a basis for Tr
s(V) (this basis is described in the article on Kronecker products). In terms of these bases, the components of a (tensor) product of two (or more) tensors can be computed. For example, if F and G are two covariant tensors of rank m and n respectively (i.e. F ∈ T 0
m, and G ∈ T 0
n), then the components of their tensor product are given byIt is defined by grouping all occurring "factors" V together: writing vi for an element of V and fi for elements of the dual space,There is a product map, called the (tensor) product of tensors[9]Here V∗ is the dual vector space (which consists of all linear maps f from V to the ground field K).For non-negative integers r and s a type (r,s) tensor on a vector space V is an element ofThe isomorphism τσ is called the braiding map associated to the permutation σ.such thatbe the natural multilinear embedding of the Cartesian power of V into the tensor power of V. Then, by the universal property, there is a unique isomorphismLetA permutation σ of the set {1, 2, ..., n} determines a mapping of the nth Cartesian power of V as follows:Let n be a non-negative integer. The nth tensor power of the vector space V is the n-fold tensor product of V with itself. That isThe universal-property definition of a tensor product is valid in more categories that just the category of vector spaces. Instead of using multilinear (bilinear) maps, the general tensor product definition uses multimorphisms.[8]The category of vector spaces with tensor product is an example of a symmetric monoidal category.Similar reasoning can be used to show that the tensor product is associative, that is, there are natural isomorphismsThis characterization can simplify proofs about the tensor product. For example, the tensor product is symmetric, meaning there is a canonical isomorphism:A dyadic product is the special case of the tensor product between two vectors of the same dimension.The resultant rank is at most 4, and thus the resultant dimension is 4. Here rank denotes the tensor rank (number of requisite indices), while the matrix rank counts the number of degrees of freedom in the resulting array.respectively, then the tensor product of these two matrices isBy choosing bases of all vector spaces involved, the linear maps S and T can be represented by matrices. Then, the matrix describing the tensor product S ⊗ T is the Kronecker product of the two matrices. For example, if V, X, W, and Y above are all two-dimensional and bases have been fixed for all of them, and S and T are given by the matricesIf S and T are both injective, surjective, or continuous then S ⊗ T is, respectively, injective, surjective, continuous.In this way, the tensor product becomes a bifunctor from the category of vector spaces to itself, covariant in both arguments.[7]defined byThe tensor product also operates on linear maps between vector spaces. Specifically, given two linear maps S : V → X and T : W → Y between vector spaces, the tensor product of the two linear maps S and T is a linear mapGiven bases {vi} and {wj} for V and W respectively, the tensors {vi ⊗ wj} form a basis for V ⊗ W. Therefore, if V and W are finite-dimensional, the dimension of the tensor product is the product of dimensions of the original spaces; for instance Rm ⊗ Rn is isomorphic to Rmn.Elements of V ⊗ W are often referred to as tensors, although this term refers to many other related concepts as well.[5] If v belongs to V and w belongs to W, then the equivalence class of (v, w) is denoted by v ⊗ w, which is called the tensor product of v with w. In physics and engineering, this use of the "⊗" symbol refers specifically to the outer product operation; the result of the outer product v ⊗ w is one of the standard ways of representing the equivalence class v ⊗ w.[6] An element of V ⊗ W that can be written in the form v ⊗ w is called a pure or simple tensor. In general, an element of the tensor product space is not a pure tensor, but rather a finite linear combination of pure tensors. For example, if v1 and v2 are linearly independent, and w1 and w2 are also linearly independent, then v1 ⊗ w1 + v2 ⊗ w2 cannot be written as a pure tensor. The number of simple tensors required to express an element of a tensor product is called the tensor rank (not to be confused with tensor order, which is the number of spaces one has taken the product of, in this case 2; in notation, the number of indices), and for linear operators or matrices, thought of as (1, 1) tensors (elements of the space V ⊗ V∗), it agrees with matrix rank.all hold (unlike in F(V × W)), which is exactly what is desired. In these latter expressions, the (v1, w), etc., are images in the quotient of vectors in the free product under the quotient map. Usually, some other notation is employed for them, see below.In the quotient, where N is mapped to the zero vector, the following equalities,The following expression explicitly gives the subspace N:[4]The result can be proven to be independent of which representatives of the involved classes have been chosen. In other words, the operations are well-defined.in the involved equivalence classes outputting the one equivalence class of the result.The operations of V ⊗ W, i.e. the map of vector addition + : U × U → U and scalar multiplication ⋅ : K × U → U are defined to be the respective operations +F and ⋅F from F(V × W), acting on any representativesFrom the Cartesian product V × W, the free vector space F(V × W) over K is formed. The vectors of V ⊗ W are then defined to be the equivalence classes of the congruence generated by the following relations on F(V × W):In general, given two vector spaces V and W over a field K, the tensor product U of V and W, denoted as U = V ⊗ W is defined as the vector space whose elements and operations are constructed as follows:Let us first consider a special case: let us say V, W are free vector spaces for the sets S, T respectively. That is, V = F(S), W = F(T). In this special case, the tensor product is defined as F(S) ⊗ F(T) = F(S × T). In most typical cases, any vector space can be immediately understood as the free vector space for some set, so this definition suffices. However, there is also an explicit way of constructing the tensor product directly from V, W, without appeal to S, T.By construction, the (possibly infinite) dimension of the vector space F(S) equals the cardinality of the set S.Then {δs | s ∈ S} is a basis for F(S), since each element g of F(S) can be uniquely written as a linear combination of δs, and because of the restriction that g has finite support, this linear combination consists of finitely many terms. Because of this explicit expression, an element of F(S) is often called a formal sum of symbols in S.The definition of ⊗ requires the notion of the free vector space F(S) on some set S, a vector space whose basis is indexed by S. F(S) is defined as the set of all functions g from S to a given field K that have finite support; i.e., g is identically zero outside some finite subset of S. It is a vector space over K with the usual addition and scalar multiplication of functions. It has a basis parameterized by S. Indeed, for each s in S we define[1]The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below). Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over V × W. This approach is described below.The tensor product of two vector spaces V and W over a field K is another vector space over K. It is denoted V ⊗K W, or V ⊗ W when the underlying field K is understood.More generally, the tensor product can be extended to other categories of mathematical objects in addition to vector spaces, such as to matrices, tensors, algebras, topological vector spaces, and modules. In each such case the tensor product is characterized by a similar universal property: it is the freest bilinear operation. The general concept of a "tensor product" is captured by monoidal categories; that is, the class of all things that have a tensor product is a monoidal category.In particular, this distinguishes the tensor product from the direct sum vector space, whose dimension is the sum of the dimensions of the two summands:The tensor product of (finite dimensional) vector spaces has dimension equal to the product of the dimensions of the two factors:In mathematics, the tensor product V ⊗ W of two vector spaces V and W (over the same field) is itself a vector space, together with an operation of bilinear composition, denoted by ⊗, from ordered pairs in the Cartesian product V × W into V ⊗ W, in a way that generalizes the outer product. The tensor product of V and W is the vector space generated by the symbols v ⊗ w, with v ∈ V and w ∈ W, in which the relations of bilinearity are imposed for the product operation ⊗, and no other relations are assumed to hold. The tensor product space is thus the "freest" (or most general) such vector space, in the sense of having the fewest constraints.
Numerical linear algebra
Common problems in numerical linear algebra include computing the following: LU decomposition, QR decomposition, singular value decomposition, eigenvalues.Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, fluid dynamics, and many other areas. Such software relies heavily on the development, analysis, and implementation of state-of-the-art algorithms for solving various numerical linear algebra problems, in large part because of the role of matrices in finite difference and finite element methods.
Floating-point arithmetic
Bounded floating point is a patented method of representing real numbers as an extension of standard floating point by adding a field to the standard representation of real numbers. This new field contains a subfield for the number of insignificant bits in the real number representation. The value of this field is the logarithm of the upper bound on the value represented. Additional subfields allow for the accumulation of rounding error. The upper bound on the logarithm of the accumulated rounding error contributes to the number of insignificant bits.[43]John Gustafson (scientist) has suggested a representation of real numbers he calls "Universal Numbers" (unums).[39] An extension of variable length arithmetic, the unum format is variable width format with the normal sign, and variable length exponent, and variable length significand, with one ubit, and variable length fields identifying the exponent and significand lengths.[39]:4 The ubit defines whether the least significant unum bit is correct or off by up to one ulp[39]:4 bounding the real value represented. The efficacy of unums has been contested by William Kahan[40][41] and others [42]Interval arithmetic utilizes a pair of values representing the limits of the real value represented.[37][38] Algorithms are available for computing these upper and lower bounds.Variable length arithmetic represents numbers by a sequence of decimal digits. A special code or bit indicates the end of a number and operations are performed serially, digit by digit. Still, real numbers are represented in a fixed space and are, therefore, subject to error. The IBM 1620 (1959, 1962) used variable length number representation.The floating point standard defines "precision" and the number of digits uses to represent a real number and does not refer to the accuracy of that representation. Common definitions are "single precision" (32-bit) and "double precision" (64-bit). Extension of precision increases the number of bits required to represent a real value. Though more likely to produce the desirable accuracy, there is still no indication of the correctness of the result.Monte Carlo arithmetic provides a dynamic evaluation of an algorithm's sensitivity to floating point error. William Kahan challenges the efficacy of Monte Carlo arithmetic.[36]Neither rounding error nor cancellation error are recognized nor recorded by standard floating point operations and therefore the results of floating point operations are equivocal.Similarly, when subtracting similar number, the value must be normalized by shifting the significand left. This introduces useless bits into the right hand side of the significand, called cancellation.[34]:20 This may remove sufficient significant digits so that the represented value is worthless and is called "catastrophic cancellation." Cancellation otherwise still introduces error called "benign cancellation."The primary sources of floating point errors are alignment and normalization. Alignment is the shifting operation that must occur when adding or subtracting numbers with differing exponents. Normally, the fractional value (significand plus the hidden bit) is greater than or equal to 1. and less than 2. The process of shifting the significand to create this situation is called "normalization". Right shifting operations will provide digits in the result that will no longer fit in the specified format. These bits can be dropped to round the result down[34]:6 , or can be used to round the result up. Some numbers cannot be accurately depicted in an fixed format computer representation and must be rounded. Either case introduces a "rounding error", resulting in a slightly smaller or slightly larger representation of the real number represented. In a single operation, rounding may not introduce a significant error, but collectively, over a large number of operations, the result could be so erroneous so as to be useless.While the two forms of the recurrence formula are clearly mathematically equivalent,[nb 7] the first subtracts 1 from a number extremely close to 1, leading to an increasingly problematic loss of significant digits. As the recurrence is applied repeatedly, the accuracy improves at first, but then it deteriorates. It never gets better than about 8 digits, even though 53-bit arithmetic should be capable of about 16 digits of precision. When the second form of the recurrence is used, the value converges to 15 digits of precision.Here is a computation using IEEE "double" (a significand with 53 bits of precision) arithmetic:Round-off error can affect the convergence and accuracy of iterative numerical procedures. As an example, Archimedes approximated π by calculating the perimeters of polygons inscribing and circumscribing a circle, starting with hexagons, and successively doubling the number of sides. As noted above, computations may be rearranged in a way that is mathematically equivalent but less prone to error (numerical analysis). Two forms of the recurrence formula for the circumscribed polygon are[citation needed]:The low 3 digits of the addends are effectively lost. Suppose, for example, that one needs to add many numbers, all approximately equal to 3. After 1000 of them have been added, the running sum is about 3000; the lost digits are not regained. The Kahan summation algorithm may be used to reduce the errors.[24]Summation of a vector of floating-point values is a basic algorithm in scientific computing, and so an awareness of when loss of significance can occur is essential. For example, if one is adding a very large number of numbers, the individual addends are very small compared with the sum. This can lead to loss of significance. A typical addition would then be something likeSmall errors in floating-point arithmetic can grow when mathematical algorithms perform operations an enormous number of times. A few examples are matrix inversion, eigenvector computation, and differential equation solving. These algorithms must be very carefully designed, using numerical approaches such as Iterative refinement, if they are to work well.[33]The use of the equality test (if (x==y) ...) requires care when dealing with floating-point numbers. Even simple expressions like 0.6/0.2-3==0 will, on most computers, fail to be true[31] (in IEEE 754 double precision, for example, 0.6/0.2-3 is approximately equal to -4.44089209850063e-16). Consequently, such tests are sometimes replaced with "fuzzy" comparisons (if (abs(x-y) < epsilon) ..., where epsilon is sufficiently small and tailored to the application, such as 1.0E−13). The wisdom of doing this varies greatly, and can require numerical analysis to bound epsilon.[24] Values derived from the primary data representation and their comparisons should be performed in a wider, extended, precision to minimize the risk of such inconsistencies due to round-off errors.[28] It is often better to organize the code in such a way that such tests are unnecessary. For example, in computational geometry, exact tests of whether a point lies off or on a line or plane defined by other points can be performed using adaptive precision or exact arithmetic methods.[32]As decimal fractions can often not be exactly represented in binary floating-point, such arithmetic is at its best when it is simply being used to measure real-world quantities over a wide range of scales (such as the orbital period of a moon around Saturn or the mass of a proton), and at its worst when it is expected to model the interactions of quantities expressed as decimal strings that are expected to be exact.[26][28] An example of the latter case is financial calculations. For this reason, financial software tends not to use a binary floating-point number representation.[30] The "decimal" data type of the C# and Python programming languages, and the decimal formats of the IEEE 754-2008 standard, are designed to avoid the problems of binary floating-point representations when applied to human-entered exact decimal values, and make the arithmetic always behave as expected when numbers are printed in decimal.A detailed treatment of the techniques for writing high-quality floating-point software is beyond the scope of this article, and the reader is referred to,[24][28] and the other references at the bottom of this article. Kahan suggests several rules of thumb that can substantially decrease by orders of magnitude[28] the risk of numerical anomalies, in addition to, or in lieu of, a more careful numerical analysis. These include: as noted above, computing all expressions and intermediate results in the highest precision supported in hardware (a common rule of thumb is to carry twice the precision of the desired result i.e. compute in double precision for a final single precision result, or in double extended or quad precision for up to double precision results[14]); and rounding input data and results to only the precision required and supported by the input data (carrying excess precision in the final result beyond that required and supported by the input data can be misleading, increases storage cost and decreases speed, and the excess bits can affect convergence of numerical procedures:[29] notably, the first form of the iterative example given below converges correctly when using this rule of thumb). Brief descriptions of several additional issues and techniques follow.To maintain the properties of such carefully constructed numerically stable programs, careful handling by the compiler is required. Certain "optimizations" that compilers might make (for example, reordering operations) can work against the goals of well-behaved software. There is some controversy about the failings of compilers and language designs in this area: C99 is an example of a language where such optimizations are carefully specified so as to maintain numerical precision. See the external references at the bottom of this article.then the algorithm becomes numerically stable and can compute to full double precision.If, however, intermediate computations are all performed in extended precision (e.g. by setting line [1] to C99 long double), then up to full precision in the final double result can be maintained.[nb 6] Alternatively, a numerical analysis of the algorithm reveals that if the following non-obvious change to line [2] is made:For example, the following algorithm is a direct implementation to compute the function A(x) = (x−1) / (exp(x−1) − 1) which is well-conditioned at 1.0,[nb 5] however it can be shown to be numerically unstable and lose up to half the significant digits carried by the arithmetic when computed near 1.0.[13]Although, as noted previously, individual arithmetic operations of IEEE 754 are guaranteed accurate to within half a ULP, more complicated formulae can suffer from larger errors due to round-off. The loss of accuracy can be substantial if a problem or its data are ill-conditioned, meaning that the correct result is hypersensitive to tiny perturbations in its data. However, even functions that are well-conditioned can suffer from large loss of accuracy if an algorithm numerically unstable for that data is used: apparently equivalent formulations of expressions in a programming language can differ markedly in their numerical stability. One approach to remove the risk of such loss of accuracy is the design and analysis of numerically stable algorithms, which is an aim of the branch of mathematics known as numerical analysis. Another approach that can protect against the risk of numerical instabilities is the computation of intermediate (scratch) values in an algorithm at a higher precision than the final result requires,[25] which can remove, or reduce by orders of magnitude,[26] such risk: IEEE 754 quadruple precision and extended precision are designed for this purpose when computing at double precision.[27][nb 4]which is the sum of two slightly perturbed (on the order of Εmach) input data, and so is backward stable. For more realistic examples in numerical linear algebra see Higham 2002[24] and other references below.and soBackward error analysis, the theory of which was developed and popularized by James H. Wilkinson, can be used to establish that an algorithm implementing a numerical function is numerically stable.[22] The basic approach is to show that although the calculated result, due to roundoff errors, will not be exactly correct, it is the exact solution to a nearby problem with slightly perturbed input data. If the perturbation required is small, on the order of the uncertainty in the input data, then the results are in some sense as accurate as the data "deserves". The algorithm is then defined as backward stable. Stability is a measure of the sensitivity to rounding errors of a given numerical procedure; by contrast, the condition number of a function for a given problem indicates the inherent sensitivity of the function to small perturbations in its input and is independent of the implementation used to solve the problem.[23]This is important since it bounds the relative error in representing any non-zero real number x within the normalized range of a floating-point system:whereas rounding to nearest,With rounding to zero,Machine precision is a quantity that characterizes the accuracy of a floating-point system, and is used in backward error analysis of floating-point algorithms. It is also known as unit roundoff or machine epsilon. Usually denoted Εmach, its value depends on the particular rounding being used.In addition to loss of significance, inability to represent numbers such as π and 0.1 exactly, and other slight inaccuracies, the following phenomena may occur:They are also not necessarily distributive. That is, (a + b) × c may not be the same as a × c + b × c:While floating-point addition and multiplication are both commutative (a + b = b + a and a × b = b × a), they are not necessarily associative. That is, (a + b) + c is not necessarily equal to a + (b + c). Using 7-digit significand decimal arithmetic:By the same token, an attempted computation of sin(π) will not yield zero. The result will be (approximately) 0.1225×10−15 in double precision, or −0.8742×10−7 in single precision.[nb 3]will give a result of 16331239353195370.0. In single precision (using the tanf function), the result will be −22877332.0.Also, the non-representability of π (and π/2) means that an attempted computation of tan(π/2) will not yield a result of infinity, nor will it even overflow. It is simply not possible for standard floating-point hardware to attempt to compute tan(π/2), because π/2 cannot be represented exactly. This computation in C:But the representable number closest to 0.01 isSquaring it with single-precision floating-point hardware (with rounding) givesSquaring this number givesFor example, the non-representability of 0.1 and 0.01 (in binary) means that the result of attempting to square 0.1 is neither 0.01 nor the representable number closest to it. In 24-bit (single precision) representation, 0.1 (decimal) was given previously as e = −4; s = 110011001100110011001101, which isThe fact that floating-point numbers cannot precisely represent all real numbers, and that floating-point operations cannot precisely represent true arithmetic operations, leads to many surprising situations. This is related to the finite precision with which computers generally represent numbers.Overflow and invalid exceptions can typically not be ignored, but do not necessarily represent errors: for example, a root-finding routine, as part of its normal operation, may evaluate a passed-in function at values outside of its domain, returning NaN and an invalid exception flag to be ignored until finding a useful start point.[17]IEEE 754 specifies five arithmetic exceptions that are to be recorded in the status flags ("sticky bits"):The original IEEE 754 standard, however, failed to recommend operations to handle such sets of arithmetic exception flag bits. So while these were implemented in hardware, initially programming language implementations typically did not provide a means to access them (apart from assembler). Over time some programming language standards (e.g., C99/C11 and Fortran) have been updated to specify methods to access and change status flag bits. The 2008 version of the IEEE 754 standard now specifies a few operations for accessing and handling the arithmetic flag bits. The programming model is based on a single thread of execution and use of them by multiple threads has to be handled by a means outside of the standard (e.g. C11 specifies that the flags have thread-local storage).Here, the required default method of handling exceptions according to IEEE 754 is discussed (the IEEE 754 optional trapping and other "alternate exception handling" modes are not discussed). Arithmetic exceptions are (by default) required to be recorded in "sticky" status flag bits. That they are "sticky" means that they are not reset by the next (arithmetic) operation, but stay set until explicitly reset. The use of "sticky" flags thus allows for testing of exceptional conditions to be delayed until after a full floating-point expression or subroutine: without them exceptional conditions that could not be otherwise ignored would require explicit testing immediately after every floating-point operation. By default, an operation always returns a result according to specification without interrupting computation. For instance, 1/0 returns +∞, while also setting the divide-by-zero flag bit (this default of ∞ is designed so as to often return a finite result when used in subsequent operations and so be safely ignored).Prior to the IEEE standard, such conditions usually caused the program to terminate, or triggered some kind of trap that the programmer might be able to catch. How this worked was system-dependent, meaning that floating-point programs were not portable. (Note that the term "exception" as used in IEEE 754 is a general term meaning an exceptional condition, which is not necessarily an error, and is a different usage to that typically defined in programming languages such as a C++ or Java, in which an "exception" is an alternative flow of control, closer to what is termed a "trap" in IEEE 754 terminology).Floating-point computation in a computer can run into three kinds of problems:There are no cancellation or absorption problems with multiplication or division, though small errors may accumulate as operations are performed in succession.[12] In practice, the way these operations are carried out in digital logic can be quite complex (see Booth's multiplication algorithm and Division algorithm).[nb 2] For a fast, simple method, see the Horner method.Similarly, division is accomplished by subtracting the divisor's exponent from the dividend's exponent, and dividing the dividend's significand by the divisor's significand.To multiply, the significands are multiplied while the exponents are added, and the result is rounded and normalized.The best representation of this difference is e = −1; s = 4.877000, which differs more than 20% from e = −1; s = 4.000000. In extreme cases, all significant digits of precision can be lost (although gradual underflow ensures that the result will not be zero unless the two operands were equal). This cancellation illustrates the danger in assuming that all of the digits of a computed result are meaningful. Dealing with the consequences of these errors is a topic in numerical analysis; see also Accuracy problems.Another problem of loss of significance occurs when two nearly equal numbers are subtracted. In the following example e = 5; s = 1.234571 and e = 5; s = 1.234567 are representations of the rationals 123457.1467 and 123456.659.In the above conceptual examples it would appear that a large number of extra digits would need to be provided by the adder to ensure correct rounding; however, for binary addition or subtraction using careful implementation techniques only two extra guard bits and one extra sticky bit need to be carried beyond the precision of the operands.[12]Note that the lowest three digits of the second operand (654) are essentially lost. This is round-off error. In extreme cases, the sum of two non-zero numbers may be equal to one of them:This is the true result, the exact sum of the operands. It will be rounded to seven digits and then normalized if necessary. The final result isIn detail:A simple method to add floating-point numbers is to first represent them with the same exponent. In the example below, the second number is shifted right by three digits, and one then proceeds with the usual addition method:For ease of presentation and understanding, decimal radix with 7 digit precision will be used in the examples, as in the IEEE 754 decimal32 format. The fundamental principles are the same in any radix or precision, except that normalization is optional (it does not affect the numerical value of the result). Here, s denotes the significand and e denotes the exponent.Alternative modes are useful when the amount of error being introduced must be bounded. Applications that require a bounded error are multi-precision floating-point, and interval arithmetic. The alternative rounding modes are also useful in diagnosing numerical instability: if the results of a subroutine vary substantially between rounding to + and − infinity then it is likely numerically unstable and affected by round-off error.[16]Alternative rounding options are also available. IEEE 754 specifies the following rounding modes:Rounding is used when the exact result of a floating-point operation (or a conversion to floating-point format) would need more digits than there are digits in the significand. IEEE 754 requires correct rounding: that is, the rounded result is as if infinitely precise arithmetic was used to compute the value and then rounded (although in implementation only three extra bits are needed to ensure this). There are several different rounding schemes (or rounding modes). Historically, truncation was the typical approach. Since the introduction of IEEE 754, the default method (round to nearest, ties to even, sometimes called Banker's Rounding) is more commonly used. This method rounds the ideal (infinitely precise) result of an arithmetic operation to the nearest representable value, and gives that representation as the result.[nb 1] In the case of a tie, the value that would make the significand end in an even digit is chosen. The IEEE 754 standard requires the same rounding to be applied to all fundamental algebraic operations, including square root and conversions, when there is a numeric (non-NaN) result. It means that the results of IEEE 754 operations are completely determined in all bits of the result, except for the representation of NaNs. ("Library" functions such as cosine and log are not mandated.)The arithmetical difference between two consecutive representable floating-point numbers which have the same exponent is called a unit in the last place (ULP). For example, if there is no representable number lying between the representable numbers 1.45a70c22hex and 1.45a70c24hex, the ULP is 2×16−8, or 2−31. For numbers with a base-2 exponent part of 0, i.e. numbers with an absolute value higher than or equal to 1 but lower than 2, an ULP is exactly 2−23 or about 10−7 in single precision, and exactly 2−53 or about 10−16 in double precision. The mandated behavior of IEEE-compliant hardware is that the result be within one-half of a ULP.The result of rounding differs from the true value by about 0.03 parts per million, and matches the decimal representation of π in the first 7 digits. The difference is the discretization error and is limited by the machine epsilon.whereas a more accurate approximation of the true value of π isIn binary single-precision floating-point, this is represented as s = 1.10010010000111111011011 with e = 1. This has a decimal value ofwhen approximated by rounding to a precision of 24 bits.but isAs a further example, the real number π, represented in binary as an infinite sequence of bits iswhich is actually 0.100000001490116119384765625 in decimal.When rounded to 24 bits this becomeswhere, as previously, s is the significand and e is the exponent.Whether or not a rational number has a terminating expansion depends on the base. For example, in base-10 the number 1/2 has a terminating expansion (0.5) while the number 1/3 does not (0.333...). In base-2 only rationals with denominators that are powers of 2 (such as 1/2 or 3/16) are terminating. Any rational with a denominator that has a prime factor other than 2 will have an infinite binary expansion. This means that numbers which appear to be short and exact when written in decimal format may need to be approximated when converted to binary floating-point. For example, the decimal number 0.1 is not representable in binary floating-point of any finite precision; the exact binary representation would have a "1100" sequence continuing endlessly:When a number is represented in some format (such as a character string) which is not a native floating-point representation supported in a computer implementation, then it will require a conversion before it can be used in that implementation. If the number can be represented exactly in the floating-point format then the conversion is exact. If there is not an exact representation then the conversion requires a choice of which floating-point number to use to represent the original value. The representation chosen will have a different value from the original, and the value thus adjusted is called the rounded value.By their nature, all numbers expressed in floating-point format are rational numbers with a terminating expansion in the relevant base (for example, a terminating decimal expansion in base-10, or a terminating binary expansion in base-2). Irrational numbers, such as π or √2, or non-terminating rational numbers, must be approximated. The number of digits (or bits) of precision also limits the set of rational numbers that can be represented exactly. For example, the number 123456789 cannot be exactly represented if only eight decimal digits of precision are available.It is a common misconception that the more esoteric features of the IEEE 754 standard discussed here, such as extended formats, NaN, infinities, subnormals etc., are only of interest to numerical analysts, or for advanced numerical applications; in fact the opposite is true: these features are designed to give safe robust defaults for numerically unsophisticated programmers, in addition to supporting sophisticated numerical libraries by experts. The key designer of IEEE 754, William Kahan notes that it is incorrect to "... [deem] features of IEEE Standard 754 for Binary Floating-Point Arithmetic that ...[are] not appreciated to be features usable by none but numerical experts. The facts are quite the opposite. In 1977 those features were designed into the Intel 8087 to serve the widest possible market... Error-analysis tells us how to design floating-point arithmetic, like IEEE Standard 754, moderately tolerant of well-meaning ignorance among programmers".[13]The representation of NaNs specified by the standard has some unspecified bits that could be used to encode the type or source of error; but there is no standard for that encoding. In theory, signaling NaNs could be used by a runtime system to flag uninitialized variables, or extend the floating-point numbers with other special values without slowing down the computations with ordinary values, although such extensions are not common.IEEE 754 specifies a special value called "Not a Number" (NaN) to be returned as the result of certain "invalid" operations, such as 0/0, ∞×0, or sqrt(−1). In general, NaNs will be propagated i.e. most operations involving a NaN will result in a NaN, although functions that would give some defined result for any given floating-point value will do so for NaNs as well, e.g. NaN ^ 0 = 1. There are two kinds of NaNs: the default quiet NaNs and, optionally, signaling NaNs. A signaling NaN in any arithmetic operation (including numerical comparisons) will cause an "invalid" exception to be signaled.IEEE 754 requires infinities to be handled in a reasonable way, such asThe infinities of the extended real number line can be represented in IEEE floating-point datatypes, just like ordinary floating-point values like 1, 1.5, etc. They are not error values in any way, though they are often (but not always, as it depends on the rounding) used as replacement values when there is an overflow. Upon a divide-by-zero exception, a positive or negative infinity is returned as an exact result. An infinity can also be introduced as a numeral (like C's "INFINITY" macro, or "∞" if the programming language allows that syntax).Modern floating-point hardware usually handles subnormal values (as well as normal values), and does not require software emulation for subnormals.Subnormal values fill the underflow gap with values where the absolute distance between them is the same as for adjacent values just outside the underflow gap. This is an improvement over the older practice to just have zero in the underflow gap, and where underflowing results were replaced by zero (flush to zero).In the IEEE 754 standard, zero is signed, meaning that there exist both a "positive zero" (+0) and a "negative zero" (−0). In most run-time environments, positive zero is usually printed as "0" and the negative zero as "-0". The two values behave as equal in numerical comparisons, but some operations return different results for +0 and −0. For instance, 1/(−0) returns negative infinity, while 1/+0 returns positive infinity (so that the identity 1/(1/±∞) = ±∞ is maintained). Other common functions with a discontinuity at x=0 which might treat +0 and −0 differently include log(x), signum(x), and the principal square root of y + xi for any negative number y. As with any approximation scheme, operations involving "negative zero" can occasionally cause confusion. For example, in IEEE 754, x = y does not always imply 1/x = 1/y, as 0 = −0 but 1/0 ≠ 1/−0.[12]More significantly, bit shifting allows one to compute the square (shift left by 1) or take the square root (shift right by 1). This leads to approximate computations of the square root; combined with the previous technique for taking the inverse, this allows the fast inverse square root computation, which was important in graphics processing in the late 1980s and 1990s. This can be exploited in some other applications, such as volume ramping in digital sound processing.[clarification needed]The sum of the exponent bias (127) and the exponent (1) is 128, so this is represented in single precision format asFor example, it was shown above that π, rounded to 24 bits of precision, has:In the IEEE binary interchange formats the leading 1 bit of a normalized significand is not actually stored in the computer datum. It is called the "hidden" or "implicit" bit. Because of this, single precision format actually has a significand with 24 bits of precision, double precision format has 53, and quad has 113.While the exponent can be positive or negative, in binary formats it is stored as an unsigned number that has a fixed "bias" added to it. Values of all 0s in this field are reserved for the zeros and subnormal numbers; values of all 1s are reserved for the infinities and NaNs. The exponent range for normalized numbers is [−126, 127] for single precision, [−1022, 1023] for double, or [−16382, 16383] for quad. Normalized numbers exclude subnormal values, zeros, infinities, and NaNs.Floating-point numbers are typically packed into a computer datum as the sign bit, the exponent field, and the significand or mantissa, from left to right. For the IEEE 754 binary formats (basic and extended) which have extant hardware implementations, they are apportioned as follows:Comparison of floating-point numbers, as defined by the IEEE standard, is a bit different from usual integer comparison. Negative and positive zero compare equal, and every NaN compares unequal to every value, including itself. All values except NaN are strictly smaller than +∞ and strictly greater than −∞. Finite floating-point numbers are ordered in the same way as their values (in the set of real numbers).The standard specifies some special values, and their representation: positive infinity (+∞), negative infinity (−∞), a negative zero (−0) distinct from ordinary ("positive") zero, and "not a number" values (NaNs).Any integer with absolute value less than 224 can be exactly represented in the single precision format, and any integer with absolute value less than 253 can be exactly represented in the double precision format. Furthermore, a wide range of powers of 2 times such a number can be represented. These properties are sometimes used for purely integer data, to get 53-bit integers on platforms that have double precision floats but only 32-bit integers.Increasing the precision of the floating point representation generally reduces the amount of accumulated round-off error caused by intermediate calculations.[9] Less common IEEE formats include:The standard provides for many closely related formats, differing in only a few details. Five of these formats are called basic formats and others are termed extended formats; three of these are especially widely used in computer hardware and languages:The IEEE standardized the computer representation for binary floating-point numbers in IEEE 754 (a.k.a. IEC 60559) in 1985. This first standard is followed by almost all modern machines. It was revised in 2008. IBM mainframes support IBM's own hexadecimal floating point format and IEEE 754-2008 decimal floating point in addition to the IEEE 754 binary format. The Cray T90 series had an IEEE version, but the SV1 still uses Cray floating-point format.In addition there are representable values strictly between −UFL and UFL. Namely, positive and negative zeros, as well as denormalized numbers.The number of normalized floating-point numbers in a system (B, P, L, U) whereOn a typical computer system, a 'double precision' (64-bit) binary floating-point number has a coefficient of 53 bits (one of which is implied), an exponent of 11 bits, and one sign bit. Since 210 = 1024, the complete range of floating-point numbers in this format is from approximately 2−1023 = 10−308 to 21023 = 10308 (see IEEE 754).A floating-point number consists of two fixed-point components, whose range depends exclusively on the number of bits or digits in their representation. Whereas components linearly depend on their range, the floating-point range linearly depends on the significant range and exponentially on the range of exponent component, which attaches outstandingly wider range to the number.Among the x86 innovations are these:In 1989, mathematician and computer scientist William Kahan was honored with the Turing Award for being the primary architect behind this proposal; he was aided by his student (Jerome Coonen) and a visiting professor (Harold Stone).[8]Initially, computers used many different representations for floating-point numbers. The lack of standardization at the mainframe level was an ongoing problem by the early 1970s for those writing and maintaining higher-level source code; these manufacturer floating-point standards differed in the word sizes, the representations, and the rounding behavior and general accuracy of operations. Floating-point compatibility across multiple computing systems was in desperate need of standardization by the early 1980s, leading to the creation of the IEEE 754 standard once the 32-bit (or 64-bit) word had become commonplace. This standard was significantly based on a proposal from Intel, which was designing the i8087 numerical coprocessor; Motorola, which was designing the 68000 around the same time, gave significant input as well.The IBM 7094, also introduced in 1962, supports single-precision and double-precision representations, but with no relation to the UNIVAC's representations. Indeed, in 1964, IBM introduced proprietary hexadecimal floating-point representations in its System/360 mainframes; these same representations are still available for use in modern z/Architecture systems. However, in 1998, IBM included IEEE-compatible binary floating-point arithmetic to its mainframes; in 2005, IBM also added IEEE-compatible decimal floating-point arithmetic.The UNIVAC 1100/2200 series, introduced in 1962, supported two floating-point representations:The mass-produced IBM 704 followed in 1954; it introduced the use of a biased exponent. For many decades after that, floating-point hardware was typically an optional feature, and computers that had it were said to be "scientific computers", or to have "scientific computation" (SC) capability (see also Extensions for Scientific Computation (XSC)). It was not until the launch of the Intel i486 in 1989 that general-purpose personal computers had floating-point capability in hardware as a standard feature.The Pilot ACE has binary floating-point arithmetic, and it became operational in 1950 at National Physical Laboratory, UK. Thirty-three were later sold commercially as the English Electric DEUCE. The arithmetic is actually implemented in software, but with a one megahertz clock rate, the speed of floating-point and fixed-point operations in this machine were initially faster than those of many competing computers.The first commercial computer with floating-point hardware was Zuse's Z4 computer, designed in 1942–1945. In 1946, Bell Laboratories introduced the Mark V, which implemented decimal floating-point numbers.[7]The floating-point representation is by far the most common way of representing in computers an approximation to real numbers. However, there are alternatives:It can be required that the most significant digit of the significand of a non-zero number be non-zero (except when the corresponding exponent would be smaller than the minimum one). This process is called normalization. For binary formats (which uses only the digits 5000000000000000000♠0 and 7000100000000000000♠1) this non-zero digit is necessarily 7000100000000000000♠1. Therefore, it does not need to be represented in memory; allowing the format to have one more bit of precision. This rule is variously called the leading bit convention, the implicit bit convention, or the hidden bit convention.[2]where p is the precision (7001240000000000000♠24 in this example), n is the position of the bit of the significand from the left (starting at 5000000000000000000♠0 and finishing at 7001230000000000000♠23 here) and e is the exponent (7000100000000000000♠1 in this example).When this is stored in memory using the IEEE 754 encoding, this becomes the significand s. The significand is assumed to have a binary point to the right of the leftmost bit. So, the binary representation of π is calculated from left-to-right as follows:In this binary expansion, let us denote the positions from 0 (leftmost bit, or most significant bit) to 32 (rightmost bit). The 24-bit significand will stop at position 23, shown as the underlined bit 5000000000000000000♠0 above. The next bit, at position 24, is called the round bit or rounding bit. It is used to round the 33-bit approximation to the nearest 24-bit number (there are specific rules for halfway values, which is not the case here). This bit, which is 7000100000000000000♠1 in this example, is added to the integer formed by the leftmost 24 bits, yielding:A floating-point number is a rational number, because it can be represented as one integer divided by another; for example 7003145000000000000♠1.45×103 is (145/100)*1000 or 7005145000000000000♠145,000/100. The base determines the fractions that can be represented; for instance, 1/5 cannot be represented exactly as a floating-point number using a binary base, but 1/5 can be represented exactly using a decimal base (6999200000000000000♠0.2, or 6999200000000000000♠2×10−1). However, 1/3 cannot be represented exactly by either binary (0.010101...) or decimal (0.333...), but in base 3, it is trivial (0.1 or 1×3−1) . The occasions on which infinite expansions occur depend on the base and its prime factors, as described in the article on Positional Notation.Historically, several number bases have been used for representing floating-point numbers, with base two (binary) being the most common, followed by base ten (decimal), and other less common varieties, such as base sixteen (hexadecimal notation), and even base three (see Setun).where s is the significand (ignoring any implied decimal point), p is the precision (the number of digits in the significand), b is the base (in our example, this is the number ten), and e is the exponent.Symbolically, this final value is:Using base-10 (the familiar decimal notation) as an example, the number 7005152853504700000♠152,853.5047, which has ten decimal digits of precision, is represented as the significand 7009152853504700000♠1,528,535,047 together with 5 as the exponent. To determine the actual value, a decimal point is placed after the first digit of the significand and the result is multiplied by 105 to give 7005152853504700000♠1.528535047×105, or 7005152853504700000♠152,853.5047. In storing such a number, the base (10) need not be stored, since it will be the same for the entire range of supported numbers, and can thus be inferred.To derive the value of the floating-point number, the significand is multiplied by the base raised to the power of the exponent, equivalent to shifting the radix point from its implied position by a number of places equal to the value of the exponent—to the right if the exponent is positive or to the left if the exponent is negative.Floating-point representation is similar in concept to scientific notation. Logically, a floating-point number consists of:In scientific notation, the given number is scaled by a power of 10, so that it lies within a certain range—typically between 1 and 10, with the radix point appearing immediately after the first digit. The scaling factor, as a power of ten, is then indicated separately at the end of the number. For example, the orbital period of Jupiter's moon Io is 7005152853504700000♠152,853.5047 seconds, a value that would be represented in standard-form scientific notation as 7005152853504700000♠1.528535047×105 seconds.There are several mechanisms by which strings of digits can represent numbers. In common mathematical notation, the digit string can be of any length, and the location of the radix point is indicated by placing an explicit "point" character (dot or comma) there. If the radix point is not specified, then the string implicitly represents an integer and the unstated radix point would be off the right-hand end of the string, next to the least significant digit. In fixed-point systems, a position in the string is specified for the radix point. So a fixed-point scheme might be to use a string of 8 decimal digits with the decimal point in the middle, whereby "00012345" would represent 0001.2345.A number representation specifies some way of encoding a number, usually as a string of digits.A floating-point unit (FPU, colloquially a math coprocessor) is a part of a computer system specially designed to carry out operations on floating-point numbers.The speed of floating-point operations, commonly measured in terms of FLOPS, is an important characteristic of a computer system, especially for applications that involve intensive mathematical calculations.Over the years, a variety of floating-point representations have been used in computers. However, since the 1990s, the most commonly encountered representation is that defined by the IEEE 754 Standard.A floating-point system can be used to represent, with a fixed number of digits, numbers of different orders of magnitude: e.g. the distance between galaxies or the diameter of an atomic nucleus can be expressed with the same unit of length. The result of this dynamic range is that the numbers that can be represented are not uniformly spaced; the difference between two consecutive representable numbers grows with the chosen scale.[1]The term floating point refers to the fact that a number's radix point (decimal point, or, more commonly in computers, binary point) can "float"; that is, it can be placed anywhere relative to the significant digits of the number. This position is indicated as the exponent component, and thus the floating-point representation can be thought of as a kind of scientific notation.where significand is an integer (i.e., in Z), base is an integer greater than or equal to two, and exponent is also an integer. For example:In computing, floating-point arithmetic is arithmetic using formulaic representation of real numbers as an approximation so as to support a trade-off between range and precision. For this reason, floating-point computation is often found in systems which include very small and very large real numbers, which require fast processing times. A number is, in general, represented approximately to a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen. A number that can be represented exactly is of the following form:
MATLAB
Several easter eggs exist in MATLAB.[109] These include hidden pictures,[110] and jokes. For example, typing in "spy" used to generate a picture of the spies from Spy vs Spy, but now displays an image of a dog. Typing in "why" randomly outputs a philosophical answer. Other commands include "penny", "toilet", "image", and "life". Not every Easter egg appears in every version of MATLAB.For a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.[95]The number (or release number) is the version reported by Concurrent License Manager program FLEXlm.GNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave), therefore, making GNU Octave a superset of the MATLAB language.MATLAB has a number of competitors.[46] Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, and SageMath, which are intended to be mostly compatible with the MATLAB language; the Julia programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua/Torch for Lua, SciRuby for Ruby, and Numeric.js for JavaScript.It has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.[44] The regulators dropped the investigation after the complainant withdrew their accusation and no evidence of wrongdoing was found.[45]Each toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.MATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in.[7][41] Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET[42] or Java[43] application building environment, future development will still be tied to the MATLAB language.Libraries also exist to import and export MathML.[40]As alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.[38][39]Libraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB,[32][33] and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox[34] which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB Interface),[35][36] (which should not be confused with the unrelated Java Metadata Interface that is also called JMI). Official MATLAB API for Java was added in 2016.[37]MATLAB can call functions and subroutines written in the programming languages C or Fortran.[27] A wrapper function is created allowing MATLAB data types to be passed and returned. The dynamically loadable object files created by compiling such functions are termed "MEX-files" (for MATLAB executable).[28][29] Since 2014 increasing two-way interfacing with Python is being added.[30][31]In MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.[26]A MATLAB program can produce three-dimensional graphics using the functions surf, plot3 or mesh.produces the following figure of the sine function:MATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE[24] (GUI development environment) for graphically designing GUIs.[25] It also has tightly integrated graph-plotting features. For example, the function plot can be used to produce a graph from two vectors x and y. The code:When put into a file named hello.m, this can be executed with the following commands:An example of a simple class is provided below.can alter any member of object only if object is an instance of a reference class.Method call behavior is different between value and reference classes. For example, a call to a methodMATLAB supports object-oriented programming including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.[22] However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has handle as a super-class (for reference classes) or not (for value classes).[23]MATLAB supports elements of lambda calculus by introducing function handles,[19] or function references, which are implemented either in .m files or anonymous[20]/nested functions.[21]When creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores. Functions are often case sensitive. [18]MATLAB has structure data types.[15] Since all variables in MATLAB are arrays, a more adequate name is "structure array", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names[16] (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.[17]Most MATLAB functions can accept matrices and will apply themselves to each element. For example, mod(2*J,n) will multiply every element in "J" by 2, and then reduce each element modulo "n". MATLAB does include standard "for" and "while" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function magic.m, creates a magic square M for odd values of n (MATLAB function meshgrid is used here to generate square matrices I and J containing 1:n).Transposing a vector or a matrix is done either by the function transpose or by adding prime to the matrix.A square identity matrix of size n can be generated using the function eye, and matrices of any size with zeros or ones can be generated with the functions zeros and ones, respectively.Sets of indices can be specified by expressions such as "2:4", which evaluates to [2, 3, 4]. For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:Matrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).Indexing is one-based,[14] which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.assigns to the variable named ari an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.the increment value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.defines a variable named array (or assigns a new value to an existing variable with the name array) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the initial value), increments with each step from the previous value by 2 (the increment value), and stops once it reaches (or to avoid exceeding) 9 (the terminator value).A simple array is defined using the colon syntax: initial:increment:terminator. For instance:Variables are defined using the assignment operator, =. MATLAB is a weakly typed programming language because types are implicitly converted.[12] It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,[13] and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:The MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.[11]MATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.[8]Cleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s.[8] He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC.[9] In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.[10]As of 2017, MATLAB has roughly 1 million users across industry and academia.[7] MATLAB users come from various backgrounds of engineering, science, and economics.Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment. A proprietary programming language developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, C#, Java, Fortran and Python.
Numerical stability
Yet another definition is used in numerical partial differential equations. An algorithm for solving a linear evolutionary partial differential equation is stable if the total variation of the numerical solution at a fixed time remains bounded as the step size goes to zero. The Lax equivalence theorem states that an algorithm converges if it is consistent and stable (in this sense). Stability is sometimes achieved by including numerical diffusion. Numerical diffusion is a mathematical term which ensures that roundoff and other errors in the calculation get spread out and do not add up to cause the calculation to "blow up". Von Neumann stability analysis is a commonly used procedure for the stability analysis of finite difference schemes as applied to linear partial differential equations. These results do not hold for nonlinear PDEs, where a general, consistent definition of stability is complicated by many properties absent in linear equations.In numerical ordinary differential equations, various concepts of numerical stability exist, for instance A-stability. They are related to some concept of stability in the dynamical systems sense, often Lyapunov stability. It is important to use a stable method when solving a stiff equation.The above definitions are particularly relevant in situations where truncation errors are not important. In other contexts, for instance when solving differential equations, a different definition of numerical stability is used.An algorithm is forward stable if its forward error divided by the condition number of the problem is small. This means that an algorithm is forward stable if it has a forward error of magnitude similar to some backward stable algorithm.The usual definition of numerical stability uses a more general concept, called mixed stability, which combines the forward error and the backward error. An algorithm is stable in this sense if it solves a nearby problem approximately, i.e., if there exists a Δx such that both Δx is small and f (x + Δx) − y* is small. Hence, a backward stable algorithm is always stable.The algorithm is said to be backward stable if the backward error is small for all inputs x. Of course, "small" is a relative term and its definition will depend on the context. Often, we want the error to be of the same order as, or perhaps only a few orders of magnitude bigger than, the unit round-off.instead of the absolute error Δx.In many cases, it is more natural to consider the relative errorConsider the problem to be solved by the numerical algorithm as a function f mapping the data x to the solution y. The result of the algorithm, say y*, will usually deviate from the "true" solution y. The main causes of error are round-off error and truncation error. The forward error of the algorithm is the difference between the result and the solution; in this case, Δy = y* − y. The backward error is the smallest Δx such that f (x + Δx) = y*; in other words, the backward error tells us what problem the algorithm actually solved. The forward and backward error are related by the condition number: the forward error is at most as big in magnitude as the condition number multiplied by the magnitude of the backward error.There are different ways to formalize the concept of stability. The following definitions of forward, backward, and mixed stability are often used in numerical linear algebra.An opposite phenomenon is instability. Typically, an algorithm involves an approximate method, and in some cases one could prove that the algorithm would approach the right solution in some limit. Even in this case, there is no guarantee that it would converge to the correct solution, because the floating-point round-off or truncation errors can be magnified, instead of damped, causing the deviation from the exact solution to grow exponentially.[1]Some numerical algorithms may damp out the small fluctuations (errors) in the input data; others might magnify such errors. Calculations that can be proven not to magnify approximation errors are called numerically stable. One of the common tasks of numerical analysis is to try to select algorithms which are robust – that is to say, do not produce a wildly different result for very small change in the input data.In numerical linear algebra the principal concern is instabilities caused by proximity to singularities of various kinds, such as very small or nearly colliding eigenvalues. On the other hand, in numerical algorithms for differential equations the concern is the growth of round-off errors and/or initially small fluctuations in initial data which might cause a large deviation of final answer from the exact solution[citation needed].In the mathematical subfield of numerical analysis, numerical stability is a generally desirable property of numerical algorithms. The precise definition of stability depends on the context. One is numerical linear algebra and the other is algorithms for solving ordinary and partial differential equations by discrete approximation.
Basic Linear Algebra Subprograms
Several extensions to BLAS for handling sparse matrices have been suggested over the course of the library's history; a small set of sparse matrix kernel routines were finally standardized in 2002.[55]Due to the ubiquity of matrix multiplications in many scientific applications, including for the implementation of the rest of Level 3 BLAS,[20] and because faster algorithms exist beyond the obvious repetition of matrix-vector multiplication, gemm is a prime target of optimization for BLAS implementers. E.g., by decomposing one or both of A, B into block matrices, gemm can be implemented recursively. This is one of the motivations for including the β parameter,[dubious – discuss] so the results of previous blocks can be accumulated. Note that this decomposition requires the special case β = 1 which many implementations optimize for, thereby eliminating one multiplication for each value of C. This decomposition allows for better locality of reference both in space and time of the data used in the product. This, in turn, takes advantage of the cache on the system.[21] For systems with more than one level of cache, the blocking can be applied a second time to the order in which the blocks are used in the computation. Both of these levels of optimization are used in implementations such as ATLAS. More recently, implementations by Kazushige Goto have shown that blocking only for the L2 cache, combined with careful amortizing of copying to contiguous memory to reduce TLB misses, is superior to ATLAS.[22] A highly tuned implementation based on these ideas is part of the GotoBLAS and OpenBLAS.where T is a triangular matrix, among other functionality.Also included in Level 3 are routines for solvingwhere A and B can optionally be transposed or hermitian-conjugated inside the routine and all three matrices may be strided. The ordinary matrix multiplication A B can be performed by setting α to one and C to an all-zeros matrix of the appropriate size.This level, formally published in 1990,[18] contains matrix-matrix operations, including a "general matrix multiplication" (gemm), of the formwith T being triangular. Design of the Level 2 BLAS started in 1984, with results published in 1988.[19] The Level 2 subroutines are especially intended to improve performance of programs using BLAS on vector processors, where Level 1 BLAS are suboptimal "because they hide the matrix-vector nature of the operations from the compiler."[19]as well as a solver for x in the linear equationThis level contains matrix-vector operations including, among other things, a generalized matrix-vector multiplication (gemv):(called "axpy") and several other operations.This level consists of all the routines described in the original presentation of BLAS (1979),[1] which defined only vector operations on strided arrays: dot products, vector norms, a generalized vector addition of the formBLAS functionality is categorized into three sets of routines called "levels", which correspond to both the chronological order of definition and publication, as well as the degree of the polynomial in the complexities of algorithms; Level 1 BLAS operations typically take linear time, O(n), Level 2 operations quadratic time and Level 3 operations cubic time.[18] Modern BLAS implementations typically provide all three levels.Automatically Tuned Linear Algebra Software (ATLAS) attempts to make a BLAS implementation with higher performance. ATLAS defines many BLAS operations in terms of some core routines and then tries to automatically tailor the core routines to have good performance. A search is performed to choose good block sizes. The block sizes may depend on the computer's cache size and architecture. Tests are also made to see if copying arrays and vectors improves performance. For example, it may be advantageous to copy arguments so that they are cache-line aligned so user-supplied routines can use SIMD instructions.The original BLAS concerned only densely stored vectors and matrices. Further extensions to BLAS, such as for sparse matrices, have been addressed.[17]Other machine features became available and could also be exploited. Consequently, BLAS was augmented from 1984 to 1986 with level-2 kernel operations that concerned vector-matrix operations. Memory hierarchy was also recognized as something to exploit. Many computers have cache memory that is much faster than main memory; keeping matrix manipulations localized allows better usage of the cache. In 1987 and 1988, the level 3 BLAS were identified to do matrix-matrix operations. The level 3 BLAS encouraged block-partitioned algorithms. The LAPACK library uses level 3 BLAS.[16]The BLAS abstraction allows customization for high performance. For example, LINPACK is a general purpose library that can be used on many different machines without modification. LINPACK could use a generic version of BLAS. To gain performance, different machines might use tailored versions of BLAS. As computer architectures became more sophisticated, vector machines appeared. BLAS for a vector machine could use the machine's fast vector operations. (While vector processors eventually fell out of favor, vector instructions in modern CPUs are essential for optimal performance in BLAS routines.)Initially, these subroutines used hard-coded loops for their low-level operations. For example, if a subroutine need to perform a matrix multiplication, then the subroutine would have three nested loops. Linear algebra programs have many common low-level operations (the so-called "kernel" operations, not related to operating systems).[13] Between 1973 and 1977, several of these kernel operations were identified.[14] These kernel operations became defined subroutines that math libraries could call. The kernel calls had advantages over hard-coded loops: the library routine would be more readable, there were fewer chances for bugs, and the kernel implementation could be optimized for speed. A specification for these kernel operations using scalars and vectors, the level-1 Basic Linear Algebra Subroutines (BLAS), was published in 1979.[15] BLAS was used to implement the linear algebra subroutine library LINPACK.With the advent of numerical programming, sophisticated subroutine libraries became useful. These libraries would contain subroutines for common high-level mathematical operations such as root finding, matrix inversion, and solving systems of equations. The language of choice was FORTRAN. The most prominent numerical programming library was IBM's Scientific Subroutine Package (SSP).[12] These subroutine libraries allowed programmers to concentrate on their specific problems and avoid re-implementing well-known algorithms. The library routines would also be better than average implementations; matrix algorithms, for example, might use full pivoting to get better numerical accuracy. The library routines would also have more efficient routines. For example, a library may include a program to solve a matrix that is upper triangular. The libraries would include single-precision and double-precision versions of some algorithms.Many numerical software applications use BLAS-compatible libraries to do linear algebra computations, including Armadillo, LAPACK, LINPACK, GNU Octave, Mathematica,[9] MATLAB,[10] NumPy,[11] R, and Julia.Most libraries that offer linear algebra routines conform to the BLAS interface, allowing library users to develop programs that are agnostic of the BLAS library being used. Examples of BLAS libraries include: AMD Core Math Library (ACML), ATLAS, Intel Math Kernel Library (MKL), and OpenBLAS. ACML is no longer supported by its producer.[5] ATLAS is a portable library that automatically optimizes itself for an arbitrary architecture. MKL is a freeware[6] and proprietary[7] vendor library optimized for x86 and x86-64 with a performance emphasis on Intel processors.[8] OpenBLAS is an open-source library that is hand-optimized for many of the popular architectures. The LINPACK benchmarks rely heavily on the BLAS routine gemm for its performance measurements.It originated as a Fortran library in 1979[1] and its interface was standardized by the BLAS Technical (BLAST) Forum, whose latest BLAS report can be found on the netlib website.[2] This Fortran library is known as the reference implementation (sometimes confusingly referred to as the BLAS library) and is not optimized for speed but is in the public domain.[3][4]Basic Linear Algebra Subprograms (BLAS) is a specification that prescribes a set of low-level routines for performing common linear algebra operations such as vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication. They are the de facto standard low-level routines for linear algebra libraries; the routines have bindings for both C and Fortran. Although the BLAS specification is general, BLAS implementations are often optimized for speed on a particular machine, so using them can bring substantial performance benefits. BLAS implementations will take advantage of special floating point hardware such as vector registers or SIMD instructions.
Sparse matrix
The term sparse matrix was possibly coined by Harry Markowitz who triggered some pioneering work but then left the field.[7]Several software libraries support sparse matrices, and provide solvers for sparse matrix equations. The following are open-source:Both iterative and direct methods exist for sparse matrix solving.There are other methods than the Cholesky decomposition in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the "false non-zeros" can be different for different methods. And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in.The fill-in of a matrix are those entries that change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm, it is useful to minimize the fill-in by switching rows and columns in the matrix. The symbolic Cholesky decomposition can be used to calculate the worst possible fill-in before doing the actual Cholesky decomposition.A symmetric sparse matrix arises as the adjacency matrix of an undirected graph; it can be stored efficiently as an adjacency list.A very efficient structure for an extreme case of band matrices, the diagonal matrix, is to store just the entries in the main diagonal as a one-dimensional array, so a diagonal n × n matrix requires only n entries.By rearranging the rows and columns of a matrix A it may be possible to obtain a matrix A′ with a lower bandwidth. A number of algorithms are designed for bandwidth minimization.Matrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices.An important special type of sparse matrices is band matrix, defined as follows. The lower bandwidth of a matrix A is the smallest number p such that the entry ai,j vanishes whenever i > j + p. Similarly, the upper bandwidth is the smallest number p such that ai,j = 0 whenever i < j − p (Golub & Van Loan 1996, §1.2.1). For example, a tridiagonal matrix has lower bandwidth 1 and upper bandwidth 1. As another example, the following sparse matrix has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity.CSC is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. For example, CSC is (val, row_ind, col_ptr), where val is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; row_ind is the row indices corresponding to the values; and, col_ptr is the list of val indexes where each column starts. The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products. See scipy.sparse.csc_matrix. This is the traditional format for specifying a sparse matrix in MATLAB (via the sparse function).The (old and new) Yale sparse matrix formats are instances of the CSR scheme. The old Yale format works exactly as described above, with three arrays; the new format achieves a further compression by combining IA and JA into a single array.[6]Note that in this format, the first value of IA is always zero and the last is always NNZ, so they are in some sense redundant (although in programming languages where the array length needs to be explicitly stored, NNZ would not be redundant). Nonetheless, this does avoid the need to handle an exceptional case when computing the length of each row, as it guarantees the formula IA[i + 1] − IA[i] works for any row i. Moreover, the memory cost of this redundant storage is likely insignificant for a sufficiently large matrix.The whole is stored as 21 entries.is a 4 × 6 matrix (24 entries) with 8 nonzero elements, soIn this case the CSR representation contains 13 entries, compared to 16 in the original matrix. The CSR format saves on memory only when NNZ < (m (n − 1) − 1) / 2. Another example, the matrixSo, in array JA, the element "5" from A has column index 0, "8" and "6" have index 1, and element "3" has index 2.is a 4 × 4 matrix with 4 nonzero elements, henceFor example, the matrixThe CSR format stores a sparse m × n matrix M in row form using three (one-dimensional) arrays (A, IA, JA). Let NNZ denote the number of nonzero entries in M. (Note that zero-based indices shall be used here.)The compressed sparse row (CSR) or compressed row storage (CRS) format represents a matrix M by three (one-dimensional) arrays, that respectively contain nonzero values, the extents of rows, and column indices. It is similar to COO, but compresses the row indices, hence the name. This format allows fast row access and matrix-vector multiplications (Mx). The CSR format has been in use since at least the mid-1960s, with the first complete description appearing in 1967.[4]COO stores a list of (row, column, value) tuples. Ideally, the entries are sorted first by row index and then by column index, to improve random access times. This is another format that is good for incremental matrix construction.[3]LIL stores one list per row, with each entry containing the column index and the value. Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.[2]DOK consists of a dictionary that maps (row, column)-pairs to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.[1]Formats can be divided into two groups:In the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The trade-off is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element ai,j of the matrix and is accessed by the two indices i and j. Conventionally, i is the row index, numbered from top to bottom, and j is the column index, numbered from left to right. For an m × n matrix, the amount of memory required to store the matrix in this format is proportional to m × n (disregarding the fact that the dimensions of the matrix also need to be stored).When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeroes. Sparse data is by nature more easily compressed and thus requires significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations.Conceptually, sparsity corresponds to systems that are loosely coupled. Consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls had springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory, which have a low density of significant data or connections.In numerical analysis and computer science, a sparse matrix or sparse array is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is called the sparsity of the matrix (which is equal to 1 minus the density of the matrix).
Comparison of linear algebra libraries

Comparison of numerical analysis software
The operating systems the software can run on natively (without emulation).Colors indicate features available asThe operating systems the software can run on natively (without emulation).The following tables provide a comparison of numerical analysis software.
Category:Linear algebra

List of linear algebra topics

Portal:Algebra

Template:Areas of mathematics

Template talk:Areas of mathematics

Areas of mathematics
Geometry deals with spatial relationships, using fundamental qualities or axioms. Such axioms can be used in conjunction with mathematical definitions for points, straight lines, curves, surfaces, and solids to draw logical conclusions. See also List of geometry topicsCombinatorics is the study of finite or discrete collections of objects that satisfy specified criteria. In particular, it is concerned with "counting" the objects in those collections (enumerative combinatorics) and with deciding whether certain "optimal" objects exist (extremal combinatorics). It includes graph theory, used to describe inter-connected objects (a graph in this sense is a network, or collection of connected points). See also the list of combinatorics topics, list of graph theory topics and glossary of graph theory. A combinatorial flavour is present in many parts of problem-solving.Modern analysis is a vast and rapidly expanding branch of mathematics that touches almost every other subdivision of the discipline, finding direct and indirect applications in topics as diverse as number theory, cryptography, and abstract algebra. It is also the language of science itself and is used across chemistry, biology, and physics, from astrophysics to X-ray crystallography.Within the world of mathematics, analysis is the branch that focuses on change: rates of change, accumulated change, and multiple things changing relative to (or independently of) one another.The study of structure begins with numbers, first the familiar natural numbers and integers and their arithmetical operations, which are recorded in elementary algebra. The deeper properties of these numbers are studied in number theory. The investigation of methods to solve equations leads to the field of abstract algebra, which, among other things, studies rings and fields, structures that generalize the properties possessed by everyday numbers. Long standing questions about compass and straightedge construction were finally settled by Galois theory. The physically important concept of vectors, generalized to vector spaces, is studied in linear algebra.Arithmetic is the study of numbers and the properties of operations between them.An ideal system of classification permits adding new areas into the organization of previous knowledge, and fitting surprising discoveries and unexpected interactions into the outline. For example, the Langlands program has found unexpected connections between areas previously thought unconnected, at least Galois groups, Riemann surfaces and number theory.A traditional division of mathematics is into pure mathematics, mathematics studied for its intrinsic interest, and applied mathematics, mathematics which can be directly applied to real world problems.[1] This division is not always clear and many subjects have been developed as pure mathematics to find unexpected applications later on. Broad divisions, such as discrete mathematics and computational mathematics, have emerged more recently.Mathematics encompasses a growing variety and depth of subjects over history, and comprehension requires a system to categorize and organize the many subjects into more general areas of mathematics. A number of different classification schemes have arisen, and though they share some similarities, there are differences due in part to the different purposes they serve. In addition, as mathematics continues to be developed, these classification schemes must change as well to account for newly created areas or newly discovered links between different areas. Classification is made more difficult by some subjects, often the most active, which straddle the boundary between different areas.
Mathematics
A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. A solution to each of these problems carries a $1 million reward, and only one (the Riemann hypothesis) is duplicated in Hilbert's problems.The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.Arguably the most prestigious award in mathematics is the Fields Medal,[58][59] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[56] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[57]Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments;[55] the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.[c]In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.The study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "P = NP?" problem, one of the Millennium Prize Problems.[54] Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science,[citation needed] as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.[53] Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.Axioms in traditional thought were "self-evident truths", but that conception is problematic.[51] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[52]Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject.[b] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.[50]Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".Most of the mathematical notation in use today was not invented until the 16th century.[45] Before that, mathematics was written out in words, limiting mathematical discovery.[46] Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.[47] Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.[48] Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.[49]For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[42] Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erdős.[43][44] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics".[40] As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages.[41] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[39]The opinions of mathematicians on this matter are varied. Many mathematicians[38] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others[who?] feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.[citation needed]An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper.[34] However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians[who?] that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."[35] Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.[36][37]The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences".[12] More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery".[33] In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to natural science follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[15]Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems".[32] A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.Intuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other."[29] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870).[30] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).[31]Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[29] All have severe problems, none has widespread acceptance, and no reconciliation seems possible.[29]Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century.[27] Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[28] Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals.[6] There is not even consensus on whether mathematics is an art or a science.[7] A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[6] Some just say, "Mathematics is what mathematicians do."[6]The apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural τα μαθηματικά (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.[25] In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in English-speaking North America, math.[26]In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.[24]Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "teachers" rather than "mathematicians" in the modern sense.The word mathematics comes from Ancient Greek μάθημα (máthēma), meaning "that which is learnt",[22] "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times.[23] Its adjective is μαθηματικός (mathēmatikós), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, μαθηματικὴ τέχνη (mathēmatikḗ tékhnē), Latin: ars mathematica, meant "the mathematical art".Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[21]During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics: most of them include the contributions from Persian mathematicians such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.Between 600 and 300 BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics.[20]In Babylonian mathematics, elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus.[citation needed]Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[19] The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time.As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.[18]The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals,[17] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.[16]Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth."[11] Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences".[12] Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions".[13] David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise."[14] Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[15]Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[10]Mathematicians seek out patterns[8][9] and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.Mathematics (from Greek μάθημα máthēma, "knowledge, study, learning") is the study of such topics as quantity,[1] structure,[2] space,[1] and change.[3][4][5] It has no generally accepted definition.[6][7]
