{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "# >>>f = io.open(\"test\", mode=\"r\", encoding=\"utf-8\")\n",
    "# Then after calling f's read() function, an encoded Unicode object is returned.\n",
    "\n",
    "# >>>f.read()\n",
    "# u'Capit\\xe1l\\n\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 titles\n",
      "19 contents\n",
      "['Elementary algebra', 'Euclidean space', 'Mathematics', 'Linear equation', 'Linear map', 'Matrix (mathematics)', 'Vector space', 'Geometry', 'Line (geometry)', 'Plane (geometry)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b\\'Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers,[1] algebra introduces quantities without fixed values, known as variables.[2] This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.\\'b\\'The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.\\'b\\'\\'b\\'\\'b\\'Elementary algebra builds on and extends arithmetic[20] by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.\\'b\\'Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,\\'b\\'If x and y are integers, rationals, or real numbers, then xy=0 implies x=0 or y=0. Suppose abc=0. Then, substituting a for x and bc for y, we learn a=0 or bc=0. Then we can substitute again, letting x=b and y=c, to show that if bc=0 then b=0 or c=0. Therefore, if abc=0, then a=0 or (b=0 or c=0), so abc=0 implies a=0 or b=0 or c=0.\\'b\\'Consider if the original fact were stated as \"ab=0 implies a=0 or b=0.\" Then when we say \"suppose abc=0,\" we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if abc=0 then a=0 or b=0 or c=0 if instead of letting a=a and b=bc we substitute a for a and b for bc (and with bc=0, substituting b for a and c for b). This shows that substituting for the terms in a statement isn\\\\\\'t always the same as letting the terms from the statement equal the substituted terms. In this situation it\\\\\\'s clear that if we substitute an expression a into the a term of the original equation, the a substituted does not refer to the a in the statement \"ab=0 implies a=0 or b=0.\"\\'b\\'The following sections lay out examples of some of the types of algebraic equations that may be encountered.\\'b\\'Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:\\'b\\'To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable.[32] This problem and its solution are as follows:\\'b\\'In words: the child is 4 years old.\\'b\\'A linear equation with two variables has many (i.e. an infinite number of) solutions.[33] For example:\\'b\"This cannot be worked out by itself. If the son\\'s age was made known, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.\"b\\'To solve a linear equation with two variables (unknowns), requires two related equations. For example, if it was also revealed that:\\'b\\'Now there are two related linear equations, each with two unknowns, which enables the production of a linear equation with just one variable, by subtracting one from the other (called the elimination method):[34]\\'b\\'In other words, the son is aged 12, and since the father 22 years older, he must be 34. In 10 years time, the son will be 22, and the father will be twice his age, 44. This problem is illustrated on the associated plot of the equations.\\'b\\'For other ways to solve this kind of equations, see below, System of linear equations.\\'b\\'where the symbol \"\\\\xc2\\\\xb1\" indicates that both\\'b\\'are solutions of the quadratic equation.\\'b\\'Quadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:\\'b\\'which is the same thing as\\'b\\'has no real number solution since no real number squared equals \\\\xe2\\\\x88\\\\x921. Sometimes a quadratic equation has a root of multiplicity 2, such as:\\'b\\'For this equation, \\\\xe2\\\\x88\\\\x921 is a root of multiplicity 2. This means \\\\xe2\\\\x88\\\\x921 appears two times, since the equation can be rewritten in factored form as\\'b\\'All quadratic equations have exactly two solutions in complex numbers (but they may be equal to each other), a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equation\\'b\\'has solutions\\'b\\'then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain\\'b\\'whence\\'b\\'or\\'b\\'For example, if\\'b\\'then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get\\'b\\'whence\\'b\\'from which we obtain\\'b\\'equivalently\\'b\\'equivalently\\'b\\'For example, if:\\'b\\'then\\'b\\'There are different methods to solve a system of linear equations with two variables.\\'b\\'An example of solving a system of linear equations is by using the elimination method:\\'b\\'Multiplying the terms in the second equation by 2:\\'b\\'Adding the two equations together to get:\\'b\\'which simplifies to\\'b\\'Another way of solving the same system of linear equations is by substitution.\\'b\\'and multiplying by \\\\xe2\\\\x88\\\\x921:\\'b\\'Adding 2 on each side of the equation:\\'b\\'which simplifies to\\'b\\'Using this value in one of the equations, the same solution as in the previous method is obtained.\\'b\\'In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is\\'b\\'As 0\\\\xe2\\\\x89\\\\xa02, the second equation in the system has no solution. Therefore, the system has no solution. However, not all inconsistent systems are recognized at first sight. As an example, let us consider the system\\'b\\'Multiplying by 2 both sides of the second equation, and adding it to the first one results in\\'b\\'which has clearly no solution.\\'b\\'And using this value in the first equation in the system:\\'b\\'Systems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system is\\'b\\'When trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express all solutions numerically because there are an infinite number of them if there are any.\\'b\\'A system with a greater number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.\\'',\n",
       " 'b\\'In geometry, Euclidean space encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces. It is named after the Ancient Greek mathematician Euclid of Alexandria.[1] The term \"Euclidean\" distinguishes these spaces from other types of spaces considered in modern geometry. Euclidean spaces also generalize to higher dimensions.\\'b\\'Classical Greek geometry defined the Euclidean plane and Euclidean three-dimensional space using certain postulates, while the other properties of these spaces were deduced as theorems. Geometric constructions are also used to define rational numbers. When algebra and mathematical analysis became developed enough, this relation reversed and now it is more common to define Euclidean space using Cartesian coordinates and the ideas of analytic geometry. It means that points of the space are specified with collections of real numbers, and geometric shapes are defined as equations and inequalities. This approach brings the tools of algebra and calculus to bear on questions of geometry and has the advantage that it generalizes easily to Euclidean spaces of more than three dimensions.\\'b\\'From the modern viewpoint, there is essentially only one Euclidean space of each dimension. While Euclidean space is defined by a set of axioms, these axioms do not specify how the points are to be represented.[2] Euclidean space can, as one possible choice of representation, be modeled using Cartesian coordinates. In this case, the Euclidean space is then modeled by the real coordinate space (Rn) of the same dimension. In one dimension, this is the real line; in two dimensions, it is the Cartesian plane; and in higher dimensions it is a coordinate space with three or more real number coordinates. Mathematicians denote the n-dimensional Euclidean space by En if they wish to emphasize its Euclidean nature, but Rn is used as well since the latter is assumed to have the standard Euclidean structure, and these two structures are not always distinguished. Euclidean spaces have finite dimension.[3]\\'b\\'\\'b\\'\\'b\\'One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation about a fixed point in the plane, in which every point in the plane turns about that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).\\'b\\'In order to make all of this mathematically precise, the theory must clearly define the notions of distance, angle, translation, and rotation for a mathematically described space. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a \"mathematical\" space is a number, not something expressed in inches or metres. The standard way to define such space, as carried out in the remainder of this article, is to define the Euclidean plane as a two-dimensional real vector space equipped with an inner product.[3] The reason for working with arbitrary vector spaces instead of Rn is that it is often preferable to work in a coordinate-free manner (that is, without choosing a preferred basis). For then:\\'b\\'Once the Euclidean plane has been described in this language, it is actually a simple matter to extend its concept to arbitrary dimensions. For the most part, the vocabulary, formulae, and calculations are not made any more difficult by the presence of more dimensions. (However, rotations are more subtle in high dimensions, and visualizing high-dimensional spaces remains difficult, even for experienced mathematicians.)\\'b\\'A Euclidean space is not technically a vector space but rather an affine space, on which a vector space acts by translations, or, conversely, a Euclidean vector is the difference (displacement) in an ordered pair of points, not a single point. Intuitively, the distinction says merely that there is no canonical choice of where the origin should go in the space, because it can be translated anywhere. When a certain point is chosen, it can be declared the origin and subsequent calculations may ignore the difference between a point and its coordinate vector, as said above. See point\\\\xe2\\\\x80\\\\x93vector distinction for details.\\'b\\'These are distances between points and the angles between lines or vectors, which satisfy certain conditions (see below), which makes a set of points a Euclidean space. The natural way to obtain these quantities is by introducing and using the standard inner product (also known as the dot product) on Rn.[3] The inner product of any two real n-vectors x and y is defined by\\'b\\'where xi and yi are ith coordinates of vectors x and y respectively. The result is always a real number.\\'b\\'The inner product of x with itself is always non-negative. This product allows us to define the \"length\" of a vector x through square root:\\'b\\'This length function satisfies the required properties of a norm and is called the Euclidean norm on Rn.\\'b\\'Finally, one can use the norm to define a metric (or distance function) on Rn by\\'b\\'This distance function is called the Euclidean metric. This formula expresses a special case of the Pythagorean theorem.\\'b\\'This distance function (which makes a metric space) is sufficient to define all Euclidean geometry, including the dot product. Thus, a real coordinate space together with this Euclidean structure is called Euclidean space. Its vectors form an inner product space (in fact a Hilbert space), and a normed vector space.\\'b\\'The metric space structure is the main reason behind the use of real numbers R, not some other ordered field, as the mathematical foundation of Euclidean (and many other) spaces. Euclidean space is a complete metric space, a property which is impossible to achieve operating over rational numbers, for example.\\'b\\'The (non-reflex) angle \\\\xce\\\\xb8 (0\\\\xc2\\\\xb0 \\\\xe2\\\\x89\\\\xa4 \\\\xce\\\\xb8 \\\\xe2\\\\x89\\\\xa4 180\\\\xc2\\\\xb0) between vectors x and y is then given by\\'b\\'where arccos is the arccosine function. It is useful only for n > 1,[footnote 1] and the case n = 2 is somewhat special. Namely, on an oriented Euclidean plane one can define an angle between two vectors as a number defined modulo 1 turn (usually denoted as either 2\\\\xcf\\\\x80 or 360\\\\xc2\\\\xb0), such that \\\\xe2\\\\x88\\\\xa0y\\\\xe2\\\\x80\\\\x89x = \\\\xe2\\\\x88\\\\x92\\\\xe2\\\\x88\\\\xa0x\\\\xe2\\\\x80\\\\x89y. This oriented angle is equal either to the angle \\\\xce\\\\xb8 from the formula above or to \\\\xe2\\\\x88\\\\x92\\\\xce\\\\xb8. If one non-zero vector is fixed (such as the first basis vector), then each non-zero vector is uniquely defined by its magnitude and angle.\\'b\\'The angle does not change if vectors x and y are multiplied by positive numbers.\\'b\\'Unlike the aforementioned situation with distance, the scale of angles is the same in pure mathematics, physics, and computing. It does not depend on the scale of distances; all distances may be multiplied by some fixed factor, and all angles will be preserved. Usually, the angle is considered a dimensionless quantity, but there are different units of measurement, such as radian (preferred in pure mathematics and theoretical physics) and degree (\\\\xc2\\\\xb0) (preferred in most applications).\\'b\\'Symmetries of a Euclidean space are transformations which preserve the Euclidean metric (called isometries). Although aforementioned translations are most obvious of them, they have the same structure for any affine space and do not show a distinctive character of Euclidean geometry. Another family of symmetries leave one point fixed, which may be seen as the origin without loss of generality. All transformations, which preserves the origin and the Euclidean metric, are linear maps. Such transformations Q must, for any x and y, satisfy:\\'b\\'Such transforms constitute a group called the orthogonal group O(n). Its elements Q are exactly solutions of a matrix equation\\'b\\'where QT is the transpose of Q and I is the identity matrix.\\'b\\'But a Euclidean space is orientable.[footnote 2] Each of these transformations either preserves or reverses orientation depending on whether its determinant is +1 or \\\\xe2\\\\x88\\\\x921 respectively. Only transformations which preserve orientation, which form the special orthogonal group SO(n), are considered (proper) rotations. This group has, as a Lie group, the same dimension n(n \\\\xe2\\\\x88\\\\x92 1) /2 and is the identity component of O(n).\\'b\\'Groups SO(n) are well-studied for n \\\\xe2\\\\x89\\\\xa4 4. There are no non-trivial rotations in 0- and 1-spaces. Rotations of a Euclidean plane (n = 2) are parametrized by the angle (modulo 1 turn). Rotations of a 3-space are parametrized with axis and angle, whereas a rotation of a 4-space is a superposition of two 2-dimensional rotations around perpendicular planes.\\'b\\'Among linear transforms in O(n) which reverse the orientation are hyperplane reflections. This is the only possible case for n \\\\xe2\\\\x89\\\\xa4 2, but starting from three dimensions, such isometry in the general position is a rotoreflection.\\'b\\'The Euclidean group E(n), also referred to as the group of all isometries ISO(n), treats translations, rotations, and reflections in a uniform way, considering them as group actions in the context of group theory, and especially in Lie group theory. These group actions preserve the Euclidean structure.\\'b\\'As the group of all isometries, ISO(n), the Euclidean group is important because it makes Euclidean geometry a case of Klein geometry, a theoretical framework including many alternative geometries.\\'b\\'The structure of Euclidean spaces \\\\xe2\\\\x80\\\\x93 distances, lines, vectors, angles (up to sign), and so on \\\\xe2\\\\x80\\\\x93 is invariant under the transformations of their associated Euclidean group. For instance, translations form a commutative subgroup that acts freely and transitively on En, while the stabilizer of any point there is the aforementioned O(n).\\'b\\'Along with translations, rotations, reflections, as well as the identity transformation, Euclidean motions comprise also glide reflections (for n \\\\xe2\\\\x89\\\\xa5 2), screw operations and rotoreflections (for n \\\\xe2\\\\x89\\\\xa5 3), and even more complex combinations of primitive transformations for n \\\\xe2\\\\x89\\\\xa5 4.\\'b\\'The group structure determines which conditions a metric space needs to satisfy to be a Euclidean space:\\'b\\'Cartesian coordinates are arguably the standard, but not the only possible option for a Euclidean space. Skew coordinates are compatible with the affine structure of En, but make formulae for angles and distances more complicated.\\'b\\'Another approach, which goes in line with ideas of differential geometry and conformal geometry, is orthogonal coordinates, where coordinate hypersurfaces of different coordinates are orthogonal, although curved. Examples include the polar coordinate system on Euclidean plane, the second important plane coordinate system.\\'b\\'See below about expression of the Euclidean structure in curvilinear coordinates.\\'b\\'The simplest (after points) objects in Euclidean space are flats, or Euclidean subspaces of lesser dimension. Points are 0-dimensional flats, 1-dimensional flats are called (straight) lines, and 2-dimensional flats are planes. (n \\\\xe2\\\\x88\\\\x92 1)-dimensional flats are called hyperplanes.\\'b\\'Any two distinct points lie on exactly one line. Any line and a point outside it lie on exactly one plane. More generally, the properties of flats and their incidence of Euclidean space are shared with affine geometry, whereas the affine geometry is devoid of distances and angles.\\'b\\'The sum of angles of a triangle is an important problem, which exerted a great influence to 19th-century mathematics. In a Euclidean space it invariably equals to 180\\\\xc2\\\\xb0, or a half-turn\\'b\\'This is not only a line which a pair (A,\\\\xe2\\\\x80\\\\x89B) of distinct points defines. Points on the line which lie between A and B, together with A and B themselves, constitute a line segment A\\\\xe2\\\\x80\\\\x89B. Any line segment has the length, which equals to distance between A and B. If A = B, then the segment is degenerate and its length equals to 0, otherwise the length is positive.\\'b\\'A (non-degenerate) triangle is defined by three points not lying on the same line. Any triangle lies on one plane. The concept of triangle is not specific to Euclidean spaces, but Euclidean triangles have numerous special properties and define many derived objects.\\'b\\'A triangle can be thought of as a 3-gon on a plane, a special (and the first meaningful in Euclidean geometry) case of a polygon.\\'b\\'Polytope is a concept that generalizes polygons on a plane and polyhedra in 3-dimensional space (which are among the earliest studied geometrical objects). A simplex is a generalization of a line segment (1-simplex) and a triangle (2-simplex). A tetrahedron is a 3-simplex.\\'b\\'The concept of a polytope belongs to affine geometry, which is more general than Euclidean. But Euclidean geometry distinguish regular polytopes. For example, affine geometry does not see the difference between an equilateral triangle and a right triangle, but in Euclidean space the former is regular and the latter is not.\\'b\\'Root systems are special sets of Euclidean vectors. A root system is often identical to the set of vertices of a regular polytope.\\'b\\'Since Euclidean space is a metric space, it is also a topological space with the natural topology induced by the metric. The metric topology on En is called the Euclidean topology, and it is identical to the standard topology on Rn. A set is open if and only if it contains an open ball around each of its points; in other words, open balls form a base of the topology. The topological dimension of the Euclidean n-space equals n, which implies that spaces of different dimension are not homeomorphic. A finer result is the invariance of domain, which proves that any subset of n-space, that is (with its subspace topology) homeomorphic to an open subset of n-space, is itself open.\\'b\\'Aside from countless uses in fundamental mathematics, a Euclidean model of the physical space can be used to solve many practical problems with sufficient precision. Two usual approaches are a fixed, or stationary reference frame (i.e. the description of a motion of objects as their positions that change continuously with time), and the use of Galilean space-time symmetry (such as in Newtonian mechanics). To both of them the modern Euclidean geometry provides a convenient formalism; for example, the space of Galilean velocities is itself a Euclidean space (see relative velocity for details).\\'b\\'Topographical maps and technical drawings are planar Euclidean. An idea behind them is the scale invariance of Euclidean geometry, that permits to represent large objects in a small sheet of paper, or a screen.\\'b\\'Although Euclidean spaces are no longer considered to be the only possible setting for a geometry, they act as prototypes for other geometric objects. Ideas and terminology from Euclidean geometry (both traditional and analytic) are pervasive in modern mathematics, where other geometric objects share many similarities with Euclidean spaces, share part of their structure, or embed Euclidean spaces.\\'b\"A smooth manifold is a Hausdorff topological space that is locally diffeomorphic to Euclidean space. Diffeomorphism does not respect distance and angle, but if one additionally prescribes a smoothly varying inner product on the manifold\\'s tangent spaces, then the result is what is called a Riemannian manifold. Put differently, a Riemannian manifold is a space constructed by deforming and patching together Euclidean spaces. Such a space enjoys notions of distance and angle, but they behave in a curved, non-Euclidean manner. The simplest Riemannian manifold, consisting of Rn with a constant inner product, is essentially identical to Euclidean n-space itself. Less trivial examples are n-sphere and hyperbolic spaces. Discovery of the latter in the 19th century was branded as the non-Euclidean geometry.\"b\\'Also, the concept of a Riemannian manifold permits an expression of the Euclidean structure in any smooth coordinate system, via metric tensor. From this tensor one can compute the Riemann curvature tensor. Where the latter equals to zero, the metric structure is locally Euclidean (it means that at least some open set in the coordinate space is isometric to a piece of Euclidean space), no matter whether coordinates are affine or curvilinear.\\'b\\'If one replaces the inner product of a Euclidean space with an indefinite quadratic form, the result is a pseudo-Euclidean space. Smooth manifolds built from such spaces are called pseudo-Riemannian manifolds. Perhaps their most famous application is the theory of relativity, where flat spacetime is a pseudo-Euclidean space called Minkowski space, where rotations correspond to motions of hyperbolic spaces mentioned above. Further generalization to curved spacetimes form pseudo-Riemannian manifolds, such as in general relativity.\\'b\\'Another line of generalization is to consider other number fields than one of real numbers. Over complex numbers, a Hilbert space can be seen as a generalization of Euclidean dot product structure, although the definition of the inner product becomes a sesquilinear form for compatibility with metric structure.\\'']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('page.txt','rb') as f:\n",
    "#     background = f.read()\n",
    "\n",
    "#Make sure to add the errors='ignore'! Otherwise some charcters cannot be converted and cause an error.\n",
    "f = io.open(\"linalg.txt\", mode=\"r\", encoding=\"utf-8\", errors='ignore')\n",
    "#After calling f's read() function, an encoded Unicode object is returned.\n",
    "background = f.read()\n",
    "\n",
    "subtopics = io.open(\"subtopics.txt\", mode=\"r\", encoding=\"utf-8\", errors='ignore')\n",
    "subtopics = subtopics.read().split('\\n')\n",
    "titles = [subtopics[i] for i in range(len(subtopics)) if i % 2 == 0]\n",
    "contents = [subtopics[i] for i in range(len(subtopics)) if i % 2 == 1]\n",
    "\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(contents)) + ' contents')\n",
    "print(titles[:10])\n",
    "contents[:2]\n",
    "# print(len(subtopics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Linear algebra\\nb'Linear algebra is the branch of mathematics concerning linear equations such as'b'linear functions such as'b'and their representations through matrices and vector spaces\",\n",
       " \"[1][2][3]'b'Linear algebra is central to almost all areas of mathematics\",\n",
       " ' For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations',\n",
       " ' Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions',\n",
       " ' Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models',\n",
       " ' For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation',\n",
       " '\\'b\\'\\'b\\'\\'b\"The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations',\n",
       " \" Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear system, now called Cramer's Rule\",\n",
       " ' Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy',\n",
       " '[4]\"b\\'The study of matrix algebra first emerged in England in the mid-1800s',\n",
       " ' In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra',\n",
       " ' In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\"',\n",
       " ' While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses',\n",
       " ' Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object',\n",
       " ' He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\"',\n",
       " '[4]\\'b\\'In 1882, H\\\\xc3\\\\xbcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\"',\n",
       " '[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged',\n",
       " ' Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra',\n",
       " ' The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics',\n",
       " ' The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations',\n",
       " \"[4]'b'The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination\",\n",
       " \"'b'Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s\",\n",
       " '[7] Following work by the School Mathematics Study Group, U',\n",
       " 'S',\n",
       " ' high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s',\n",
       " '[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school',\n",
       " ' This was met with a backlash in the 1980s that removed linear algebra from the curriculum',\n",
       " '[9] In 1993, the U',\n",
       " 'S',\n",
       " '-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation',\n",
       " \"[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as \\\\'so many other disciplines use it\\\\'\",\n",
       " '[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination',\n",
       " \"[13][14]'b'The main structures of linear algebra are vector spaces\",\n",
       " ' A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms',\n",
       " ' Elements of V are called vectors, and elements of F are called scalars',\n",
       " ' The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w',\n",
       " ' The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av',\n",
       " ' The operations of addition and multiplication in a vector space must satisfy the following axioms',\n",
       " '[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F',\n",
       " \"'b'The first four axioms are those of V being an abelian group under vector addition\",\n",
       " ' Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices',\n",
       " ' Linear algebra is concerned with properties common to all vector spaces',\n",
       " \"'b'Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure\",\n",
       " \" Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map'b'that is compatible with addition and scalar multiplication:'b'for any vectors u,v \\\\xe2\\\\x88\\\\x88 V and a scalar a \\\\xe2\\\\x88\\\\x88 F\",\n",
       " \"'b'Additionally for any vectors u, v \\\\xe2\\\\x88\\\\x88 V and scalars a, b \\\\xe2\\\\x88\\\\x88 F:'b'When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic\",\n",
       " ' Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view',\n",
       " ' One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero',\n",
       " ' If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping',\n",
       " \"'b'Linear transformations have geometric significance\",\n",
       " ' For example, 2 \\\\xc3\\\\x97 2 real matrices denote standard planar mappings that preserve the origin',\n",
       " \"'b'Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces\",\n",
       " ' For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces',\n",
       " ' Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " \", vk:'b'where a1, a2, \",\n",
       " '',\n",
       " '',\n",
       " ', ak are scalars',\n",
       " ' The set of all linear combinations of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk is called their span, which forms a subspace',\n",
       " \"'b'A linear combination of any system of vectors with all zero coefficients is the zero vector of V\",\n",
       " ' If this is the only way to express the zero vector as a linear combination of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk then these vectors are linearly independent',\n",
       " ' Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set',\n",
       " ' Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace',\n",
       " ' Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V',\n",
       " ' Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis',\n",
       " '[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible',\n",
       " ' For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed',\n",
       " \"'b'Any two bases of a vector space V have the same cardinality, which is called the dimension of V\",\n",
       " ' The dimension of a vector space is well-defined by the dimension theorem for vector spaces',\n",
       " ' If a basis of V has finite number of elements, V is called a finite-dimensional vector space',\n",
       " ' If V is finite-dimensional and U is a subspace of V, then dim U \\\\xe2\\\\x89\\\\xa4 dim V',\n",
       " \" If U1 and U2 are subspaces of V, then'b'One often restricts consideration to finite-dimensional vector spaces\",\n",
       " ' A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism',\n",
       " \"'b'A particular basis {v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ',\n",
       " '',\n",
       " '',\n",
       " \", an) is the linear combination'b'The condition that v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn assures that these coordinates are unique (i',\n",
       " 'e',\n",
       " ' there is only one linear combination of the basis vectors that is equal to v)',\n",
       " ' In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn',\n",
       " ' Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn',\n",
       " ' Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \\\\xe2\\\\x86\\\\x92 W may be encoded by an m \\\\xc3\\\\x97 n matrix A with entries in the field F, called the matrix of T with respect to these bases',\n",
       " ' Two matrices that encode the same linear transformation in different bases are called similar',\n",
       " ' Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects',\n",
       " ' This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely',\n",
       " \"'b'There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V\",\n",
       " ' While Rn has a standard basis {e1, e2, ',\n",
       " '',\n",
       " '',\n",
       " ', en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V)',\n",
       " '\\'b\"One major application of the matrix theory is calculation of determinants, a central concept in linear algebra',\n",
       " ' While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis',\n",
       " ' It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20])',\n",
       " ' If the determinant is zero, then the nullspace is nontrivial',\n",
       " ' Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent)',\n",
       " \" Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method\",\n",
       " '\"b\\'In general, the action of a linear transformation may be quite complex',\n",
       " ' Attention to low-dimensional examples gives an indication of the variety of their types',\n",
       " ' One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T',\n",
       " ' If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector',\n",
       " ' The scalar \\\\xce\\\\xbb such that Tv = \\\\xce\\\\xbbv is called a characteristic value or eigenvalue of T',\n",
       " \"'b'To find an eigenvector or an eigenvalue, we note that'b'where I is the identity matrix\",\n",
       " ' For there to be nontrivial solutions to that equation, det(T \\\\xe2\\\\x88\\\\x92 \\\\xce\\\\xbb I) = 0',\n",
       " ' The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R',\n",
       " ' Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist',\n",
       " ' It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors',\n",
       " ' If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \\\\xce\\\\xbb1, \\\\xce\\\\xbb2, ',\n",
       " '',\n",
       " '',\n",
       " ', \\\\xce\\\\xbbn, and if v = a1v1 + ',\n",
       " '',\n",
       " '',\n",
       " \" + an vn, then,'b'Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix\",\n",
       " ' Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form',\n",
       " ' Not all matrices are diagonalizable (even over an algebraically closed field)',\n",
       " \"'b'Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product\",\n",
       " ' The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles',\n",
       " \" Formally, an inner product is a map'b'that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]'b'Note that in R, it is symmetric\",\n",
       " \"'b'We can define the length of a vector v in V by'b'and we can prove the Cauchy\\\\xe2\\\\x80\\\\x93Schwarz inequality:'b'In particular, the quantity'b'and so we can call this quantity the cosine of the angle between the two vectors\",\n",
       " \"'b'The inner product facilitates the construction of many useful concepts\",\n",
       " \" For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying'b'If T satisfies TT* = T*T, we call T normal\",\n",
       " ' It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V',\n",
       " \"'b'Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science\",\n",
       " ' Below are just some examples of applications of linear algebra',\n",
       " \"'b'Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method\",\n",
       " \" Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:'b'The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2\",\n",
       " ' This will put the system into triangular form',\n",
       " ' Then, using back-substitution, each unknown can be solved for',\n",
       " \"'b'In the example, x is eliminated from L2 by adding (3/2)L1 to L2\",\n",
       " ' x is then eliminated from L3 by adding L1 to L3',\n",
       " \" Formally:'b'The result is:'b'Now y is eliminated from L3 by adding \\\\xe2\\\\x88\\\\x924L2 to L3:'b'The result is:'b'This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete\",\n",
       " \"'b'The last part, back-substitution, consists of solving for the known in reverse order\",\n",
       " \" It can thus be seen that'b'Then, z can be substituted into L2, which can then be solved to obtain'b'Next, z and y can be substituted into L1, which can be solved to obtain'b'The system is solved\",\n",
       " \"'b'We can, in general, write any system of linear equations as a matrix equation:'b'The least squares method is used to determine the best-fit line for a set of data\",\n",
       " '[24] This line will minimize the sum of the squares of the residuals',\n",
       " \"'b'Fourier series are a representation of a function f: [\\\\xe2\\\\x88\\\\x92\\\\xcf\\\\x80, \\\\xcf\\\\x80] \\\\xe2\\\\x86\\\\x92 R as a trigonometric series:'b'This series expansion is extremely useful in solving partial differential equations\",\n",
       " ' In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points',\n",
       " '\\'b\\'The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series)',\n",
       " \" Moreover, this space is also an inner product space with the inner product'b'The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \\\\xe2\\\\x89\\\\xa5 0 are an orthonormal basis for the space of Fourier-expandable functions\",\n",
       " ' We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions',\n",
       " \" For instance, to find the coefficient ak, we take the inner product with hk:'b'Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E\",\n",
       " ' When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces',\n",
       " \"'b'Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]'b'where a, b and c are not all zero\",\n",
       " \" Then,'b'or'b'where x = (x, y, 1) is the 3\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x891 set of homogeneous coordinates associated with the point (x, y)\",\n",
       " \"[26]'b'Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space\",\n",
       " ' The x\\\\xe2\\\\x88\\\\x92y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1)',\n",
       " \"'b'The linear equation, \\\\xce\\\\xbb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \\\\xce\\\\xb1x1 + \\\\xce\\\\xb2x2 is also on the line, for any real \\\\xce\\\\xb1 and \\\\xce\\\\xb2\",\n",
       " \"'b'Now consider the equations of the two lines \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2,'b'which forms a system of linear equations\",\n",
       " \" The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,'b'or using homogeneous coordinates,'b'The point of intersection of these two lines is the unique non-zero solution of these equations\",\n",
       " ' In homogeneous coordinates, the solutions are multiples of the following solution:[26]\\'b\"if the rows of B are linearly independent (i',\n",
       " 'e',\n",
       " ', \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2 represent distinct lines)',\n",
       " \" Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns\",\n",
       " '[27] Notice that this yields a point in the z = 1 plane only when the 2\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x892 submatrix associated with x3 has a non-zero determinant',\n",
       " '\"b\\'It is interesting to consider the case of three lines, \\\\xce\\\\xbb1, \\\\xce\\\\xbb2 and \\\\xce\\\\xbb3, which yield the matrix equation,\\'b\\'which in homogeneous form yields,\\'b\\'Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E',\n",
       " ' For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero',\n",
       " ' Another way to say this is that the columns of the matrix must be linearly dependent',\n",
       " \"'b'Another way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2\",\n",
       " ' Here R denotes the set of real numbers',\n",
       " \" Let x=(x, y) be an arbitrary vector in E and consider the linear function \\\\xce\\\\xbb: E\\\\xe2\\\\x86\\\\x92R, given by'b'or'b'This transformation has the important property that if Ay=d, then'b'This shows that the sum of vectors in E map to the sum of their images in R\",\n",
       " ' This is the defining characteristic of a linear map, or linear transformation',\n",
       " '[25] For this case, where the image space is a real number the map is called a linear functional',\n",
       " \"[27]'b'Consider the linear functional a little more carefully\",\n",
       " ' Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj',\n",
       " \" It is now possible to see that'b'Thus, the columns of the matrix A are the image of the basis vectors of E in R\",\n",
       " \"'b'This is true for any pair of vectors used to define coordinates in E\",\n",
       " ' Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E',\n",
       " ' This means a vector x has coordinates (\\\\xce\\\\xb1,\\\\xce\\\\xb2), such that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w',\n",
       " \" Then, we have the linear functional'b'where Av=d and Aw=e are the images of the basis vectors v and w\",\n",
       " \" This is written in matrix form as'b'This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E\",\n",
       " ' Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1)',\n",
       " \" Our goal is to find the real numbers \\\\xce\\\\xb1, \\\\xce\\\\xb2, so that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w, that is'b'To solve this equation for \\\\xce\\\\xb1, \\\\xce\\\\xb2, we compute the linear coordinate functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 for the basis v, w, which are given by,[26]'b'The functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 compute the components of x along the basis vectors v and w, respectively, that is,'b'which can be written in matrix form as'b'These coordinate functionals have the properties,'b'These equations can be assembled into the single matrix equation,'b'Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors\",\n",
       " \"[25][27]'b'The set of points in the plane E that map to the same image in R under the linear functional \\\\xce\\\\xbb define a line in E\",\n",
       " ' This line is the image of the inverse map, \\\\xce\\\\xbb\\\\xe2\\\\x88\\\\x921: R\\\\xe2\\\\x86\\\\x92E',\n",
       " \" This inverse image is the set of the points x=(x, y) that solve the equation,'b'Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c\",\n",
       " \"'b'In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation'b'Solve for y and obtain the inverse image as the set of points,'b'For convenience the free parameter x has been relabeled t\",\n",
       " \"'b'The vector p defines the intersection of the line with the y-axis, known as the y-intercept\",\n",
       " \" The vector h satisfies the homogeneous equation,'b'Notice that if h is a solution to this homogeneous equation, then t h is also a solution\",\n",
       " \"'b'The set of points of a linear functional that map to zero define the kernel of the linear functional\",\n",
       " ' The line can be considered to be the set of points h in the kernel translated by the vector p',\n",
       " \"[25][27]'b'Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics\",\n",
       " ' In module theory, one replaces the field of scalars by a ring',\n",
       " ' The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense',\n",
       " ' Nevertheless, many theorems from linear algebra become false in module theory',\n",
       " ' For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis',\n",
       " \"'b'In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables\",\n",
       " ' This line of inquiry naturally leads to the idea of the dual space, the vector space V\\\\xe2\\\\x88\\\\x97 consisting of linear maps f: V \\\\xe2\\\\x86\\\\x92 F where F is the field of scalars',\n",
       " ' Multilinear maps T: Vn \\\\xe2\\\\x86\\\\x92 F can be described via tensor products of elements of V\\\\xe2\\\\x88\\\\x97',\n",
       " \"'b'If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \\\\xc3\\\\x97 V \\\\xe2\\\\x86\\\\x92 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials)\",\n",
       " \"'b'Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces\",\n",
       " \"'b'Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices\",\n",
       " ' It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra',\n",
       " ' The concept of eigenvalues and eigenvectors is especially important',\n",
       " \"'b'Algebraic geometry considers the solutions of systems of polynomial equations\",\n",
       " \"'b'There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to\",\n",
       " \"'Linear algebra\\nb'Linear algebra is the branch of mathematics concerning linear equations such as'b'linear functions such as'b'and their representations through matrices and vector spaces\",\n",
       " \"[1][2][3]'b'Linear algebra is central to almost all areas of mathematics\",\n",
       " ' For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations',\n",
       " ' Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions',\n",
       " ' Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models',\n",
       " ' For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation',\n",
       " '\\'b\\'\\'b\\'\\'b\"The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations',\n",
       " \" Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear system, now called Cramer's Rule\",\n",
       " ' Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy',\n",
       " '[4]\"b\\'The study of matrix algebra first emerged in England in the mid-1800s',\n",
       " ' In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra',\n",
       " ' In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\"',\n",
       " ' While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses',\n",
       " ' Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object',\n",
       " ' He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\"',\n",
       " '[4]\\'b\\'In 1882, H\\\\xc3\\\\xbcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\"',\n",
       " '[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged',\n",
       " ' Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra',\n",
       " ' The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics',\n",
       " ' The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations',\n",
       " \"[4]'b'The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination\",\n",
       " \"'b'Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s\",\n",
       " '[7] Following work by the School Mathematics Study Group, U',\n",
       " 'S',\n",
       " ' high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s',\n",
       " '[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school',\n",
       " ' This was met with a backlash in the 1980s that removed linear algebra from the curriculum',\n",
       " '[9] In 1993, the U',\n",
       " 'S',\n",
       " '-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation',\n",
       " \"[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as \\\\'so many other disciplines use it\\\\'\",\n",
       " '[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination',\n",
       " \"[13][14]'b'The main structures of linear algebra are vector spaces\",\n",
       " ' A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms',\n",
       " ' Elements of V are called vectors, and elements of F are called scalars',\n",
       " ' The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w',\n",
       " ' The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av',\n",
       " ' The operations of addition and multiplication in a vector space must satisfy the following axioms',\n",
       " '[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F',\n",
       " \"'b'The first four axioms are those of V being an abelian group under vector addition\",\n",
       " ' Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices',\n",
       " ' Linear algebra is concerned with properties common to all vector spaces',\n",
       " \"'b'Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure\",\n",
       " \" Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map'b'that is compatible with addition and scalar multiplication:'b'for any vectors u,v \\\\xe2\\\\x88\\\\x88 V and a scalar a \\\\xe2\\\\x88\\\\x88 F\",\n",
       " \"'b'Additionally for any vectors u, v \\\\xe2\\\\x88\\\\x88 V and scalars a, b \\\\xe2\\\\x88\\\\x88 F:'b'When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic\",\n",
       " ' Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view',\n",
       " ' One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero',\n",
       " ' If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping',\n",
       " \"'b'Linear transformations have geometric significance\",\n",
       " ' For example, 2 \\\\xc3\\\\x97 2 real matrices denote standard planar mappings that preserve the origin',\n",
       " \"'b'Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces\",\n",
       " ' For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces',\n",
       " ' Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " \", vk:'b'where a1, a2, \",\n",
       " '',\n",
       " '',\n",
       " ', ak are scalars',\n",
       " ' The set of all linear combinations of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk is called their span, which forms a subspace',\n",
       " \"'b'A linear combination of any system of vectors with all zero coefficients is the zero vector of V\",\n",
       " ' If this is the only way to express the zero vector as a linear combination of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk then these vectors are linearly independent',\n",
       " ' Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set',\n",
       " ' Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace',\n",
       " ' Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V',\n",
       " ' Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis',\n",
       " '[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible',\n",
       " ' For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed',\n",
       " \"'b'Any two bases of a vector space V have the same cardinality, which is called the dimension of V\",\n",
       " ' The dimension of a vector space is well-defined by the dimension theorem for vector spaces',\n",
       " ' If a basis of V has finite number of elements, V is called a finite-dimensional vector space',\n",
       " ' If V is finite-dimensional and U is a subspace of V, then dim U \\\\xe2\\\\x89\\\\xa4 dim V',\n",
       " \" If U1 and U2 are subspaces of V, then'b'One often restricts consideration to finite-dimensional vector spaces\",\n",
       " ' A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism',\n",
       " \"'b'A particular basis {v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ',\n",
       " '',\n",
       " '',\n",
       " \", an) is the linear combination'b'The condition that v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn assures that these coordinates are unique (i',\n",
       " 'e',\n",
       " ' there is only one linear combination of the basis vectors that is equal to v)',\n",
       " ' In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn',\n",
       " ' Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn',\n",
       " ' Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \\\\xe2\\\\x86\\\\x92 W may be encoded by an m \\\\xc3\\\\x97 n matrix A with entries in the field F, called the matrix of T with respect to these bases',\n",
       " ' Two matrices that encode the same linear transformation in different bases are called similar',\n",
       " ' Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects',\n",
       " ' This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely',\n",
       " \"'b'There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V\",\n",
       " ' While Rn has a standard basis {e1, e2, ',\n",
       " '',\n",
       " '',\n",
       " ', en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V)',\n",
       " '\\'b\"One major application of the matrix theory is calculation of determinants, a central concept in linear algebra',\n",
       " ' While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis',\n",
       " ' It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20])',\n",
       " ' If the determinant is zero, then the nullspace is nontrivial',\n",
       " ' Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent)',\n",
       " \" Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method\",\n",
       " '\"b\\'In general, the action of a linear transformation may be quite complex',\n",
       " ' Attention to low-dimensional examples gives an indication of the variety of their types',\n",
       " ' One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T',\n",
       " ' If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector',\n",
       " ' The scalar \\\\xce\\\\xbb such that Tv = \\\\xce\\\\xbbv is called a characteristic value or eigenvalue of T',\n",
       " \"'b'To find an eigenvector or an eigenvalue, we note that'b'where I is the identity matrix\",\n",
       " ' For there to be nontrivial solutions to that equation, det(T \\\\xe2\\\\x88\\\\x92 \\\\xce\\\\xbb I) = 0',\n",
       " ' The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R',\n",
       " ' Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist',\n",
       " ' It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors',\n",
       " ' If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \\\\xce\\\\xbb1, \\\\xce\\\\xbb2, ',\n",
       " '',\n",
       " '',\n",
       " ', \\\\xce\\\\xbbn, and if v = a1v1 + ',\n",
       " '',\n",
       " '',\n",
       " \" + an vn, then,'b'Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix\",\n",
       " ' Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form',\n",
       " ' Not all matrices are diagonalizable (even over an algebraically closed field)',\n",
       " \"'b'Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product\",\n",
       " ' The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles',\n",
       " \" Formally, an inner product is a map'b'that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]'b'Note that in R, it is symmetric\",\n",
       " \"'b'We can define the length of a vector v in V by'b'and we can prove the Cauchy\\\\xe2\\\\x80\\\\x93Schwarz inequality:'b'In particular, the quantity'b'and so we can call this quantity the cosine of the angle between the two vectors\",\n",
       " \"'b'The inner product facilitates the construction of many useful concepts\",\n",
       " \" For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying'b'If T satisfies TT* = T*T, we call T normal\",\n",
       " ' It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V',\n",
       " \"'b'Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science\",\n",
       " ' Below are just some examples of applications of linear algebra',\n",
       " \"'b'Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method\",\n",
       " \" Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:'b'The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2\",\n",
       " ' This will put the system into triangular form',\n",
       " ' Then, using back-substitution, each unknown can be solved for',\n",
       " \"'b'In the example, x is eliminated from L2 by adding (3/2)L1 to L2\",\n",
       " ' x is then eliminated from L3 by adding L1 to L3',\n",
       " \" Formally:'b'The result is:'b'Now y is eliminated from L3 by adding \\\\xe2\\\\x88\\\\x924L2 to L3:'b'The result is:'b'This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete\",\n",
       " \"'b'The last part, back-substitution, consists of solving for the known in reverse order\",\n",
       " \" It can thus be seen that'b'Then, z can be substituted into L2, which can then be solved to obtain'b'Next, z and y can be substituted into L1, which can be solved to obtain'b'The system is solved\",\n",
       " \"'b'We can, in general, write any system of linear equations as a matrix equation:'b'The least squares method is used to determine the best-fit line for a set of data\",\n",
       " '[24] This line will minimize the sum of the squares of the residuals',\n",
       " \"'b'Fourier series are a representation of a function f: [\\\\xe2\\\\x88\\\\x92\\\\xcf\\\\x80, \\\\xcf\\\\x80] \\\\xe2\\\\x86\\\\x92 R as a trigonometric series:'b'This series expansion is extremely useful in solving partial differential equations\",\n",
       " ' In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points',\n",
       " '\\'b\\'The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series)',\n",
       " \" Moreover, this space is also an inner product space with the inner product'b'The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \\\\xe2\\\\x89\\\\xa5 0 are an orthonormal basis for the space of Fourier-expandable functions\",\n",
       " ' We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions',\n",
       " \" For instance, to find the coefficient ak, we take the inner product with hk:'b'Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E\",\n",
       " ' When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces',\n",
       " \"'b'Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]'b'where a, b and c are not all zero\",\n",
       " \" Then,'b'or'b'where x = (x, y, 1) is the 3\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x891 set of homogeneous coordinates associated with the point (x, y)\",\n",
       " \"[26]'b'Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space\",\n",
       " ' The x\\\\xe2\\\\x88\\\\x92y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1)',\n",
       " \"'b'The linear equation, \\\\xce\\\\xbb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \\\\xce\\\\xb1x1 + \\\\xce\\\\xb2x2 is also on the line, for any real \\\\xce\\\\xb1 and \\\\xce\\\\xb2\",\n",
       " \"'b'Now consider the equations of the two lines \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2,'b'which forms a system of linear equations\",\n",
       " \" The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,'b'or using homogeneous coordinates,'b'The point of intersection of these two lines is the unique non-zero solution of these equations\",\n",
       " ' In homogeneous coordinates, the solutions are multiples of the following solution:[26]\\'b\"if the rows of B are linearly independent (i',\n",
       " 'e',\n",
       " ', \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2 represent distinct lines)',\n",
       " \" Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns\",\n",
       " '[27] Notice that this yields a point in the z = 1 plane only when the 2\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x892 submatrix associated with x3 has a non-zero determinant',\n",
       " '\"b\\'It is interesting to consider the case of three lines, \\\\xce\\\\xbb1, \\\\xce\\\\xbb2 and \\\\xce\\\\xbb3, which yield the matrix equation,\\'b\\'which in homogeneous form yields,\\'b\\'Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E',\n",
       " ' For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero',\n",
       " ' Another way to say this is that the columns of the matrix must be linearly dependent',\n",
       " \"'b'Another way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2\",\n",
       " ' Here R denotes the set of real numbers',\n",
       " \" Let x=(x, y) be an arbitrary vector in E and consider the linear function \\\\xce\\\\xbb: E\\\\xe2\\\\x86\\\\x92R, given by'b'or'b'This transformation has the important property that if Ay=d, then'b'This shows that the sum of vectors in E map to the sum of their images in R\",\n",
       " ' This is the defining characteristic of a linear map, or linear transformation',\n",
       " '[25] For this case, where the image space is a real number the map is called a linear functional',\n",
       " \"[27]'b'Consider the linear functional a little more carefully\",\n",
       " ' Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj',\n",
       " \" It is now possible to see that'b'Thus, the columns of the matrix A are the image of the basis vectors of E in R\",\n",
       " \"'b'This is true for any pair of vectors used to define coordinates in E\",\n",
       " ' Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E',\n",
       " ' This means a vector x has coordinates (\\\\xce\\\\xb1,\\\\xce\\\\xb2), such that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w',\n",
       " \" Then, we have the linear functional'b'where Av=d and Aw=e are the images of the basis vectors v and w\",\n",
       " \" This is written in matrix form as'b'This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E\",\n",
       " ' Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1)',\n",
       " \" Our goal is to find the real numbers \\\\xce\\\\xb1, \\\\xce\\\\xb2, so that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w, that is'b'To solve this equation for \\\\xce\\\\xb1, \\\\xce\\\\xb2, we compute the linear coordinate functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 for the basis v, w, which are given by,[26]'b'The functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 compute the components of x along the basis vectors v and w, respectively, that is,'b'which can be written in matrix form as'b'These coordinate functionals have the properties,'b'These equations can be assembled into the single matrix equation,'b'Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors\",\n",
       " \"[25][27]'b'The set of points in the plane E that map to the same image in R under the linear functional \\\\xce\\\\xbb define a line in E\",\n",
       " ' This line is the image of the inverse map, \\\\xce\\\\xbb\\\\xe2\\\\x88\\\\x921: R\\\\xe2\\\\x86\\\\x92E',\n",
       " \" This inverse image is the set of the points x=(x, y) that solve the equation,'b'Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c\",\n",
       " \"'b'In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation'b'Solve for y and obtain the inverse image as the set of points,'b'For convenience the free parameter x has been relabeled t\",\n",
       " \"'b'The vector p defines the intersection of the line with the y-axis, known as the y-intercept\",\n",
       " \" The vector h satisfies the homogeneous equation,'b'Notice that if h is a solution to this homogeneous equation, then t h is also a solution\",\n",
       " \"'b'The set of points of a linear functional that map to zero define the kernel of the linear functional\",\n",
       " ' The line can be considered to be the set of points h in the kernel translated by the vector p',\n",
       " \"[25][27]'b'Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics\",\n",
       " ' In module theory, one replaces the field of scalars by a ring',\n",
       " ' The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense',\n",
       " ' Nevertheless, many theorems from linear algebra become false in module theory',\n",
       " ' For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis',\n",
       " \"'b'In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables\",\n",
       " ' This line of inquiry naturally leads to the idea of the dual space, the vector space V\\\\xe2\\\\x88\\\\x97 consisting of linear maps f: V \\\\xe2\\\\x86\\\\x92 F where F is the field of scalars',\n",
       " ' Multilinear maps T: Vn \\\\xe2\\\\x86\\\\x92 F can be described via tensor products of elements of V\\\\xe2\\\\x88\\\\x97',\n",
       " \"'b'If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \\\\xc3\\\\x97 V \\\\xe2\\\\x86\\\\x92 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials)\",\n",
       " \"'b'Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces\",\n",
       " \"'b'Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices\",\n",
       " ' It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra',\n",
       " ' The concept of eigenvalues and eigenvectors is especially important',\n",
       " \"'b'Algebraic geometry considers the solutions of systems of polynomial equations\",\n",
       " \"'b'There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to\",\n",
       " \"'Linear algebrab'Linear algebra is the branch of mathematics concerning linear equations such as'b'linear functions such as'b'and their representations through matrices and vector spaces\",\n",
       " \"[1][2][3]'b'Linear algebra is central to almost all areas of mathematics\",\n",
       " ' For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations',\n",
       " ' Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions',\n",
       " ' Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models',\n",
       " ' For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation',\n",
       " '\\'b\\'\\'b\\'\\'b\"The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations',\n",
       " \" Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear system, now called Cramer's Rule\",\n",
       " ' Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy',\n",
       " '[4]\"b\\'The study of matrix algebra first emerged in England in the mid-1800s',\n",
       " ' In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra',\n",
       " ' In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\"',\n",
       " ' While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses',\n",
       " ' Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object',\n",
       " ' He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\"',\n",
       " '[4]\\'b\\'In 1882, H\\\\xc3\\\\xbcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\"',\n",
       " '[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged',\n",
       " ' Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra',\n",
       " ' The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics',\n",
       " ' The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations',\n",
       " \"[4]'b'The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination\",\n",
       " \"'b'Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s\",\n",
       " '[7] Following work by the School Mathematics Study Group, U',\n",
       " 'S',\n",
       " ' high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s',\n",
       " '[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school',\n",
       " ' This was met with a backlash in the 1980s that removed linear algebra from the curriculum',\n",
       " '[9] In 1993, the U',\n",
       " 'S',\n",
       " '-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation',\n",
       " \"[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as \\\\'so many other disciplines use it\\\\'\",\n",
       " '[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination',\n",
       " \"[13][14]'b'The main structures of linear algebra are vector spaces\",\n",
       " ' A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms',\n",
       " ' Elements of V are called vectors, and elements of F are called scalars',\n",
       " ' The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w',\n",
       " ' The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av',\n",
       " ' The operations of addition and multiplication in a vector space must satisfy the following axioms',\n",
       " '[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F',\n",
       " \"'b'The first four axioms are those of V being an abelian group under vector addition\",\n",
       " ' Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices',\n",
       " ' Linear algebra is concerned with properties common to all vector spaces',\n",
       " \"'b'Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure\",\n",
       " \" Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map'b'that is compatible with addition and scalar multiplication:'b'for any vectors u,v \\\\xe2\\\\x88\\\\x88 V and a scalar a \\\\xe2\\\\x88\\\\x88 F\",\n",
       " \"'b'Additionally for any vectors u, v \\\\xe2\\\\x88\\\\x88 V and scalars a, b \\\\xe2\\\\x88\\\\x88 F:'b'When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic\",\n",
       " ' Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view',\n",
       " ' One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero',\n",
       " ' If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping',\n",
       " \"'b'Linear transformations have geometric significance\",\n",
       " ' For example, 2 \\\\xc3\\\\x97 2 real matrices denote standard planar mappings that preserve the origin',\n",
       " \"'b'Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces\",\n",
       " ' For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces',\n",
       " ' Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " \", vk:'b'where a1, a2, \",\n",
       " '',\n",
       " '',\n",
       " ', ak are scalars',\n",
       " ' The set of all linear combinations of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk is called their span, which forms a subspace',\n",
       " \"'b'A linear combination of any system of vectors with all zero coefficients is the zero vector of V\",\n",
       " ' If this is the only way to express the zero vector as a linear combination of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk then these vectors are linearly independent',\n",
       " ' Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set',\n",
       " ' Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace',\n",
       " ' Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V',\n",
       " ' Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis',\n",
       " '[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible',\n",
       " ' For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed',\n",
       " \"'b'Any two bases of a vector space V have the same cardinality, which is called the dimension of V\",\n",
       " ' The dimension of a vector space is well-defined by the dimension theorem for vector spaces',\n",
       " ' If a basis of V has finite number of elements, V is called a finite-dimensional vector space',\n",
       " ' If V is finite-dimensional and U is a subspace of V, then dim U \\\\xe2\\\\x89\\\\xa4 dim V',\n",
       " \" If U1 and U2 are subspaces of V, then'b'One often restricts consideration to finite-dimensional vector spaces\",\n",
       " ' A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism',\n",
       " \"'b'A particular basis {v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ',\n",
       " '',\n",
       " '',\n",
       " \", an) is the linear combination'b'The condition that v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn assures that these coordinates are unique (i',\n",
       " 'e',\n",
       " ' there is only one linear combination of the basis vectors that is equal to v)',\n",
       " ' In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn',\n",
       " ' Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn',\n",
       " ' Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \\\\xe2\\\\x86\\\\x92 W may be encoded by an m \\\\xc3\\\\x97 n matrix A with entries in the field F, called the matrix of T with respect to these bases',\n",
       " ' Two matrices that encode the same linear transformation in different bases are called similar',\n",
       " ' Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects',\n",
       " ' This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely',\n",
       " \"'b'There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V\",\n",
       " ' While Rn has a standard basis {e1, e2, ',\n",
       " '',\n",
       " '',\n",
       " ', en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V)',\n",
       " '\\'b\"One major application of the matrix theory is calculation of determinants, a central concept in linear algebra',\n",
       " ' While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis',\n",
       " ' It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20])',\n",
       " ' If the determinant is zero, then the nullspace is nontrivial',\n",
       " ' Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent)',\n",
       " \" Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method\",\n",
       " '\"b\\'In general, the action of a linear transformation may be quite complex',\n",
       " ' Attention to low-dimensional examples gives an indication of the variety of their types',\n",
       " ' One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T',\n",
       " ' If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector',\n",
       " ' The scalar \\\\xce\\\\xbb such that Tv = \\\\xce\\\\xbbv is called a characteristic value or eigenvalue of T',\n",
       " \"'b'To find an eigenvector or an eigenvalue, we note that'b'where I is the identity matrix\",\n",
       " ' For there to be nontrivial solutions to that equation, det(T \\\\xe2\\\\x88\\\\x92 \\\\xce\\\\xbb I) = 0',\n",
       " ' The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R',\n",
       " ' Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist',\n",
       " ' It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors',\n",
       " ' If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \\\\xce\\\\xbb1, \\\\xce\\\\xbb2, ',\n",
       " '',\n",
       " '',\n",
       " ', \\\\xce\\\\xbbn, and if v = a1v1 + ',\n",
       " '',\n",
       " '',\n",
       " \" + an vn, then,'b'Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix\",\n",
       " ' Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form',\n",
       " ' Not all matrices are diagonalizable (even over an algebraically closed field)',\n",
       " \"'b'Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product\",\n",
       " ' The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles',\n",
       " \" Formally, an inner product is a map'b'that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]'b'Note that in R, it is symmetric\",\n",
       " \"'b'We can define the length of a vector v in V by'b'and we can prove the Cauchy\\\\xe2\\\\x80\\\\x93Schwarz inequality:'b'In particular, the quantity'b'and so we can call this quantity the cosine of the angle between the two vectors\",\n",
       " \"'b'The inner product facilitates the construction of many useful concepts\",\n",
       " \" For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying'b'If T satisfies TT* = T*T, we call T normal\",\n",
       " ' It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V',\n",
       " \"'b'Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science\",\n",
       " ' Below are just some examples of applications of linear algebra',\n",
       " \"'b'Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method\",\n",
       " \" Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:'b'The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2\",\n",
       " ' This will put the system into triangular form',\n",
       " ' Then, using back-substitution, each unknown can be solved for',\n",
       " \"'b'In the example, x is eliminated from L2 by adding (3/2)L1 to L2\",\n",
       " ' x is then eliminated from L3 by adding L1 to L3',\n",
       " \" Formally:'b'The result is:'b'Now y is eliminated from L3 by adding \\\\xe2\\\\x88\\\\x924L2 to L3:'b'The result is:'b'This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete\",\n",
       " \"'b'The last part, back-substitution, consists of solving for the known in reverse order\",\n",
       " \" It can thus be seen that'b'Then, z can be substituted into L2, which can then be solved to obtain'b'Next, z and y can be substituted into L1, which can be solved to obtain'b'The system is solved\",\n",
       " \"'b'We can, in general, write any system of linear equations as a matrix equation:'b'The least squares method is used to determine the best-fit line for a set of data\",\n",
       " '[24] This line will minimize the sum of the squares of the residuals',\n",
       " \"'b'Fourier series are a representation of a function f: [\\\\xe2\\\\x88\\\\x92\\\\xcf\\\\x80, \\\\xcf\\\\x80] \\\\xe2\\\\x86\\\\x92 R as a trigonometric series:'b'This series expansion is extremely useful in solving partial differential equations\",\n",
       " ' In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points',\n",
       " '\\'b\\'The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series)',\n",
       " \" Moreover, this space is also an inner product space with the inner product'b'The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \\\\xe2\\\\x89\\\\xa5 0 are an orthonormal basis for the space of Fourier-expandable functions\",\n",
       " ' We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions',\n",
       " \" For instance, to find the coefficient ak, we take the inner product with hk:'b'Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E\",\n",
       " ' When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces',\n",
       " \"'b'Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]'b'where a, b and c are not all zero\",\n",
       " \" Then,'b'or'b'where x = (x, y, 1) is the 3\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x891 set of homogeneous coordinates associated with the point (x, y)\",\n",
       " \"[26]'b'Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space\",\n",
       " ' The x\\\\xe2\\\\x88\\\\x92y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1)',\n",
       " \"'b'The linear equation, \\\\xce\\\\xbb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \\\\xce\\\\xb1x1 + \\\\xce\\\\xb2x2 is also on the line, for any real \\\\xce\\\\xb1 and \\\\xce\\\\xb2\",\n",
       " \"'b'Now consider the equations of the two lines \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2,'b'which forms a system of linear equations\",\n",
       " \" The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,'b'or using homogeneous coordinates,'b'The point of intersection of these two lines is the unique non-zero solution of these equations\",\n",
       " ' In homogeneous coordinates, the solutions are multiples of the following solution:[26]\\'b\"if the rows of B are linearly independent (i',\n",
       " 'e',\n",
       " ', \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2 represent distinct lines)',\n",
       " \" Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns\",\n",
       " '[27] Notice that this yields a point in the z = 1 plane only when the 2\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x892 submatrix associated with x3 has a non-zero determinant',\n",
       " '\"b\\'It is interesting to consider the case of three lines, \\\\xce\\\\xbb1, \\\\xce\\\\xbb2 and \\\\xce\\\\xbb3, which yield the matrix equation,\\'b\\'which in homogeneous form yields,\\'b\\'Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E',\n",
       " ' For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero',\n",
       " ' Another way to say this is that the columns of the matrix must be linearly dependent',\n",
       " \"'b'Another way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2\",\n",
       " ' Here R denotes the set of real numbers',\n",
       " \" Let x=(x, y) be an arbitrary vector in E and consider the linear function \\\\xce\\\\xbb: E\\\\xe2\\\\x86\\\\x92R, given by'b'or'b'This transformation has the important property that if Ay=d, then'b'This shows that the sum of vectors in E map to the sum of their images in R\",\n",
       " ' This is the defining characteristic of a linear map, or linear transformation',\n",
       " '[25] For this case, where the image space is a real number the map is called a linear functional',\n",
       " \"[27]'b'Consider the linear functional a little more carefully\",\n",
       " ' Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj',\n",
       " \" It is now possible to see that'b'Thus, the columns of the matrix A are the image of the basis vectors of E in R\",\n",
       " \"'b'This is true for any pair of vectors used to define coordinates in E\",\n",
       " ' Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E',\n",
       " ' This means a vector x has coordinates (\\\\xce\\\\xb1,\\\\xce\\\\xb2), such that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w',\n",
       " \" Then, we have the linear functional'b'where Av=d and Aw=e are the images of the basis vectors v and w\",\n",
       " \" This is written in matrix form as'b'This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E\",\n",
       " ' Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1)',\n",
       " \" Our goal is to find the real numbers \\\\xce\\\\xb1, \\\\xce\\\\xb2, so that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w, that is'b'To solve this equation for \\\\xce\\\\xb1, \\\\xce\\\\xb2, we compute the linear coordinate functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 for the basis v, w, which are given by,[26]'b'The functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 compute the components of x along the basis vectors v and w, respectively, that is,'b'which can be written in matrix form as'b'These coordinate functionals have the properties,'b'These equations can be assembled into the single matrix equation,'b'Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors\",\n",
       " \"[25][27]'b'The set of points in the plane E that map to the same image in R under the linear functional \\\\xce\\\\xbb define a line in E\",\n",
       " ' This line is the image of the inverse map, \\\\xce\\\\xbb\\\\xe2\\\\x88\\\\x921: R\\\\xe2\\\\x86\\\\x92E',\n",
       " \" This inverse image is the set of the points x=(x, y) that solve the equation,'b'Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c\",\n",
       " \"'b'In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation'b'Solve for y and obtain the inverse image as the set of points,'b'For convenience the free parameter x has been relabeled t\",\n",
       " \"'b'The vector p defines the intersection of the line with the y-axis, known as the y-intercept\",\n",
       " \" The vector h satisfies the homogeneous equation,'b'Notice that if h is a solution to this homogeneous equation, then t h is also a solution\",\n",
       " \"'b'The set of points of a linear functional that map to zero define the kernel of the linear functional\",\n",
       " ' The line can be considered to be the set of points h in the kernel translated by the vector p',\n",
       " \"[25][27]'b'Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics\",\n",
       " ' In module theory, one replaces the field of scalars by a ring',\n",
       " ' The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense',\n",
       " ' Nevertheless, many theorems from linear algebra become false in module theory',\n",
       " ' For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis',\n",
       " \"'b'In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables\",\n",
       " ' This line of inquiry naturally leads to the idea of the dual space, the vector space V\\\\xe2\\\\x88\\\\x97 consisting of linear maps f: V \\\\xe2\\\\x86\\\\x92 F where F is the field of scalars',\n",
       " ' Multilinear maps T: Vn \\\\xe2\\\\x86\\\\x92 F can be described via tensor products of elements of V\\\\xe2\\\\x88\\\\x97',\n",
       " \"'b'If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \\\\xc3\\\\x97 V \\\\xe2\\\\x86\\\\x92 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials)\",\n",
       " \"'b'Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces\",\n",
       " \"'b'Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices\",\n",
       " ' It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra',\n",
       " ' The concept of eigenvalues and eigenvectors is especially important',\n",
       " \"'b'Algebraic geometry considers the solutions of systems of polynomial equations\",\n",
       " \"'b'There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to\",\n",
       " \"'Linear algebrab'Linear algebra is the branch of mathematics concerning linear equations such as'b'linear functions such as'b'and their representations through matrices and vector spaces\",\n",
       " \"[1][2][3]'b'Linear algebra is central to almost all areas of mathematics\",\n",
       " ' For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations',\n",
       " ' Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions',\n",
       " ' Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models',\n",
       " ' For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation',\n",
       " '\\'b\\'\\'b\\'\\'b\"The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations',\n",
       " \" Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear system, now called Cramer's Rule\",\n",
       " ' Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy',\n",
       " '[4]\"b\\'The study of matrix algebra first emerged in England in the mid-1800s',\n",
       " ' In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra',\n",
       " ' In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\"',\n",
       " ' While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses',\n",
       " ' Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object',\n",
       " ' He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\"',\n",
       " '[4]\\'b\\'In 1882, H\\\\xc3\\\\xbcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\"',\n",
       " '[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged',\n",
       " ' Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra',\n",
       " ' The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics',\n",
       " ' The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations',\n",
       " \"[4]'b'The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination\",\n",
       " \"'b'Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s\",\n",
       " '[7] Following work by the School Mathematics Study Group, U',\n",
       " 'S',\n",
       " ' high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s',\n",
       " '[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school',\n",
       " ' This was met with a backlash in the 1980s that removed linear algebra from the curriculum',\n",
       " '[9] In 1993, the U',\n",
       " 'S',\n",
       " '-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation',\n",
       " \"[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as \\\\'so many other disciplines use it\\\\'\",\n",
       " '[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination',\n",
       " \"[13][14]'b'The main structures of linear algebra are vector spaces\",\n",
       " ' A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms',\n",
       " ' Elements of V are called vectors, and elements of F are called scalars',\n",
       " ' The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w',\n",
       " ' The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av',\n",
       " ' The operations of addition and multiplication in a vector space must satisfy the following axioms',\n",
       " '[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F',\n",
       " \"'b'The first four axioms are those of V being an abelian group under vector addition\",\n",
       " ' Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices',\n",
       " ' Linear algebra is concerned with properties common to all vector spaces',\n",
       " \"'b'Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure\",\n",
       " \" Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map'b'that is compatible with addition and scalar multiplication:'b'for any vectors u,v \\\\xe2\\\\x88\\\\x88 V and a scalar a \\\\xe2\\\\x88\\\\x88 F\",\n",
       " \"'b'Additionally for any vectors u, v \\\\xe2\\\\x88\\\\x88 V and scalars a, b \\\\xe2\\\\x88\\\\x88 F:'b'When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic\",\n",
       " ' Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view',\n",
       " ' One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero',\n",
       " ' If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping',\n",
       " \"'b'Linear transformations have geometric significance\",\n",
       " ' For example, 2 \\\\xc3\\\\x97 2 real matrices denote standard planar mappings that preserve the origin',\n",
       " \"'b'Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces\",\n",
       " ' For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces',\n",
       " ' Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " \", vk:'b'where a1, a2, \",\n",
       " '',\n",
       " '',\n",
       " ', ak are scalars',\n",
       " ' The set of all linear combinations of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk is called their span, which forms a subspace',\n",
       " \"'b'A linear combination of any system of vectors with all zero coefficients is the zero vector of V\",\n",
       " ' If this is the only way to express the zero vector as a linear combination of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk then these vectors are linearly independent',\n",
       " ' Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set',\n",
       " ' Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace',\n",
       " ' Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V',\n",
       " ' Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis',\n",
       " '[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible',\n",
       " ' For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed',\n",
       " \"'b'Any two bases of a vector space V have the same cardinality, which is called the dimension of V\",\n",
       " ' The dimension of a vector space is well-defined by the dimension theorem for vector spaces',\n",
       " ' If a basis of V has finite number of elements, V is called a finite-dimensional vector space',\n",
       " ' If V is finite-dimensional and U is a subspace of V, then dim U \\\\xe2\\\\x89\\\\xa4 dim V',\n",
       " \" If U1 and U2 are subspaces of V, then'b'One often restricts consideration to finite-dimensional vector spaces\",\n",
       " ' A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism',\n",
       " \"'b'A particular basis {v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ',\n",
       " '',\n",
       " '',\n",
       " \", an) is the linear combination'b'The condition that v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn assures that these coordinates are unique (i',\n",
       " 'e',\n",
       " ' there is only one linear combination of the basis vectors that is equal to v)',\n",
       " ' In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn',\n",
       " ' Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn',\n",
       " ' Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \\\\xe2\\\\x86\\\\x92 W may be encoded by an m \\\\xc3\\\\x97 n matrix A with entries in the field F, called the matrix of T with respect to these bases',\n",
       " ' Two matrices that encode the same linear transformation in different bases are called similar',\n",
       " ' Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects',\n",
       " ' This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely',\n",
       " \"'b'There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V\",\n",
       " ' While Rn has a standard basis {e1, e2, ',\n",
       " '',\n",
       " '',\n",
       " ', en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V)',\n",
       " '\\'b\"One major application of the matrix theory is calculation of determinants, a central concept in linear algebra',\n",
       " ' While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis',\n",
       " ' It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20])',\n",
       " ' If the determinant is zero, then the nullspace is nontrivial',\n",
       " ' Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent)',\n",
       " \" Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method\",\n",
       " '\"b\\'In general, the action of a linear transformation may be quite complex',\n",
       " ' Attention to low-dimensional examples gives an indication of the variety of their types',\n",
       " ' One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T',\n",
       " ' If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector',\n",
       " ' The scalar \\\\xce\\\\xbb such that Tv = \\\\xce\\\\xbbv is called a characteristic value or eigenvalue of T',\n",
       " \"'b'To find an eigenvector or an eigenvalue, we note that'b'where I is the identity matrix\",\n",
       " ' For there to be nontrivial solutions to that equation, det(T \\\\xe2\\\\x88\\\\x92 \\\\xce\\\\xbb I) = 0',\n",
       " ' The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R',\n",
       " ' Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist',\n",
       " ' It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors',\n",
       " ' If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \\\\xce\\\\xbb1, \\\\xce\\\\xbb2, ',\n",
       " '',\n",
       " '',\n",
       " ', \\\\xce\\\\xbbn, and if v = a1v1 + ',\n",
       " '',\n",
       " '',\n",
       " \" + an vn, then,'b'Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix\",\n",
       " ' Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form',\n",
       " ' Not all matrices are diagonalizable (even over an algebraically closed field)',\n",
       " \"'b'Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product\",\n",
       " ' The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles',\n",
       " \" Formally, an inner product is a map'b'that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]'b'Note that in R, it is symmetric\",\n",
       " \"'b'We can define the length of a vector v in V by'b'and we can prove the Cauchy\\\\xe2\\\\x80\\\\x93Schwarz inequality:'b'In particular, the quantity'b'and so we can call this quantity the cosine of the angle between the two vectors\",\n",
       " \"'b'The inner product facilitates the construction of many useful concepts\",\n",
       " \" For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying'b'If T satisfies TT* = T*T, we call T normal\",\n",
       " ' It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V',\n",
       " \"'b'Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science\",\n",
       " ' Below are just some examples of applications of linear algebra',\n",
       " \"'b'Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method\",\n",
       " \" Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:'b'The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2\",\n",
       " ' This will put the system into triangular form',\n",
       " ' Then, using back-substitution, each unknown can be solved for',\n",
       " \"'b'In the example, x is eliminated from L2 by adding (3/2)L1 to L2\",\n",
       " ' x is then eliminated from L3 by adding L1 to L3',\n",
       " \" Formally:'b'The result is:'b'Now y is eliminated from L3 by adding \\\\xe2\\\\x88\\\\x924L2 to L3:'b'The result is:'b'This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete\",\n",
       " \"'b'The last part, back-substitution, consists of solving for the known in reverse order\",\n",
       " \" It can thus be seen that'b'Then, z can be substituted into L2, which can then be solved to obtain'b'Next, z and y can be substituted into L1, which can be solved to obtain'b'The system is solved\",\n",
       " \"'b'We can, in general, write any system of linear equations as a matrix equation:'b'The least squares method is used to determine the best-fit line for a set of data\",\n",
       " '[24] This line will minimize the sum of the squares of the residuals',\n",
       " \"'b'Fourier series are a representation of a function f: [\\\\xe2\\\\x88\\\\x92\\\\xcf\\\\x80, \\\\xcf\\\\x80] \\\\xe2\\\\x86\\\\x92 R as a trigonometric series:'b'This series expansion is extremely useful in solving partial differential equations\",\n",
       " ' In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points',\n",
       " '\\'b\\'The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series)',\n",
       " \" Moreover, this space is also an inner product space with the inner product'b'The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \\\\xe2\\\\x89\\\\xa5 0 are an orthonormal basis for the space of Fourier-expandable functions\",\n",
       " ' We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions',\n",
       " \" For instance, to find the coefficient ak, we take the inner product with hk:'b'Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E\",\n",
       " ' When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces',\n",
       " \"'b'Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]'b'where a, b and c are not all zero\",\n",
       " \" Then,'b'or'b'where x = (x, y, 1) is the 3\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x891 set of homogeneous coordinates associated with the point (x, y)\",\n",
       " \"[26]'b'Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space\",\n",
       " ' The x\\\\xe2\\\\x88\\\\x92y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1)',\n",
       " \"'b'The linear equation, \\\\xce\\\\xbb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \\\\xce\\\\xb1x1 + \\\\xce\\\\xb2x2 is also on the line, for any real \\\\xce\\\\xb1 and \\\\xce\\\\xb2\",\n",
       " \"'b'Now consider the equations of the two lines \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2,'b'which forms a system of linear equations\",\n",
       " \" The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,'b'or using homogeneous coordinates,'b'The point of intersection of these two lines is the unique non-zero solution of these equations\",\n",
       " ' In homogeneous coordinates, the solutions are multiples of the following solution:[26]\\'b\"if the rows of B are linearly independent (i',\n",
       " 'e',\n",
       " ', \\\\xce\\\\xbb1 and \\\\xce\\\\xbb2 represent distinct lines)',\n",
       " \" Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns\",\n",
       " '[27] Notice that this yields a point in the z = 1 plane only when the 2\\\\xe2\\\\x80\\\\x89\\\\xc3\\\\x97\\\\xe2\\\\x80\\\\x892 submatrix associated with x3 has a non-zero determinant',\n",
       " '\"b\\'It is interesting to consider the case of three lines, \\\\xce\\\\xbb1, \\\\xce\\\\xbb2 and \\\\xce\\\\xbb3, which yield the matrix equation,\\'b\\'which in homogeneous form yields,\\'b\\'Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E',\n",
       " ' For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero',\n",
       " ' Another way to say this is that the columns of the matrix must be linearly dependent',\n",
       " \"'b'Another way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2\",\n",
       " ' Here R denotes the set of real numbers',\n",
       " \" Let x=(x, y) be an arbitrary vector in E and consider the linear function \\\\xce\\\\xbb: E\\\\xe2\\\\x86\\\\x92R, given by'b'or'b'This transformation has the important property that if Ay=d, then'b'This shows that the sum of vectors in E map to the sum of their images in R\",\n",
       " ' This is the defining characteristic of a linear map, or linear transformation',\n",
       " '[25] For this case, where the image space is a real number the map is called a linear functional',\n",
       " \"[27]'b'Consider the linear functional a little more carefully\",\n",
       " ' Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj',\n",
       " \" It is now possible to see that'b'Thus, the columns of the matrix A are the image of the basis vectors of E in R\",\n",
       " \"'b'This is true for any pair of vectors used to define coordinates in E\",\n",
       " ' Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E',\n",
       " ' This means a vector x has coordinates (\\\\xce\\\\xb1,\\\\xce\\\\xb2), such that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w',\n",
       " \" Then, we have the linear functional'b'where Av=d and Aw=e are the images of the basis vectors v and w\",\n",
       " \" This is written in matrix form as'b'This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E\",\n",
       " ' Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1)',\n",
       " \" Our goal is to find the real numbers \\\\xce\\\\xb1, \\\\xce\\\\xb2, so that x=\\\\xce\\\\xb1v+\\\\xce\\\\xb2w, that is'b'To solve this equation for \\\\xce\\\\xb1, \\\\xce\\\\xb2, we compute the linear coordinate functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 for the basis v, w, which are given by,[26]'b'The functionals \\\\xcf\\\\x83 and \\\\xcf\\\\x84 compute the components of x along the basis vectors v and w, respectively, that is,'b'which can be written in matrix form as'b'These coordinate functionals have the properties,'b'These equations can be assembled into the single matrix equation,'b'Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors\",\n",
       " \"[25][27]'b'The set of points in the plane E that map to the same image in R under the linear functional \\\\xce\\\\xbb define a line in E\",\n",
       " ' This line is the image of the inverse map, \\\\xce\\\\xbb\\\\xe2\\\\x88\\\\x921: R\\\\xe2\\\\x86\\\\x92E',\n",
       " \" This inverse image is the set of the points x=(x, y) that solve the equation,'b'Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c\",\n",
       " \"'b'In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation'b'Solve for y and obtain the inverse image as the set of points,'b'For convenience the free parameter x has been relabeled t\",\n",
       " \"'b'The vector p defines the intersection of the line with the y-axis, known as the y-intercept\",\n",
       " \" The vector h satisfies the homogeneous equation,'b'Notice that if h is a solution to this homogeneous equation, then t h is also a solution\",\n",
       " \"'b'The set of points of a linear functional that map to zero define the kernel of the linear functional\",\n",
       " ' The line can be considered to be the set of points h in the kernel translated by the vector p',\n",
       " \"[25][27]'b'Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics\",\n",
       " ' In module theory, one replaces the field of scalars by a ring',\n",
       " ' The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense',\n",
       " ' Nevertheless, many theorems from linear algebra become false in module theory',\n",
       " ' For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis',\n",
       " \"'b'In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables\",\n",
       " ' This line of inquiry naturally leads to the idea of the dual space, the vector space V\\\\xe2\\\\x88\\\\x97 consisting of linear maps f: V \\\\xe2\\\\x86\\\\x92 F where F is the field of scalars',\n",
       " ' Multilinear maps T: Vn \\\\xe2\\\\x86\\\\x92 F can be described via tensor products of elements of V\\\\xe2\\\\x88\\\\x97',\n",
       " \"'b'If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \\\\xc3\\\\x97 V \\\\xe2\\\\x86\\\\x92 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials)\",\n",
       " \"'b'Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces\",\n",
       " \"'b'Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices\",\n",
       " ' It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra',\n",
       " ' The concept of eigenvalues and eigenvectors is especially important',\n",
       " \"'b'Algebraic geometry considers the solutions of systems of polynomial equations\",\n",
       " \"'b'There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to\",\n",
       " \"'Linear algebrab'Linear algebra is the branch of mathematics concerning linear equations such as'b'linear functions such as'b'and their representations through matrices and vector spaces\",\n",
       " \"[1][2][3]'b'Linear algebra is central to almost all areas of mathematics\",\n",
       " ' For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations',\n",
       " ' Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions',\n",
       " ' Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models',\n",
       " ' For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation',\n",
       " '\\'b\\'\\'b\\'\\'b\"The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations',\n",
       " \" Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear system, now called Cramer's Rule\",\n",
       " ' Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy',\n",
       " '[4]\"b\\'The study of matrix algebra first emerged in England in the mid-1800s',\n",
       " ' In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra',\n",
       " ' In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\"',\n",
       " ' While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses',\n",
       " ' Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object',\n",
       " ' He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\"',\n",
       " '[4]\\'b\\'In 1882, H\\\\xc3\\\\xbcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\"',\n",
       " '[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged',\n",
       " ' Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra',\n",
       " ' The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics',\n",
       " ' The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations',\n",
       " \"[4]'b'The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination\",\n",
       " \"'b'Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s\",\n",
       " '[7] Following work by the School Mathematics Study Group, U',\n",
       " 'S',\n",
       " ' high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s',\n",
       " '[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school',\n",
       " ' This was met with a backlash in the 1980s that removed linear algebra from the curriculum',\n",
       " '[9] In 1993, the U',\n",
       " 'S',\n",
       " '-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation',\n",
       " \"[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as \\\\'so many other disciplines use it\\\\'\",\n",
       " '[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination',\n",
       " \"[13][14]'b'The main structures of linear algebra are vector spaces\",\n",
       " ' A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms',\n",
       " ' Elements of V are called vectors, and elements of F are called scalars',\n",
       " ' The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w',\n",
       " ' The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av',\n",
       " ' The operations of addition and multiplication in a vector space must satisfy the following axioms',\n",
       " '[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F',\n",
       " \"'b'The first four axioms are those of V being an abelian group under vector addition\",\n",
       " ' Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices',\n",
       " ' Linear algebra is concerned with properties common to all vector spaces',\n",
       " \"'b'Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure\",\n",
       " \" Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map'b'that is compatible with addition and scalar multiplication:'b'for any vectors u,v \\\\xe2\\\\x88\\\\x88 V and a scalar a \\\\xe2\\\\x88\\\\x88 F\",\n",
       " \"'b'Additionally for any vectors u, v \\\\xe2\\\\x88\\\\x88 V and scalars a, b \\\\xe2\\\\x88\\\\x88 F:'b'When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic\",\n",
       " ' Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view',\n",
       " ' One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero',\n",
       " ' If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping',\n",
       " \"'b'Linear transformations have geometric significance\",\n",
       " ' For example, 2 \\\\xc3\\\\x97 2 real matrices denote standard planar mappings that preserve the origin',\n",
       " \"'b'Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces\",\n",
       " ' For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces',\n",
       " ' Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " \", vk:'b'where a1, a2, \",\n",
       " '',\n",
       " '',\n",
       " ', ak are scalars',\n",
       " ' The set of all linear combinations of vectors v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk is called their span, which forms a subspace',\n",
       " \"'b'A linear combination of any system of vectors with all zero coefficients is the zero vector of V\",\n",
       " ' If this is the only way to express the zero vector as a linear combination of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vk then these vectors are linearly independent',\n",
       " ' Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set',\n",
       " ' Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace',\n",
       " ' Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V',\n",
       " ' Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis',\n",
       " '[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible',\n",
       " ' For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed',\n",
       " \"'b'Any two bases of a vector space V have the same cardinality, which is called the dimension of V\",\n",
       " ' The dimension of a vector space is well-defined by the dimension theorem for vector spaces',\n",
       " ' If a basis of V has finite number of elements, V is called a finite-dimensional vector space',\n",
       " ' If V is finite-dimensional and U is a subspace of V, then dim U \\\\xe2\\\\x89\\\\xa4 dim V',\n",
       " \" If U1 and U2 are subspaces of V, then'b'One often restricts consideration to finite-dimensional vector spaces\",\n",
       " ' A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism',\n",
       " \"'b'A particular basis {v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ',\n",
       " '',\n",
       " '',\n",
       " \", an) is the linear combination'b'The condition that v1, v2, \",\n",
       " '',\n",
       " '',\n",
       " ', vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn assures that these coordinates are unique (i',\n",
       " 'e',\n",
       " ' there is only one linear combination of the basis vectors that is equal to v)',\n",
       " ' In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn',\n",
       " ' Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn',\n",
       " ' Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \\\\xe2\\\\x86\\\\x92 W may be encoded by an m \\\\xc3\\\\x97 n matrix A with entries in the field F, called the matrix of T with respect to these bases',\n",
       " ' Two matrices that encode the same linear transformation in different bases are called similar',\n",
       " ' Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects',\n",
       " ' This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely',\n",
       " \"'b'There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V\",\n",
       " ' While Rn has a standard basis {e1, e2, ',\n",
       " '',\n",
       " '',\n",
       " ', en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V)',\n",
       " '\\'b\"One major application of the matrix theory is calculation of determinants, a central concept in linear algebra',\n",
       " ' While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis',\n",
       " ' It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20])',\n",
       " ' If the determinant is zero, then the nullspace is nontrivial',\n",
       " ' Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent)',\n",
       " \" Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method\",\n",
       " '\"b\\'In general, the action of a linear transformation may be quite complex',\n",
       " ' Attention to low-dimensional examples gives an indication of the variety of their types',\n",
       " ' One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T',\n",
       " ' If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector',\n",
       " ' The scalar \\\\xce\\\\xbb such that Tv = \\\\xce\\\\xbbv is called a characteristic value or eigenvalue of T',\n",
       " \"'b'To find an eigenvector or an eigenvalue, we note that'b'where I is the identity matrix\",\n",
       " ' For there to be nontrivial solutions to that equation, det(T \\\\xe2\\\\x88\\\\x92 \\\\xce\\\\xbb I) = 0',\n",
       " ' The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R',\n",
       " ' Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist',\n",
       " ' It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors',\n",
       " ' If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ',\n",
       " '',\n",
       " '',\n",
       " ', vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \\\\xce\\\\xbb1, \\\\xce\\\\xbb2, ',\n",
       " '',\n",
       " '',\n",
       " ', \\\\xce\\\\xbbn, and if v = a1v1 + ',\n",
       " '',\n",
       " '',\n",
       " \" + an vn, then,'b'Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix\",\n",
       " ' Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form',\n",
       " ' Not all matrices are diagonalizable (even over an algebraically closed field)',\n",
       " \"'b'Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product\",\n",
       " ' The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles',\n",
       " \" Formally, an inner product is a map'b'that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]'b'Note that in R, it is symmetric\",\n",
       " \"'b'We can define the length of a vector v in V by'b'and we can prove the Cauchy\\\\xe2\\\\x80\\\\x93Schwarz inequality:'b'In particular, the quantity'b'and so we can call this quantity the cosine of the angle between the two vectors\",\n",
       " \"'b'The inner product facilitates the construction of many useful concepts\",\n",
       " \" For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying'b'If T satisfies TT* = T*T, we call T normal\",\n",
       " ' It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V',\n",
       " \"'b'Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science\",\n",
       " ' Below are just some examples of applications of linear algebra',\n",
       " \"'b'Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method\",\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[1]',\n",
       " '[2]',\n",
       " '[3]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[5]',\n",
       " '[6]',\n",
       " '[4]',\n",
       " '[4]',\n",
       " '[7]',\n",
       " '[8]',\n",
       " '[9]',\n",
       " '[10]',\n",
       " '[11]',\n",
       " '[12]',\n",
       " '[13]',\n",
       " '[14]',\n",
       " '[15]',\n",
       " '[16]',\n",
       " '[17]',\n",
       " '[19]',\n",
       " '[20]',\n",
       " '[21]',\n",
       " '[22]',\n",
       " '[24]',\n",
       " '[25]',\n",
       " '[26]',\n",
       " '[26]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[26]',\n",
       " '[25]',\n",
       " '[27]',\n",
       " '[25]',\n",
       " '[27]']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\[\\d+\\]\", background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "background_nofoot = re.sub(r\"\\[\\d+\\]\", \"\", background)\n",
    "# background_nofoot = re.sub(r'\\d+', '', background_nofoot)\n",
    "background_nofoot = re.sub(r'\\w*\\d\\w*', '', background_nofoot)\n",
    "#background_nofoot = re.sub(r'/[\\x00-\\x1F\\x7F\\xA0]/u',r'', background_nofoot) \n",
    "#background_nofoot.decode(\"utf-8\").encode(\"ascii\", \"ignore\")\n",
    "#background_nofoot = re.sub(r'[^\\x00-\\xff]','',background_nofoot)\n",
    "\n",
    "splitter = re.compile(r\"\"\"\n",
    "    [.!?]       # split on punctuation\n",
    "    \"\"\", re.VERBOSE)\n",
    "\n",
    "splitter = re.compile(r\"\"\"\n",
    "    (?<![A-Z])  # last character cannot be uppercase\n",
    "    [.!?]       # match punctuation\n",
    "    \\s+         # followed by whitespace\n",
    "    (?=[A-Z])   # next character must be uppercase\n",
    "    \"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for sentence in splitter.split(background_nofoot):\n",
    "#     print(sentence.strip())\n",
    "#     print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_splitter = re.compile(r\"\"\"\n",
    "    (\\w+)\n",
    "    \"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_words = [word_splitter.findall(sent)\n",
    "              for sent in splitter.split(background_nofoot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_words_lower = [[w.lower() for w in sent]\n",
    "                    for sent in sent_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1803"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_words_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allwords=[w for sent in sent_words_lower for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'abelian', 'about', 'abstract', 'accept', 'action', 'actions', 'adding', 'addition', 'additional', 'additionally', 'advancement', 'again', 'aggregate', 'ak', 'algebra', 'algebrab', 'algebraic', 'algebraically', 'algebras', 'algorithm', 'algorithms', 'all', 'allowing', 'allows', 'almost', 'along', 'also', 'although', 'always', 'american', 'an', 'analogue', 'analysis', 'and', 'angle', 'angles', 'another', 'answered', 'any', 'appeared', 'application', 'applications', 'approach', 'approximation', 'arbitrary', 'are', 'areas', 'arthur', 'article', 'articles', 'as', 'asked', 'assembled', 'assigned', 'associate', 'associated', 'associative', 'assume', 'assures', 'at', 'attempted', 'attention', 'av', 'aw', 'axiom', 'axiomatically', 'axioms', 'axis', 'ay', 'b', 'back', 'backlash', 'based', 'bases', 'basic', 'basically', 'basis', 'be', 'became', 'because', 'become', 'been', 'being', 'below', 'besides', 'best', 'better', 'between', 'beyond', 'bijective', 'bilinear', 'binary', 'book', 'both', 'branch', 'bring', 'but', 'by', 'c', 'calculation', 'call', 'called', 'can', 'cannot', 'cardinality', 'carefully', 'case', 'cauchy', 'cayley', 'central', 'centuries', 'century', 'characteristic', 'characterizing', 'checking', 'choice', 'chosen', 'clearly', 'closed', 'coefficient', 'coefficients', 'college', 'columns', 'combination', 'combinations', 'come', 'common', 'compatible', 'complete', 'complex', 'component', 'components', 'compositions', 'computations', 'compute', 'computer', 'computers', 'computing', 'concept', 'concepts', 'concerned', 'concerning', 'concrete', 'concretely', 'condition', 'conjugate', 'connection', 'consider', 'consideration', 'considered', 'considers', 'consist', 'consisting', 'consists', 'construct', 'constructed', 'constructible', 'construction', 'contains', 'continuous', 'convenience', 'convergence', 'converges', 'converging', 'coordinate', 'coordinates', 'correspond', 'cos', 'cosine', 'could', 'courses', 'cramer', 'crown', 'crucially', 'curriculum', 'd', 'data', 'dealing', 'decomposition', 'decompositions', 'define', 'defined', 'defines', 'defining', 'definition', 'denote', 'denotes', 'depend', 'dependent', 'describe', 'described', 'det', 'determinant', 'determinants', 'determine', 'determined', 'developed', 'development', 'diagonal', 'diagonalizable', 'different', 'differential', 'dim', 'dimension', 'dimensional', 'disciplines', 'discontinuous', 'discussed', 'distinct', 'distinction', 'distinguishes', 'divide', 'dividing', 'do', 'does', 'dual', 'during', 'e', 'each', 'easily', 'easy', 'educators', 'efficient', 'efficiently', 'eigenbasis', 'eigenvalue', 'eigenvalues', 'eigenvector', 'eigenvectors', 'elements', 'eliminate', 'eliminated', 'elimination', 'emerged', 'en', 'encode', 'encoded', 'encompasses', 'engineering', 'england', 'enough', 'entries', 'equal', 'equation', 'equations', 'equipped', 'especially', 'essential', 'essentially', 'even', 'every', 'exactly', 'example', 'examples', 'exist', 'exists', 'expandable', 'expansion', 'explicit', 'express', 'extended', 'extension', 'extremely', 'f', 'facilitates', 'false', 'faster', 'field', 'fields', 'find', 'finding', 'finite', 'first', 'fit', 'fixed', 'fn', 'following', 'follows', 'for', 'form', 'formal', 'formally', 'formed', 'formerly', 'forming', 'forms', 'formulated', 'foundational', 'four', 'fourier', 'france', 'free', 'from', 'function', 'functional', 'functionals', 'functions', 'fundamental', 'further', 'furthermore', 'gabriel', 'gauss', 'gaussian', 'general', 'generalized', 'geodesy', 'geometric', 'geometry', 'get', 'given', 'gives', 'giving', 'gn', 'goal', 'grade', 'graduate', 'grassmann', 'group', 'guaranteed', 'guarantees', 'h', 'had', 'half', 'has', 'have', 'he', 'helped', 'here', 'hermann', 'hermitian', 'high', 'his', 'hk', 'hn', 'homogeneous', 'how', 'hyperplanes', 'i', 'idea', 'ideas', 'identification', 'identified', 'identify', 'identity', 'if', 'image', 'images', 'important', 'in', 'include', 'included', 'including', 'increased', 'indeed', 'independence', 'independent', 'indication', 'inequality', 'initially', 'inner', 'inquiry', 'instance', 'instead', 'intercept', 'interested', 'interesting', 'interpretation', 'intersection', 'into', 'introduced', 'introduction', 'invariant', 'inverse', 'inverses', 'inversion', 'involving', 'is', 'isomorphic', 'isomorphism', 'issues', 'it', 'its', 'itself', 'j', 'james', 'jewel', 'joseph', 'just', 'kernel', 'know', 'known', 'last', 'later', 'latin', 'leads', 'least', 'led', 'leibniz', 'length', 'let', 'letter', 'like', 'line', 'linear', 'linearly', 'lines', 'lipschitz', 'list', 'listed', 'little', 'low', 'lp', 'm', 'main', 'major', 'make', 'manner', 'many', 'map', 'mapped', 'mapping', 'mappings', 'maps', 'mathematical', 'mathematics', 'matrices', 'matrix', 'may', 'me', 'means', 'mechanics', 'met', 'method', 'methods', 'mid', 'might', 'minimize', 'mining', 'mixes', 'modeled', 'modeling', 'modelling', 'models', 'modern', 'module', 'modules', 'more', 'moreover', 'most', 'mostly', 'much', 'multilinear', 'multiple', 'multiples', 'multiplication', 'multivariable', 'must', 'n', 'natural', 'naturally', 'nature', 'necessarily', 'nevertheless', 'new', 'next', 'nice', 'no', 'non', 'nonlinear', 'nontrivial', 'nonzero', 'normal', 'not', 'note', 'notice', 'now', 'nullspace', 'number', 'numbers', 'nx', 'object', 'objects', 'obtain', 'obtained', 'of', 'often', 'on', 'once', 'one', 'only', 'operates', 'operation', 'operations', 'operator', 'opposed', 'or', 'order', 'ordered', 'orientation', 'origin', 'orthogonal', 'orthonormal', 'other', 'our', 'out', 'outputs', 'over', 'p', 'pair', 'pairs', 'parameter', 'parameterized', 'part', 'partial', 'particular', 'particularly', 'parts', 'pasha', 'peano', 'phenomena', 'planar', 'plane', 'planes', 'point', 'points', 'polynomial', 'polynomials', 'possible', 'precede', 'precise', 'precisely', 'presentations', 'preserve', 'preserves', 'previous', 'principles', 'product', 'products', 'programming', 'properties', 'property', 'prove', 'provides', 'published', 'pure', 'put', 'quantity', 'quantum', 'question', 'quite', 'r', 'range', 'rank', 'rationals', 'real', 'realized', 'rearrange', 'recognize', 'recommended', 'redundant', 'refers', 'relabeled', 'related', 'relative', 'relativity', 'remain', 'remove', 'removed', 'replaces', 'represent', 'representation', 'representations', 'represented', 'representing', 'research', 'reserved', 'residuals', 'respect', 'respectively', 'restricts', 'result', 'reverse', 'reviews', 'ring', 'rn', 'rotations', 'rows', 'rule', 's', 'same', 'satisfies', 'satisfy', 'satisfying', 'say', 'scalar', 'scalars', 'school', 'schools', 'science', 'sciences', 'second', 'secondary', 'see', 'seeing', 'seeks', 'seems', 'seen', 'select', 'sense', 'sequences', 'series', 'set', 'sets', 'setting', 'several', 'should', 'shows', 'significance', 'similar', 'similarly', 'simple', 'simpler', 'simulations', 'sin', 'since', 'single', 'singular', 'so', 'social', 'solution', 'solutions', 'solve', 'solved', 'solving', 'some', 'space', 'spaces', 'span', 'spans', 'speaking', 'special', 'specific', 'spread', 'square', 'squares', 'standard', 'states', 'statistics', 'still', 'strategy', 'stress', 'structure', 'structures', 'students', 'studies', 'study', 'studying', 'subject', 'submatrix', 'subsequently', 'subset', 'subsets', 'subspace', 'subspaces', 'substituted', 'substitution', 'successful', 'such', 'suit', 'sum', 'suppose', 'svd', 'sylvester', 'symmetric', 'system', 'systematic', 'systems', 't', 'take', 'takes', 'taking', 'teach', 'teaching', 'technically', 'technique', 'techniques', 'tensor', 'term', 'terms', 'tevfik', 'textbooks', 'that', 'the', 'their', 'them', 'themselves', 'then', 'theorem', 'theorems', 'theoretical', 'theories', 'theory', 'there', 'therefore', 'these', 'they', 'things', 'third', 'this', 'those', 'three', 'through', 'thus', 'titled', 'to', 'today', 'took', 'tool', 'tools', 'topics', 'transform', 'transformation', 'transformations', 'translated', 'treating', 'triangular', 'trigonometric', 'true', 'tt', 'turns', 'tv', 'twentieth', 'two', 'types', 'typically', 'u', 'ubiquity', 'uncertainty', 'under', 'undergraduate', 'unique', 'unit', 'unknown', 'unknowns', 'unnatural', 'upon', 'use', 'used', 'useful', 'using', 'usually', 'utilize', 'v', 'value', 'values', 'variables', 'variety', 'various', 'vector', 'vectors', 'via', 'view', 'viewed', 'visualization', 'vk', 'vn', 'w', 'was', 'way', 'ways', 'we', 'well', 'were', 'what', 'when', 'where', 'whereas', 'whether', 'which', 'while', 'will', 'with', 'womb', 'work', 'would', 'write', 'written', 'wrote', 'x', 'xbb', 'xbbn', 'xbbv', 'xbcseyin', 'xce', 'xcf', 'xi', 'y', 'year', 'yield', 'yields', 'yj', 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "terms = sorted(set(allwords))\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
