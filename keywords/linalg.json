{
    "pages": [
        {
            "links": [
                "/wiki/Algebraic_geometry",
                "/wiki/Systems_of_polynomial_equations",
                "/wiki/Representation_theory",
                "/wiki/Functional_analysis",
                "/wiki/Mathematical_analysis",
                "/wiki/Lp_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Multilinear_algebra",
                "/wiki/Dual_space",
                "/wiki/Tensor_product",
                "/wiki/Module_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Free_module",
                "/wiki/Linear_map",
                "/wiki/Linear_functional",
                "/wiki/Cramer%27s_rule",
                "/wiki/System_of_linear_equations",
                "/wiki/Homogeneous_coordinates",
                "/wiki/Linear_equation",
                "/wiki/Inner_product_space",
                "/wiki/Partial_differential_equation",
                "/wiki/Dirichlet_conditions",
                "/wiki/Fourier_series",
                "/wiki/Linear_least_squares_(mathematics)",
                "/wiki/Triangular_form",
                "/wiki/Normal_matrix",
                "/wiki/Hermitian_conjugate",
                "/wiki/Cauchy%E2%80%93Schwarz_inequality",
                "/wiki/Axiom",
                "/wiki/Inner_product",
                "/wiki/Bilinear_form",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Complex_number",
                "/wiki/Invariant_(mathematics)#Invariant_set",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Characteristic_value",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/Inverse_element",
                "/wiki/Nullspace",
                "/wiki/Cramer%27s_rule",
                "/wiki/Gaussian_elimination",
                "/wiki/Standard_basis",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Coordinate_system",
                "/wiki/Linear_combination",
                "/wiki/Cardinality",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Well-defined",
                "/wiki/Dimension_theorem_for_vector_spaces",
                "/wiki/Linearly_independent",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Axiom_of_choice",
                "/wiki/Rationals",
                "/wiki/Linear_span",
                "/wiki/Linear_subspace",
                "/wiki/Nullspace",
                "/wiki/Linear_combination",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Origin_(mathematics)",
                "/wiki/Bijective",
                "/wiki/Isomorphic",
                "/wiki/Determinant",
                "/wiki/Range_(mathematics)",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Linear_transformation",
                "/wiki/Map_(mathematics)",
                "/wiki/Abelian_group",
                "/wiki/Sequence",
                "/wiki/Function_(mathematics)",
                "/wiki/Polynomial_ring",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Set_(mathematics)",
                "/wiki/Binary_operation",
                "/wiki/Element_(mathematics)",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Axiom",
                "/wiki/School_Mathematics_Study_Group",
                "/wiki/Secondary_school",
                "/wiki/Singular_value_decomposition",
                "/wiki/Determinants",
                "/wiki/Gaussian_elimination",
                "/wiki/H%C3%BCseyin_Tevfik_Pasha",
                "/wiki/Peano",
                "/wiki/Abstract_algebra",
                "/wiki/Quantum_mechanics",
                "/wiki/Special_relativity",
                "/wiki/Statistics",
                "/wiki/Algorithm",
                "/wiki/Hermann_Grassmann",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Arthur_Cayley",
                "/wiki/Determinant",
                "/wiki/Systems_of_linear_equations",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Gabriel_Cramer",
                "/wiki/Cramer%27s_Rule",
                "/wiki/Gauss",
                "/wiki/Gaussian_elimination",
                "/wiki/Geodesy",
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Functional_analysis",
                "/wiki/Engineering",
                "/wiki/Mathematical_model",
                "/wiki/Nonlinear_system",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Mathematics",
                "/wiki/Linear_equation"
            ],
            "text": "There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to.Algebraic geometry considers the solutions of systems of polynomial equations.Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \u00d7 V \u2192 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V\u2217 consisting of linear maps f: V \u2192 F where F is the field of scalars. Multilinear maps T: Vn \u2192 F can be described via tensor products of elements of V\u2217.Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.The set of points of a linear functional that map to zero define the kernel of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.[25][27]Notice that if h is a solution to this homogeneous equation, then t h is also a solution.The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,For convenience the free parameter x has been relabeled t.Solve for y and obtain the inverse image as the set of points,In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equationNotice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c.The set of points in the plane E that map to the same image in R under the linear functional \u03bb define a line in E. This line is the image of the inverse map, \u03bb\u22121: R\u2192E. This inverse image is the set of the points x=(x, y) that solve the equation,Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.[25][27]These equations can be assembled into the single matrix equation,These coordinate functionals have the properties,which can be written in matrix form asThe functionals \u03c3 and \u03c4 compute the components of x along the basis vectors v and w, respectively, that is,To solve this equation for \u03b1, \u03b2, we compute the linear coordinate functionals \u03c3 and \u03c4 for the basis v, w, which are given by,[26]This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E. Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is to find the real numbers \u03b1, \u03b2, so that x=\u03b1v+\u03b2w, that iswhere Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form asThis is true for any pair of vectors used to define coordinates in E. Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E. This means a vector x has coordinates (\u03b1,\u03b2), such that x=\u03b1v+\u03b2w. Then, we have the linear functionalThus, the columns of the matrix A are the image of the basis vectors of E in R.Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj. It is now possible to see thatThis shows that the sum of vectors in E map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation.[25] For this case, where the image space is a real number the map is called a linear functional.[27]This transformation has the important property that if Ay=d, thenorAnother way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in E and consider the linear function \u03bb: E\u2192R, given byClearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E. For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.which in homogeneous form yields,It is interesting to consider the case of three lines, \u03bb1, \u03bb2 and \u03bb3, which yield the matrix equation,if the rows of B are linearly independent (i.e., \u03bb1 and \u03bb2 represent distinct lines). Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns.[27] Notice that this yields a point in the z = 1 plane only when the 2\u2009\u00d7\u20092 submatrix associated with x3 has a non-zero determinant.The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates, the solutions are multiples of the following solution:[26]or using homogeneous coordinates,which forms a system of linear equations. The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,Now consider the equations of the two lines \u03bb1 and \u03bb2,The linear equation, \u03bb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \u03b1x1 + \u03b2x2 is also on the line, for any real \u03b1 and \u03b2.Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space. The x\u2212y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1).where x = (x, y, 1) is the 3\u2009\u00d7\u20091 set of homogeneous coordinates associated with the point (x, y).[26]orwhere a, b and c are not all zero. Then,Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E. When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \u2265 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient ak, we take the inner product with hk:The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner productThis series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.Fourier series are a representation of a function f: [\u2212\u03c0, \u03c0] \u2192 R as a trigonometric series:The least squares method is used to determine the best-fit line for a set of data.[24] This line will minimize the sum of the squares of the residuals.We can, in general, write any system of linear equations as a matrix equation:The system is solved.Next, z and y can be substituted into L1, which can be solved to obtainThen, z can be substituted into L2, which can then be solved to obtainThe last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen thatThis result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.The result is:Now y is eliminated from L3 by adding \u22124L2 to L3:The result is:In the example, x is eliminated from L2 by adding (3/2)L1 to L2. x is then eliminated from L3 by adding L1 to L3. Formally:The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.If T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfyingand so we can call this quantity the cosine of the angle between the two vectors.In particular, the quantityand we can prove the Cauchy\u2013Schwarz inequality:We can define the length of a vector v in V byNote that in R, it is symmetric.that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a mapSuch a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).where I is the identity matrix. For there to be nontrivial solutions to that equation, det(T \u2212 \u03bb I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ..., vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \u03bb1, \u03bb2, ..., \u03bbn, and if v = a1v1 + ... + an vn, then,To find an eigenvector or an eigenvalue, we note thatIn general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T. If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector. The scalar \u03bb such that Tv = \u03bbv is called a characteristic value or eigenvalue of T.One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20]). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V. While Rn has a standard basis {e1, e2, ..., en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V).The condition that v1, v2, ..., vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ..., vn assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to v). In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn. Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn. Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \u2192 W may be encoded by an m \u00d7 n matrix A with entries in the field F, called the matrix of T with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.A particular basis {v1, v2, ..., vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ..., an) is the linear combinationOne often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism.Any two bases of a vector space V have the same cardinality, which is called the dimension of V. The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of V has finite number of elements, V is called a finite-dimensional vector space. If V is finite-dimensional and U is a subspace of V, then dim U \u2264 dim V. If U1 and U2 are subspaces of V, thenA linear combination of any system of vectors with all zero coefficients is the zero vector of V. If this is the only way to express the zero vector as a linear combination of v1, v2, ..., vk then these vectors are linearly independent. Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V. Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis.[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.where a1, a2, ..., ak are scalars. The set of all linear combinations of vectors v1, v2, ..., vk is called their span, which forms a subspace.Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ..., vk:Linear transformations have geometric significance. For example, 2 \u00d7 2 real matrices denote standard planar mappings that preserve the origin.When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.Additionally for any vectors u, v \u2208 V and scalars a, b \u2208 F:for any vectors u,v \u2208 V and a scalar a \u2208 F.that is compatible with addition and scalar multiplication:Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a mapThe first four axioms are those of V being an abelian group under vector addition. Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.The main structures of linear algebra are vector spaces. A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The operations of addition and multiplication in a vector space must satisfy the following axioms.[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s.[7] Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s.[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum.[9] In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation.[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as 'so many other disciplines use it'.[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination.[13][14]The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.In 1882, H\u00fcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\".[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[4]The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".[4]The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations. Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.[4]Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation.and their representations through matrices and vector spaces.[1][2][3]linear functions such asLinear algebra is the branch of mathematics concerning linear equations such as",
            "title": "\nLinear algebra\n",
            "url": "https://en.wikipedia.org/wiki/Linear_algebra"
        },
        {
            "links": [
                "/wiki/Algebraic_statistics",
                "/wiki/Control_theory",
                "/wiki/Robotics",
                "/wiki/Algebraic_geometric_code",
                "/wiki/Computational_phylogenetics",
                "/wiki/Geometric_modelling",
                "/wiki/Homological_mirror_symmetry",
                "/wiki/Game_theory",
                "/wiki/Matching_(graph_theory)",
                "/wiki/Soliton",
                "/wiki/Integer_programming",
                "/wiki/Jean-Pierre_Serre",
                "/wiki/GAGA",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Analytic_variety",
                "/wiki/Analytic_function",
                "/wiki/Algebraic_variety",
                "/wiki/Complex_manifold",
                "/wiki/Mathematical_singularity",
                "/wiki/Derived_algebraic_geometry",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Bruno_Buchberger",
                "/wiki/Cylindrical_algebraic_decomposition",
                "/wiki/George_E._Collins",
                "/wiki/Abelian_variety",
                "/wiki/Group_(mathematics)",
                "/wiki/Elliptic_curve",
                "/wiki/Fermat%27s_last_theorem",
                "/wiki/Elliptic_curve_cryptography",
                "/wiki/Jean-Pierre_Serre",
                "/wiki/Alexander_Grothendieck",
                "/wiki/Sheaf_theory",
                "/wiki/Scheme_(mathematics)",
                "/wiki/Homological_algebra",
                "/wiki/Number_theory",
                "/wiki/Singularity_theory",
                "/wiki/Moduli_space",
                "/wiki/B._L._van_der_Waerden",
                "/wiki/Oscar_Zariski",
                "/wiki/Andr%C3%A9_Weil",
                "/wiki/Commutative_algebra",
                "/wiki/Valuation_theory",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Italian_school_of_algebraic_geometry",
                "/wiki/Generic_point",
                "/wiki/Commutative_algebra",
                "/wiki/Hilbert%27s_basis_theorem",
                "/wiki/Hilbert%27s_Nullstellensatz",
                "/wiki/Francis_Sowerby_Macaulay",
                "/wiki/Multivariate_resultant",
                "/wiki/Elimination_theory",
                "/wiki/Singularity_theory",
                "/wiki/Bernhard_Riemann",
                "/wiki/Riemann_surface",
                "/wiki/Non-Euclidean_geometry",
                "/wiki/Abelian_integral",
                "/wiki/Edmond_Laguerre",
                "/wiki/Arthur_Cayley",
                "/wiki/Quadratic_form",
                "/wiki/Felix_Klein",
                "/wiki/Transformation_group",
                "/wiki/Kleinian_geometry",
                "/wiki/Birational_transformation",
                "/wiki/Italian_school_of_algebraic_geometry",
                "/wiki/Algebraic_surface",
                "/wiki/Birational_isomorphism",
                "/wiki/G%C3%A9rard_Desargues",
                "/wiki/Synthetic_geometry",
                "/wiki/Projective_geometry",
                "/wiki/Analytic_geometry",
                "/wiki/Isaac_Newton",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Leonhard_Euler",
                "/wiki/Renaissance",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Niccol%C3%B2_Fontana_Tartaglia",
                "/wiki/Blaise_Pascal",
                "/wiki/Franciscus_Vieta",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Coordinate_geometry",
                "/wiki/Diophantine_equations",
                "/wiki/Hellenistic_Greece",
                "/wiki/Delian_problem",
                "/wiki/Menaechmus",
                "/wiki/Archimedes",
                "/wiki/Apollonius_of_Perga",
                "/wiki/Conic_sections",
                "/wiki/Mathematics_in_medieval_Islam",
                "/wiki/Ibn_al-Haytham",
                "/wiki/Persian_people",
                "/wiki/Omar_Khayy%C3%A1m",
                "/wiki/Cubic_equation",
                "/wiki/Sharaf_al-Din_al-Tusi",
                "/wiki/Grothendieck_site",
                "/wiki/Higher_category_theory",
                "/wiki/Derived_affine_scheme",
                "/wiki/Differential_graded_commutative_algebra",
                "/wiki/Quillen_model_category",
                "/wiki/Quasicategory",
                "/wiki/Derived_algebraic_geometry",
                "/wiki/Carlos_Simpson",
                "/wiki/Bertrand_To%C3%ABn",
                "/wiki/Jacob_Lurie",
                "/wiki/Bertrand_To%C3%ABn",
                "/wiki/Maxim_Kontsevich",
                "/wiki/Universal_algebraic_geometry",
                "/wiki/Variety_(universal_algebra)",
                "/wiki/Nikolai_Durov",
                "/wiki/Tropical_geometry",
                "/wiki/Absolute_geometry",
                "/wiki/Arakelov%27s_geometry",
                "/wiki/Alexander_Grothendieck",
                "/wiki/Scheme_(mathematics)",
                "/wiki/Grothendieck_topology",
                "/wiki/%C3%89tale_topology",
                "/wiki/Nisnevich_topology",
                "/wiki/Deligne-Mumford_stack",
                "/wiki/Formal_scheme",
                "/wiki/Ind-scheme",
                "/wiki/Algebraic_space",
                "/wiki/Algebraic_stack",
                "/wiki/Faug%C3%A8re%27s_F4_and_F5_algorithms",
                "/wiki/First-order_logic",
                "/wiki/Atomic_formula",
                "/wiki/Tarski%E2%80%93Seidenberg_theorem",
                "/wiki/Quantifier_elimination",
                "/wiki/Faug%C3%A8re_F5_algorithm",
                "/wiki/Regular_chain",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Ideal_(ring_theory)#Ideal_generated_by_a_set",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Numerical_algebraic_geometry",
                "/wiki/Homotopy_continuation",
                "/wiki/Floating_point",
                "/wiki/Marseille",
                "/wiki/Hilbert%27s_sixteenth_problem",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Homogeneous_ideal",
                "/wiki/Homogeneous_coordinate_ring",
                "/wiki/Integral_domain",
                "/wiki/Projective_space",
                "/wiki/Homogeneous_coordinates",
                "/wiki/B%C3%A9zout%27s_theorem",
                "/wiki/Projective_plane",
                "/wiki/Regular_point_of_an_algebraic_variety",
                "/wiki/Line_at_infinity",
                "/wiki/Cusp_(singularity)",
                "/wiki/Riemann-Roch_theorem_for_algebraic_curves",
                "/wiki/Cubic_curve",
                "/wiki/Parabola",
                "/wiki/Resolution_of_singularities",
                "/wiki/Smooth_completion",
                "/wiki/Heisuke_Hironaka",
                "/wiki/Function_inverse",
                "/wiki/Ring_homomorphism",
                "/wiki/Integral_domain",
                "/wiki/Field_of_fractions",
                "/wiki/Function_field_of_an_algebraic_variety",
                "/wiki/Rational_function",
                "/wiki/Domain_of_a_function",
                "/wiki/Complement_(set_theory)",
                "/wiki/Ring_homomorphism",
                "/wiki/Equivalence_of_categories",
                "/wiki/Dual_(category_theory)",
                "/wiki/Reduced_ring",
                "/wiki/Scheme_theory",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Morphism_of_algebraic_varieties",
                "/wiki/Image_(mathematics)",
                "/wiki/Coordinate_ring",
                "/wiki/Normal_space",
                "/wiki/Topological_space",
                "/wiki/Tietze_extension_theorem",
                "/wiki/Continuous_function",
                "/wiki/Topological_space",
                "/wiki/Smooth_function",
                "/wiki/Differentiable_manifold",
                "/wiki/Analytic_function",
                "/wiki/Irreducible_component",
                "/wiki/Algebraic_variety",
                "/wiki/Prime_ideal",
                "/wiki/Hilbert%27s_basis_theorem",
                "/wiki/Zariski_topology",
                "/wiki/Hilbert%27s_Nullstellensatz",
                "/wiki/Radical_of_an_ideal",
                "/wiki/Galois_connection",
                "/wiki/Closure_operator",
                "/wiki/Galois_connection#Examples",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Polynomial_function",
                "/wiki/Field_(mathematics)",
                "/wiki/Algebraically_closed_field",
                "/wiki/Affine_space",
                "/wiki/Polynomial",
                "/wiki/Systems_of_polynomial_equations",
                "/wiki/N-sphere",
                "/wiki/Sphere",
                "/wiki/Euclidean_space",
                "/wiki/Topology",
                "/wiki/Differential_geometry",
                "/wiki/Complex_geometry",
                "/wiki/Grothendieck",
                "/wiki/Scheme_theory",
                "/wiki/Sheaf_theory",
                "/wiki/Differential_manifold",
                "/wiki/Complex_manifold",
                "/wiki/Hilbert%27s_Nullstellensatz",
                "/wiki/Maximal_ideal",
                "/wiki/Coordinate_ring",
                "/wiki/Wiles%27s_proof_of_Fermat%27s_Last_Theorem",
                "/wiki/Fermat%27s_last_theorem",
                "/wiki/Complex_analysis",
                "/wiki/Topology",
                "/wiki/Number_theory",
                "/wiki/Systems_of_polynomial_equations",
                "/wiki/Equation_solving",
                "/wiki/Algebraic_variety",
                "/wiki/Solution_set",
                "/wiki/Systems_of_polynomial_equations",
                "/wiki/Plane_algebraic_curve",
                "/wiki/Line_(geometry)",
                "/wiki/Circle",
                "/wiki/Parabola",
                "/wiki/Ellipse",
                "/wiki/Hyperbola",
                "/wiki/Cubic_curve",
                "/wiki/Elliptic_curve",
                "/wiki/Lemniscate_of_Bernoulli",
                "/wiki/Cassini_oval",
                "/wiki/Polynomial_equation",
                "/wiki/Singular_point_of_a_curve",
                "/wiki/Inflection_point",
                "/wiki/Point_at_infinity",
                "/wiki/Topology",
                "/wiki/Mathematics",
                "/wiki/Zero_of_a_function",
                "/wiki/Multivariate_polynomial",
                "/wiki/Commutative_algebra",
                "/wiki/Geometry"
            ],
            "text": "Algebraic geometry now finds applications in statistics,[8] control theory,[9][10] robotics,[11] error-correcting codes,[12] phylogenetics[13] and geometric modelling.[14] There are also connections to string theory,[15] game theory,[16] graph matchings,[17] solitons[18] and integer programming.[19]Modern analytic geometry is essentially equivalent to real and complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper GAGA, the name of which is French for Algebraic geometry and analytic geometry. Nevertheless, the two fields remain distinct, as the methods of proof are quite different and algebraic geometry includes also geometry in finite characteristic.An analytic variety is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the included concept of real or complex algebraic variety. Any complex manifold is an analytic variety. Since analytic varieties may have singular points, not all analytic varieties are manifolds.See also: derived algebraic geometry.In parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gr\u00f6bner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.An important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's last theorem and are also used in elliptic curve cryptography.In the 1950s and 1960s Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely led by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities and moduli.B. L. van der Waerden, Oscar Zariski and Andr\u00e9 Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.In the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connexion between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.[7]The second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.It took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of homogeneous polynomial forms, and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.During the same period, Blaise Pascal and G\u00e9rard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek ruler and compass construction. Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the calculus of infinitesimals of Lagrange and Euler.Such techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccol\u00f2 Fontana \"Tartaglia\" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry.[6] The French mathematicians Franciscus Vieta and later Ren\u00e9 Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of algebraic curves, such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).Some of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length x so that the cube of side x contained the same volume as the rectangular box a2b for given sides a and b. Menaechmus (circa 350 BC) considered the problem geometrically by intersecting the pair of plane conics ay\u00a0=\u00a0x2 and xy\u00a0=\u00a0ab.[1] The later work, in the 3rd century BC, of Archimedes and Apollonius studied more systematically problems on conic sections,[2] and also involved the use of coordinates.[1] The Arab mathematicians were able to solve by purely algebraic means certain cubic equations, and then to interpret the results geometrically. This was done, for instance, by Ibn al-Haytham in the 10th century AD.[3] Subsequently, Persian mathematician Omar Khayy\u00e1m (born 1048 A.D.) discovered a method for solving cubic equations by intersecting a parabola with a circle.[4] A few years after Omar Khayy\u00e1m, Sharaf al-Din al-Tusi's \"Treatise on equations\" has been described as inaugurating the beginning of algebraic geometry.[5]Algebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the derived algebraic geometry, introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand To\u00ebn, Gabrielle Vezzosi, Michel Vaqui\u00e9 and others; and developed further by Jacob Lurie, Bertrand To\u00ebn, and Gabrielle Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from early 1990s by Maxim Kontsevich and followers.The language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.Another formal generalization is possible to universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term variety of algebras should not be confused with algebraic variety.Sometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup.Most remarkably, in late 1950s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field k, and the category of finitely generated reduced k-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the \u00e9tale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne-Mumford stacks, both often called algebraic stacks.The modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.Among these algorithms which solve a sub problem of the problems solved by Gr\u00f6bner bases, one may cite testing if an affine variety is empty and solving nonhomogeneous polynomial systems which have a finite number of solutions. Such algorithms are rarely implemented because, on most entries Faug\u00e8re's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity (probably because the evaluation of the complexity of Gr\u00f6bner basis algorithms on a particular class of entries is a difficult task which has been done only in a few special cases).As an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand, CAD is yet, in practice, the best algorithm to count the number of connected components.Since 1973, most of the research on this subject is devoted either to improve CAD or to find alternate algorithms in special cases of general interest.While Gr\u00f6bner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.The complexity of CAD is doubly exponential in the number of variables. This means that CAD allows, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.This theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators and (\u2227), or (\u2228), not (\u00ac), for all (\u2200) and exists (\u2203). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (\u2200, \u2203).CAD is an algorithm which was introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski\u2013Seidenberg theorem on quantifier elimination over the real numbers.Gr\u00f6bner bases are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faug\u00e8re F5 algorithm realizes this complexity, as it may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow one to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gr\u00f6bner basis is strongly related to the intrinsic difficulty of the problem.Gr\u00f6bner basis computations do not allow one to compute directly the primary decomposition of I nor the prime ideals defining the irreducible components of V, but most algorithms for this involve Gr\u00f6bner basis computation. The algorithms which are not based on Gr\u00f6bner bases use regular chains but may need Gr\u00f6bner bases in some exceptional situations.Given an ideal I defining an algebraic set V:A Gr\u00f6bner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.A body of mathematical theory complementary to symbolic methods called numerical algebraic geometry has been developed over the last several decades. The main computational method is homotopy continuation. This supports, for example, a model of floating point computation for solving problems of algebraic geometry.Since then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.One may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France in June 1979. At this meeting,One of the challenging problems of real algebraic geometry is the unsolved Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8.Real algebraic geometry is the study of the real points of algebraic geometry.The only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand, the field of the rational functions or function field is a useful notion, which, similarly to the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.A polynomial in n + 1 variables vanishes at all points of a line passing through the origin if and only if it is homogeneous. In this case, one says that the polynomial vanishes at the corresponding point of Pn. This allows us to define a projective algebraic set in Pn as the set V(f1, ..., fk), where a finite set of homogeneous polynomials {f1, ..., fk} vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The projective varieties are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the projective coordinates ring being defined as the quotient of the graded ring or the polynomials in n + 1 variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.Nowadays, the projective space Pn of dimension n is usually defined as the set of the lines passing through a point, considered as the origin, in the affine space of dimension n + 1, or equivalently to the set of the vector lines in a vector space of dimension n + 1. When a coordinate system has been chosen in the space of dimension n + 1, all the points of a line have the same set of coordinates, up to the multiplication by an element of k. This defines the homogeneous coordinates of a point of Pn as a sequence of n + 1 elements of the base field k, defined up to the multiplication by a nonzero element of k (the same for the whole sequence).Thus many of the properties of algebraic varieties, including birational equivalence and all the topological properties, depend on the behavior \"at infinity\" and so it is natural to study the varieties in projective space. Furthermore, the introduction of projective techniques made many theorems in algebraic geometry simpler and sharper: For example, B\u00e9zout's theorem on the number of intersection points between two varieties can be stated in its sharpest form only in projective space. For these reasons, projective space plays a fundamental role in algebraic geometry.The consideration of the projective completion of the two curves, which is their prolongation \"at infinity\" in the projective plane, allows us to quantify this difference: the point at infinity of the parabola is a regular point, whose tangent is the line at infinity, while the point at infinity of the cubic curve is a cusp. Also, both curves are rational, as they are parameterized by x, and the Riemann-Roch theorem implies that the cubic curve must have a singularity, which must be at infinity, as all its points in the affine space are regular.Compare this to the variety V(y\u00a0\u2212\u00a0x3). This is a cubic curve. As x goes to positive infinity, the slope of the line from the origin to the point (x,\u00a0x3) goes to positive infinity just as before. But unlike before, as x goes to negative infinity, the slope of the same line goes to positive infinity as well; the exact opposite of the parabola. So the behavior \"at infinity\" of V(y\u00a0\u2212\u00a0x3) is different from the behavior \"at infinity\" of V(y\u00a0\u2212\u00a0x2).To see how this might come about, consider the variety V(y \u2212 x2). If we draw it, we get a parabola. As x goes to positive infinity, the slope of the line from the origin to the point (x,\u00a0x2) also goes to positive infinity. As x goes to negative infinity, the slope of the same line goes to negative infinity.Just as the formulas for the roots of second, third, and fourth degree polynomials suggest extending real numbers to the more algebraically complete setting of the complex numbers, many properties of algebraic varieties suggest extending affine space to a more geometrically complete projective space. Whereas the complex numbers are obtained by adding the number i, a root of the polynomial x2 + 1, projective space is obtained by adding in appropriate points \"at infinity\", points where parallel lines may meet.The problem of resolution of singularities is to know if every algebraic variety is birationally equivalent to a variety whose projective completion is nonsingular (see also smooth completion). It was solved in the affirmative in characteristic 0 by Heisuke Hironaka in 1964 and is yet unsolved in finite characteristic.which may also be viewed as a rational map from the line to the circle.Two affine varieties are birationally equivalent if there are two rational functions between them which are inverse one to the other in the regions where both are defined. Equivalently, they are birationally equivalent if their function fields are isomorphic.As with regular maps, one may define a rational map from a variety V to a variety V'. As with the regular maps, the rational maps from V to V' may be identified to the field homomorphisms from k(V') to k(V).If V is an affine variety, its coordinate ring is an integral domain and has thus a field of fractions which is denoted k(V) and called the field of the rational functions on V or, shortly, the function field of V. Its elements are the restrictions to V of the rational functions over the affine space containing V. The domain of a rational function f is not V but the complement of the subvariety (a hypersurface) where the denominator of f vanishes.In contrast to the preceding sections, this section concerns only varieties and not algebraic sets. On the other hand, the definitions extend naturally to projective varieties (next section), as an affine variety and its projective completion have the same field of functions.Given a regular map g from V to V\u2032 and a regular function f of k[V\u2032], then f \u2218 g \u2208 k[V]. The map f \u2192 f \u2218 g is a ring homomorphism from k[V\u2032] to k[V]. Conversely, every ring homomorphism from k[V\u2032] to k[V] defines a regular map from V to V\u2032. This defines an equivalence of categories between the category of algebraic sets and the opposite category of the finitely generated reduced k-algebras. This equivalence is one of the starting points of scheme theory.The definition of the regular maps apply also to algebraic sets. The regular maps are also called morphisms, as they make the collection of all affine algebraic sets into a category, where the objects are the affine algebraic sets and the morphisms are the regular maps. The affine varieties is a subcategory of the category of the algebraic sets.If V\u2032 is a variety contained in Am, we say that f is a regular map from V to V\u2032 if the range of f is contained in V\u2032.Using regular functions from an affine variety to A1, we can define regular maps from one affine variety to another. First we will define a regular map from a variety into affine space: Let V be a variety contained in An. Choose m regular functions on V, and call them f1, ..., fm. We define a regular map f from V to Am by letting f = (f1, ..., fm). In other words, each fi determines one coordinate of the range of f.Since regular functions on V come from regular functions on An, there is a relationship between the coordinate rings. Specifically, if a regular function on V is the restriction of two functions f and g in k[An], then f\u00a0\u2212\u00a0g is a polynomial function which is null on V and thus belongs to I(V). Thus k[V] may be identified with k[An]/I(V).Just as with the regular functions on affine space, the regular functions on V form a ring, which we denote by k[V]. This ring is called the coordinate ring of V.It may seem unnaturally restrictive to require that a regular function always extend to the ambient space, but it is very similar to the situation in a normal topological space, where the Tietze extension theorem guarantees that a continuous function on a closed subset always extends to the ambient topological space.Just as continuous functions are the natural maps on topological spaces and smooth functions are the natural maps on differentiable manifolds, there is a natural class of functions on an algebraic set, called regular functions or polynomial functions. A regular function on an algebraic set V contained in An is the restriction to V of a regular function on An. For an algebraic set defined on the field of the complex numbers, the regular functions are smooth and even analytic.Some authors do not make a clear distinction between algebraic sets and varieties and use irreducible variety to make the distinction when needed.An algebraic set is called irreducible if it cannot be written as the union of two smaller algebraic sets. Any algebraic set is a finite union of irreducible algebraic sets and this decomposition is unique. Thus its elements are called the irreducible components of the algebraic set. An irreducible algebraic set is also called a variety. It turns out that an algebraic set is a variety if and only if it may be defined as the vanishing set of a prime ideal of the polynomial ring.For various reasons we may not always want to work with the entire ideal corresponding to an algebraic set U. Hilbert's basis theorem implies that ideals in k[An] are always finitely generated.The answer to the first question is provided by introducing the Zariski topology, a topology on An whose closed sets are the algebraic sets, and which directly reflects the algebraic structure of k[An]. Then U = V(I(U)) if and only if U is an algebraic set or equivalently a Zariski-closed set. The answer to the second question is given by Hilbert's Nullstellensatz. In one of its forms, it says that I(V(S)) is the radical of the ideal generated by S. In more abstract language, there is a Galois connection, giving rise to two closure operators; they can be identified, and naturally play a basic role in the theory; the example is elaborated at Galois connection.Two natural questions to ask are:Given a subset U of An, can one recover the set of polynomials which generate it? If U is any subset of An, define I(U) to be the set of all polynomials whose vanishing set contains U. The I stands for ideal: if two polynomials f and g both vanish on U, then f+g vanishes on U, and if h is any polynomial, then hf vanishes on U, so I(U) is always an ideal of the polynomial ring k[An].A subset of An which is V(S), for some S, is called an algebraic set. The V stands for variety (a specific type of algebraic set to be defined below).We say that a polynomial vanishes at a point if evaluating it at that point gives zero. Let S be a set of polynomials in k[An]. The vanishing set of S (or vanishing locus or zero set) is the set V(S) of all points in An where every polynomial in S vanishes. Symbolically,When a coordinate system is chosen, the regular functions on the affine n-space may be identified with the ring of polynomial functions in n variables over k. Therefore, the set of the regular functions on An is a ring, which is denoted k[An].A function f\u00a0: An \u2192 A1 is said to be polynomial (or regular) if it can be written as a polynomial, that is, if there is a polynomial p in k[x1,...,xn] such that f(M) = p(t1,...,tn) for every point M with coordinates (t1,...,tn) in An. The property of a function to be polynomial (or regular) does not depend on the choice of a coordinate system in An.First we start with a field k. In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that k is algebraically closed. We consider the affine space of dimension n over k, denoted An(k) (or more simply An, when k is clear from the context). When one fixes a coordinate system, one may identify An(k) with kn. The purpose of not working with kn is to emphasize that one \"forgets\" the vector space structure that kn carries.A \"slanted\" circle in R3 can be defined as the set of all points (x,y,z) which satisfy the two polynomial equationsIn classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere of radius 1 in three-dimensional Euclidean space R3 could be defined as the set of all points (x,y,z) withMuch of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on \"intrinsic\" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles's proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.In the 20th century, algebraic geometry split into several subareas.Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving geometrical problems about these sets of zeros.",
            "title": "\nAlgebraic geometry\n",
            "url": "https://en.wikipedia.org/wiki/Algebraic_geometry"
        },
        {
            "links": [
                "/wiki/Regular_semi-algebraic_system",
                "/wiki/Maple_(software)",
                "/wiki/RegularChains",
                "/wiki/Regular_chain",
                "/wiki/Maple_(software)",
                "/wiki/MPSolve",
                "/wiki/MPSolve",
                "/wiki/Maple_(software)",
                "/wiki/Maple_(software)",
                "/wiki/Floating_point",
                "/wiki/Homotopy",
                "/wiki/System_of_nonlinear_equations",
                "/wiki/Primary_decomposition",
                "/wiki/Radical_of_an_ideal",
                "/wiki/Modular_arithmetic",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Monomial_order",
                "/wiki/Triangular_decomposition",
                "/wiki/Regular_chain",
                "/wiki/Regular_semi-algebraic_system",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Regular_chain",
                "/wiki/Algebraic_closure",
                "/wiki/Rational_number",
                "/wiki/Cylindrical_algebraic_decomposition",
                "/wiki/Algebraic_geometry",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Monomial_order",
                "/wiki/Monomial_order#Graded_reverse_lexicographic_order",
                "/wiki/B%C3%A9zout%27s_theorem",
                "/wiki/Zero-dimensional_space",
                "/wiki/Algebraic_variety",
                "/wiki/Dimension_of_an_algebraic_variety",
                "/wiki/Underdetermined_system",
                "/wiki/Overdetermined_system",
                "/wiki/Inconsistent_equations",
                "/wiki/Hilbert%27s_Nullstellensatz",
                "/wiki/Linear_combination",
                "/wiki/Number_field",
                "/wiki/Trigonometric_polynomial",
                "/wiki/Algebraically_closed",
                "/wiki/Field_extension",
                "/wiki/Rational_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Finite_field",
                "/wiki/Simultaneous_equations",
                "/wiki/Polynomial_ring",
                "/wiki/Field_(mathematics)"
            ],
            "text": "While the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm[17] is based on the concept of a regular semi-algebraic system.The command RegularChains[RealTriangularize] is part of the Maple library RegularChains, written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command RegularChains[Triangularize] for computing the solutions whose coordinates are in the algebraic closure of the coefficient field. The underlying algorithms are based on the notion of a regular chain.The fourth solver is the Maple command RegularChains[RealTriangularize]. For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.The third solver is Bertini,[15][16] written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision. In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.The second solver is PHCpack,[11][14] written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.To extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.The rational univariate representation may be computed with Maple function Groebner[RationalUnivariateRepresentation].Internally, this solver, designed by F. Rouillier computes first a Gr\u00f6bner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.The Maple function RootFinding[Isolate] takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.There are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.The roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.To deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.Then a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the systemThis method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore, it is computed by, at least, four different methods and the best value, say N, is kept.This is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades.[11]Nevertheless, two methods deserve to be mentioned here.The general numerical algorithms which are designed for any system of nonlinear equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow one to find all solutions. In particular, when a general method does not find any solution, this is usually not an indication that there is no solution.Contrarily to triangular decompositions and equiprojectable decompositions, the RUR is not defined in positive dimension.Moreover, the univariate polynomial h(x0) of the RUR may be factorized, and this gives a RUR for every irreducible factor. This provides the prime decomposition of the given ideal (that is the primary decomposition of the radical of the ideal). In practice, this provides an output with much smaller coefficients, especially in the case of systems with high multiplicities.For zero-dimensional systems, the RUR allows retrieval of the numeric values of the solutions by solving a single univariate polynomial and substituting them in rational functions. This allows production of certified approximations of the solutions to any given precision.The RUR is uniquely defined for a given separating variable, independently of any algorithm, and it preserves the multiplicities of the roots. This is a notable difference with triangular decompositions (even the equiprojectable decomposition), which, in general, do not preserve multiplicities. The RUR shares with equiprojectable decomposition the property of producing an output with coefficients of relatively small size.For example, for the system in the previous section, every linear combination of the variable, except the multiples of x, y and x + y, is a separating variable. If one chooses t = x \u2013 y/2 as a separating variable, then the RUR isGiven a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.where h is a univariate polynomial in x0 of degree D and g0, ..., gn are univariate polynomials in x0 of degree less than D.A RUR of a zero-dimensional system consists in a linear combination x0 of the variables, called separating variable, and a system of equations[9]The rational univariate representation or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by F. Rouillier.[8]The second issue is generally solved by outputting regular chains of a special form, sometimes called shape lemma, for which all di but the first one are equal to 1. For getting such regular chains, one may have to add a further variable, called separating variable, which is given the index 0. The rational univariate representation, described below, allows computing such a special regular chain, satisfying Dahan\u2013Schost bound, by starting from either a regular chain or a Gr\u00f6bner basis.The first issue has been solved by Dahan and Schost:[5][6] Among the sets of regular chains that represent a given set of solutions, there is a set for which the coefficients are explicitly bounded in terms of the size of the input system, with a nearly optimal bound. This set, called equiprojectable decomposition, depends only on the choice of the coordinates. This allows the use of modular methods for computing efficiently the equiprojectable decomposition.[7]This representation of the solutions are fully convenient for coefficients in a finite field. However, for rational coefficients, two aspects have to be taken care of:There is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gr\u00f6bner basis for the graded reverse lexicographic order (grevlex), then deducing the lexicographical Gr\u00f6bner basis by FGLM algorithm[3] and finally applying the Lextriangular algorithm.[4]There are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional)[2] into regular chains (or regular semi-algebraic systems).Every zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.The solutions of this system are obtained by solving the first univariate equation, substituting the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from fi has degree di and thus that the system has d1 ... dn solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).To such a regular chain is associated a triangular system of equationsThe usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists of a sequence of polynomials f1(x1), f2(x1, x2), ..., fn(x1, ..., xn) such that, for every i such that 1 \u2264 i \u2264 nThe other way to represent the solutions is said to be algebraic. It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field k of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation involving the solving of only one univariate polynomial for each solution is preferable: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.For zero-dimensional systems, solving consists of computing all the solutions. There are two different ways of outputting the solutions. The most common, possible for real or complex solutions, consists of outputting numeric approximations of the solutions. Such a solution is called numeric. A solution is certified if it is provided with a bound on the error of the approximations which separates the different solutions.A natural example of an open question about solving positive-dimensional systems is the following: decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them. The only published algorithm which allows one to solve this question is cylindrical algebraic decomposition, which is not efficient enough, in practice, to be used for this.If the system is positive-dimensional, it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean \"finding a description of the solutions from which the relevant properties of the solutions are easy to extract\". There is no commonly accepted such description. In fact there are many different \"relevant properties\", which involve almost every subfield of algebraic geometry.The first thing to do in solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gr\u00f6bner basis of the left-hand sides of the equations. The system is inconsistent if this Gr\u00f6bner basis is reduced to 1. The system is zero-dimensional if, for every variable there is a leading monomial of some element of the Gr\u00f6bner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).This exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with B\u00e9zout's bound higher than, say, 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).A zero-dimensional system with as many equations as variables is said to be well-behaved.[1] B\u00e9zout's theorem asserts that a well-behaved system whose equations have degrees d1, ..., dn has at most d1...dn solutions. This bound is sharp. If all the degrees are equal to d, this bound becomes dn and is exponential in the number of variables.A system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension K of\u00a0k. This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be positive-dimensional.A system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension K of\u00a0k.A system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems, when constructed with random coefficients, are inconsistent. For example, the system \u00a0x3\u00a0\u2212\u00a01 =\u00a00, x2\u00a0\u2212\u00a01\u00a0=\u00a00 is overdetermined (having two equations but only one unknown), but it is not inconsistent since it has the solution x =1.In the case of a finite field, the same transformation allows always to suppose that the field k has a prime order.The elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.When solving a system over a finite field k with q elements, one is primarily interested in the solutions in k. As the elements of k are exactly the solutions of the equation xq\u00a0\u2212\u00a0x\u00a0=\u00a00, it suffices, for restricting the solutions to k, to add the equation xiq\u00a0\u2212\u00a0xi\u00a0=\u00a00 for each variable\u00a0xi.is equivalent to the polynomial systemFor example, the equationA trigonometric equation is an equation g = 0 where g is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing sin(x) and cos(x) by two new variables s and c and adding the new equation s2\u00a0+\u00a0c2\u00a0\u2212\u00a01\u00a0=\u00a00.A solution is a set of the values for the xi which make all of the equations true and which belong to some algebraically closed field extension K of k. When k is the field of rational numbers, K is the field of complex numbers.Usually, the field k is either the field of rational numbers or a finite field, although most of the theory applies to any field.A system of polynomial equations is a set of simultaneous equations f1 = 0, ..., fh = 0 where the fi are polynomials in several variables, say x1, ..., xn, over some field k.",
            "title": "\nSystem of polynomial equations\n",
            "url": "https://en.wikipedia.org/wiki/Systems_of_polynomial_equations"
        },
        {
            "links": [
                "/wiki/Directed_graph",
                "/wiki/Functor",
                "/wiki/Monoid",
                "/wiki/Monoid_action",
                "/wiki/Category_of_topological_spaces",
                "/wiki/Homeomorphism",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Category_of_sets",
                "/wiki/Category_(mathematics)",
                "/wiki/Morphism",
                "/wiki/Functor",
                "/wiki/Automorphism_group",
                "/wiki/Bijection",
                "/wiki/Permutation",
                "/wiki/Group_homomorphism",
                "/wiki/Symmetric_group",
                "/wiki/Group_action",
                "/wiki/Group_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Quantum_group",
                "/wiki/Crystal_basis",
                "/wiki/Hopf_algebra",
                "/wiki/Associative_algebra",
                "/wiki/Group_ring",
                "/wiki/Group_algebra",
                "/wiki/Universal_enveloping_algebra",
                "/wiki/Hilbert_modular_form",
                "/wiki/Siegel_modular_form",
                "/wiki/Selberg_trace_formula",
                "/wiki/Robert_Langlands",
                "/wiki/Riemann-Roch_theorem",
                "/wiki/Algebraic_group",
                "/wiki/Adelic_algebraic_group",
                "/wiki/Langlands_program",
                "/wiki/Modular_form",
                "/wiki/Analytic_function",
                "/wiki/Several_complex_variables",
                "/wiki/PSL2(R)",
                "/wiki/Congruence_subgroup",
                "/wiki/Discrete_subgroup",
                "/wiki/Differential_form",
                "/wiki/Upper_half_space",
                "/wiki/Maximal_compact_subgroup",
                "/wiki/Symmetric_space",
                "/wiki/Semisimple_Lie_group",
                "/wiki/Felix_Klein",
                "/wiki/Erlangen_program",
                "/wiki/%C3%89lie_Cartan",
                "/wiki/Cartan_connection",
                "/wiki/Holonomy",
                "/wiki/Differential_operator",
                "/wiki/Several_complex_variables",
                "/wiki/Infinite_group",
                "/wiki/Linear_algebra",
                "/wiki/Quadratic_form",
                "/wiki/Determinant",
                "/wiki/Projective_geometry",
                "/wiki/David_Mumford",
                "/wiki/Geometric_invariant_theory",
                "/wiki/Group_action",
                "/wiki/Algebraic_variety",
                "/wiki/Polynomial_function",
                "/wiki/Linear_group",
                "/wiki/Group_scheme",
                "/wiki/Lie_group",
                "/wiki/Finite_groups_of_Lie_type",
                "/wiki/Zariski_topology",
                "/wiki/Lie_superalgebra",
                "/wiki/Theoretical_physics",
                "/wiki/Conformal_field_theory",
                "/wiki/Exactly_solvable_model",
                "/wiki/Macdonald_identities",
                "/wiki/Victor_Kac",
                "/wiki/Robert_Moody",
                "/wiki/Semisimple_Lie_algebra",
                "/wiki/%C3%89lie_Cartan",
                "/wiki/Cartan_subalgebra",
                "/wiki/Weight_(representation_theory)",
                "/wiki/Eigenspace",
                "/wiki/Lie_algebra",
                "/wiki/Skew-symmetric_graph",
                "/wiki/Bilinear_operation",
                "/wiki/Lie_bracket",
                "/wiki/Jacobi_identity",
                "/wiki/Tangent_space",
                "/wiki/Lie_group",
                "/wiki/Identity_element",
                "/wiki/Semidirect_product",
                "/wiki/Solvable_Lie_group",
                "/wiki/Levi_decomposition",
                "/wiki/Mackey_theory",
                "/wiki/Wigner%27s_classification",
                "/wiki/Semisimple_Lie_group",
                "/wiki/Weyl%27s_unitary_trick",
                "/wiki/Lie_group",
                "/wiki/Smooth_manifold",
                "/wiki/Homogeneous_spaces",
                "/wiki/Symmetric_space",
                "/wiki/Automorphic_form",
                "/wiki/Alexander_Grothendieck",
                "/wiki/Linear_algebraic_group",
                "/wiki/Tannakian_category",
                "/wiki/Category_(mathematics)",
                "/wiki/Tannaka%E2%80%93Krein_duality",
                "/wiki/Plancherel_theorem",
                "/wiki/Measure_(mathematics)",
                "/wiki/Unitary_dual",
                "/wiki/Square_integrable",
                "/wiki/L2-space",
                "/wiki/Pontrjagin_duality",
                "/wiki/Peter%E2%80%93Weyl_theorem",
                "/wiki/Fourier_series",
                "/wiki/Fourier_transform",
                "/wiki/Dual_vector_space",
                "/wiki/Harmonic_analysis",
                "/wiki/Mathematical_analysis",
                "/wiki/Local_compactness",
                "/wiki/Harish-Chandra_module",
                "/wiki/Sesquilinear_form",
                "/wiki/Semisimple_Lie_group",
                "/wiki/Lie_group",
                "/wiki/Representation_theory_of_SL2(R)",
                "/wiki/Representation_theory_of_the_Lorentz_group",
                "/wiki/Unitary_dual",
                "/wiki/Locally_compact",
                "/wiki/Topological_group",
                "/wiki/Strongly_continuous",
                "/wiki/Character_theory",
                "/wiki/Peter%E2%80%93Weyl_theorem",
                "/wiki/Hilbert_space",
                "/wiki/Unitary_operator",
                "/wiki/Quantum_mechanics",
                "/wiki/Hermann_Weyl",
                "/wiki/Representation_theory_of_the_Poincar%C3%A9_group",
                "/wiki/Eugene_Wigner",
                "/wiki/George_Mackey",
                "/wiki/Harish-Chandra",
                "/wiki/Mathematics",
                "/wiki/Algebraic_geometry",
                "/wiki/Coding_theory",
                "/wiki/Combinatorics",
                "/wiki/Number_theory",
                "/wiki/Richard_Brauer",
                "/wiki/Classification_of_finite_simple_groups",
                "/wiki/Sylow_subgroup",
                "/wiki/Group_ring",
                "/wiki/Finite_groups_of_Lie_type",
                "/wiki/Linear_algebraic_group",
                "/wiki/Lie_group",
                "/wiki/Lie_algebra_representation",
                "/wiki/Weight_(representation_theory)",
                "/wiki/Compact_group",
                "/wiki/Haar_measure",
                "/wiki/Abstract_harmonic_analysis",
                "/wiki/Orthogonal_complement",
                "/wiki/Unitary_representation",
                "/wiki/Positive_characteristic",
                "/wiki/Finite_field",
                "/wiki/Coprime",
                "/wiki/Group_order",
                "/wiki/Common_factor",
                "/wiki/Modular_representation_theory",
                "/wiki/Character_theory",
                "/wiki/Characteristic_zero",
                "/wiki/Maschke%27s_theorem",
                "/wiki/Projection_(linear_algebra)",
                "/wiki/Crystallographic_group",
                "/wiki/Clebsch%E2%80%93Gordan_coefficients",
                "/wiki/Weyl%27s_theorem_on_complete_reducibility",
                "/wiki/Direct_sum_of_representations",
                "/wiki/Direct_sum_of_vector_spaces",
                "/wiki/Schur%27s_lemma",
                "/wiki/Zero_map",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Image_(mathematics)",
                "/wiki/Endomorphism",
                "/wiki/Division_algebra",
                "/wiki/Algebraically_closed",
                "/wiki/Zero_vector_space",
                "/wiki/Subrepresentation",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Up_to_isomorphism",
                "/wiki/Isomorphism",
                "/wiki/Commutative_diagram",
                "/wiki/Faithful_representation",
                "/wiki/Injective",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Dimension_of_a_vector_space",
                "/wiki/Lie_algebra#Definition_and_first_properties",
                "/wiki/Identity_element",
                "/wiki/Group_action",
                "/wiki/Group_(mathematics)",
                "/wiki/Linear_map",
                "/wiki/Function_composition",
                "/wiki/General_linear_group#General_linear_group_of_a_vector_space",
                "/wiki/Linear_map#Endomorphisms_and_automorphisms",
                "/wiki/Algebra",
                "/wiki/Group_(mathematics)",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Column_vector",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Abstract_algebra",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Category_theory",
                "/wiki/Functor",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Algebraic_geometry",
                "/wiki/Module_theory",
                "/wiki/Analytic_number_theory",
                "/wiki/Differential_geometry",
                "/wiki/Operator_theory",
                "/wiki/Algebraic_combinatorics",
                "/wiki/Topology",
                "/wiki/Abstract_algebra",
                "/wiki/Linear_algebra",
                "/wiki/Hilbert_space",
                "/wiki/Mathematical_analysis",
                "/wiki/Physics",
                "/wiki/Symmetry_group",
                "/wiki/Mathematics",
                "/wiki/Abstract_algebra",
                "/wiki/Algebraic_structure",
                "/wiki/Element_(set_theory)",
                "/wiki/Linear_transformation",
                "/wiki/Vector_space",
                "/wiki/Module_(mathematics)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Algebraic_operation",
                "/wiki/Matrix_addition",
                "/wiki/Matrix_multiplication",
                "/wiki/Algebra",
                "/wiki/Group_(mathematics)",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Group_representation"
            ],
            "text": "One special case has had a significant impact on representation theory, namely the representation theory of quivers.[11] A quiver is simply a directed graph (with loops and multiple arrows allowed), but it can be made into a category (and also an algebra) by considering paths in the graph. Representations of such categories/algebras have illuminated several aspects of representation theory, for instance by allowing non-semisimple representation theory questions about a group to be reduced in some cases to semisimple representation theory questions about a quiver.More generally, one can relax the assumption that the category being represented has only one object. In full generality, this is simply the theory of functors between categories, and little can be said.Since groups are categories, one can also consider representation of other categories. The simplest generalization is to monoids, which are categories with one object. Groups are monoids for which every morphism is invertible. General monoids have representations in any category. In the category of sets, these are monoid actions, but monoid representations on vector spaces and other objects can be studied.Two types of representations closely related to linear representations are:For another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from G to the homeomorphism group of a topological space X.In the case where C is VectF, the category of vector spaces over a field F, this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of G in the category of sets.Every group G can be viewed as a category with a single object; morphisms in this category are just the elements of G. Given an arbitrary category C, a representation of G in C is a functor from G to C. Such a functor selects an object X in C and a group homomorphism from G to Aut(X), the automorphism group of X.This condition and the axioms for a group imply that \u03c1(g) is a bijection (or permutation) for all g in G. Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group SX of X.A set-theoretic representation (also known as a group action or permutation representation) of a group G on a set X is given by a function \u03c1 from G to XX, the set of functions from X to X, such that for all g1, g2 in G and all x in X:The Hopf algebras associated to groups have a commutative algebra structure, and so general Hopf algebras are known as quantum groups, although this term is often restricted to certain Hopf algebras arising as deformations of groups or their universal enveloping algebras. The representation theory of quantum groups has added surprising insights to the representation theory of Lie groups and Lie algebras, for instance through the crystal basis of Kashiwara.Hopf algebras provide a way to improve the representation theory of associative algebras, while retaining the representation theory of groups and Lie algebras as special cases. In particular, the tensor product of two representations is a representation, as is the dual vector space.When considering representations of an associative algebra, one can forget the underlying field, and simply regard the associative algebra as a ring, and its representations as modules. This approach is surprisingly fruitful: many results in representation theory can be interpreted as special cases of results about modules over a ring.In one sense, associative algebra representations generalize both representations of groups and Lie algebras. A representation of a group induces a representation of a corresponding group ring or group algebra, while representations of a Lie algebra correspond bijectively to representations of its universal enveloping algebra. However, the representation theory of general associative algebras does not have all of the nice properties of the representation theory of groups and Lie algebras.Before the development of the general theory, many important special cases were worked out in detail, including the Hilbert modular forms and Siegel modular forms. Important results in the theory include the Selberg trace formula and the realization by Robert Langlands that the Riemann-Roch theorem could be applied to calculate the dimension of the space of automorphic forms. The subsequent notion of \"automorphic representation\" has proved of great technical value for dealing with the case that G is an algebraic group, treated as an adelic algebraic group. As a result, an entire philosophy, the Langlands program has developed around the relation between representation and number theoretic properties of automorphic forms.[40]Automorphic forms are a generalization of modular forms to more general analytic functions, perhaps of several complex variables, with similar transformation properties.[39] The generalization involves replacing the modular group PSL2 (R) and a chosen congruence subgroup by a semisimple Lie group G and a discrete subgroup \u0393. Just as modular forms can be viewed as differential forms on a quotient of the upper half space H = PSL2 (R)/SO(2), automorphic forms can be viewed as differential forms (or similar objects) on \u0393\\G/K, where K is (typically) a maximal compact subgroup of G. Some care is required, however, as the quotient typically has singularities. The quotient of a semisimple Lie group by a compact subgroup is a symmetric space and so the theory of automorphic forms is intimately related to harmonic analysis on symmetric spaces.The representation theory of semisimple Lie groups has its roots in invariant theory[30] and the strong links between representation theory and algebraic geometry have many parallels in differential geometry, beginning with Felix Klein's Erlangen program and \u00c9lie Cartan's connections, which place groups and symmetry at the heart of geometry.[38] Modern developments link representation theory and invariant theory to areas as diverse as holonomy, differential operators and the theory of several complex variables.Invariant theory of infinite groups is inextricably linked with the development of linear algebra, especially, the theories of quadratic forms and determinants. Another subject with strong mutual influence is projective geometry, where invariant theory can be used to organize the subject, and during the 1960s, new life was breathed into the subject by David Mumford in the form of his geometric invariant theory.[37]Invariant theory studies actions on algebraic varieties from the point of view of their effect on functions, which form representations of the group. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. The modern approach analyses the decomposition of these representations into irreducibles.[36]Linear algebraic groups (or more generally, affine group schemes) are analogues in algebraic geometry of Lie groups, but over more general fields than just R or C. In particular, over finite fields, they give rise to finite groups of Lie type. Although linear algebraic groups have a classification that is very similar to that of Lie groups, their representation theory is rather different (and much less well understood) and requires different techniques, since the Zariski topology is relatively weak, and techniques from analysis are no longer available.[35]Lie superalgebras are generalizations of Lie algebras in which the underlying vector space has a Z2-grading, and skew-symmetry and Jacobi identity properties of the Lie bracket are modified by signs. Their representation theory is similar to the representation theory of Lie algebras.[34]Affine Lie algebras are a special case of Kac\u2013Moody algebras, which have particular importance in mathematics and theoretical physics, especially conformal field theory and the theory of exactly solvable models. Kac discovered an elegant proof of certain combinatorial identities, Macdonald identities, which is based on the representation theory of affine Kac\u2013Moody algebras.There are many classes of infinite-dimensional Lie algebras whose representations have been studied. Among these, an important class are the Kac\u2013Moody algebras.[33] They are named after Victor Kac and Robert Moody, who independently discovered them. These algebras form a generalization of finite-dimensional semisimple Lie algebras, and share many of their combinatorial properties. This means that they have a class of representations that can be understood in the same way as representations of semisimple Lie algebras.Lie algebras, like Lie groups, have a Levi decomposition into semisimple and solvable parts, with the representation theory of solvable Lie algebras being intractable in general. In contrast, the finite-dimensional representations of semisimple Lie algebras are completely understood, after work of \u00c9lie Cartan. A representation of a semisimple Lie algebra g is analysed by choosing a Cartan subalgebra, which is essentially a generic maximal subalgebra h of g on which the Lie bracket is zero (\"abelian\"). The representation of g can be decomposed into weight spaces that are eigenspaces for the action of h and the infinitesimal analogue of characters. The structure of semisimple Lie algebras then reduces the analysis of representations to easily understood combinatorics of the possible weights that can occur.[31]A Lie algebra over a field F is a vector space over F equipped with a skew-symmetric bilinear operation called the Lie bracket, which satisfies the Jacobi identity. Lie algebras arise in particular as tangent spaces to Lie groups at the identity element, leading to their interpretation as \"infinitesimal symmetries\".[31] An important approach to the representation theory of Lie groups is to study the corresponding representation theory of Lie algebras, but representations of Lie algebras also have an intrinsic interest.[32]A general Lie group is a semidirect product of a solvable Lie group and a semisimple Lie group (the Levi decomposition).[31] The classification of representations of solvable Lie groups is intractable in general, but often easy in practical cases. Representations of semidirect products can then be analysed by means of general results called Mackey theory, which is a generalization of the methods used in Wigner's classification of representations of the Poincar\u00e9 group.The representation theory of Lie groups can be developed first by considering the compact groups, to which results of compact representation theory apply.[26] This theory can be extended to finite-dimensional representations of semisimple Lie groups using Weyl's unitary trick: each semisimple real Lie group G has a complexification, which is a complex Lie group Gc, and this complex Lie group has a maximal compact subgroup K. The finite-dimensional representations of G closely correspond to those of K.A Lie group is a group that is also a smooth manifold. Many classical groups of matrices over the real or complex numbers are Lie groups.[30] Many of the groups important in physics and chemistry are Lie groups, and their representation theory is crucial to the application of group theory in those fields.[5]Harmonic analysis has also been extended from the analysis of functions on a group G to functions on homogeneous spaces for G. The theory is particularly well developed for symmetric spaces and provides a theory of automorphic forms (discussed below).If the group is neither abelian nor compact, no general theory is known with an analogue of the Plancherel theorem or Fourier inversion, although Alexander Grothendieck extended Tannaka\u2013Krein duality to a relationship between linear algebraic groups and tannakian categories.Another approach involves considering all unitary representations, not just the irreducible ones. These form a category, and Tannaka\u2013Krein duality provides a way to recover a compact group from its category of unitary representations.A major goal is to provide a general form of the Fourier transform and the Plancherel theorem. This is done by constructing a measure on the unitary dual and an isomorphism between the regular representation of G on the space L2(G) of square integrable functions on G and its representation on the space of L2 functions on the unitary dual. Pontrjagin duality and the Peter\u2013Weyl theorem achieve this for abelian and compact G respectively.[27][29]The duality between the circle group S1 and the integers Z, or more generally, between a torus Tn and Zn is well known in analysis as the theory of Fourier series, and the Fourier transform similarly expresses the fact that the space of characters on a real vector space is the dual vector space. Thus unitary representation theory and harmonic analysis are intimately related, and abstract harmonic analysis exploits this relationship, by developing the analysis of functions on locally compact topological groups and related spaces.[7]For non-compact G, the question of which representations are unitary is a subtle one. Although irreducible unitary representations must be \"admissible\" (as Harish-Chandra modules) and it is easy to detect which admissible representations have a nondegenerate invariant sesquilinear form, it is hard to determine when this form is positive definite. An effective description of the unitary dual, even for relatively well-behaved groups such as real reductive Lie groups (discussed below), remains an important open problem in representation theory. It has been solved for many particular groups, such as SL(2,R) and the Lorentz group.[28]A major goal is to describe the \"unitary dual\", the space of irreducible unitary representations of G.[26] The theory is most well-developed in the case that G is a locally compact (Hausdorff) topological group and the representations are strongly continuous.[7] For G abelian, the unitary dual is just the space of characters, while for G compact, the Peter\u2013Weyl theorem shows that the irreducible unitary representations are finite-dimensional and the unitary dual is discrete.[27] For example, if G is the circle group S1, then the characters are given by integers, and the unitary dual is Z.A unitary representation of a group G is a linear representation \u03c6 of G on a real or (usually) complex Hilbert space V such that \u03c6(g) is a unitary operator for every g \u2208 G. Such representations have been widely applied in quantum mechanics since the 1920s, thanks in particular to the influence of Hermann Weyl,[23] and this has inspired the development of the theory, most notably through the analysis of representations of the Poincar\u00e9 group by Eugene Wigner.[24] One of the pioneers in constructing a general theory of unitary representations (for any group G rather than just for particular groups useful in applications) was George Mackey, and an extensive theory was developed by Harish-Chandra and others in the 1950s and 1960s.[25]As well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory, combinatorics and number theory.Modular representations of a finite group G are representations over a field whose characteristic is not coprime to |G|, so that Maschke's theorem no longer holds (because |G| is not invertible in F and so one cannot divide by it).[21] Nevertheless, Richard Brauer extended much of character theory to modular representations, and this theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were \"too small\".[22]Representations of a finite group G are also linked directly to algebra representations via the group algebra F[G], which is a vector space over F with the elements of G as a basis, equipped with the multiplication operation defined by the group operation, linearity, and the requirement that the group operation and scalar multiplication commute.Over arbitrary fields, another class of finite groups that have a good representation theory are the finite groups of Lie type. Important examples are linear algebraic groups over finite fields. The representation theory of linear algebraic groups and Lie groups extends these examples to infinite-dimensional groups, the latter being intimately related to Lie algebra representations. The importance of character theory for finite groups has an analogue in the theory of weights for representations of Lie groups and Lie algebras.Results such as Maschke's theorem and the unitary property that rely on averaging can be generalized to more general groups by replacing the average with an integral, provided that a suitable notion of integral can be defined. This can be done for compact topological groups (including compact Lie groups), using Haar measure, and the resulting theory is known as abstract harmonic analysis.Unitary representations are automatically semisimple, since Maschke's result can be proven by taking the orthogonal complement of a subrepresentation. When studying representations of groups that are not finite, the unitary representations provide a good generalization of the real and complex representations of a finite group.for all g in G and v, w in W. Hence any G-representation is unitary.Maschke's theorem holds more generally for fields of positive characteristic p, such as the finite fields, as long as the prime p is coprime to the order of G. When p and |G| have a common factor, there are G-representations that are not semisimple, which are studied in a subbranch called modular representation theory.The finite-dimensional G-representations can be understood using character theory: the character of a representation \u03c6: G \u2192 GL(V) is the class function \u03c7\u03c6: G \u2192 F defined by\u03c0G is equivariant, and its kernel is the required complement.Over a field of characteristic zero, the representation of a finite group G has a number of convenient properties. First, the representations of G are semisimple (completely reducible). This is a consequence of Maschke's theorem, which states that any subrepresentation V of a G-representation W has a G-invariant complement. One proof is to choose any projection \u03c0 from W to V and replace it by its average \u03c0G defined byGroup representations are a very important tool in the study of finite groups.[19] They also arise in the applications of finite group theory to geometry and crystallography.[20] Representations of finite groups exhibit many of the features of the general theory and point the way to other branches and topics in representation theory.Representation theory is notable for the number of branches it has, and the diversity of the approaches to studying representations of groups and algebras. Although, all the theories have in common the basic concepts discussed already, they differ considerably in detail. The differences are at least 3-fold:In general, the tensor product of irreducible representations is not irreducible; the process of decomposing a tensor product as a direct sum of irreducible representations is known as Clebsch\u2013Gordan theory.In cases where complete reducibility does not hold, one must understand how indecomposable representations can be built from irreducible representations as extensions of a quotient by a subrepresentation.In favorable circumstances, every finite-dimensional representation is a direct sum of irreducible representations: such representations are said to be semisimple. In this case, it suffices to understand only the irreducible representations. Examples where this \"complete reducibility\" phenomenon occur include finite and compact groups, and semisimple Lie algebras.The direct sum of two representations carries no more information about the group G than the two representations do individually. If a representation is the direct sum of two proper nontrivial subrepresentations, it is said to be decomposable. Otherwise, it is said to be indecomposable.If (V,\u03c6) and (W,\u03c8) are representations of (say) a group G, then the direct sum of V and W is a representation, in a canonical way, via the equationIrreducible representations are the building blocks of representation theory: if a representation V is not irreducible then it is built from a subrepresentation and a quotient that are both \"simpler\" in some sense; for instance, if V is finite-dimensional, then both the subrepresentation and the quotient have smaller dimension.The definition of an irreducible representation implies Schur's lemma: an equivariant map \u03b1: V \u2192 W between irreducible representations is either the zero map or an isomorphism, since its kernel and image are subrepresentations. In particular, when V = W, this shows that the equivariant endomorphisms of V form an associative division algebra over the underlying field F. If F is algebraically closed, the only equivariant endomorphisms of an irreducible representation are the scalar multiples of the identity.If V has exactly two subrepresentations, namely the trivial subspace {0} and V itself, then the representation is said to be irreducible; if V has a proper nontrivial subrepresentation, the representation is said to be reducible.[15]If (V,\u03c8) is a representation of (say) a group G, and W is a linear subspace of V that is preserved by the action of G in the sense that g \u00b7 w \u2208 W for all w \u2208 W (Serre [14] calls these W stable under G), then W is called a subrepresentation: by defining \u03c6(g) to be the restriction of \u03c8(g) to W, (W, \u03c6) is a representation of G and the inclusion of W into V is an equivariant map. The quotient space V/W can also be made into a representation of G.Isomorphic representations are, for practical purposes, \"the same\"; they provide the same information about the group or algebra being represented. Representation theory therefore seeks to classify representations up to isomorphism.Equivariant maps for representations of an associative or Lie algebra are defined similarly. If \u03b1 is invertible, then it is said to be an isomorphism, in which case V and W (or, more precisely, \u03c6 and \u03c8) are isomorphic representations, also phrased as equivalent representations. An equivariant map is often called an intertwining map of representations. Also, in the case of a group G, it is on occasion called a G-map.for all g in G, i.e. the following diagram commutes:for all g in G and v in V. In terms of \u03c6: G \u2192 GL(V) and \u03c8: G \u2192 GL(W), this meansIf V and W are vector spaces over F, equipped with representations \u03c6 and \u03c8 of a group G, then an equivariant map from V to W is a linear map \u03b1: V \u2192 W such thatAn effective or faithful representation is a representation (V,\u03c6) for which the homomorphism \u03c6 is injective.When V is of finite dimension n, one can choose a basis for V to identify V with Fn and hence recover a matrix representation with entries in the field F.The vector space V is called the representation space of \u03c6 and its dimension (if finite) is called the dimension of the representation (sometimes degree, as in [14]). It is also common practice to refer to V itself as the representation when the homomorphism \u03c6 is clear from the context; otherwise the notation (V,\u03c6) can be used to denote a representation.and similarly in the other cases. This approach is both more concise and more abstract. From this point of view:The second way to define a representation focuses on the map \u03c6 sending g in G to a linear map \u03c6(g): V \u2192 V, which satisfieswhere [x1, x2] is the Lie bracket, which generalizes the matrix commutator MN \u2212 NM.where e is the identity element of G and g1g2 is the product in G. The requirement for associative algebras is analogous, except that associative algebras do not always have an identity element, in which case equation (1) is ignored. Equation (2) is an abstract expression of the associativity of matrix multiplication. This doesn't hold for the matrix commutator and also there is no identity element for the commutator. Hence for Lie algebras, the only requirement is that for any x1, x2 in A and v in V:with two properties. First, for any g in G (or a in A), the mapThere are two ways to say what a representation is.[13] The first uses the idea of an action, generalizing the way that matrices act on column vectors by matrix multiplication. A representation of a group G or (associative or Lie) algebra A on a vector space V is a mapThis generalizes to any field F and any vector space V over F, with linear maps replacing matrices and composition replacing matrix multiplication: there is a group GL(V,F) of automorphisms of V, an associative algebra EndF(V) of all endomorphisms of V, and a corresponding Lie algebra gl(V,F).There are three main sorts of algebraic objects for which this can be done: groups, associative algebras and Lie algebras.[12]Let V be a vector space over a field F.[3] For instance, suppose V is Rn or Cn, the standard n-dimensional space of column vectors over the real or complex numbers respectively. In this case, the idea of representation theory is to do abstract algebra concretely by using n \u00d7 n matrices of real or complex numbers.The success of representation theory has led to numerous generalizations. One of the most general is in category theory.[11] The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces. This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.Secondly, there are diverse approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.[10]Representation theory is pervasive across fields of mathematics, for two reasons. First, the applications of representation theory are diverse:[6] in addition to its impact on algebra, representation theory:Representation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood.[3] Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups.[4] Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.[5]Representation theory is a branch of mathematics that studies abstract algebraic structures by representing their elements as linear transformations of vector spaces, and studies modules over these abstract algebraic structures.[1] In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations in terms of matrix addition and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.[2]",
            "title": "\nRepresentation theory\n",
            "url": "https://en.wikipedia.org/wiki/Representation_theory"
        },
        {
            "links": [
                "/wiki/Vector_space_basis",
                "/wiki/Zorn%27s_lemma",
                "/wiki/Schauder_basis",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Axiom_of_choice",
                "/wiki/Boolean_prime_ideal_theorem",
                "/wiki/Baire_category_theorem",
                "/wiki/List_of_functional_analysis_topics",
                "/wiki/Topological_space",
                "/wiki/Compact_space",
                "/wiki/Hausdorff_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Baire_category_theorem",
                "/wiki/Normed_space",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/Open_mapping_theorem_(functional_analysis)",
                "/wiki/Stefan_Banach",
                "/wiki/Juliusz_Schauder",
                "/wiki/Bounded_linear_operator",
                "/wiki/Banach_space",
                "/wiki/Surjective",
                "/wiki/Open_map",
                "/wiki/Sublinear_function",
                "/wiki/Linear_functional",
                "/wiki/Linear_subspace",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Bounded_operator",
                "/wiki/Vector_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Normed_vector_space",
                "/wiki/Dual_space",
                "/wiki/Operator_theory",
                "/wiki/Spectral_measure#Spectral_measure",
                "/wiki/Multiplication_operator",
                "/wiki/Measure_space",
                "/wiki/Ess_sup",
                "/wiki/Spectral_theorem",
                "/wiki/Stefan_Banach",
                "/wiki/Hugo_Steinhaus",
                "/wiki/Hans_Hahn_(mathematician)",
                "/wiki/Uniform_boundedness_principle",
                "/wiki/Banach%E2%80%93Steinhaus_theorem",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Open_mapping_theorem_(functional_analysis)",
                "/wiki/Continuous_linear_operator",
                "/wiki/Banach_space",
                "/wiki/Derivative",
                "/wiki/Fr%C3%A9chet_derivative",
                "/wiki/Continuous_dual",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Isometry",
                "/wiki/Banach_space",
                "/wiki/Orthonormal_basis",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Linear_transformation",
                "/wiki/C*-algebra",
                "/wiki/Operator_algebra",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/Topological_vector_space",
                "/wiki/Complete_space",
                "/wiki/Normed_vector_space",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Linear_algebra",
                "/wiki/Measure_(mathematics)",
                "/wiki/Integral",
                "/wiki/Probability",
                "/wiki/Functional_(mathematics)",
                "/wiki/Calculus_of_variations",
                "/wiki/Higher-order_function",
                "/wiki/Jacques_Hadamard",
                "/wiki/Vito_Volterra",
                "/wiki/Maurice_Ren%C3%A9_Fr%C3%A9chet",
                "/wiki/Paul_L%C3%A9vy_(mathematician)",
                "/wiki/Frigyes_Riesz",
                "/wiki/Lw%C3%B3w_School_of_Mathematics",
                "/wiki/Poland",
                "/wiki/Stefan_Banach",
                "/wiki/Mathematical_analysis",
                "/wiki/Vector_space",
                "/wiki/Inner_product_space#Definition",
                "/wiki/Norm_(mathematics)#Definition",
                "/wiki/Topological_space#Definition",
                "/wiki/Linear_transformation",
                "/wiki/Function_space",
                "/wiki/Fourier_transform",
                "/wiki/Continuous_function",
                "/wiki/Unitary_operator",
                "/wiki/Differential_equations",
                "/wiki/Integral_equations"
            ],
            "text": "Functional analysis in its present form[update] includes the following tendencies:Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn\u2013Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice.List of functional analysis topics.The closed graph theorem states the following: If X is a topological space and Y is a compact Hausdorff space, then the graph of a linear map T from X to Y is closed if and only if T is continuous.[3]The proof uses the Baire category theorem, and completeness of both X and Y is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if X and Y are taken to be Fr\u00e9chet spaces.The open mapping theorem, also known as the Banach\u2013Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:[2]then there exists a linear extension \u03c8\u00a0: V \u2192 R of \u03c6 to the whole space V, i.e., there exists a linear functional \u03c8 such thatHahn\u2013Banach theorem:[2] If p\u00a0: V \u2192 R is a sublinear function, and \u03c6\u00a0: U \u2192 R is a linear functional on a linear subspace U \u2286 V which is dominated by p on U, i.e.The Hahn\u2013Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are \"enough\" continuous linear functionals defined on every normed vector space to make the study of the dual space \"interesting\".This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.where T is the multiplication operator:Theorem:[1] Let A be a bounded self-adjoint operator on a Hilbert space H. Then there is a measure space (X, \u03a3, \u03bc) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H \u2192 L2\u03bc(X) such thatThere are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let A be the operator of multiplication by t on L2[0, 1], that isThe theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.The uniform boundedness principle or Banach\u2013Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn\u2013Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.Important results of functional analysis include:Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fr\u00e9chet derivative article.In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.More generally, functional analysis includes the study of Fr\u00e9chet spaces and other topological vector spaces not endowed with a norm.The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra. The theory of nonlinear functionals was continued by students of Hadamard, in particular Fr\u00e9chet and L\u00e9vy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.",
            "title": "\nFunctional analysis\n",
            "url": "https://en.wikipedia.org/wiki/Functional_analysis"
        },
        {
            "links": [
                "/wiki/Sound",
                "/wiki/Radio_wave",
                "/wiki/Seismic_waves",
                "/wiki/Functional_analysis",
                "/wiki/Quantum_mechanics",
                "/wiki/Classical_mechanics",
                "/wiki/Theory_of_relativity",
                "/wiki/Quantum_mechanics",
                "/wiki/Differential_equation",
                "/wiki/Newton%27s_second_law",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Einstein_field_equations",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Celestial_mechanics",
                "/wiki/Numerical_linear_algebra",
                "/wiki/Stochastic_differential_equation",
                "/wiki/Markov_chain",
                "/wiki/Algorithm",
                "/wiki/Approximation",
                "/wiki/Symbolic_computation",
                "/wiki/Discrete_mathematics",
                "/wiki/Deterministic_system_(mathematics)",
                "/wiki/Classical_mechanics",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Equations_of_motion",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Variable_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Derivative#Higher_derivatives",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology",
                "/wiki/Vector_space",
                "/wiki/Inner_product_space#Definition",
                "/wiki/Norm_(mathematics)#Definition",
                "/wiki/Topological_space#Definition",
                "/wiki/Linear_transformation",
                "/wiki/Function_space",
                "/wiki/Fourier_transform",
                "/wiki/Continuous_function",
                "/wiki/Unitary_operator",
                "/wiki/Differential_equations",
                "/wiki/Integral_equations",
                "/wiki/Analytic_function",
                "/wiki/Meromorphic_function",
                "/wiki/Real_number",
                "/wiki/Imaginary_number",
                "/wiki/Laplace%27s_equation",
                "/wiki/Physics",
                "/wiki/Function_(mathematics)",
                "/wiki/Complex_numbers",
                "/wiki/Algebraic_geometry",
                "/wiki/Number_theory",
                "/wiki/Applied_mathematics",
                "/wiki/Physics",
                "/wiki/Hydrodynamics",
                "/wiki/Thermodynamics",
                "/wiki/Mechanical_engineering",
                "/wiki/Electrical_engineering",
                "/wiki/Quantum_field_theory",
                "/wiki/Real_number",
                "/wiki/Function_(mathematics)",
                "/wiki/Sequence",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Limit_of_a_function",
                "/wiki/Sequence",
                "/wiki/Calculus",
                "/wiki/Continuous_function",
                "/wiki/Smooth_function",
                "/wiki/Set_(mathematics)",
                "/wiki/Element_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Countable",
                "/wiki/Totally_ordered",
                "/wiki/Natural_numbers",
                "/wiki/Real_line",
                "/wiki/Complex_plane",
                "/wiki/Euclidean_space",
                "/wiki/Vector_space",
                "/wiki/Integer",
                "/wiki/Measure_theory",
                "/wiki/Functional_analysis",
                "/wiki/Topological_vector_space",
                "/wiki/Mathematics",
                "/wiki/Set_(mathematics)",
                "/wiki/Distance",
                "/wiki/Metric_(mathematics)",
                "/wiki/Pathological_(mathematics)",
                "/wiki/Nowhere_continuous_function",
                "/wiki/Weierstrass_function",
                "/wiki/Space-filling_curve",
                "/wiki/Camille_Jordan",
                "/wiki/Jordan_measure",
                "/wiki/Georg_Cantor",
                "/wiki/Naive_set_theory",
                "/wiki/Ren%C3%A9-Louis_Baire",
                "/wiki/Baire_category_theorem",
                "/wiki/Set_theory",
                "/wiki/Henri_Lebesgue",
                "/wiki/David_Hilbert",
                "/wiki/Hilbert_space",
                "/wiki/Integral_equation",
                "/wiki/Normed_vector_space",
                "/wiki/Stefan_Banach",
                "/wiki/Functional_analysis",
                "/wiki/Bernhard_Riemann",
                "/wiki/Integral",
                "/wiki/Arithmetization_of_analysis",
                "/wiki/Karl_Weierstrass",
                "/wiki/(%CE%B5,_%CE%B4)-definition_of_limit",
                "/wiki/Limit_of_a_function",
                "/wiki/Continuum_(set_theory)",
                "/wiki/Real_number",
                "/wiki/Richard_Dedekind",
                "/wiki/Dedekind_cut",
                "/wiki/Complete_metric_space",
                "/wiki/Simon_Stevin",
                "/wiki/Decimal_expansion",
                "/wiki/Theorem",
                "/wiki/Riemann_integral",
                "/wiki/Classification_of_discontinuities",
                "/wiki/Leonhard_Euler",
                "/wiki/Function_(mathematics)",
                "/wiki/Bernard_Bolzano",
                "/wiki/Augustin_Louis_Cauchy",
                "/wiki/Generality_of_algebra",
                "/wiki/Infinitesimal",
                "/wiki/Cauchy_sequence",
                "/wiki/Complex_analysis",
                "/wiki/Sim%C3%A9on_Denis_Poisson",
                "/wiki/Joseph_Liouville",
                "/wiki/Joseph_Fourier",
                "/wiki/Harmonic_analysis",
                "/wiki/Karl_Weierstrass",
                "/wiki/(%CE%B5,_%CE%B4)-definition_of_limit",
                "/wiki/Descartes",
                "/wiki/Fermat",
                "/wiki/Analytic_geometry",
                "/wiki/Isaac_Newton",
                "/wiki/Gottfried_Leibniz",
                "/wiki/Infinitesimal_calculus",
                "/wiki/Calculus_of_variations",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Partial_differential_equation",
                "/wiki/Fourier_analysis",
                "/wiki/Generating_function",
                "/wiki/Discrete_mathematics",
                "/wiki/Madhava_of_Sangamagrama",
                "/wiki/Series_(mathematics)",
                "/wiki/Power_series",
                "/wiki/Taylor_series",
                "/wiki/Trigonometric_functions",
                "/wiki/Trigonometric_functions",
                "/wiki/Trigonometric_functions",
                "/wiki/Inverse_trigonometric_functions",
                "/wiki/Trigonometric_functions",
                "/wiki/Kerala_School_of_Astronomy_and_Mathematics",
                "/wiki/Scientific_Revolution",
                "/wiki/Zeno_of_Elea",
                "/wiki/Zeno%27s_paradoxes#The_dichotomy_paradox",
                "/wiki/Greek_mathematics",
                "/wiki/Eudoxus_of_Cnidus",
                "/wiki/Archimedes",
                "/wiki/Method_of_exhaustion",
                "/wiki/Infinitesimals",
                "/wiki/The_Method_of_Mechanical_Theorems",
                "/wiki/Chinese_mathematics",
                "/wiki/Liu_Hui",
                "/wiki/Zu_Chongzhi",
                "/wiki/Cavalieri%27s_principle",
                "/wiki/Sphere",
                "/wiki/Indian_mathematics",
                "/wiki/Bh%C4%81skara_II",
                "/wiki/Derivative",
                "/wiki/Rolle%27s_theorem",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Function_(mathematics)",
                "/wiki/Calculus",
                "/wiki/Geometry",
                "/wiki/Space_(mathematics)",
                "/wiki/Mathematical_object",
                "/wiki/Topological_space",
                "/wiki/Metric_space",
                "/wiki/Mathematics",
                "/wiki/Limit_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Integral",
                "/wiki/Measure_(mathematics)",
                "/wiki/Series_(mathematics)",
                "/wiki/Analytic_function"
            ],
            "text": "Techniques from analysis are used in many areas of mathematics, including:When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal. A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[24]Functional analysis is also a major factor in quantum mechanics.The vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schr\u00f6dinger equation, and the Einstein field equations.Techniques from analysis are also found in other areas such as:Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st\u00a0century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.Modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).[22]Differential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly.A differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders.[18][19][20] Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense.[16][17] The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.Complex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers.[15] It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable.[13][14] In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.One of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n \u2192 \u221e, denotedA sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.Much of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).In mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.Also, \"monsters\" (nowhere continuous functions, continuous but nowhere differentiable functions, space-filling curves) began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue solved the problem of measure, and Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.In the middle of the 19th century Riemann introduced his theory of integration. The last third of the century saw the arithmetization of analysis by Weierstrass, who thought that geometric reasoning was inherently misleading, and introduced the \"epsilon-delta\" definition of limit. Then, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the \"gaps\" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the \"size\" of the set of discontinuities of real functions.In the 18th century, Euler introduced the notion of mathematical function.[11] Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816,[12] but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler. Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals. Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y. He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis. The contributions of these mathematicians and others, such as Weierstrass, developed the (\u03b5, \u03b4)-definition of limit approach, thus founding the modern field of mathematical analysis.The modern foundations of mathematical analysis were established in 17th century Europe.[3] Descartes and Fermat independently developed analytic geometry, and a few decades later Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18th\u00a0century, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.In the 14th century, Madhava of Sangamagrama developed infinite series expansions, like the power series and the Taylor series, of functions such as sine, cosine, tangent and arctangent.[10] Alongside his development of the Taylor series of the trigonometric functions, he also estimated the magnitude of the error terms created by truncating these series and gave a rational approximation of an infinite series. His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.Mathematical analysis formally developed in the 17th century during the Scientific Revolution,[3] but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy.[4] Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids.[5] The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century.[6] In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century AD to find the area of a circle.[7] Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century.[8] The Indian mathematician Bh\u0101skara II gave examples of the derivative and used what is now known as Rolle's theorem in the 12th century.[9]These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis. Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects (a metric space).Mathematical analysis is the branch of mathematics dealing with limits and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2]",
            "title": "\nMathematical analysis\n",
            "url": "https://en.wikipedia.org/wiki/Mathematical_analysis"
        },
        {
            "links": [
                "/wiki/Density_on_a_manifold",
                "/wiki/Muckenhoupt_weights",
                "/wiki/Hilbert_transform",
                "/wiki/Hardy%E2%80%93Littlewood_maximal_operator",
                "/wiki/Radon%E2%80%93Nikodym_theorem",
                "/wiki/Norm_(mathematics)",
                "/wiki/Measure_space",
                "/wiki/Marcinkiewicz_interpolation",
                "/wiki/Harmonic_analysis",
                "/wiki/Singular_integrals",
                "/wiki/Banach_space",
                "/wiki/Triangle_inequality",
                "/wiki/Lorentz_space",
                "/wiki/Markov%27s_inequality",
                "/wiki/Measurable_function",
                "/wiki/Cumulative_distribution_function",
                "/wiki/Paul_L%C3%A9vy_(mathematician)",
                "/wiki/Convergence_in_measure",
                "/wiki/Convergence_in_probability",
                "/wiki/Hardy_space",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Counting_measure",
                "/wiki/F-space",
                "/wiki/Locally_bounded",
                "/wiki/F-space",
                "/wiki/Locally_convex",
                "/wiki/Clarkson%27s_inequalities",
                "/wiki/Uniformly_convex_space",
                "/wiki/Complete_space",
                "/wiki/Quasi-norm",
                "/wiki/Metrization_theorem",
                "/wiki/Topological_space",
                "/wiki/Borel_algebra",
                "/wiki/Open_set",
                "/wiki/Operator_norm",
                "/wiki/Closed_graph_theorem",
                "/wiki/H%C3%B6lder%27s_inequality",
                "/wiki/Absolutely_continuous",
                "/wiki/Ba_space",
                "/wiki/Saharon_Shelah",
                "/wiki/Zermelo%E2%80%93Fraenkel_set_theory",
                "/wiki/Axiom_of_dependent_choice",
                "/wiki/Baire_property",
                "/wiki/Sigma-finite",
                "/wiki/Reflexive_space#Definitions",
                "/wiki/Reflexive_space",
                "/wiki/Dual_space#Transpose_of_a_continuous_linear_map",
                "/wiki/H%C3%B6lder%27s_inequality",
                "/wiki/Isometry",
                "/wiki/H%C3%B6lder%27s_inequality#Extremal_equality",
                "/wiki/Radon%E2%80%93Nikodym_theorem",
                "/wiki/Isomorphism",
                "/wiki/Banach_space",
                "/wiki/Natural_number",
                "/wiki/Counting_measure",
                "/wiki/Commutative",
                "/wiki/C*-algebra",
                "/wiki/Von_Neumann_algebra",
                "/wiki/Bounded_operator",
                "/wiki/Multiplication_operator",
                "/wiki/Fourier_series",
                "/wiki/Quantum_mechanics",
                "/wiki/Quadratically_integrable_function",
                "/wiki/Riemann_integral",
                "/wiki/Hilbert_space",
                "/wiki/Bochner_space",
                "/wiki/Banach_space",
                "/wiki/Riesz-Fischer_theorem",
                "/wiki/Lebesgue_integral",
                "/wiki/Essential_supremum",
                "/wiki/Normed_vector_space",
                "/wiki/Quotient_space_(topology)",
                "/wiki/Kernel_(set_theory)",
                "/wiki/Almost_everywhere",
                "/wiki/Vector_space",
                "/wiki/Absolute_value",
                "/wiki/Lebesgue_integrable",
                "/wiki/Measure_space",
                "/wiki/Measurable_function",
                "/wiki/Absolute_value",
                "/wiki/Banach_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Integral",
                "/wiki/Supremum",
                "/wiki/Harmonic_series_(mathematics)",
                "/wiki/Series_(mathematics)",
                "/wiki/Sequence",
                "/wiki/Complex_number",
                "/wiki/Norm_(mathematics)",
                "/wiki/Scientific_computing",
                "/wiki/Information_theory",
                "/wiki/Statistics",
                "/wiki/Compressed_sensing",
                "/wiki/Signal_processing",
                "/wiki/Harmonic_analysis",
                "/wiki/David_Donoho",
                "/wiki/Abuse_of_terminology",
                "/wiki/Banach",
                "/wiki/F-space",
                "/wiki/F-space",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Locally_convex",
                "/wiki/Metric_space",
                "/wiki/F-space",
                "/wiki/Homogeneous_function",
                "/wiki/Subadditivity",
                "/wiki/Cauchy%E2%80%93Schwarz_inequality",
                "/wiki/Manhattan_distance",
                "/wiki/Banach_space",
                "/wiki/Norm_(mathematics)",
                "/wiki/L-infinity",
                "/wiki/Chebyshev_distance",
                "/wiki/Taxicab_geometry",
                "/wiki/Real_number",
                "/wiki/Taxicab_geometry",
                "/wiki/Mathematics",
                "/wiki/Physics",
                "/wiki/Computer_science",
                "/wiki/Real_number",
                "/wiki/Vector_space",
                "/wiki/Euclidean_norm",
                "/wiki/Hilbert_space",
                "/wiki/Quantum_mechanics",
                "/wiki/Stochastic_calculus",
                "/wiki/Fourier_transform",
                "/wiki/Fourier_series",
                "/wiki/Riesz%E2%80%93Thorin_theorem",
                "/wiki/Hausdorff%E2%80%93Young_inequality",
                "/wiki/Taxicab_geometry",
                "/wiki/Norm_(mathematics)#Euclidean_norm",
                "/wiki/LASSO",
                "/wiki/Tikhonov_regularization",
                "/wiki/Elastic_net_regularization",
                "/wiki/Statistics",
                "/wiki/Central_tendency",
                "/wiki/Statistical_dispersion",
                "/wiki/Mean",
                "/wiki/Median",
                "/wiki/Standard_deviation",
                "/wiki/Central_tendency#Solutions_to_variational_problems",
                "/wiki/Mathematics",
                "/wiki/Function_space",
                "/wiki/Norm_(mathematics)#p-norm",
                "/wiki/Vector_space",
                "/wiki/Henri_Lebesgue",
                "/wiki/Nicolas_Bourbaki",
                "/wiki/Frigyes_Riesz",
                "/wiki/Banach_space",
                "/wiki/Functional_analysis",
                "/wiki/Topological_vector_space"
            ],
            "text": "One may also define spaces Lp(M) on a manifold, called the intrinsic Lp spaces of the manifold, using densities.As Lp-spaces, the weighted spaces have nothing special, since Lp(S, w\u2009d\u03bc) is equal to Lp(S, d\u03bd). But they are the natural framework for several results in harmonic analysis (Grafakos 2004); they appear for example in the Muckenhoupt theorem: for 1 < p < \u221e, the classical Hilbert transform is defined on Lp(T, \u03bb) where T denotes the unit circle and \u03bb the Lebesgue measure; the (nonlinear) Hardy\u2013Littlewood maximal operator is bounded on Lp(Rn, \u03bb). Muckenhoupt's theorem describes weights w such that the Hilbert transform remains bounded on Lp(T, w\u2009d\u03bb) and the maximal operator on Lp(Rn, w\u2009d\u03bb).or, in terms of the Radon\u2013Nikodym derivative, w = d\u03bd/d\u03bc\u2009 the norm for Lp(S, w\u2009d\u03bc) is explicitlyAs before, consider a measure space (S, \u03a3, \u03bc). Let w\u00a0: S \u2192 [0, \u221e) be a measurable function. The w-weighted Lp space is defined as Lp(S, w\u2009d\u03bc), where w\u2009d\u03bc means the measure \u03bd defined byA major result that uses the Lp,w-spaces is the Marcinkiewicz interpolation theorem, which has broad applications to harmonic analysis and the study of singular integrals.is comparable to the Lp,w-norm. Further in the case p\u00a0> 1, this expression defines a norm if r\u00a0= 1. Hence for p\u00a0> 1 the weak Lp spaces are Banach spaces (Grafakos 2004).For any 0\u00a0< r\u00a0< p the expressionUnder the convention that two functions are equal if they are equal \u03bc almost everywhere, then the spaces Lp,w are complete (Grafakos 2004).In fact, one hasand in particular Lp(S, \u03bc)\u00a0\u2282 Lp,w(S, \u03bc).The Lp,w-norm is not a true norm, since the triangle inequality fails to hold. Nevertheless, for f in Lp(S, \u03bc),The weak Lp coincide with the Lorentz spaces Lp,\u221e, so this notation is also used to denote them.The best constant C for this inequality is the Lp,w-norm of f, and is denoted byA function f is said to be in the space weak Lp(S, \u03bc), or Lp,w(S, \u03bc), if there is a constant C\u00a0> 0 such that, for all t\u00a0> 0,If f is in Lp(S, \u03bc) for some p with 1\u00a0\u2264 p\u00a0< \u221e, then by Markov's inequality,Let (S, \u03a3, \u03bc) be a measure space, and f a measurable function with real or complex values on S. The distribution function of f is defined for t\u00a0> 0 byThe resulting space L0(Rn, \u03bb) coincides as topological vector space with L0(Rn, g(x)\u2009d\u03bb(x)), for any positive \u03bb\u2013integrable density g.For the infinite Lebesgue measure \u03bb on Rn, the definition of the fundamental system of neighborhoods could be modified as followswhere \u03c6 is bounded continuous concave and non-decreasing on [0, \u221e), with \u03c6(0) = 0 and \u03c6(t) > 0 when t > 0 (for example, \u03c6(t) = min(t, 1)). Such a metric is called L\u00e9vy-metric for L0. Under this metric the space L0 is complete (it is again an F-space). The space L0 is in general not locally bounded, and not locally convex.The topology can be defined by any metric d of the formThe description is easier when \u03bc is finite. If \u03bc is a finite measure on (S, \u03a3), the 0 function admits for the convergence in measure the following fundamental system of neighborhoodsThe vector space of (equivalence classes of) measurable functions on (S, \u03a3, \u03bc) is denoted L0(S, \u03a3, \u03bc) (Kalton, Peck & Roberts 1984). By definition, it contains all the Lp, and is equipped with the topology of convergence in measure. When \u03bc is a probability measure (i.e., \u03bc(S) = 1), this mode of convergence is named convergence in probability.The situation of having no linear functionals is highly undesirable for the purposes of doing analysis. In the case of the Lebesgue measure on Rn, rather than work with Lp for 0 < p < 1, it is common to work with the Hardy space H\u2009p whenever possible, as this has quite a few linear functionals: enough to distinguish points from one another. However, the Hahn\u2013Banach theorem still fails in H\u2009p for p < 1 (Duren 1970, \u00a77.5).The only nonempty convex open set in Lp([0, 1]) is the entire space (Rudin 1991, \u00a71.47). As a particular consequence, there are no nonzero linear functionals on Lp([0, 1]): the dual space is the zero space. In the case of the counting measure on the natural numbers (producing the sequence space Lp(\u03bc) = \u2113\u2009p), the bounded linear functionals on \u2113\u2009p are exactly those that are bounded on \u2113\u20091, namely those given by sequences in \u2113\u2009\u221e. Although \u2113\u2009p does contain non-trivial convex open sets, it fails to have enough of them to give a base for the topology.The space Lp for 0 < p < 1 is an F-space: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p \u2265 1. It is the prototypical example of an F-space that, for most reasonable measure spaces, is not locally convex: in \u2113\u2009p or Lp([0, 1]), every open convex set containing the 0 function is unbounded for the p-quasi-norm; therefore, the 0 vector does not possess a fundamental system of convex neighborhoods. Specifically, this is true if the measure space S contains an infinite family of disjoint measurable sets of finite positive measure.This result may be used to prove Clarkson's inequalities, which are in turn used to establish the uniform convexity of the spaces Lp for 1 < p < \u221e (Adams & Fournier 2003).In this setting Lp satisfies a reverse Minkowski inequality, that is for u, v in Lpis a metric on Lp(\u03bc). The resulting metric space is complete; the verification is similar to the familiar case when p \u2265 1.and so the functionAs before, we may introduce the p-norm ||\u2009f\u2009||p = Np(\u2009f\u2009)1/p, but || \u00b7 ||\u2009p does not satisfy the triangle inequality in this case, and defines only a quasi-norm. The inequality (a + b)\u2009p \u2264 a\u2009p + b\u2009p, valid for a, b \u2265 0 implies that (Rudin 1991, \u00a71.47)Let (S, \u03a3, \u03bc) be a measure space. If 0 < p < 1, then Lp(\u03bc) can be defined as above: it is the vector space of those measurable functions \u2009f\u2009 such thatwhereSeveral properties of general functions in Lp(Rd) are first proved for continuous and compactly supported functions (sometimes for step functions), then extended by density to all functions. For example, it is proved this way that translations are continuous on Lp(Rd), in the following sense:This applies in particular when S = Rd and when \u03bc is the Lebesgue measure. The space of continuous and compactly supported functions is dense in Lp(Rd). Similarly, the space of integrable step functions is dense in Lp(Rd); this space is the linear span of indicator functions of bounded intervals when d = 1, of bounded rectangles when d = 2 and more generally of products of bounded intervals.If S can be covered by an increasing sequence (Vn) of open sets that have finite measure, then the space of p\u2013integrable continuous functions is dense in Lp(S, \u03a3, \u03bc). More precisely, one can use bounded continuous functions that vanish outside one of the open sets Vn.It follows that there exists \u03c6 continuous on S such thatSuppose V \u2282 S is an open set with \u03bc(V) < \u221e. It can be proved that for every Borel set A \u2208 \u03a3 contained in V, and for every \u03b5 > 0, there exist a closed set F and an open set U such thatMore can be said when S is a metrizable topological space and \u03a3 its Borel \u03c3\u2013algebra, i.e., the smallest \u03c3\u2013algebra of subsets of S containing the open sets.Let (S, \u03a3, \u03bc) be a measure space. An integrable simple function \u2009f\u2009 on S is one of the formThroughout this section we assume that: 1 \u2264 p < \u221e.the case of equality being achieved exactly when \u2009f\u2009 = 1 \u03bc-a.e.The constant appearing in the above inequality is optimal, in the sense that the operator norm of the identity I\u00a0: Lq(S, \u03bc) \u2192 Lp(S, \u03bc) is preciselyleading toNeither condition holds for the real line with the Lebesgue measure. In both cases the embedding is continuous, in that the identity operator is a bounded linear map from Lq to Lp in the first case, and Lp to Lq in the second. (This is a consequence of the closed graph theorem and properties of Lp spaces.) Indeed, if the domain S has finite measure, one can make the following explicit calculation using H\u00f6lder's inequalityColloquially, if 1 \u2264 p < q \u2264 \u221e, then Lp(S, \u03bc) contains functions that are more locally singular, while elements of Lq(S, \u03bc) can be more spread out. Consider the Lebesgue measure on the half line (0, \u221e). A continuous function in L1 might blow up near 0 but must decay sufficiently fast toward infinity. On the other hand, continuous functions in L\u221e need not decay at all but no blow-up is allowed. The precise technical result is the following.[6] Suppose that 0 < p < q \u2264 \u221e. Then:The dual of L\u221e is subtler. Elements of L\u221e(\u03bc)\u2217 can be identified with bounded signed finitely additive measures on S that are absolutely continuous with respect to \u03bc. See ba space for more details. If we assume the axiom of choice, this space is much bigger than L1(\u03bc) except in some trivial cases. However, Saharon Shelah proved that there are relatively consistent extensions of Zermelo\u2013Fraenkel set theory (ZF + DC + \"Every subset of the real numbers has the Baire property\") in which the dual of \u2113\u221e is \u21131.[5]If the measure \u03bc on S is sigma-finite, then the dual of L1(\u03bc) is isometrically isomorphic to L\u221e(\u03bc) (more precisely, the map \u03ba1 corresponding to p = 1 is an isometry from L\u221e(\u03bc) onto L1(\u03bc)\u2217).This map coincides with the canonical embedding J of Lp(\u03bc) into its bidual. Moreover, the map jp is onto, as composition of two onto isometries, and this proves reflexivity.For 1 < p < \u221e, the space Lp(\u03bc) is reflexive. Let \u03bap be as above and let \u03baq\u00a0: Lp(\u03bc) \u2192 Lq(\u03bc)\u2217 be the corresponding linear isometry. Consider the map from Lp(\u03bc) to Lp(\u03bc)\u2217\u2217, obtained by composing \u03baq with the transpose (or adjoint) of the inverse of \u03bap:The fact that \u03bap(g) is well defined and continuous follows from H\u00f6lder's inequality. \u03bap\u00a0: Lq(\u03bc) \u2192 Lp(\u03bc)\u2217 is a linear mapping which is an isometry by the extremal case of H\u00f6lder's inequality. It is also possible to show (for example with the Radon\u2013Nikodym theorem, see[4]) that any G \u2208 Lp(\u03bc)\u2217 can be expressed this way: i.e., that \u03bap is onto. Since \u03bap is onto and isometric, it is an isomorphism of Banach spaces. With this (isometric) isomorphism in mind, it is usual to say simply that Lq is the dual Banach space of Lp.For 1 \u2264 p \u2264 \u221e the \u2113p spaces are a special case of Lp spaces, when S = N, and \u03bc is the counting measure on N. More generally, if one considers any set S with the counting measure, the resulting Lp space is denoted \u2113p(S). For example, the space \u2113p(Z) is the space of all sequences indexed by the integers, and when defining the p-norm on such a space, one sums over all the integers. The space \u2113p(n), where n is the set with n elements, is Rn with its p-norm as defined above. As any Hilbert space, every space L2 is linearly isometric to a suitable \u21132(I), where the cardinality of the set I is the cardinality of an arbitrary Hilbertian basis for this particular L2.If we use complex-valued functions, the space L\u221e is a commutative C*-algebra with pointwise multiplication and conjugation. For many measure spaces, including all sigma-finite ones, it is in fact a commutative von Neumann algebra. An element of L\u221e defines a bounded operator on any Lp space by multiplication.The additional inner product structure allows for a richer theory, with applications to, for instance, Fourier series and quantum mechanics. Functions in L2 are sometimes called quadratically integrable functions, square-integrable functions or square-summable functions, but sometimes these terms are reserved for functions that are square-integrable in some other sense, such as in the sense of a Riemann integral (Titchmarsh 1976).Similar to the \u2113p spaces, L2 is the only Hilbert space among Lp spaces. In the complex case, the inner product on L2 is defined byWhen the underlying measure space S is understood, Lp(S, \u03bc) is often abbreviated Lp(\u03bc), or just Lp. The above definitions generalize to Bochner spaces.For 1 \u2264 p \u2264 \u221e, Lp(S, \u03bc) is a Banach space. The fact that Lp is complete is often referred to as the Riesz-Fischer theorem. Completeness can be checked using the convergence theorems for Lebesgue integrals.As before, if there exists q < \u221e such that \u2009f\u2009 \u2208 L\u221e(S, \u03bc) \u2229 Lq(S, \u03bc), thenFor p = \u221e, the space L\u221e(S, \u03bc) is defined as follows. We start with the set of all measurable functions from S to C or R which are bounded. Again two such functions are identified if they are equal almost everywhere. Denote this set by L\u221e(S, \u03bc). For a function \u2009f\u2009 in this set, the essential supremum of its absolute value serves as an appropriate norm:In the quotient space, two functions \u2009f\u2009 and g are identified if \u2009f\u2009 = g almost everywhere. The resulting normed vector space is, by definition, This can be made into a normed vector space in a standard way; one simply takes the quotient space with respect to the kernel of || \u00b7 ||p. Since for any measurable function \u2009f\u2009, we have that ||\u2009f\u2009||p = 0 if and only if \u2009f\u2009 = 0 almost everywhere, the kernel of || \u00b7 ||p does not depend upon p,That the sum of two p-th power integrable functions is again p-th power integrable follows from the inequalityfor every scalar \u03bb.The set of such functions forms a vector space, with the following natural operations:An Lp space may be defined as a space of functions for which the p-th power of the absolute value is Lebesgue integrable,[3] where functions which agree almost everywhere are identified. More generally, let 1 \u2264 p < \u221e and (S, \u03a3, \u03bc) be a measure space. Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, thatThe p-norm thus defined on \u2113\u2009p is indeed a norm, and \u2113\u2009p together with this norm is a Banach space. The fully general Lp space is obtained\u2014as seen below \u2014 by considering vectors, not only with finitely or countably-infinitely many components, but with \"arbitrarily many components\"; in other words, functions. An integral instead of a sum is used to define the p-norm.if the right-hand side is finite, or the left-hand side is infinite. Thus, we will consider \u2113\u2009p spaces for 1 \u2264 p \u2264 \u221e.and the corresponding space \u2113\u2009\u221e of all bounded sequences. It turns out that[2]One also defines the \u221e-norm using the supremum:diverges for p = 1 (the harmonic series), but is convergent for p > 1.is not in \u2113\u20091, but it is in \u2113\u2009p for p > 1, as the seriesOne can check that as p increases, the set \u2113\u2009p grows larger. For example, the sequenceHere, a complication arises, namely that the series on the right is not always convergent, so for example, the sequence made up of only ones, (1, 1, 1, ...), will have an infinite p-norm for 1 \u2264 p < \u221e. The space \u2113\u2009p is then defined as the set of all infinite sequences of real (or complex) numbers such that the p-norm is finite.Define the p-norm:The space of sequences has a natural vector space structure by applying addition and scalar multiplication coordinate by coordinate. Explicitly, the vector sum and the scalar action for infinite sequences of real (or complex) numbers are given by:The p-norm can be extended to vectors that have an infinite number of components, which yields the space \u2113\u2009p. This contains as special cases:This is not a norm because it is not homogeneous. Despite these defects as a mathematical norm, the non-zero counting \"norm\" has uses in scientific computing, information theory, and statistics\u2013notably in compressed sensing in signal processing and computational harmonic analysis.Another function was called the \u21130 \"norm\" by David Donoho\u2014whose quotation marks warn that this function is not a proper norm\u2014is the number of non-zero entries of the vector x. Many authors abuse terminology by omitting the quotation marks. Defining 00 = 0, the zero \"norm\" of x is equal towhich is discussed by Stefan Rolewicz in Metric Linear Spaces.[1] The \u21130-normed space is studied in functional analysis, probability theory, and harmonic analysis.The mathematical definition of the \u21130 norm was established by Banach's Theory of Linear Operations. The space of sequences has a complete metric topology provided by the F-normThere is one \u21130 norm and another function called the \u21130 \"norm\" (with quotation marks).shows that the infinite-dimensional sequence space \u2113p defined below, is no longer locally convex.[citation needed]Although the p-unit ball Bnp around the origin in this metric is \"concave\", the topology defined on Rn by the metric dp is the usual vector space topology of Rn, hence \u2113np is a locally convex topological vector space. Beyond this qualitative statement, a quantitative way to measure the lack of convexity of \u2113np is to denote by Cp(n) the smallest constant C such that the multiple C\u00a0Bnp of the p-unit ball contains the convex hull of Bnp, equal to Bn1. The fact that for fixed p < 1 we havedefines a metric. The metric space (Rn, dp) is denoted by \u2113np.Hence, the functiondefines a subadditive function at the cost of losing absolute homogeneity. It does define an F-norm, though, which is homogeneous of degree p.defines an absolutely homogeneous function for 0 < p < 1; however, the resulting function does not define a norm, because it is not subadditive. On the other hand, the formulaIn Rn for n > 1, the formulaIn general, for vectors in Cn where 0 < r < p:This inequality depends on the dimension n of the underlying vector space and follows directly from the Cauchy\u2013Schwarz inequality.For the opposite direction, the following relation between the 1-norm and the 2-norm is known:This fact generalizes to p-norms in that the p-norm ||x||p of any given vector x does not grow with p:The grid distance or rectilinear distance (sometimes called the \"Manhattan distance\") between two points is never shorter than the length of the line segment between them (the Euclidean or \"as the crow flies\" distance). Formally, this means that the Euclidean norm of any vector is bounded by its 1-norm:Abstractly speaking, this means that Rn together with the p-norm is a Banach space. This Banach space is the Lp-space over Rn.For all p \u2265 1, the p-norms and maximum norm as defined above indeed satisfy the properties of a \"length function\" (or norm), which are that:See L-infinity.The L\u221e-norm or maximum norm (or uniform norm) is the limit of the Lp-norms for p \u2192 \u221e. It turns out that this limit is equivalent to the following definition:The Euclidean norm from above falls into this class and is the 2-norm, and the 1-norm is the norm that corresponds to the rectilinear distance.Of course the absolute value bars are unnecessary when p is a rational number and, in reduced form, has an even numerator.For a real number p \u2265 1, the p-norm or Lp-norm of x is defined byThe Euclidean distance between two points x and y is the length ||x \u2212 y||2 of the straight line between the two points. In many situations, the Euclidean distance is insufficient for capturing the actual distances in a given space. An analogy to this is suggested by taxi drivers in a grid street plan who should measure distance not in terms of the length of the straight line to their destination, but in terms of the rectilinear distance, which takes into account that streets are either orthogonal or parallel to each other. The class of p-norms generalizes these two examples and has an abundance of applications in many parts of mathematics, physics, and computer science.\nThe length of a vector x = (x1, x2, ..., xn) in the n-dimensional real vector space Rn is usually given by the Euclidean norm:Hilbert spaces are central to many applications, from quantum mechanics to stochastic calculus. The spaces L2 and \u21132 are both Hilbert spaces. In fact, by choosing a Hilbert basis (i.e., a maximal orthonormal subset of L2 or any Hilbert space), one sees that all Hilbert spaces are isometric to \u21132(E), where E is a set with an appropriate cardinality.By contrast, if p\u00a0> 2, the Fourier transform does not map into Lq.The Fourier transform for the real line (or, for periodic functions, see Fourier series), maps Lp(R) to Lq(R) (or Lp(T) to \u2113q) respectively, where 1\u00a0\u2264 p\u00a0\u2264 2 and 1/p\u00a0+ 1/q\u00a0= 1. This is a consequence of the Riesz\u2013Thorin interpolation theorem, and is made precise with the Hausdorff\u2013Young inequality.In penalized regression, 'L1 penalty' and 'L2 penalty' refer to penalizing either the L1 norm of a solution's vector of parameter values (i.e. the sum of its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector.In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as solutions to variational problems.In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910). Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces. Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, finance, engineering, and other disciplines.",
            "title": "\nLp space\n",
            "url": "https://en.wikipedia.org/wiki/Lp_space"
        },
        {
            "links": [
                "/wiki/Commutative_algebra",
                "/wiki/Module_(mathematics)",
                "/wiki/Eduard_Study",
                "/wiki/Free_module",
                "/wiki/Index_notation",
                "/wiki/Metric_space",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Pullback_(differential_geometry)",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Pushforward_(differential)",
                "/wiki/Mathematical_physics",
                "/wiki/Einstein_notation",
                "/wiki/Up_to",
                "/wiki/Isomorphism",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Center_(ring_theory)",
                "/wiki/Zero_ring",
                "/wiki/Injective_function",
                "/wiki/Ring_(mathematics)",
                "/wiki/Ring_homomorphism",
                "/wiki/Module_(mathematics)",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Bruno_Buchberger",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Dual_number",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Unit_(algebra)",
                "/wiki/Commutativity",
                "/wiki/Associativity",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Linear_subspace",
                "/wiki/Isomorphism",
                "/wiki/Bijective",
                "/wiki/Homomorphism",
                "/wiki/Linear_map",
                "/wiki/Non-associative_algebra",
                "/wiki/Cross_product",
                "/wiki/Mathematics",
                "/wiki/Physics",
                "/wiki/Lie_algebra",
                "/wiki/Hypercomplex_number",
                "/wiki/Real_numbers",
                "/wiki/Null_vector",
                "/wiki/Multiplicative_inverse",
                "/wiki/Division_algebras",
                "/wiki/Quaternions",
                "/wiki/Commutativity",
                "/wiki/Commutative",
                "/wiki/Bilinear_operator",
                "/wiki/Associativity",
                "/wiki/Associative_algebra",
                "/wiki/Vector_space",
                "/wiki/Binary_operation",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Real_number",
                "/wiki/Imaginary_unit",
                "/wiki/Euclidean_vector",
                "/wiki/Commutative_ring",
                "/wiki/Bilinear_form",
                "/wiki/Inner_product_space",
                "/wiki/Algebraic_geometry",
                "/wiki/Identity_element",
                "/wiki/Identity_matrix",
                "/wiki/Unital_ring",
                "/wiki/Associative",
                "/wiki/Associative_algebra",
                "/wiki/Nonassociative_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Real_matrix",
                "/wiki/Square_matrix",
                "/wiki/Real_number",
                "/wiki/Matrix_addition",
                "/wiki/Matrix_multiplication",
                "/wiki/Euclidean_space",
                "/wiki/Vector_cross_product",
                "/wiki/Jacobi_identity",
                "/wiki/Mathematics",
                "/wiki/Vector_space",
                "/wiki/Bilinear_map",
                "/wiki/Product_(mathematics)",
                "/wiki/Algebraic_structure",
                "/wiki/Set_(mathematics)",
                "/wiki/Addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)"
            ],
            "text": "In some areas of mathematics, such as commutative algebra, it is common to consider the more general concept of an algebra over a ring, where a commutative unital ring R replaces the field K. The only part of the definition that changes is that A is assumed to be an R-module (instead of a vector space over K).The fourth algebra is non-commutative, others are commutative.There exist five three-dimensional algebras. Each algebra consists of linear combinations of three basis elements, 1 (the identity element), a and b. Taking into account the definition of an identity element, it is sufficient to specifyIt remains to specifyThere exist two two-dimensional algebras. Each algebra consists of linear combinations (with complex coefficients) of two basis elements, 1 (the identity element) and a. According to the definition of an identity element,Two-dimensional, three-dimensional and four-dimensional unital associative algebras over the field of complex numbers were completely classified up to isomorphism by Eduard Study.[4]If K is only a commutative ring and not a field, then the same process works if A is a free module over K. If it isn't, then the multiplication is still completely determined by its action on a set that spans A; however, the structure constants can't be specified arbitrarily in this case, and knowing only the structure constants does not specify the algebra up to isomorphism.If you apply this to vectors written in index notation, then this becomesWhen the algebra can be endowed with a metric, then the structure coefficients are generally written with upper and lower indices, so as to distinguish their transformation properties under coordinate transformations. Specifically, lower indices are covariant indices, and transform via pullbacks, while upper indices are contravariant, transforming under pushforwards. Thus, in mathematical physics, the structure coefficients are often written ci,jk, and their defining rule is written using the Einstein notation asNote however that several different sets of structure coefficients can give rise to isomorphic algebras.where e1,...,en form a basis of A.Thus, given the field K, any finite-dimensional algebra can be specified up to isomorphism by giving its dimension (say n), and specifying n3 structure coefficients ci,j,k, which are scalars. These structure coefficients determine the multiplication in A via the following rule:For algebras over a field, the bilinear multiplication from A \u00d7 A to A is completely determined by the multiplication of basis elements of A. Conversely, once a basis for A has been chosen, the products of basis elements can be set arbitrarily, and then extended in a unique way to a bilinear operator on A, i.e., so the resulting multiplication satisfies the algebra laws.Given two such associative unital K-algebras A and B, a unital K-algebra morphism f: A \u2192 B is a ring morphism that commutes with the scalar multiplication defined by \u03b7, which one may write asgiven bywhere Z(A) is the center of A. Since \u03b7 is a ring morphism, then one must have either that A is the zero ring, or that \u03b7 is injective. This definition is equivalent to that above, with scalar multiplicationThe definition of an associative K-algebra with unit is also frequently given in an alternative way. In this case, an algebra over a field K is a ring A together with a ring homomorphismExamples detailed in the main article include:These unital zero algebras may be more generally useful, as they allow to translate any general property of the algebras to properties of vector spaces or modules. For example, the theory of Gr\u00f6bner bases was introduced by Bruno Buchberger for ideals in a polynomial ring R = K[x1, ..., xn] over a field. The construction of the unital zero algebra over a free R-module allows extending this theory as a Gr\u00f6bner basis theory for sub modules of a free module. This extension allows, for computing a Gr\u00f6bner basis of a submodule, to use, without any modification, any algorithm and any software for computing Gr\u00f6bner bases of ideals.An example of unital zero algebra is the algebra of dual numbers, the unital zero R-algebra built from a one dimensional real vector space.One may define a unital zero algebra by taking the direct sum of modules of a field (or more generally a ring) K and a K-vector space (or module) V, and defining the product of every pair of elements of V to be zero. That is, if \u03bb, \u03bc \u2208 k and u, v \u2208 V, then (\u03bb + u) (\u03bc + v) = \u03bb\u03bc + (\u03bbv + \u03bcu). If e1, ... ed is a basis of V, the unital zero algebra is the quotient of the polynomial ring K[E1, ..., En] by the ideal generated by the EiEj for every pair (i, j).An algebra is called zero algebra if uv = 0 for all u, v in the algebra,[2] not to be confused with the algebra with one element. It is inherently non-unital (except in the case of only one element), associative and commutative.An algebra is unital or unitary if it has a unit or identity element I with Ix = x = xI for all x in the algebra.Algebras over fields come in many different types. These types are specified by insisting on some further axioms, such as commutativity or associativity of the multiplication operation, which are not required in the broad definition of an algebra. The theories corresponding to the different types of algebras are often very different.It is important to notice that this definition is different from the definition of an ideal of a ring, in that here we require the condition (2). Of course if the algebra is unital, then condition (3) implies condition (2).If (3) were replaced with x \u00b7 z is in L, then this would define a right ideal. A two-sided ideal is a subset that is both a left and a right ideal. The term ideal on its own is usually taken to mean a two-sided ideal. Of course when the algebra is commutative, then all of these notions of ideal are equivalent. Notice that conditions (1) and (2) together are equivalent to L being a linear subspace of A. It follows from condition (3) that every left or right ideal is a subalgebra.A left ideal of a K-algebra is a linear subspace that has the property that any element of the subspace multiplied on the left by any element of the algebra produces an element of the subspace. In symbols, we say that a subset L of a K-algebra A is a left ideal if for every x and y in L, z in A and c in K, we have the following three statements.In the above example of the complex numbers viewed as a two-dimensional algebra over the real numbers, the one-dimensional real line is a subalgebra.A subalgebra of an algebra over a field K is a linear subspace that has the property that the product of any two of its elements is again in the subspace. In other words, a subalgebra of an algebra is a subset of elements that is closed under addition, multiplication, and scalar multiplication. In symbols, we say that a subset L of a K-algebra A is a subalgebra if for every x, y in L and c in K, we have that x \u00b7 y, x + y, and cx are all in L.A K-algebra isomorphism is a bijective K-algebra homomorphism. For all practical purposes, isomorphic algebras differ only by notation.Given K-algebras A and B, a K-algebra homomorphism is a K-linear map f: A \u2192 B such that f(xy) = f(x) f(y) for all x,y in A. The space of all K-algebra homomorphisms between A and B is frequently written asPrevious examples are associative algebras. An example of a non-associative algebra is a three dimensional vector space equipped with the cross product. This is a simple example of a class of nonassociative algebras, which is widely used in mathematics and physics, the Lie algebras.The quaternions were soon followed by several other hypercomplex number systems, which were the early examples of algebras over a field.The real numbers may be viewed as a one-dimensional vector space with a compatible multiplication, and hence a one-dimensional algebra over itself. Likewise, as we saw above, the complex numbers form a two-dimensional vector space over the field of real numbers, and hence form a two dimensional algebra over the reals. In both these examples, every non-zero vector has an inverse, making them both division algebras. Although there are no division algebras in 3 dimensions, in 1843, the quaternions were defined and provided the now famous 4-dimensional example of an algebra over the real numbers, where one can not only multiply vectors, but also divide. Any quaternion may be written as (a, b, c, d) = a + bi + cj + dk. Unlike the complex numbers, the quaternions are an example of a non-commutative algebra: for instance, (0,1,0,0) \u00b7 (0,0,1,0) = (0,0,0,1) but (0,0,1,0) \u00b7 (0,1,0,0) = (0,0,0,\u22121).Notice that when a binary operation on a vector space is commutative, as in the above example of the complex numbers, it is left distributive exactly when it is right distributive. But in general, for non-commutative operations (such as the next example of the quaternions), they are not equivalent, and therefore require separate axioms.These three axioms are another way of saying that the binary operation is bilinear. An algebra over K is sometimes also called a K-algebra, and K is called the base field of A. The binary operation is often referred to as multiplication in A. The convention adopted in this article is that multiplication of elements of an algebra is not necessarily associative, although some authors use the term algebra to refer to an associative algebra.Let K be a field, and let A be a vector space over K equipped with an additional binary operation from A \u00d7 A to A, denoted here by \u00b7 (i.e. if x and y are any two elements of A, x \u00b7 y is the product of x and y). Then A is an algebra over K if the following identities hold for all elements x, y, and z of A, and all elements (often called scalars) a and b of K:This example fits into the following definition by taking the field K to be the real numbers, and the vector space A to be the complex numbers.The following statements are basic properties of the complex numbers. If x, y, z are complex numbers and a, b are real numbers, thenAny complex number may be written a + bi, where a and b are real numbers and i is the imaginary unit. In other words, a complex number is represented by the vector (a, b) over the field of real numbers. So the complex numbers form a two-dimensional real vector space, where addition is given by (a, b) + (c, d) = (a + c, b + d) and scalar multiplication is given by c(a, b) = (ca, cb), where all of a, b, c and d are real numbers. We use the symbol \u00b7 to multiply two vectors together, which we use complex multiplication to define: (a, b) \u00b7 (c, d) = (ac \u2212 bd, ad + bc).Replacing the field of scalars by a commutative ring leads to the more general notion of an algebra over a ring. Algebras are not to be confused with vector spaces equipped with a bilinear form, like inner product spaces, as, for such a space, the result of a product is not in the space, but rather in the field of coefficients.Many authors use the term algebra to mean associative algebra, or unital associative algebra, or in some subjects such as algebraic geometry, unital associative commutative algebra.An algebra is unital or unitary if it has an identity element with respect to the multiplication. The ring of real square matrices of order n forms a unital algebra since the identity matrix of order n is the identity element with respect to matrix multiplication. It is an example of a unital associative algebra, a (unital) ring that is also a vector space.The multiplication operation in an algebra may or may not be associative, leading to the notions of associative algebras and nonassociative algebras. Given an integer n, the ring of real square matrices of order n is an example of an associative algebra over the field of real numbers under matrix addition and matrix multiplication since matrix multiplication is associative. Three-dimensional Euclidean space with multiplication given by the vector cross product is an example of a nonassociative algebra over the field of real numbers since the vector cross product is nonassociative, satisfying the Jacobi identity instead.In mathematics, an algebra over a field (often simply called an algebra) is a vector space equipped with a bilinear product. Thus, an algebra is an algebraic structure, which consists of a set, together with operations of multiplication, addition, and scalar multiplication by elements of the underlying field, and satisfies the axioms implied by \"vector space\" and \"bilinear\".[1]",
            "title": "\nAlgebra over a field\n",
            "url": "https://en.wikipedia.org/wiki/Algebra_over_a_field"
        },
        {
            "links": [],
            "text": "",
            "title": "\nMultilinear algebra\n",
            "url": "https://en.wikipedia.org/wiki/Multilinear_algebra"
        },
        {
            "links": [
                "/wiki/Reflexive_space#Locally_convex_spaces",
                "/wiki/Topological_vector_space",
                "/wiki/Locally_convex_topological_vector_space",
                "/wiki/Hausdorff_space",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Isometry",
                "/wiki/Bijection",
                "/wiki/Reflexive_space",
                "/wiki/Space_of_linear_maps#G-topologies_on_X_induced_by_the_continuous_dual",
                "/wiki/Separable_space",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Surjection",
                "/wiki/Strong_topology_(polar_topology)",
                "/wiki/Hermitian_adjoint",
                "/wiki/Compact_operator",
                "/wiki/Arzel%C3%A0%E2%80%93Ascoli_theorem",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Riesz%E2%80%93Markov%E2%80%93Kakutani_representation_theorem",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Antiisomorphic",
                "/wiki/Bra%E2%80%93ket_notation",
                "/wiki/Quantum_mechanics",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Supremum_norm",
                "/wiki/H%C3%B6lder%27s_inequality",
                "/wiki/Lp_space#The_p-norm_in_countably_infinite_dimensions",
                "/wiki/Sequence",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/First_isomorphism_theorem",
                "/wiki/Kernel_(algebra)",
                "/wiki/Galois_connection",
                "/wiki/Vector_subspace",
                "/wiki/Vacuously_true",
                "/wiki/Annihilator_(ring_theory)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Transpose",
                "/wiki/Injective",
                "/wiki/Isomorphism",
                "/wiki/Algebra_over_a_field",
                "/wiki/Composition_of_maps",
                "/wiki/Antihomomorphism",
                "/wiki/Category_theory",
                "/wiki/Contravariant_functor",
                "/wiki/Adjoint_of_an_operator",
                "/wiki/Pullback_(differential_geometry)",
                "/wiki/Linear_map",
                "/wiki/Transpose#Transpose_of_linear_maps",
                "/wiki/Complex_numbers",
                "/wiki/Sesquilinear_form",
                "/wiki/Complex_conjugate_vector_space",
                "/wiki/Natural_isomorphism",
                "/wiki/Bilinear_form",
                "/wiki/Cardinal_number",
                "/wiki/Isomorphic",
                "/wiki/Direct_sum_of_modules#Properties",
                "/wiki/Direct_product",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Sequence",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Linearly_independent",
                "/wiki/Vector_(geometric)",
                "/wiki/Real_number",
                "/wiki/Matrix_multiplication",
                "/wiki/Finite-dimensional",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Dual_basis",
                "/wiki/Bilinear_mapping",
                "/wiki/Natural_pairing",
                "/wiki/One-form",
                "/wiki/Tensor",
                "/wiki/Measure_(mathematics)",
                "/wiki/Distribution_(mathematics)",
                "/wiki/Hilbert_space",
                "/wiki/Functional_analysis",
                "/wiki/Topological_vector_space",
                "/wiki/Mathematics",
                "/wiki/Vector_space",
                "/wiki/Linear_functional"
            ],
            "text": "be continuous for the chosen topology on V\u2032. Further, there is still a choice of a topology on V\u2032\u2032, and continuity of \u03a8 depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V\u2032, so that the continuous double dual V\u2032\u2032 is not uniquely defined as a set. Saying that \u03a8 maps from V to V\u2032\u2032, or in other words, that \u03a8(x) is continuous on V\u2032 for every x \u2208 V, is a reasonable minimal requirement on the topology of V\u2032, namely that the evaluation mappingsWhen V is a topological vector space, one can still define \u03a8(x) by the same formula, for every x \u2208 V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map \u03a8 trivial. However, if V is Hausdorff and locally convex, the map \u03a8 is injective from V to the algebraic dual V\u2032\u2217 of the continuous dual, again as a consequence of the Hahn\u2013Banach theorem.[15]As a consequence of the Hahn\u2013Banach theorem, this map is in fact an isometry, meaning \u2016 \u03a8(x) \u2016 = \u2016 x \u2016 for all x in\u00a0V. Normed spaces for which the map \u03a8 is a bijection are called reflexive.In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator \u03a8\u00a0: V \u2192 V\u2032\u2032 from a normed space V into its continuous double dual V\u2032\u2032, defined byThe topology of V and the topology of real or complex numbers can be used to induce on V\u2032 a dual space topology.If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space \u2113\u20091 is separable, but its dual \u2113\u2009\u221e is not.and it follows from the Hahn\u2013Banach theorem that j\u2032 induces an isometric isomorphism V\u2032\u2009/\u2009W\u22a5 \u2192 W\u2032.Then, the dual of the quotient V\u2009/\u2009W\u2009 can be identified with W\u22a5, and the dual of W can be identified with the quotient V\u2032\u2009/\u2009W\u22a5.[14] Indeed, let P denote the canonical surjection from V onto the quotient V\u2009/\u2009W\u2009; then, the transpose P\u2032 is an isometric isomorphism from (V\u2009/\u2009W\u2009)\u2032 into V\u2032, with range equal to W\u22a5. If j denotes the injection map from W into V, then the kernel of the transpose j\u2032 is the annihilator of W:Assume that W is a closed linear subspace of a normed space\u00a0V, and consider the annihilator of W in V\u2032,When T is a continuous linear map between two topological vector spaces V and W, then the transpose T\u2032 is continuous when W\u2032 and V\u2032 are equipped with\"compatible\" topologies: for example when, for X = V and X = W, both duals X\u2032 have the strong topology \u03b2(X\u2032, X) of uniform convergence on bounded sets of X, or both have the weak-\u2217 topology \u03c3(X\u2032, X) of pointwise convergence on\u00a0X. The transpose T\u2032 is continuous from \u03b2(W\u2032, W) to \u03b2(V\u2032, V), or from \u03c3(W\u2032, W) to \u03c3(V\u2032, V).When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V\u2032. For every bounded linear map T on V, the transpose and the adjoint operators are linked byWhen T is a compact linear map between two Banach spaces V and W, then the transpose T\u2032 is compact. This can be proved using the Arzel\u00e0\u2013Ascoli theorem.When V and W are normed spaces, the norm of the transpose in L(W\u2032, V\u2032) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn\u2013Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T\u2032 is injective.The resulting functional T\u2032(\u03c6) is in V\u2032. The assignment T \u2192 T\u2032 produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W\u2032 to V\u2032. When T and U are composable continuous linear maps, thenIf T\u00a0: V \u2192 W is a continuous linear map between two topological vector spaces, then the (continuous) transpose T\u2032\u00a0: W\u2032 \u2192 V\u2032 is defined by the same formula as before:By the Riesz\u2013Markov\u2013Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra\u2013ket notation used by physicists in the mathematical formulation of quantum mechanics.In a similar manner, the continuous dual of \u2113\u20091 is naturally identified with \u2113\u2009\u221e (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with \u2113\u20091.is finite. Define the number q by 1/p + 1/q = 1. Then the continuous dual of \u2113\u2009p is naturally identified with \u2113\u2009q: given an element \u03c6 \u2208 (\u2113\u2009p)\u2032, the corresponding element of \u2113\u2009q is the sequence (\u03c6(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) \u2208 \u2113\u2009q, the corresponding continuous linear functional \u03c6 on \u2113\u2009p is defined by \u03c6(b) = \u2211n anbn for all b = (bn) \u2208 \u2113\u2009p (see H\u00f6lder's inequality).Let 1 < p < \u221e be a real number and consider the Banach space \u2113\u2009p of all sequences a = (an) for whichHere are the three most important special cases.form its local base.As a particular consequence, if V is a direct sum of two subspaces A and B, then V\u2217 is a direct sum of A0 and B0.If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional f\u00a0: V \u2192 F factors through V/W if and only if W is in the kernel of f. There is thus an isomorphismafter identifying W with its image in the second dual space under the double duality isomorphism V \u2248 V\u2217\u2217. Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.If V is finite-dimensional, and W is a vector subspace, thenIn particular if A and B are subspaces of V, it follows thatand equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, thenMoreover, if A and B are two subsets of V, thenThe annihilator of a subset is itself a vector space. In particular, \u22050 = V\u2217 is all of V\u2217 (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S \u2282 T \u2282 V, thenLet S be a subset of V. The annihilator of S in V\u2217, denoted here S0, is the collection of linear functionals f \u2208 V\u2217 such that [f, s] = 0 for all s \u2208 S. That is, S0 consists of all linear functionals f\u00a0: V \u2192 F such that the restriction to S vanishes: f|S = 0.If the linear map f is represented by the matrix A with respect to two bases of V and W, then f\u2217 is represented by the transpose matrix AT with respect to the dual bases of W\u2217 and V\u2217, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f\u2217 is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.The assignment f \u21a6 f\u2217 produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W\u2217 to V\u2217; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)\u2217 = g\u2217f\u2217. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify (f\u2217)\u2217 with f using the natural injection into the double dual.where the bracket [\u00b7,\u00b7] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual. This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.The following identity holds for all \u03c6 \u2208 W\u2217 and v \u2208 V:for every \u03c6 \u2208 W\u2217. The resulting functional f\u2217(\u03c6) in V\u2217 is called the pullback of \u03c6 along f.If f\u00a0: V \u2192 W is a linear map, then the transpose (or dual) f\u2217\u00a0: W\u2217 \u2192 V\u2217 is defined byThe conjugate space V\u2217 can be identified with the set of all additive complex-valued functionals f: V \u2192 C such thatIf the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form \u27e8\u00b7,\u00b7\u27e9 determines an isomorphism of V with the complex conjugate of the dual spaceThus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V\u2217 and nondegenerate bilinear forms on V.defined bywhere the right hand side is defined as the functional on V taking each w \u2208 V to \u27e8v,w\u27e9. In other words, the bilinear form determines a linear mappingIf V is finite-dimensional, then V is isomorphic to V\u2217. But there is in general no natural isomorphism between these two spaces.[7] Any bilinear form \u27e8\u00b7,\u00b7\u27e9 on V gives a mapping of V into its dual space viaThus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.is a special case of a general result relating direct sums (of modules) to direct products.On the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identificationNote that (FA)0 may be identified (essentially by definition) with the direct sum of infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphismsAgain the sum is finite because f\u03b1 is nonzero for only finitely many \u03b1.The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values \u03b8\u03b1 = T(e\u03b1) it takes on the basis of V, and any function \u03b8\u00a0: A \u2192 F (with \u03b8(\u03b1) = \u03b8\u03b1) defines a linear functional T on V byin V (the sum is finite by the assumption on f, and any v \u2208 V may be written in this way by the definition of the basis).This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {e\u03b1\u00a0: \u03b1 \u2208 A} identifies V with the space (FA)0 of functions f\u00a0: A \u2192 F such that f\u03b1 = f(\u03b1) is nonzero for only finitely many \u03b1 \u2208 A, where such a function f is identified with the vectorConsider, for instance, the space R\u221e, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i \u2208 N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R\u221e is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R\u221e to give the number \u2211anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R\u221e is countably infinite, whereas RN does not have a countable basis.If V is not finite-dimensional but has a basis[6] e\u03b1 indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements e\u03b1 (\u03b1 \u2208 A) of the dual space, but they will not form a basis.If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V\u2217 form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V\u2217 can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one \"counts\" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V\u2217 are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]In particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n \u00d7 1 matrix and a 1 \u00d7 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 \u00d7 n matrix, i.e., M must be a row vector.for any choice of coefficients ci \u2208 F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equationsIf V is finite-dimensional, then V\u2217 has the same dimension as V. Given a basis {e1, ..., en} in V, it is possible to construct a specific basis in V\u2217, called the dual basis. This dual basis is a set {e1, ..., en} of linear functionals on V, defined by the relationThe pairing of a functional \u03c6 in the dual space V\u2217 and an element x of V is sometimes denoted by a bracket: \u03c6(x) = [x,\u03c6] [2] or \u03c6(x) = \u27e8\u03c6,x\u27e9.[3] This pairing defines a nondegenerate bilinear mapping[4] \u27e8\u00b7,\u00b7\u27e9\u00a0: V\u2217 \u00d7 V \u2192 F called the natural pairing.for all \u03c6 and \u03c8 \u2208 V\u2217, x \u2208 V, and a \u2208 F. Elements of the algebraic dual space V\u2217 are sometimes called covectors or one-forms.Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space. When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.",
            "title": "\nDual space\n",
            "url": "https://en.wikipedia.org/wiki/Dual_space"
        },
        {
            "links": [
                "/wiki/MATLAB",
                "/wiki/Higher-order_function",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Fortran",
                "/wiki/Differentiable_function",
                "/wiki/Array_programming_languages",
                "/wiki/APL_programming_language",
                "/wiki/J_programming_language",
                "/wiki/Monoidal_category",
                "/wiki/Product_(category_theory)",
                "/wiki/Graph_homomorphism",
                "/wiki/Kronecker_product",
                "/wiki/Adjacency_matrix",
                "/wiki/Tensor_product#Tensor_product_of_linear_maps",
                "/wiki/Tensor_product#Product_of_tensors",
                "/wiki/Tensor#As_multilinear_maps",
                "/wiki/Kronecker_product",
                "/wiki/Galois_extension",
                "/wiki/Tensor_product_of_fields",
                "/wiki/Galois_theory",
                "/wiki/Irreducible_polynomial",
                "/wiki/Algebra_(ring_theory)",
                "/wiki/Tor_functor",
                "/wiki/Derived_tensor_product",
                "/wiki/Right_exact_functor",
                "/wiki/Tensor_product_of_modules#Balanced_product",
                "/wiki/Abelian_group",
                "/wiki/Free_module",
                "/wiki/Tensor_product#Definition",
                "/wiki/Module_(mathematics)",
                "/wiki/Commutative_ring",
                "/wiki/Ring_(mathematics)",
                "/wiki/Transpose",
                "/wiki/Adjoint_functor",
                "/wiki/Dual_space",
                "/wiki/Tensor_contraction",
                "/wiki/Dual_vector_space",
                "/wiki/Linear_map",
                "/wiki/Dual_basis",
                "/wiki/Kronecker_product#Relation_to_the_abstract_tensor_product",
                "/wiki/Coordinate_vector",
                "/wiki/Tensor",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Dual_vector_space",
                "/wiki/Linear_map",
                "/wiki/Tensor",
                "/wiki/Permutation",
                "/wiki/Symmetric_monoidal_category",
                "/wiki/Canonical_isomorphism",
                "/wiki/Dyadic_product",
                "/wiki/Tensor_rank",
                "/wiki/Matrix_rank",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Kronecker_product",
                "/wiki/Bifunctor",
                "/wiki/Functor#Covariance_and_contravariance",
                "/wiki/Linear_map",
                "/wiki/Outer_product",
                "/wiki/Simple_tensor",
                "/wiki/Linearly_independent",
                "/wiki/Tensor_rank",
                "/wiki/Tensor_order",
                "/wiki/Matrix_rank",
                "/wiki/Cartesian_product",
                "/wiki/Equivalence_class",
                "/wiki/Congruence_relation",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Formal_sum",
                "/wiki/Free_vector_space",
                "/wiki/Set_(mathematics)",
                "/wiki/Index_set",
                "/wiki/Finite_support",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Category_(mathematics)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensors",
                "/wiki/Algebra_over_a_field",
                "/wiki/Topological_vector_spaces",
                "/wiki/Module_(mathematics)",
                "/wiki/Bilinear_operator",
                "/wiki/Monoidal_category",
                "/wiki/Direct_sum",
                "/wiki/Mathematics",
                "/wiki/Field_(mathematics)",
                "/wiki/Bilinear_map",
                "/wiki/Cartesian_product",
                "/wiki/Outer_product",
                "/wiki/Free_object"
            ],
            "text": "However, these kinds of notation are not universally present in array languages. Other array languages may require explicit treatment of indices (for example, MATLAB), and/or may not support higher-order functions such as the Jacobian derivative (for example, Fortran/APL).Note that J's treatment also allows the representation of some tensor fields, as a and b may be functions instead of constants. This product of two functions is a derived function, and if a and b are differentiable, then a */ b is differentiable.Array programming languages may have this pattern built in. For example, in APL the tensor product is expressed as \u25cb.\u00d7 (for example A \u25cb.\u00d7 B or A \u25cb.\u00d7 B \u25cb.\u00d7 C). In J the tensor product is the dyadic form of */ (for example a */ b or a */ b */ c).That is, in the symmetric algebra two adjacent vectors (and therefore all of them) can be interchanged. The resulting objects are called symmetric tensors.The symmetric algebra is constructed in a similar manner:Note that when the underlying field of V does not have characteristic 2, then this definition is equivalent tois defined asTwo notable constructions in linear algebra can be constructed as quotients of the tensor product: the exterior algebra and the symmetric algebra. For example, given a vector space V, the exterior productA general context for tensor product is that of a monoidal category.It should be mentioned that, though called \"tensor product\", this is not a tensor product of graphs in the above sense; actually it is the category-theoretic product in the category of graphs and graph homomorphisms. However it is actually the Kronecker tensor product of the adjacency matrices of the graphs. Compare also the section Tensor product of linear maps above.This is a special case of the product of tensors if they are seen as multilinear maps (see also tensors as multilinear maps). Thus the components of the tensor product of multilinear forms can be computed by the Kronecker product.is isomorphic (as an A-algebra) to the Adeg(f).where now f is interpreted as the same polynomial, but with its coefficients regarded as elements of B. In the larger field B, the polynomial may become reducible, which brings in Galois theory. For example, if A = B is a Galois extension of R, thenA particular example is when A and B are fields containing a common subfield R. The tensor product of fields is closely related to Galois theory: if, say, A = R[x] / f(x), where f is some irreducible polynomial with coefficients in R, the tensor product can be calculated asFor example,Let R be a commutative ring. The tensor product of R-modules applies, in particular, if A and B are R-algebras. In this case, the tensor product A \u2297R B is an R-algebra itself by puttingis not usually injective. For example, tensoring the (injective) map given by multiplication with n, n\u00a0: Z \u2192 Z with Z/nZ yields the zero map 0\u00a0: Z/nZ \u2192 Z/nZ, which is not injective. Higher Tor functors measure the defect of the tensor product being not left exact. All higher Tor functors are assembled in the derived tensor product.Here NJ\u00a0:= \u2a01j \u2208 J N and the map is determined by sending some n \u2208 N in the jth copy of NJ to ajin (in NI). Colloquially, this may be rephrased by saying that a presentation of M gives rise to a presentation of M \u2297R N. This is referred to by saying that the tensor product is a right exact functor. It is not in general left exact, that is, given an injective map of R-modules M1 \u2192 M2, the tensor productFor vector spaces, the tensor product V \u2297 W is quickly computed since bases of V of W immediately determine a basis of V \u2297 W, as was mentioned above. For modules over a general (commutative) ring, not every module is free. For example, Z/nZ is not a free abelian group (= Z-module). The tensor product with Z/nZ is given byLet A be a right R-module and B be a left R-module B. Then the tensor product of A and B is an abelian group defined byThe universal property also carries over, slightly modified: the map \u03c6\u00a0: A \u00d7 B \u2192 A \u2297R B defined by (a, b) \u21a6 a \u2297 b is a middle linear map (referred to as \"the canonical middle linear map\".[13]); that is,[14] it satisfies:is imposed. If R is non-commutative, this is no longer an R-module, but just an abelian group.More generally, the tensor product can be defined even if the ring is non-commutative (ab \u2260 ba). In this case A has to be a right-R-module and B is a left-R-module, and instead of the last two relations above, the relationwhere now F(A \u00d7 B) is the free R-module generated by the cartesian product and G is the R-module generated by the same relations as above.The tensor product of two modules A and B over a commutative ring R is defined in exactly the same way as the tensor product of vector spaces over a field:where u\u2217 in End(V\u2217) is the transpose of u, that is, in terms of the obvious pairing on V \u2297 V\u2217,Here Hom(-,-) denotes the K-vector space of all linear maps. This is an example of adjoint functors: the tensor product is \"left adjoint\" to Hom.Furthermore, given three vector spaces U, V, W the tensor product is linked to the vector space of all linear maps, as follows:This result impliesGiven two finite dimensional vector spaces U, V, denote the dual space of U as U*, we have the following relation:The interplay of evaluation and coevaluation map can be used to characterize finite-dimensional vector spaces without referring to bases.[12]where v1, ..., vn is any basis of V, and vi\u2217 is its dual basis. Surprisingly, this map does not depend on our choice of basis.[11]On the other hand, if V is finite-dimensional, there is a canonical map in the other direction (called the coevaluation map)is called tensor contraction (for r, s > 0).The resulting mapwhich on elementary tensors is defined byA particular example is the tensor product of some vector space V with its dual vector space V\u2217 (which consists of all linear maps f from V to the ground field K). In this case, there is a canonical evaluation mapand[10] Thus, the components of the tensor product of two tensors are the ordinary product of the components of each tensor. Another example: let U be a tensor of type (1, 1) with components U\u03b1\u03b2, and let V be a tensor of type (1, 0) with components V\u2009\u03b3. ThenPicking a basis of V and the corresponding dual basis of V\u2217 naturally induces a basis for Tr\ns(V) (this basis is described in the article on Kronecker products). In terms of these bases, the components of a (tensor) product of two (or more) tensors can be computed. For example, if F and G are two covariant tensors of rank m and n respectively (i.e. F \u2208 T\u20090\nm, and G \u2208 T\u20090\nn), then the components of their tensor product are given byIt is defined by grouping all occurring \"factors\" V together: writing vi for an element of V and fi for elements of the dual space,There is a product map, called the (tensor) product of tensors[9]Here V\u2217 is the dual vector space (which consists of all linear maps f from V to the ground field K).For non-negative integers r and s a type (r,s) tensor on a vector space V is an element ofThe isomorphism \u03c4\u03c3 is called the braiding map associated to the permutation \u03c3.such thatbe the natural multilinear embedding of the Cartesian power of V into the tensor power of V. Then, by the universal property, there is a unique isomorphismLetA permutation \u03c3 of the set {1, 2, ..., n} determines a mapping of the nth Cartesian power of V as follows:Let n be a non-negative integer. The nth tensor power of the vector space V is the n-fold tensor product of V with itself. That isThe universal-property definition of a tensor product is valid in more categories that just the category of vector spaces. Instead of using multilinear (bilinear) maps, the general tensor product definition uses multimorphisms.[8]The category of vector spaces with tensor product is an example of a symmetric monoidal category.Similar reasoning can be used to show that the tensor product is associative, that is, there are natural isomorphismsThis characterization can simplify proofs about the tensor product. For example, the tensor product is symmetric, meaning there is a canonical isomorphism:A dyadic product is the special case of the tensor product between two vectors of the same dimension.The resultant rank is at most 4, and thus the resultant dimension is 4. Here rank denotes the tensor rank (number of requisite indices), while the matrix rank counts the number of degrees of freedom in the resulting array.respectively, then the tensor product of these two matrices isBy choosing bases of all vector spaces involved, the linear maps S and T can be represented by matrices. Then, the matrix describing the tensor product S \u2297 T is the Kronecker product of the two matrices. For example, if V, X, W, and Y above are all two-dimensional and bases have been fixed for all of them, and S and T are given by the matricesIf S and T are both injective, surjective, or continuous then S \u2297 T is, respectively, injective, surjective, continuous.In this way, the tensor product becomes a bifunctor from the category of vector spaces to itself, covariant in both arguments.[7]defined byThe tensor product also operates on linear maps between vector spaces. Specifically, given two linear maps S\u00a0: V \u2192 X and T\u00a0: W \u2192 Y between vector spaces, the tensor product of the two linear maps S and T is a linear mapGiven bases {vi} and {wj} for V and W respectively, the tensors {vi \u2297 wj} form a basis for V \u2297 W. Therefore, if V and W are finite-dimensional, the dimension of the tensor product is the product of dimensions of the original spaces; for instance Rm \u2297 Rn is isomorphic to Rmn.Elements of V \u2297 W are often referred to as tensors, although this term refers to many other related concepts as well.[5] If v belongs to V and w belongs to W, then the equivalence class of (v, w) is denoted by v \u2297 w, which is called the tensor product of v with w. In physics and engineering, this use of the \"\u2297\" symbol refers specifically to the outer product operation; the result of the outer product v \u2297 w is one of the standard ways of representing the equivalence class v \u2297 w.[6] An element of V \u2297 W that can be written in the form v \u2297 w is called a pure or simple tensor. In general, an element of the tensor product space is not a pure tensor, but rather a finite linear combination of pure tensors. For example, if v1 and v2 are linearly independent, and w1 and w2 are also linearly independent, then v1 \u2297 w1 + v2 \u2297 w2 cannot be written as a pure tensor. The number of simple tensors required to express an element of a tensor product is called the tensor rank (not to be confused with tensor order, which is the number of spaces one has taken the product of, in this case 2; in notation, the number of indices), and for linear operators or matrices, thought of as (1, 1) tensors (elements of the space V \u2297 V\u2217), it agrees with matrix rank.all hold (unlike in F(V \u00d7 W)), which is exactly what is desired. In these latter expressions, the (v1, w), etc., are images in the quotient of vectors in the free product under the quotient map. Usually, some other notation is employed for them, see below.In the quotient, where N is mapped to the zero vector, the following equalities,The following expression explicitly gives the subspace N:[4]The result can be proven to be independent of which representatives of the involved classes have been chosen. In other words, the operations are well-defined.in the involved equivalence classes outputting the one equivalence class of the result.The operations of V \u2297 W, i.e. the map of vector addition +\u00a0: U \u00d7 U \u2192 U and scalar multiplication \u22c5\u00a0: K \u00d7 U \u2192 U are defined to be the respective operations +F and \u22c5F from F(V \u00d7 W), acting on any representativesFrom the Cartesian product V \u00d7 W, the free vector space F(V \u00d7 W) over K is formed. The vectors of V \u2297 W are then defined to be the equivalence classes of the congruence generated by the following relations on F(V \u00d7 W):In general, given two vector spaces V and W over a field K, the tensor product U of V and W, denoted as U = V \u2297 W is defined as the vector space whose elements and operations are constructed as follows:Let us first consider a special case: let us say V, W are free vector spaces for the sets S, T respectively. That is, V = F(S), W = F(T). In this special case, the tensor product is defined as F(S) \u2297 F(T) = F(S \u00d7 T). In most typical cases, any vector space can be immediately understood as the free vector space for some set, so this definition suffices. However, there is also an explicit way of constructing the tensor product directly from V, W, without appeal to S, T.By construction, the (possibly infinite) dimension of the vector space F(S) equals the cardinality of the set S.Then {\u03b4s | s \u2208 S} is a basis for F(S), since each element g of F(S) can be uniquely written as a linear combination of \u03b4s, and because of the restriction that g has finite support, this linear combination consists of finitely many terms. Because of this explicit expression, an element of F(S) is often called a formal sum of symbols in S.The definition of \u2297 requires the notion of the free vector space F(S) on some set S, a vector space whose basis is indexed by S. F(S) is defined as the set of all functions g from S to a given field K that have finite support; i.e., g is identically zero outside some finite subset of S. It is a vector space over K with the usual addition and scalar multiplication of functions. It has a basis parameterized by S. Indeed, for each s in S we define[1]The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below). Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over V \u00d7 W. This approach is described below.The tensor product of two vector spaces V and W over a field K is another vector space over K. It is denoted V \u2297K W, or V \u2297 W when the underlying field K is understood.More generally, the tensor product can be extended to other categories of mathematical objects in addition to vector spaces, such as to matrices, tensors, algebras, topological vector spaces, and modules. In each such case the tensor product is characterized by a similar universal property: it is the freest bilinear operation. The general concept of a \"tensor product\" is captured by monoidal categories; that is, the class of all things that have a tensor product is a monoidal category.In particular, this distinguishes the tensor product from the direct sum vector space, whose dimension is the sum of the dimensions of the two summands:The tensor product of (finite dimensional) vector spaces has dimension equal to the product of the dimensions of the two factors:In mathematics, the tensor product V \u2297 W of two vector spaces V and W (over the same field) is itself a vector space, together with an operation of bilinear composition, denoted by \u2297, from ordered pairs in the Cartesian product V \u00d7 W into V \u2297 W, in a way that generalizes the outer product. The tensor product of V and W is the vector space generated by the symbols v \u2297 w, with v \u2208 V and w \u2208 W, in which the relations of bilinearity are imposed for the product operation \u2297, and no other relations are assumed to hold. The tensor product space is thus the \"freest\" (or most general) such vector space, in the sense of having the fewest constraints.",
            "title": "\nTensor product\n",
            "url": "https://en.wikipedia.org/wiki/Tensor_product"
        },
        {
            "links": [
                "/wiki/Near-rings",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Semiring",
                "/wiki/Commutative",
                "/wiki/Monoid",
                "/wiki/Semiring",
                "/wiki/Vector_space",
                "/wiki/Ringed_space",
                "/wiki/Sheaf_(mathematics)",
                "/wiki/Sheaf_of_modules",
                "/wiki/Algebraic_geometry",
                "/wiki/Preadditive_category",
                "/wiki/Additive_functor",
                "/wiki/Functor_category",
                "/wiki/Injective",
                "/wiki/Integer",
                "/wiki/Modular_arithmetic",
                "/wiki/Group_homomorphism",
                "/wiki/Ring_homomorphism",
                "/wiki/Uniform_module",
                "/wiki/Graded_module",
                "/wiki/Graded_ring",
                "/wiki/Artinian_module",
                "/wiki/Descending_chain_condition",
                "/wiki/Noetherian_module",
                "/wiki/Ascending_chain_condition",
                "/wiki/Torsion-free_module",
                "/wiki/Faithful_module",
                "/wiki/Annihilator_(ring_theory)",
                "/wiki/Indecomposable_module",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Uniform_module",
                "/wiki/Semisimple_module",
                "/wiki/Simple_module",
                "/wiki/Torsionless_module",
                "/wiki/Flat_module",
                "/wiki/Tensor_product_of_modules",
                "/wiki/Exact_sequence",
                "/wiki/Injective_module",
                "/wiki/Projective_module",
                "/wiki/Direct_summand",
                "/wiki/Free_module",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Cyclic_module",
                "/wiki/Finitely_generated_module",
                "/wiki/Linear_combination",
                "/wiki/Category_theory",
                "/wiki/Category_of_modules",
                "/wiki/Abelian_category",
                "/wiki/Kernel_(algebra)",
                "/wiki/Isomorphism_theorem",
                "/wiki/Bijective",
                "/wiki/Isomorphism",
                "/wiki/Homomorphism",
                "/wiki/Linear_map",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_homomorphism",
                "/wiki/Lattice_(order)",
                "/wiki/Modular_lattice",
                "/wiki/Subgroup",
                "/wiki/Commutative_ring",
                "/wiki/Bimodule",
                "/wiki/Endomorphism",
                "/wiki/Ring_homomorphism",
                "/wiki/Endomorphism_ring",
                "/wiki/Group_action",
                "/wiki/Monoid_action",
                "/wiki/Representation_theory",
                "/wiki/Group_ring",
                "/wiki/Unital_algebra",
                "/wiki/Glossary_of_ring_theory",
                "/wiki/Ring_(mathematics)",
                "/wiki/Abelian_group",
                "/wiki/Well-behaved",
                "/wiki/Principal_ideal_domain",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Free_module",
                "/wiki/Invariant_basis_number",
                "/wiki/Axiom_of_choice",
                "/wiki/Lp_space",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Distributive_law",
                "/wiki/Ring_(mathematics)",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Quotient_ring",
                "/wiki/Representation_theory",
                "/wiki/Group_(mathematics)",
                "/wiki/Commutative_algebra",
                "/wiki/Homological_algebra",
                "/wiki/Algebraic_geometry",
                "/wiki/Algebraic_topology",
                "/wiki/Abelian_group",
                "/wiki/Semigroup_action",
                "/wiki/Mathematics",
                "/wiki/Algebraic_structure",
                "/wiki/Abstract_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Scalar_(mathematics)"
            ],
            "text": "Over near-rings, one can consider near-ring modules, a nonabelian generalization of modules.[citation needed]One can also consider modules over a semiring. Modules over rings are abelian groups, but modules over semirings are only commutative monoids. Most applications of modules are still possible. In particular, for any semiring S the matrices over S form a semiring over which the tuples of elements from S are a module (in this generalized sense only). This allows a further generalization of the concept of vector space incorporating the semirings from theoretical computer science.Modules over commutative rings can be generalized in a different direction: take a ringed space (X, OX) and consider the sheaves of OX-modules; see sheaf of modules for more. These form a category OX-Mod, and play an important role in modern algebraic geometry. If X has only a single point, then this is a module category in the old sense over the commutative ring OX(X).Any ring R can be viewed as a preadditive category with a single object. With this understanding, a left R-module is nothing but a (covariant) additive functor from R to the category Ab of abelian groups. Right R-modules are contravariant additive functors. This suggests that, if C is any preadditive category, a covariant additive functor from C to Ab should be considered a generalized left module over C; these functors form a functor category C-Mod which is the natural generalization of the module category R-Mod.A representation is called faithful if and only if the map R \u2192 EndZ(M) is injective. In terms of modules, this means that if r is an element of R such that rx = 0 for all x in M, then r = 0. Every abelian group is a faithful module over the integers or over some modular arithmetic Z/nZ.Such a ring homomorphism R \u2192 EndZ(M) is called a representation of R over the abelian group M; an alternative and equivalent way of defining left R-modules is to say that a left R-module is an abelian group M together with a representation of R over it.If M is a left R-module, then the action of an element r in R is defined to be the map M \u2192 M that sends each x to rx (or xr in the case of a right module), and is necessarily a group endomorphism of the abelian group (M, +). The set of all group endomorphisms of M is denoted EndZ(M) and forms a ring under addition and composition, and sending a ring element r of R to its action actually defines a ring homomorphism from R to EndZ(M).Uniform. A uniform module is a module in which all pairs of nonzero submodules have nonzero intersection.Graded. A graded module is a module with a decomposition as a direct sum M = \u2a01x Mx over a graded ring R = \u2a01x Rx such that RxMy \u2282 Mx+y for all x and y.Artinian. An Artinian module is a module which satisfies the descending chain condition on submodules, that is, every decreasing chain of submodules becomes stationary after finitely many steps.Noetherian. A Noetherian module is a module which satisfies the ascending chain condition on submodules, that is, every increasing chain of submodules becomes stationary after finitely many steps. Equivalently, every submodule is finitely generated.Torsion-free. A torsion-free module is a module over a ring such that 0 is the only element annihilated by a regular element (non zero-divisor) of the ring.Faithful. A faithful module M is one where the action of each r \u2260 0 in R on M is nontrivial (i.e. r \u22c5 x \u2260 0 for some x in M). Equivalently, the annihilator of M is the zero ideal.Indecomposable. An indecomposable module is a non-zero module that cannot be written as a direct sum of two non-zero submodules. Every simple module is indecomposable, but there are indecomposable modules which are not simple (e.g. uniform modules).Semisimple. A semisimple module is a direct sum (finite or not) of simple modules. Historically these modules are also called completely reducible.Simple. A simple module S is a module that is not {0} and whose only submodules are {0} and S. Simple modules are sometimes called irreducible.[3]Torsionless module. A module is called torsionless if it embeds into its algebraic dual.Flat. A module is called flat if taking the tensor product of it with any exact sequence of R-modules preserves exactness.Injective. Injective modules are defined dually to projective modules.Projective. Projective modules are direct summands of free modules and share many of their desirable properties.Free. A free R-module is a module that has a basis, or equivalently, one that is isomorphic to a direct sum of copies of the ring R. These are the modules that behave very much like vector spaces.Cyclic. A module is called a cyclic module if it is generated by one element.Finitely generated. An R-module M is finitely generated if there exist finitely many elements x1, ..., xn in M such that every element of M is a linear combination of those elements with coefficients from the ring R.The left R-modules, together with their module homomorphisms, form a category, written as R-Mod (see category of modules for more.) This is an abelian category.The kernel of a module homomorphism f\u00a0: M \u2192 N is the submodule of M consisting of all elements that are sent to zero by f. The isomorphism theorems familiar from groups and vector spaces are also valid for R-modules.A bijective module homomorphism is an isomorphism of modules, and the two modules are called isomorphic. Two isomorphic modules are identical for all practical purposes, differing solely in the notation for their elements.This, like any homomorphism of mathematical objects, is just a mapping which preserves the structure of the objects. Another name for a homomorphism of modules over R is an R-linear map.If M and N are left R-modules, then a map f\u00a0: M \u2192 N is a homomorphism of R-modules if, for any m, n in M and r, s in R,The set of submodules of a given module M, together with the two binary operations + and \u2229, forms a lattice which satisfies the modular law: Given submodules U, N1, N2 of M such that N1 \u2282 N2, then the following two submodules are equal: (N1 + U) \u2229 N2 = N1 + (U \u2229 N2).Suppose M is a left R-module and N is a subgroup of M. Then N is a submodule (or R-submodule, to be more explicit) if, for any n in N and any r in R, the product r \u22c5 n is in N (or n \u22c5 r for a right module).If R is commutative, then left R-modules are the same as right R-modules and are simply called R-modules.A bimodule is a module that is a left module and a right module such that the two multiplications are compatible.If one writes the scalar action as fr so that fr(x) = r \u22c5 x, and f for the map that takes each r to its corresponding map fr\u00a0, then the first axiom states that every fr is a group endomorphism of M, and the other three axioms assert that the map f\u00a0: R \u2192 End(M) given by r \u21a6 fr is a ring homomorphism from R to the endomorphism ring End(M).[2] Thus a module is a ring action on an abelian group (cf. group action. Also consider monoid action of multiplicative structure of R). In this sense, module theory generalizes representation theory, which deals with group actions on vector spaces, or equivalently group ring actions.Authors who do not require rings to be unital omit condition 4 above in the definition of an R-module, and so would call the structures defined above \"unital left R-modules\". In this article, consistent with the glossary of ring theory, all rings and modules are assumed to be unital.[1]The operation of the ring on M is called scalar multiplication, and is usually written by juxtaposition, i.e. as rx for r in R and x in M, though here it is denoted as r \u22c5 x to distinguish it from the ring multiplication operation, denoted here by juxtaposition. The notation RM indicates a left R-module M. A right R-module M or MR is defined similarly, except that the ring acts on the right; i.e., scalar multiplication takes the form \u22c5\u00a0: M \u00d7 R \u2192 M, and the above axioms are written with scalars r and s on the right of x and y.Suppose that R is a ring and 1R is its multiplicative identity. A left R-module M consists of an abelian group (M, +) and an operation \u22c5\u00a0: R \u00d7 M \u2192 M such that for all r, s in R and x, y in M, we have:Much of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a \"well-behaved\" ring, such as a principal ideal domain. However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis, and even those that do, free modules, need not have a unique rank if the underlying ring does not satisfy the invariant basis number condition, unlike vector spaces, which always have a (possibly infinite) basis whose cardinality is then unique. (These last two assertions require the axiom of choice in general, but not in the case of finite-dimensional spaces, or certain well-behaved infinite-dimensional spaces such as Lp spaces.)In a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law. In a module, the scalars need only be a ring, so the module concept represents a significant generalization. In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules. In non-commutative algebra the distinction between left ideals, ideals, and modules becomes more pronounced, though some ring-theoretic conditions can be expressed either about left ideals or left modules.Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is compatible with the ring multiplication.In mathematics, a module is one of the fundamental algebraic structures used in abstract algebra. A module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary given ring (with identity) and a multiplication (on the left and/or on the right) is defined between elements of the ring and elements of the module.",
            "title": "\nModule (mathematics)\n",
            "url": "https://en.wikipedia.org/wiki/Module_(mathematics)"
        },
        {
            "links": [],
            "text": "",
            "title": "\nField (mathematics)\n",
            "url": "https://en.wikipedia.org/wiki/Field_(mathematics)"
        },
        {
            "links": [
                "/wiki/Local_ring",
                "/wiki/Perfect_ring",
                "/wiki/Dedekind_ring",
                "/wiki/Projective_modules",
                "/wiki/Flat_module",
                "/wiki/Torsion-free_module",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Free_abelian_group",
                "/wiki/Mathematics",
                "/wiki/Module_(mathematics)",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Generating_set_of_a_module",
                "/wiki/Linearly_independent",
                "/wiki/Vector_space",
                "/wiki/Ring_(mathematics)",
                "/wiki/Division_ring",
                "/wiki/Field_(mathematics)",
                "/wiki/Commutative_ring"
            ],
            "text": "See local ring, perfect ring and Dedekind ring.Many statements about free modules, which are wrong for general modules over rings, are still true for certain generalisations of free modules. Projective modules are direct summands of free modules, so one can choose an injection in a free module and use the basis of this one to prove something for the projective module. Even weaker generalisations are flat modules, which still have the property that tensoring with them preserves exact sequences, and torsion-free modules. If the ring has special properties, this hierarchy may collapse, e.g., for any perfect local Dedekind ring, every torsion-free module is flat, projective and free as well. A finitely generated torsion-free module of a commutative PID is free. A finitely generated Z-module is free if and only if it is flat.and the scalar multiplication by: for r in R and x in E,We equip it with a structure of a left module such that the addition is defined by: for x in E,Given a ring R and a set E, first as a set we letThe free module R(E) may also be constructed in the following equivalent way.A similar argument shows that every free left (resp. right) R-module is isomorphic to a direct sum of copies of R as left (resp. right) module.Given a set E and ring R, there is a free R-module that has E as a basis: namely, the direct sum of copies of R indexed by ELet R be a ring.An immediate consequence of the second half of the definition is that the coefficients in the first half are unique for each element of M.A free module is a module with a basis.[2]A free abelian group is precisely a free module over the ring Z of integers.Given any set S and ring R, there is a free R-module with basis S, which is called free module on S or module of formal linear combinations of the elements of S.In mathematics, a free module is a module that has a basis \u2013 that is, a generating set consisting of linearly independent elements. Every vector space is a free module,[1] but, if the ring of the coefficients is not a division ring (not a field in the commutative case), then there exist non-free modules.",
            "title": "\nFree module\n",
            "url": "https://en.wikipedia.org/wiki/Free_module"
        },
        {
            "links": [
                "/wiki/Compiler_optimizations",
                "/wiki/Parallelizing_compiler",
                "/wiki/Computer_graphics",
                "/wiki/Transformation_matrix",
                "/wiki/Topological_vector_space",
                "/wiki/Normed_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Continuous_linear_operator",
                "/wiki/Bounded_operator",
                "/wiki/Discontinuous_linear_operator",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Tensor",
                "/wiki/Endomorphism",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Euler_characteristic",
                "/wiki/Operator_theory",
                "/wiki/Fredholm",
                "/wiki/Atiyah%E2%80%93Singer_index_theorem",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Exact_sequence",
                "/wiki/Rank_of_a_matrix",
                "/wiki/Kernel_(matrix)#Subspace_properties",
                "/wiki/Linear_subspace",
                "/wiki/Dimension",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Image_(mathematics)",
                "/wiki/Range_(mathematics)",
                "/wiki/Isomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Group_isomorphism",
                "/wiki/General_linear_group",
                "/wiki/Isomorphism",
                "/wiki/Automorphism",
                "/wiki/Group_(math)",
                "/wiki/Automorphism_group",
                "/wiki/Endomorphisms",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Endomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Ring_(algebra)",
                "/wiki/Identity_function",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_addition",
                "/wiki/Associative_algebra",
                "/wiki/Composition_of_maps",
                "/wiki/Pointwise",
                "/wiki/Inverse_function",
                "/wiki/Relation_composition",
                "/wiki/Class_(set_theory)",
                "/wiki/Morphism",
                "/wiki/Category_(mathematics)",
                "/wiki/Dimension",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Finite-dimensional",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "text": "Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A\u2032 = B\u22121AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V \u2192 W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \u2192 V \u2192 W \u2192 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\u2013Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \u2212 dim(W), by rank\u2013nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \u03c1(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \u03bd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\u2013nullity theorem:If f\u00a0: V \u2192 W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \u00d7 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \u00d7 n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V \u2192 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \u2192 V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f\u00a0: V \u2192 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1\u00a0: V \u2192 W and f2\u00a0: V \u2192 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f\u00a0: V \u2192 W and g\u00a0: W \u2192 Z are linear, then so is their composition g \u2218 f\u00a0: V \u2192 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 \u00d7 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol \u2217 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m \u00d7 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f\u00a0: V \u2192 W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \u00d7 n matrix, then f(x) = Ax describes a linear map Rn \u2192 Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\u00a0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \u2192 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.",
            "title": "\nLinear map\n",
            "url": "https://en.wikipedia.org/wiki/Linear_map"
        },
        {
            "links": [
                "/wiki/Kronecker_delta",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Hilbert_space",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Antilinear",
                "/wiki/Euclidean_space",
                "/wiki/Dot_product",
                "/wiki/Bilinear_form",
                "/wiki/Isomorphism",
                "/wiki/Level_set",
                "/wiki/Hyperplane",
                "/wiki/General_relativity",
                "/wiki/Gravitation_(book)",
                "/wiki/Generalized_function",
                "/wiki/Distribution_(mathematics)",
                "/wiki/Test_function",
                "/wiki/Quantum_mechanics",
                "/wiki/Hilbert_space",
                "/wiki/Antilinear",
                "/wiki/Linear_isomorphism",
                "/wiki/Bra%E2%80%93ket_notation",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Numerical_quadrature",
                "/wiki/Linear_subspace",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Lagrange_interpolation",
                "/wiki/Functional_analysis",
                "/wiki/Function_space",
                "/wiki/Integral",
                "/wiki/Riemann_integral",
                "/wiki/Topological_vector_space",
                "/wiki/Continuous_function",
                "/wiki/Continuous_dual_space",
                "/wiki/Banach_space",
                "/wiki/Pointwise",
                "/wiki/Dual_space",
                "/wiki/Continuous_dual_space",
                "/wiki/Linear_algebra",
                "/wiki/One-form",
                "/wiki/Linear_map",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Euclidean_vector",
                "/wiki/Column_vector",
                "/wiki/Row_vector",
                "/wiki/Dot_product",
                "/wiki/Matrix_product",
                "/wiki/Row_vector",
                "/wiki/Column_vector",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)"
            ],
            "text": "In higher dimensions, this generalizes as followsSo each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.\u00a0 Thenwhere \u03b4 is the Kronecker delta.\u00a0 Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.Or, more succinctly,In an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.\u00a0 There is a mapping V \u2192 V\u2217 into the continuous dual space V\u2217.\u00a0 However, this mapping is antilinear rather than linear.The above defined vector v\u2217 \u2208 V\u2217 is said to be the dual vector of v \u2208 V.The inverse isomorphism is V\u2217 \u2192 V\u00a0: v\u2217 \u21a6 v, where v is the unique element of V such thatwhere the bilinear form on V is denoted \u27e8 , \u27e9 (for instance, in Euclidean space \u27e8v, w\u27e9 = v \u22c5 w is the dot product of v and w).Every non-degenerate bilinear form on a finite-dimensional vector space V induces an isomorphism V \u2192 V\u2217\u00a0: v \u21a6 v\u2217 such thatIn finite dimensions, a linear functional can be visualized in terms of its level sets.\u00a0 In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.\u00a0 This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as Gravitation by Misner, Thorne & Wheeler (1973).In the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.Linear functionals are particularly important in quantum mechanics.\u00a0 Quantum mechanical systems are represented by Hilbert spaces, which are anti\u2013isomorphic to their own dual spaces.\u00a0 A state of a quantum mechanical system can be identified with a linear functional.\u00a0 For more information see bra\u2013ket notation.This follows from the fact that the linear functionals evxi\u00a0: f \u2192 f(xi) defined above form a basis of the dual space of Pn.[1]for all f \u2208 Pn. This forms the foundation of the theory of numerical quadrature.The integration functional I defined above defines a linear functional on the subspace Pn of polynomials of degree \u2264 n. If x0, ..., xn are n + 1 distinct points in [a, b], then there are coefficients a0, ..., an for whichIf x0, ..., xn are n + 1 distinct points in [a, b], then the evaluation functionals evxi, i = 0, 1, ..., n form a basis of the dual space of Pn.\u00a0 (Lax (1996) proves this last fact using Lagrange interpolation.)The mapping f\u00a0\u2192\u00a0f(c) is linear sinceLet Pn denote the vector space of real-valued polynomial functions of degree \u2264n defined on an interval [a,\u00a0b].\u00a0 If c\u00a0\u2208\u00a0[a,\u00a0b], then let evc\u00a0: Pn \u2192 R be the evaluation functionalis a linear functional from the vector space C[a,\u00a0b] of continuous functions on the interval [a,\u00a0b] to the real numbers. The linearity of I follows from the standard facts about the integral:Linear functionals first appeared in functional analysis, the study of vector spaces of functions.\u00a0 A typical example of a linear functional is integration: the linear transformation defined by the Riemann integraland each linear functional can be expressed in this form.For each row vector [a1 \u2026 an] there is a linear functional f defined bySuppose that vectors in the real coordinate space Rn are represented as column vectorsIf V is a topological vector space, the space of continuous linear functionals \u2014 the continuous dual \u2014 is often simply called the dual space.\u00a0 If V is a Banach space, then so is its (continuous) dual.\u00a0 To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the algebraic dual space.\u00a0 In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.The set of all linear functionals from V to k, Homk(V,k), forms a vector space over k with the addition of the operations of addition and scalar multiplication (defined pointwise).\u00a0 This space is called the dual space of V, or sometimes the algebraic dual space, to distinguish it from the continuous dual space.\u00a0 It is often written V\u2217, V\u2032, or V\u142f when the field k is understood.In linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In \u211dn, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.\u00a0 In general, if V is a vector space over a field k, then a linear functional f is a function from V to k that is linear:",
            "title": "\nLinear form\n",
            "url": "https://en.wikipedia.org/wiki/Linear_functional"
        },
        {
            "links": [],
            "text": "",
            "title": "\nCramer's rule\n",
            "url": "https://en.wikipedia.org/wiki/Cramer%27s_rule"
        },
        {
            "links": [
                "/wiki/Image_(mathematics)",
                "/wiki/Linear_transformation",
                "/wiki/Translation_(geometry)",
                "/wiki/Flat_(geometry)",
                "/wiki/Euclidean_subspace",
                "/wiki/Euclidean_subspace",
                "/wiki/Kernel_(matrix)",
                "/wiki/Singular_value_decomposition#Solving_homogeneous_linear_equations",
                "/wiki/Singular_matrix",
                "/wiki/Zero_vector",
                "/wiki/Quantum_algorithm_for_linear_systems_of_equations",
                "/wiki/Iterative_method",
                "/wiki/Symmetric_matrix",
                "/wiki/Positive-definite_matrix",
                "/wiki/Cholesky_decomposition",
                "/wiki/Levinson_recursion",
                "/wiki/Toeplitz_matrix",
                "/wiki/Sparse_matrix",
                "/wiki/Cracovian",
                "/wiki/Pivot_element",
                "/wiki/LU_decomposition",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Determinant",
                "/wiki/Gaussian_elimination",
                "/wiki/Gauss-Jordan_elimination",
                "/wiki/Elementary_row_operations",
                "/wiki/Reduced_row_echelon_form",
                "/wiki/Augmented_matrix",
                "/wiki/Degrees_of_freedom_(statistics)",
                "/wiki/Dimension",
                "/wiki/Algorithm",
                "/wiki/Equation_solving",
                "/wiki/Rouch%C3%A9%E2%80%93Capelli_theorem",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Augmented_matrix",
                "/wiki/Coefficient_matrix",
                "/wiki/Independent_equation",
                "/wiki/Parallel_(geometry)",
                "/wiki/Contradiction",
                "/wiki/Linear_independence",
                "/wiki/Linear_independence",
                "/wiki/Dimension",
                "/wiki/Hyperplane",
                "/wiki/N-dimensional_space",
                "/wiki/Flat_(geometry)",
                "/wiki/Plane_(mathematics)",
                "/wiki/Three-dimensional_space",
                "/wiki/Line_(mathematics)",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Intersection_(set_theory)",
                "/wiki/Empty_set",
                "/wiki/Set_(mathematics)",
                "/wiki/Solution_set",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Column_vector",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Module_(mathematics)",
                "/wiki/Span_(linear_algebra)",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Linearly_independent",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Column_vector",
                "/wiki/Linear_combination",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Integer",
                "/wiki/Rational_number",
                "/wiki/Algebraic_structure",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Integral_domain",
                "/wiki/Ring_(mathematics)",
                "/wiki/Integer",
                "/wiki/Algebraic_structure",
                "/wiki/Linear_equation_over_a_ring",
                "/wiki/Integer_linear_programming",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Polynomial",
                "/wiki/Tropical_geometry",
                "/wiki/Linear_algebra",
                "/wiki/Algorithm",
                "/wiki/Numerical_linear_algebra",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Computer_science",
                "/wiki/Economics",
                "/wiki/Nonlinear_system",
                "/wiki/Approximation",
                "/wiki/Linearization",
                "/wiki/Mathematical_model",
                "/wiki/Computer_simulation",
                "/wiki/Complex_system",
                "/wiki/Equation_solving",
                "/wiki/Mathematics",
                "/wiki/Linear_equation",
                "/wiki/Variable_(math)"
            ],
            "text": "This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) \u2260 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:where A is an m \u00d7 n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.A homogeneous system is equivalent to a matrix equation of the formA system of linear equations is homogeneous if all of the constant terms are zero:There is also a quantum algorithm for linear systems of equations.[3]A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.is given byCramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemThe last matrix is in reduced row echelon form, and represents the system x = \u221215, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = \u221215. Therefore, the solution set is the single point (x, y, z) = (\u221215, 8, 2).Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Solving the first equation for x gives x = 5 + 2z \u2212 3y, and plugging this into the second and third equation yieldsFor example, consider the following system:The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:Here x is the free variable, and y and z are dependent.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.The solution set to this system can be described by the following equations:For example, consider the following system:To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.There are several algorithms for solving a system of linear equations.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.Putting it another way, according to the Rouch\u00e9\u2013Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.For example, the equationsA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.For a more complicated example, the equationsare not independent \u2014 they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For example, the equationsThe equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.The following pictures illustrate this trichotomy in the case of two variables:In the first case, the dimension of the solution set is, in general, equal to n \u2212 m, where n is the number of variables and m is the number of equations.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, \"in general\" means that a different behavior may occur for specific values of the coefficients of the equations.For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.A linear system may behave in any one of three possible ways:A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.The number of vectors in a basis for the span is now expressed as the rank of the matrix.where A is an m\u00d7n matrix, x is a column vector with n entries, and b is a column vector with m entries.The vector equation is equivalent to a matrix equation of the formThis allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.A general system of m linear equations with n unknowns can be written asNow substitute this expression for x into the bottom equation:The simplest kind of linear system involves two equations and two variables:Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the \"best\" integer solution (when there are many). Gr\u00f6bner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.since it makes all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given byIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,",
            "title": "\nSystem of linear equations\n",
            "url": "https://en.wikipedia.org/wiki/System_of_linear_equations"
        },
        {
            "links": [],
            "text": "",
            "title": "\nHomogeneous coordinates\n",
            "url": "https://en.wikipedia.org/wiki/Homogeneous_coordinates"
        },
        {
            "links": [
                "/wiki/Three-dimensional_space",
                "/wiki/Hyperplane",
                "/wiki/Euclidean_space",
                "/wiki/Affine_space",
                "/wiki/Equation_solving",
                "/wiki/Tax_bracket",
                "/wiki/Progressive_tax#Computation",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Linear_map",
                "/wiki/Affine_function",
                "/wiki/Graph_of_a_function",
                "/wiki/Vertical_line_test",
                "/wiki/Interpolation",
                "/wiki/Extrapolation",
                "/wiki/Simultaneous_equations",
                "/wiki/Linear_algebra",
                "/wiki/System_of_linear_equations",
                "/wiki/Gauss-Jordan",
                "/wiki/Determinant",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Line_(geometry)",
                "/wiki/Coordinate",
                "/wiki/Slope",
                "/wiki/Elementary_algebra",
                "/wiki/Constant_term",
                "/wiki/Nonlinear_system",
                "/wiki/Straight_line",
                "/wiki/Slope",
                "/wiki/Constant_term",
                "/wiki/Inconsistent_equations",
                "/wiki/Equation",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Coefficient",
                "/wiki/Field_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Non-linear_equation",
                "/wiki/Algebraic_equation",
                "/wiki/Term_(mathematics)",
                "/wiki/Constant_term",
                "/wiki/Variable_(mathematics)",
                "/wiki/Number",
                "/wiki/Parameter",
                "/wiki/Function_(mathematics)",
                "/wiki/Linear_regression"
            ],
            "text": "If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n\u00a0\u2013\u00a01)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).In other words, if ai \u2260 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 \u2260 0, and rewrite the equationIf all the coefficients are zero, then either b \u2260 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewrittenAn everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point\u2013slope form or slope\u2013intercept form.where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.andA linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.Ergo,Thus,One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.andThese are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:andSince this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.becomes:Further, this representation extends to systems of linear equations.one can rewrite the equation in matrix form:Using the order of the standard formwhere a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.Using a determinant, one gets a determinant form, easy to remember:Expanding the products and regrouping the terms leads to the general form:Multiplying both sides of this equation by (x2\u00a0\u2212\u00a0x1) yields a form of the line generally referred to as the symmetric form:where (x1,\u00a0y1) and (x2,\u00a0y2) are two points on the line with x2 \u2260 x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2\u00a0\u2212\u00a0y1)/(x2\u00a0\u2212\u00a0x1).The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y\u00a0\u2212\u00a0y1) is proportional to the difference in the x coordinate (that is, x\u00a0\u2212\u00a0x1). The proportionality constant is m (the slope of the line).where m is the slope of the line and (x1,y1) is any point on the line.where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.where A and B are not both equal to zero. The equation is usually written so that A \u2265 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is \u2212A/B. The general form is sometimes written as:In the general (or standard[1]) form the linear equation is written as:Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the \"equations of the straight line.\" In what follows, x, y, t, and \u03b8 are variables; other letters represent constants (fixed numbers).Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.where m and b designate constants (parameters). The origin of the name \"linear\" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.A common form of a linear equation in the two variables x and y isIf a = 0, then, if b = 0, every number is a solution of the equation, and, if b \u2260 0, there are no solutions (and the equation is said to be inconsistent).If a \u2260 0, there is a unique solutionA linear equation in one unknown x may always be rewrittenThis article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a \u2260 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some \"background\" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a \u2260 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).",
            "title": "\nLinear equation\n",
            "url": "https://en.wikipedia.org/wiki/Linear_equation"
        },
        {
            "links": [
                "/wiki/Geometric_algebra",
                "/wiki/Clifford_algebra",
                "/wiki/Interior_product",
                "/wiki/Exterior_product",
                "/wiki/Vector_field",
                "/wiki/Differential_form",
                "/wiki/Exterior_algebra",
                "/wiki/Simple_tensor",
                "/wiki/Nondegenerate_form",
                "/wiki/Outer_product",
                "/wiki/Nondegenerate_form",
                "/wiki/Differential_geometry",
                "/wiki/Riemannian_manifold",
                "/wiki/Pseudo-Riemannian_manifold",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Minkowski_space",
                "/wiki/Dimension_(mathematics)",
                "/wiki/Sign_(mathematics)",
                "/wiki/Sign_convention#Metric_signature",
                "/wiki/Gelfand%E2%80%93Naimark%E2%80%93Segal_construction",
                "/wiki/Mercer%27s_theorem",
                "/wiki/Semi-norm",
                "/wiki/Spectral_theorem",
                "/wiki/Normal_operator",
                "/wiki/Linear",
                "/wiki/Weierstrass_approximation_theorem",
                "/wiki/Fourier_series",
                "/wiki/Trigonometric_polynomial",
                "/wiki/Hilbert_space",
                "/wiki/Parseval%27s_identity",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Hilbert_space",
                "/wiki/Hausdorff_maximal_principle",
                "/wiki/Hilbert_space",
                "/wiki/Separable_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_space",
                "/wiki/Parallelogram_equality#Normed_vector_spaces_satisfying_the_parallelogram_law",
                "/wiki/Probability",
                "/wiki/Almost_surely",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Expected_value",
                "/wiki/Cauchy_sequence",
                "/wiki/Hilbert_space",
                "/wiki/Complete_space",
                "/wiki/Hermitian_matrix",
                "/wiki/Positive-definite_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Scaling_(geometry)",
                "/wiki/Scale_factor",
                "/wiki/Weight_function",
                "/wiki/Hermitian_form",
                "/wiki/Transpose",
                "/wiki/Real_coordinate_space",
                "/wiki/Dot_product",
                "/wiki/Euclidean_space",
                "/wiki/Real_numbers",
                "/wiki/Sesquilinear",
                "/wiki/Complex_conjugate",
                "/wiki/Basefield",
                "/wiki/Ordered_field",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Algebraic_number",
                "/wiki/Constructible_number#Transformation_into_algebra",
                "/wiki/Quantum_computation",
                "/wiki/Complete_metric_space",
                "/wiki/Hilbert_space",
                "/wiki/Physics",
                "/wiki/Matrix_algebra",
                "/wiki/Sesquilinear_form",
                "/wiki/Bra%E2%80%93ket_notation",
                "/wiki/Quantum_mechanics",
                "/wiki/Linear_functional",
                "/wiki/Dual_space",
                "/wiki/Axiom",
                "/wiki/Vector_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Field_(mathematics)",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_vector_space",
                "/wiki/Complete_space",
                "/wiki/Hilbert_space",
                "/wiki/Complete_space#Completion",
                "/wiki/Hilbert_space",
                "/wiki/Linear_algebra",
                "/wiki/Vector_space",
                "/wiki/Mathematical_structure",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Angle",
                "/wiki/Orthogonality",
                "/wiki/Euclidean_space",
                "/wiki/Dot_product",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Functional_analysis"
            ],
            "text": "As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) \u2013 the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) \u2013 and in this context the exterior product is usually called the \"outer (alternatively, wedge) product\". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.More abstractly, the outer product is the bilinear map W \u00d7 V\u2217 \u2192 Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V\u2217 \u00d7 V \u2192 F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.In a quip: \"inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out\".On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V \u2192 V\u2217) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.The term \"inner product\" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 \u00d7 n covector with an n \u00d7 1 vector, yielding a 1\u00a0\u00d7\u00a01 matrix (a scalar), while the outer product is the product of an m \u00d7 1 vector with a 1 \u00d7 n covector, yielding an m \u00d7 n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V \u2192 V\u2217) and thus hold more generally.Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that \u27e8x,y\u27e9 \u2260 0, though y need not equal x; in other words, the induced map to the dual space V \u2192 V\u2217 is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of \"+\" and \"\u2212\" to them differs depending on conventions).This construction is used in numerous contexts. The Gelfand\u2013Naimark\u2013Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x\u00a0: ||x|| = 0}. The sesquilinear form \u27e8\u00b7,\u00b7\u27e9 factors through W.If V is a vector space and \u27e8\u00b7,\u00b7\u00b7\u00b7\u27e9 a semi-definite sesquilinear form, then the function:Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.Several types of linear maps A from an inner product space V to an inner product space W are of relevance:Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [\u2212\u03c0,\u03c0] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.Orthogonality of the sequence {ek}k follows immediately from the fact that if k \u2260 j, thenis an isometric linear map with dense image.is an orthonormal basis of the space C[\u2212\u03c0,\u03c0] with the L2 inner product. The mappingTheorem. Let V be the inner product space C[\u2212\u03c0,\u03c0]. Then the sequence (indexed on set of all integers) of continuous functionsThis theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:is an isometric linear map V \u2192 l2 with a dense image.Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of\u00a0V. Then the mapParseval's identity leads immediately to the following theorem:The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]Theorem. Any complete inner product space V has an orthonormal basis.Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show thatTheorem. Any separable inner product space V has an orthonormal basis.Using an infinite-dimensional analog of the Gram-Schmidt process one may show:if \u03b1 \u2260 \u03b2 and \u27e8e\u03b1,e\u03b1\u27e9 = ||e\u03b1|| = 1 for all \u03b1, \u03b2 \u2208 A.is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis andThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collectionLet V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram\u2013Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if \u27e8ei,ej\u27e9 = 0 for every i \u2260 j and \u27e8ei,ei\u27e9 = ||ei|| = 1 for each i.This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]A linear space with a norm such as:is an inner product.For real matrices of the same size, \u27e8A,B\u27e9\u00a0:= tr(ABT) with transpose as conjugationis an inner product.[6][7][8] In this case, \u27e8X,X\u27e9 = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.For real random variables X and Y, the expected value of their productThis sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.This space is not complete; consider for example, for the interval [\u22121,1] the sequence of continuous \"step\" functions, {\u2009fk}k, defined by:The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product iswhere M is any Hermitian positive-definite matrix and y\u2020 is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.The general form of an inner product on Cn is known as the Hermitian form and is given bywhere xT is the transpose of x.More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.A simple example is the real numbers with the standard multiplication as the inner productis also known as additivity.The property of an inner product space V thatAssuming the underlying field to be R, the inner product becomes symmetric, and we obtainCombining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:From the linearity property it is derived that x = 0 implies \u27e8x,x\u27e9 = 0. while from the positive-definiteness axiom we obtain the converse, \u27e8x,x\u27e9 = 0 implies x = 0. Combining these two, we have the property that \u27e8x,x\u27e9 = 0 if and only if x = 0.In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.Conjugate symmetry and linearity in the first variable givesMoreover, sesquilinearity (see below) implies thatNotice that conjugate symmetry implies that \u27e8x,x\u27e9 is real for all x, since we have:When F = R, conjugate symmetry reduces to symmetry. That is, \u27e8x,y\u27e9 = \u27e8y,x\u27e9 for F = R; while for F = C, \u27e8x,y\u27e9 is equal to the complex conjugate.In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that \u27e8x,x\u27e9 is only required to be non-negative. We show how to treat these below.There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product \u27e8x,y\u27e9 as \u27e8y|x\u27e9 (the bra\u2013ket notation of quantum mechanics), respectively y\u2020x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V\u2217, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking \u27e8x,y\u27e9 to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both \u27e8\u00b7,\u00b7\u27e9 and \u27e8\u00b7|\u00b7\u27e9 as distinct notations differing only in which argument is conjugate linear.that satisfies the following three axioms for all vectors x, y, z \u2208 V and all scalars a \u2208 F:[2][3]Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a mapIn this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]",
            "title": "\nInner product space\n",
            "url": "https://en.wikipedia.org/wiki/Inner_product_space"
        },
        {
            "links": [
                "/wiki/Divergence_theorem",
                "/wiki/Finite_difference",
                "/wiki/Finite_element_analysis",
                "/wiki/Finite_volume_method",
                "/wiki/Finite_difference_method",
                "/wiki/Meshfree_methods",
                "/wiki/Hp-FEM",
                "/wiki/Extended_finite_element_method",
                "/wiki/Spectral_element_method",
                "/wiki/Meshfree_methods",
                "/wiki/Discontinuous_Galerkin_Method",
                "/wiki/Adomian_decomposition_method",
                "/wiki/Aleksandr_Lyapunov",
                "/wiki/Homotopy_perturbation_method",
                "/wiki/Homotopy_analysis_method",
                "/wiki/Perturbation_theory",
                "/wiki/Infinitesimal_transformation",
                "/wiki/Lie_theory",
                "/wiki/Group_theory",
                "/wiki/Lie_algebras",
                "/wiki/Differential_geometry",
                "/wiki/Lax_pair",
                "/wiki/B%C3%A4cklund_transform",
                "/wiki/Sophus_Lie",
                "/wiki/Lie_group",
                "/wiki/Infinitesimal_transformation",
                "/wiki/Contact_transformation",
                "/wiki/Perturbation_analysis",
                "/wiki/Numerical_analysis",
                "/wiki/Finite_difference",
                "/wiki/Multigrid",
                "/wiki/Finite_element_method",
                "/wiki/Computer",
                "/wiki/Supercomputer",
                "/wiki/Method_of_characteristics",
                "/wiki/H-principle",
                "/wiki/Underdetermined_system",
                "/wiki/Overdetermined_system",
                "/wiki/Cauchy%E2%80%93Kowalevski_theorem",
                "/wiki/Mathematical_analysis",
                "/wiki/Split-step_method",
                "/wiki/Nonlinear_Schr%C3%B6dinger_equation",
                "/wiki/Signal_processing",
                "/wiki/Impulse_response",
                "/wiki/Fundamental_solution",
                "/wiki/Convolution",
                "/wiki/Wayback_Machine",
                "/wiki/Heat_equation",
                "/wiki/Change_of_variables_(PDE)",
                "/wiki/Black%E2%80%93Scholes_equation#Derivation",
                "/wiki/Fourier_series",
                "/wiki/Fourier_integral",
                "/wiki/Fourier_analysis",
                "/wiki/Eigenbasis",
                "/wiki/Integral_transform",
                "/wiki/Method_of_characteristics",
                "/wiki/Method_of_characteristics",
                "/wiki/Integral_transform",
                "/wiki/Separable_partial_differential_equation",
                "/wiki/Diagonal_matrices",
                "/wiki/Ansatz",
                "/wiki/Phase_space_formulation",
                "/wiki/Method_of_quantum_characteristics",
                "/wiki/Wigner_quasi-probability_distribution",
                "/wiki/Method_of_quantum_characteristics",
                "/wiki/Euler%E2%80%93Tricomi_equation",
                "/wiki/Fourier_transform",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Quadratic_form",
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Hyperbolic_partial_differential_equation",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Euler%E2%80%93Tricomi_equation",
                "/wiki/Laplace_operator",
                "/wiki/Well-posed_problem",
                "/wiki/Uniform_convergence",
                "/wiki/Boundary_condition",
                "/wiki/Cauchy_problem",
                "/wiki/Laplace_equation",
                "/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem",
                "/wiki/Cauchy%E2%80%93Kowalevski_theorem",
                "/wiki/Cauchy_problem",
                "/wiki/Analytic_function",
                "/wiki/Lewy%27s_example",
                "/wiki/Weak_solution",
                "/wiki/Constant_(mathematics)",
                "/wiki/Uniqueness_quantification",
                "/wiki/Boundary_(topology)",
                "/wiki/Logical_implication",
                "/wiki/Linear_function",
                "/wiki/Heat_equation",
                "/wiki/Wave_equation",
                "/wiki/Laplace%27s_equation",
                "/wiki/Helmholtz_equation",
                "/wiki/Klein%E2%80%93Gordon_equation",
                "/wiki/Poisson%27s_equation",
                "/wiki/Continuous_variables",
                "/wiki/Rigid_body",
                "/wiki/Fluid",
                "/wiki/Continuous_distribution",
                "/wiki/Temperature",
                "/wiki/Pressure",
                "/wiki/Configuration_space_(physics)",
                "/wiki/Acoustics",
                "/wiki/Fluid_dynamics",
                "/wiki/Electrodynamics",
                "/wiki/Heat_transfer",
                "/wiki/Sound",
                "/wiki/Heat",
                "/wiki/Electrostatics",
                "/wiki/Electrodynamics",
                "/wiki/Fluid_dynamics",
                "/wiki/Elasticity_(physics)",
                "/wiki/Quantum_mechanics",
                "/wiki/Dynamical_system",
                "/wiki/Multidimensional_system",
                "/wiki/Stochastic_partial_differential_equation",
                "/wiki/Mathematics",
                "/wiki/Differential_equation",
                "/wiki/Multivariable_calculus",
                "/wiki/Partial_derivatives",
                "/wiki/Computer_model",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative"
            ],
            "text": "Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. \"Finite volume\" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods conserve mass by design.Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.The finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge\u2013Kutta, etc.The three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM), as well other kind of methods called Meshfree methods, which were made to solve problems where the before mentioned methods are limited. The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other hybrid versions of FEM and Meshfree methods include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.The adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality.Symmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.A general approach to solving PDE's uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, B\u00e4cklund transform and finally finding exact analytic solutions to the PDE.From 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.In some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.The method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.Nevertheless, some techniques can be used for several types of equations. The h-principle is the most powerful method to solve underdetermined equations. The Riquier\u2013Janet theory is an effective method for obtaining information about many analytic overdetermined systems.There are no generally applicable methods to solve nonlinear PDEs. Still, existence and uniqueness results (such as the Cauchy\u2013Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schr\u00f6dinger equation.This is analogous in signal processing to understanding a filter by its impulse response.Inhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.by the change of variables (for complete details see Solution of the Black Scholes Equation at the Wayback Machine (archived April 11, 2008))is reducible to the heat equationOften a PDE can be reduced to a simpler form with a known solution by a suitable change of variables. For example, the Black\u2013Scholes PDEIf the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example of the use of a Fourier integral.An important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.An integral transform may transform the PDE to a simpler one, in particular, a separable PDE. This corresponds to diagonalizing an operator.More generally, one may find characteristic surfaces.In special cases, one can find characteristic curves on which the equation reduces to an ODE \u2013 changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.This generalizes to the method of characteristics, and is also used in integral transforms.This is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices \u2013 thinking of \"the value for fixed x\" as a coordinate, each coordinate can be understood separately.In the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable \u2013 these are in turn easier to solve.Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can find any solution that solves the equation and satisfies the boundary conditions, then it is the solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.[2]In the phase space formulation of quantum mechanics, one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of \u0127. The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function.which is called elliptic-hyperbolic because it is elliptic in the region x < 0, hyperbolic in the region x > 0, and degenerate parabolic on the line x = 0.If a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler\u2013Tricomi equationThe geometric interpretation of this condition is as follows: if data for u are prescribed on the surface S, then it may be possible to determine the normal derivative of u on S from the differential equation. If the data on S and the differential equation determine the normal derivative of u on S, then S is non-characteristic. If the data on S and the differential equation do not determine the normal derivative of u on S, then the surface is characteristic, and the differential equation restricts the data on S: the differential equation is internal to S.where \u03c6 has a non-zero gradient, then S is a characteristic surface for the operator L at a given point if the characteristic form vanishes:where the coefficient matrices A\u03bd and the vector B may depend upon x and u. If a hypersurface S is given in the implicit formThe classification of partial differential equations can be extended to systems of first-order equations, where the unknown u is now a vector with m components, and the coefficient matrices A\u03bd are m by m matrices for \u03bd = 1, ..., n. The partial differential equation takes the formThe classification depends upon the signature of the eigenvalues of the coefficient matrix ai,j..If there are n independent variables x1, x2 , ..., xn, a general linear partial differential equation of second order has the formMore precisely, replacing \u2202x by X, and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.Some linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others such as the Euler\u2013Tricomi equation have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions, and to the smoothness of the solutions.where \u0394 is the Laplace operator.orIn PDEs, it is common to denote partial derivatives using subscripts. That is:This solution approaches infinity if nx is not an integer multiple of \u03c0 for any non-zero value of y. The Cauchy problem for the Laplace equation is called ill-posed or not well-posed, since the solution does not continuously depend on the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.where n is an integer. The derivative of u with respect to y approaches 0 uniformly in x as n increases, but the solution iswith boundary conditionsAn example of pathological behavior is the sequence (depending upon n) of Cauchy problems for the Laplace equationAlthough the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard\u2013Lindel\u00f6f theorem, that is far from the case for partial differential equations. The Cauchy\u2013Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties. The mathematical study of these questions is usually in the more powerful context of weak solutions.where c is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function f(y) can be determined if u is specified on the line x = 0.which has the solutionwhere f is an arbitrary function of y. The analogous ordinary differential equation isThis relation implies that the function u(x,y) is independent of x. However, the equation gives no information on the function's dependence on the variable y. Hence the general solution of this equation isA relatively simple PDE isIf f is a linear function of u and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein\u2013Gordon equation, and Poisson's equation.Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six parameters[1], but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the fluid occur in an infinite-dimensional configuration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again, there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid dynamics, electrodynamics, and heat transfer.PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.In mathematics, a partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.",
            "title": "\nPartial differential equation\n",
            "url": "https://en.wikipedia.org/wiki/Partial_differential_equation"
        },
        {
            "links": [],
            "text": "",
            "title": "\nDirichlet conditions\n",
            "url": "https://en.wikipedia.org/wiki/Dirichlet_conditions"
        },
        {
            "links": [
                "/wiki/Andrey_Kolmogorov",
                "/wiki/Uniform_boundedness_principle",
                "/wiki/Convergence_of_Fourier_series",
                "/wiki/Lennart_Carleson",
                "/wiki/Almost_everywhere",
                "/wiki/Parseval%27s_theorem",
                "/wiki/Trigonometric_polynomial",
                "/wiki/Fourier_integral",
                "/wiki/Fourier_transform",
                "/wiki/Non-abelian_group",
                "/wiki/Compact_space",
                "/wiki/Riemannian_manifold",
                "/wiki/Laplace%E2%80%93Beltrami_operator",
                "/wiki/Laplace_operator",
                "/wiki/Spherical_harmonics",
                "/wiki/Peter%E2%80%93Weyl_theorem",
                "/wiki/Compact_group",
                "/wiki/Classical_group",
                "/wiki/Convolution",
                "/wiki/Stone%E2%80%93Weierstrass_theorem",
                "/wiki/Fej%C3%A9r_kernel",
                "/wiki/Kronecker_delta",
                "/wiki/Orthonormal_set",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Bloch%27s_Theorem",
                "/wiki/Bravais_lattice",
                "/wiki/Image_compression",
                "/wiki/Jpeg",
                "/wiki/Discrete_cosine_transform",
                "/wiki/List_of_Fourier-related_transforms",
                "/wiki/Harmonic_analysis",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Laplace",
                "/wiki/%C3%89tienne-Louis_Malus",
                "/wiki/Adrien-Marie_Legendre",
                "/wiki/Mathematical_rigour",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Formalism_(mathematics)",
                "/wiki/Euler",
                "/wiki/D%27Alembert",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Convergent_series",
                "/wiki/Function_space",
                "/wiki/Harmonic_analysis",
                "/wiki/Dirac_comb",
                "/wiki/Frequency_domain",
                "/wiki/Basel_problem",
                "/wiki/Parseval%27s_theorem",
                "/wiki/Riemann_zeta_function",
                "/wiki/Hyperbolic_sine",
                "/wiki/Closed-form_expression",
                "/wiki/Heat_equation",
                "/wiki/Basel_problem",
                "/wiki/Convergence_of_Fourier_series#Convergence_at_a_given_point",
                "/wiki/Engineering",
                "/wiki/Square-integrable_function",
                "/wiki/Carleson%27s_theorem",
                "/wiki/Dirichlet_conditions",
                "/wiki/Convergence_of_Fourier_series",
                "/wiki/Weak_convergence_(Hilbert_space)",
                "/wiki/Series_(mathematics)",
                "/wiki/Sine_wave",
                "/wiki/Electrical_engineering",
                "/wiki/Oscillation",
                "/wiki/Acoustics",
                "/wiki/Optics",
                "/wiki/Signal_processing",
                "/wiki/Image_processing",
                "/wiki/Quantum_mechanics",
                "/wiki/Econometrics",
                "/wiki/Thin-shell_structure",
                "/wiki/Function_(mathematics)",
                "/wiki/Integral",
                "/wiki/Peter_Gustav_Lejeune_Dirichlet",
                "/wiki/Bernhard_Riemann",
                "/wiki/Partial_differential_equation",
                "/wiki/Sine",
                "/wiki/Cosine",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace",
                "/wiki/Linear_combination",
                "/wiki/Superposition_principle",
                "/wiki/Eigenfunction",
                "/wiki/Jean-Baptiste_Joseph_Fourier",
                "/wiki/Trigonometric_series",
                "/wiki/Leonhard_Euler",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Heat_equation",
                "/wiki/M%C3%A9moire_sur_la_propagation_de_la_chaleur_dans_les_corps_solides",
                "/wiki/Acad%C3%A9mie_fran%C3%A7aise",
                "/wiki/Deferent_and_epicycle",
                "/wiki/Mathematics",
                "/wiki/Series_(mathematics)",
                "/wiki/Help:IPA/English",
                "/wiki/Periodic_function",
                "/wiki/Sine_wave",
                "/wiki/Complex_exponential",
                "/wiki/Discrete-time_Fourier_transform",
                "/wiki/Z-transform",
                "/wiki/Nyquist%E2%80%93Shannon_sampling_theorem",
                "/wiki/Fourier_analysis"
            ],
            "text": "In 1922, Andrey Kolmogorov published an article titled \"Une s\u00e9rie de Fourier-Lebesgue divergente presque partout\" in which he gave an example of a Lebesgue-integrable function whose Fourier series diverges almost everywhere. He later constructed an example of an integrable function whose Fourier series diverges everywhere (Katznelson 1976).Since Fourier series have such good convergence properties, many are often surprised by some of the negative results. For example, the Fourier series of a continuous T-periodic function need not converge pointwise. The uniform boundedness principle yields a simple non-constructive proof of this fact.These theorems, and informal variations of them that don't specify the convergence conditions, are sometimes referred to generically as \"Fourier's theorem\" or \"the Fourier theorem\".[14][15][16][17]Many other results concerning the convergence of Fourier series are known, ranging from the moderately simple result that the series converges at x if f is differentiable at x, to Lennart Carleson's much more sophisticated result that the Fourier series of an L2 function actually converges almost everywhere.Because of the least squares property, and because of the completeness of the Fourier basis, we obtain an elementary convergence result.Note that fN is a trigonometric polynomial of degree N. Parseval's theorem implies thatWe say that p is a trigonometric polynomial of degree N when it is of the formThis is called a partial sum. We would like to know, in which sense does fN(x) converge to f(x) as N \u2192 \u221e.This generalizes the Fourier transform to L1(G) or L2(G), where G is an LCA group. If G is compact, one also obtains a Fourier series, which converges similarly to the [\u2212\u03c0,\u00a0\u03c0] case, but if G is noncompact, one obtains instead a Fourier integral. This generalization yields the usual Fourier transform when the underlying locally compact Abelian group is R.The generalization to compact groups discussed above does not generalize to noncompact, nonabelian groups. However, there is a straightforward generalization to Locally Compact Abelian (LCA) groups.If the domain is not a group, then there is no intrinsically defined convolution. However, if X is a compact Riemannian manifold, it has a Laplace\u2013Beltrami operator. The Laplace\u2013Beltrami operator is the differential operator that corresponds to Laplace operator for the Riemannian manifold X. Then, by analogy, one can consider heat equations on X. Since Fourier arrived at his basis by attempting to solve the heat equation, the natural generalization is to use the eigensolutions of the Laplace\u2013Beltrami operator as a basis. This generalizes Fourier series to spaces of the type L2(X), where X is a Riemannian manifold. The Fourier series converges in ways similar to the [\u2212\u03c0,\u00a0\u03c0] case. A typical example is to take X to be the sphere with the usual metric, in which case the Fourier basis consists of spherical harmonics.An alternative extension to compact groups is the Peter\u2013Weyl theorem, which proves results about representations of compact groups analogous to those about finite groups.One of the interesting properties of the Fourier transform which we have mentioned, is that it carries convolutions to pointwise products. If that is the property which we seek to preserve, one can produce Fourier series on any compact group. Typical examples include those classical groups that are compact. This generalizes the Fourier transform to all spaces of the form L2(G), where G is a compact group, in such a way that the Fourier transform carries convolutions to pointwise products. The Fourier series exists and converges in similar ways to the [\u2212\u03c0,\u03c0] case.furthermore, the sines and cosines are orthogonal to the constant function 1. An orthonormal basis for L2([\u2212\u03c0,\u03c0]) consisting of real functions is formed by the functions 1 and \u221a2\u00a0cos(nx),\u2009 \u221a2\u00a0sin(nx) with n\u00a0= 1,\u00a02,...\u00a0 The density of their span is a consequence of the Stone\u2013Weierstrass theorem, but follows also from the properties of classical kernels like the Fej\u00e9r kernel.(where \u03b4mn is the Kronecker delta), andThis corresponds exactly to the complex exponential formulation given above. The version with sines and cosines is also justified with the Hilbert space interpretation. Indeed, the sines and cosines form an orthogonal set:The basic Fourier series result for Hilbert spaces can be written asWe can write now h(K) as an integral with the traditional coordinate system over the volume of the primitive cell, instead of with the x1, x2 and x3 variables:(it may be advantageous for the sake of simplifying calculations, to work in such a cartesian coordinate system, in which it just so happens that a1 is parallel to the x axis, a2 lies in the x-y plane, and a3 has components of all three axes). The denominator is exactly the volume of the primitive unit cell which is enclosed by the three primitive-vectors a1, a2 and a3. In particular, we now know thatwhich after some calculation and applying some non-trivial cross-product identities can be shown to be equal to:we can solve this system of three linear equations for x, y, and z in terms of x1, x2 and x3 in order to calculate the volume element in the original cartesian coordinate system. Once we have x, y, and z in terms of x1, x2 and x3, we can calculate the Jacobian determinant:AssumingwhereAnd so it is clear that in our expansion, the sum is actually over reciprocal lattice vectors:Re-arranging:We write g as:Finally applying the same for the third coordinate, we define:We can write g once again as:Further defining:And then we can write:If we write a series for g on the interval [0, a1] for x1, we can define the following:Thus we can define a new function,where ai = |ai|.where ni are integers and ai are three linearly independent vectors. Assuming we have some function, f(r), such that it obeys the following condition for any Bravais lattice vector R: f(r) = f(r\u00a0+\u00a0R), we could make a Fourier series of it. This kind of function can be, for example, the effective potential that one electron \"feels\" inside a periodic crystal. It is useful to make a Fourier series of the potential then when applying Bloch's theorem. First, we may write any arbitrary vector r in the coordinate-system of the lattice:The three-dimensional Bravais lattice is defined as the set of vectors of the form:Aside from being useful for solving partial differential equations such as the heat equation, one notable application of Fourier series on the square is in image compression. In particular, the jpeg image compression standard uses the two-dimensional discrete cosine transform, which is a Fourier transform using the cosine basis functions.We can also define the Fourier series for functions of two variables x and y in the square [\u2212\u03c0,\u00a0\u03c0]\u00a0\u00d7\u00a0[\u2212\u03c0,\u00a0\u03c0]:Many other Fourier-related transforms have since been defined, extending the initial idea to other applications. This general area of inquiry is now sometimes called harmonic analysis. A Fourier series, however, can be used only for periodic functions, or for functions on a bounded (compact) interval.Since Fourier's time, many different approaches to defining and understanding the concept of Fourier series have been discovered, all of which are consistent with one another, but each of which emphasizes different aspects of the topic. Some of the more powerful and elegant approaches are based on mathematical ideas and tools that were not available at the time Fourier completed his original work. Fourier originally defined the Fourier series for real-valued functions of real arguments, and using the sine and cosine functions as the basis set for the decomposition.When Fourier submitted a later competition essay in 1811, the committee (which included Lagrange, Laplace, Malus and Legendre, among others) concluded: ...the manner in which the author arrives at these equations is not exempt of difficulties and...his analysis to integrate them still leaves something to be desired on the score of generality and even rigour.[citation needed]In these few lines, which are close to the modern formalism used in Fourier series, Fourier revolutionized both mathematics and physics. Although similar trigonometric series were previously used by Euler, d'Alembert, Daniel Bernoulli and Gauss, Fourier believed that such trigonometric series could represent any arbitrary function. In what sense that is actually true is a somewhat subtle issue and the attempts over many years to clarify this idea have led to important discoveries in the theories of convergence, function spaces, and harmonic analysis.This immediately gives any coefficient ak of the trigonometrical series for \u03c6(y) for any function which has such an expansion. It works because if \u03c6 has such an expansion, then (under suitable convergence assumptions) the integralThe constructed function S(f) is therefore commonly referred to as a Fourier transform, even though the Fourier integral of a periodic function is not convergent at the harmonic frequencies.[nb 2]Another commonly used frequency domain representation uses the Fourier series coefficients to modulate a Dirac comb:In engineering, particularly when the variable x represents time, the coefficient sequence is called a frequency domain representation. Square brackets are often used to emphasize that the domain of this function is a discrete set of frequencies.Another application of this Fourier series is to solve the Basel problem by using Parseval's theorem. The example generalizes and one may compute \u03b6(2n), for any positive integer\u00a0n.Here, sinh is the hyperbolic sine function. This solution of the heat equation is obtained by multiplying each term of \u00a0Eq.1 by sinh(ny)/sinh(n\u03c0). While our example function s(x) seems to have a needlessly complicated Fourier series, the heat distribution T(x,\u00a0y) is nontrivial. The function T cannot be written as a closed-form expression. This method of solving the heat problem was made possible by Fourier's work.The Fourier series expansion of our function in Example 1 looks more complicated than the simple formula s(x) = x/\u03c0, so it is not immediately apparent why one would need the Fourier series. While there are many applications, Fourier's motivation was in solving the heat equation. For example, consider a metal plate in the shape of a square whose side measures \u03c0 meters, with coordinates (x,\u00a0y) \u2208 [0,\u00a0\u03c0]\u00a0\u00d7\u00a0[0,\u00a0\u03c0]. If there is no heat source within the plate, and if three of the four sides are held at 0 degrees Celsius, while the fourth side, given by y\u00a0=\u00a0\u03c0, is maintained at the temperature gradient T(x,\u00a0\u03c0) = x degrees Celsius, for x in (0,\u00a0\u03c0), then one can show that the stationary heat distribution (or the heat distribution after a long period of time has elapsed) is given byThis example leads us to a solution to the Basel problem.When x\u00a0= \u03c0, the Fourier series converges to 0, which is the half-sum of the left- and right-limit of s at x\u00a0= \u03c0. This is a particular instance of the Dirichlet theorem for Fourier series.It can be proven that Fourier series converges to s(x) at every point x where s is differentiable, and therefore:In this case, the Fourier coefficients are given byWe now use the formula above to give a Fourier series expansion of a very simple function. Consider a sawtooth waveIn engineering applications, the Fourier series is generally presumed to converge everywhere except at discontinuities, since the functions encountered in engineering are more well behaved than the ones that mathematicians can provide as counter-examples to this presumption. In particular, if s is continuous and the derivative of s(x) (which may not exist everywhere) is square integrable, then the Fourier series of s converges absolutely and uniformly to s(x).[11]\u00a0 If a function is square-integrable on the interval [x0, x0+P], then the Fourier series converges to the function at almost every point. Convergence of Fourier series also depends on the finite number of maxima and minima in a function which is popularly known as one of the Dirichlet's condition for Fourier series. See Convergence of Fourier series. It is possible to define Fourier coefficients for more general functions or distributions, in such cases convergence in norm or weak convergence is usually of interest.This is the same formula as before except cn and c\u2212n are no longer complex conjugates. The formula for cn is also unchanged:Both components of a complex-valued function are real-valued functions that can be represented by a Fourier series. The two sets of coefficients and the partial sum are given by:When the coefficients (known as Fourier coefficients) are computed as follows:[10]The inverse relationships between the coefficients are:where:we can also write the function in these equivalent forms:In this section, s(x) denotes a function of the real variable x, and s is integrable on an interval [x0,\u00a0x0\u00a0+\u00a0P], for real numbers x0 and\u00a0P. We will attempt to represent \u00a0s\u00a0 in that interval as an infinite sum, or series, of harmonically related sinusoidal functions. Outside the interval, the series is periodic with period\u00a0P (frequency\u00a01/P). It follows that if s also has that property, the approximation is valid on the entire real line. We can begin with a finite summation (or partial sum):Although the original motivation was to solve the heat equation, it later became obvious that the same techniques could be applied to a wide array of mathematical and physical problems, and especially those involving linear differential equations with constant coefficients, for which the eigensolutions are sinusoids. The Fourier series has many such applications in electrical engineering, vibration analysis, acoustics, optics, signal processing, image processing, quantum mechanics, econometrics,[8] thin-walled shell theory,[9] etc.From a modern point of view, Fourier's results are somewhat informal, due to the lack of a precise notion of function and integral in the early nineteenth century. Later, Peter Gustav Lejeune Dirichlet[4] and Bernhard Riemann[5][6][7] expressed Fourier's results with greater precision and formality.The heat equation is a partial differential equation. Prior to Fourier's work, no solution to the heat equation was known in the general case, although particular solutions were known if the heat source behaved in a simple way, in particular, if the heat source was a sine or cosine wave. These simple solutions are now sometimes called eigensolutions. Fourier's idea was to model a complicated heat source as a superposition (or linear combination) of simple sine and cosine waves, and to write the solution as a superposition of the corresponding eigensolutions. This superposition or linear combination is called the Fourier series.The Fourier series is named in honour of Jean-Baptiste Joseph Fourier (1768\u20131830), who made important contributions to the study of trigonometric series, after preliminary investigations by Leonhard Euler, Jean le Rond d'Alembert, and Daniel Bernoulli.[nb 1] Fourier introduced the series for the purpose of solving the heat equation in a metal plate, publishing his initial results in his 1807 M\u00e9moire sur la propagation de la chaleur dans les corps solides (Treatise on the propagation of heat in solid bodies), and publishing his Th\u00e9orie analytique de la chaleur (Analytical theory of heat) in 1822. The M\u00e9moire introduced Fourier analysis, specifically Fourier series. Through Fourier's research the fact was established that an arbitrary (continuous)[2] function can be represented by a trigonometric series. The first announcement of this great discovery was made by Fourier in 1807, before the French Academy.[3] Early ideas of decomposing a periodic function into the sum of simple oscillating functions date back to the 3rd century BC, when ancient astronomers proposed an empiric model of planetary motions, based on deferents and epicycles.In mathematics, a Fourier series (English: /\u02c8f\u028a\u0259ri\u02cce\u026a/)[1] is a way to represent a function as the sum of simple sine waves. More formally, it decomposes any periodic function or periodic signal into the sum of a (possibly infinite) set of simple oscillating functions, namely sines and cosines (or, equivalently, complex exponentials). The discrete-time Fourier transform is a periodic function, often defined in terms of a Fourier series. The Z-transform, another example of application, reduces to a Fourier series for the important case |z|=1. Fourier series are also central to the original proof of the Nyquist\u2013Shannon sampling theorem. The study of Fourier series is a branch of Fourier analysis.",
            "title": "\nFourier series\n",
            "url": "https://en.wikipedia.org/wiki/Fourier_series"
        },
        {
            "links": [
                "/wiki/Rounding_error",
                "/wiki/Statistical_analysis",
                "/wiki/Linear_regression",
                "/wiki/Statistical_model",
                "/wiki/Comparison_of_statistical_packages",
                "/wiki/Round-off_error",
                "/wiki/Goodness_of_fit",
                "/wiki/Normal_distribution",
                "/wiki/Student%27s_t-distribution",
                "/wiki/Studentized_residual",
                "/wiki/Outlier",
                "/wiki/Identity_matrix",
                "/wiki/Idempotent_matrix",
                "/wiki/Hat_matrix",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Chebychev%27s_inequality",
                "/wiki/Student%27s_t-distribution",
                "/wiki/Degrees_of_freedom_(statistics)",
                "/wiki/Sampling_error",
                "/wiki/Error_propagation",
                "/wiki/Variance-covariance_matrix",
                "/wiki/Iteratively_reweighted_least_squares",
                "/wiki/Multiplicative_inverse",
                "/wiki/Variance",
                "/wiki/Diagonal_matrix",
                "/wiki/Total_least_squares",
                "/wiki/Errors-in-variables_models",
                "/wiki/Maximum_likelihood",
                "/wiki/Arithmetic_mean",
                "/wiki/Condition_number",
                "/wiki/Round-off_error",
                "/wiki/Singular_value",
                "/wiki/Factor_analysis",
                "/wiki/Singular_value_decomposition",
                "/wiki/Orthogonal_matrix",
                "/wiki/QR_decomposition",
                "/wiki/Numerical_stability",
                "/wiki/Forward_substitution",
                "/wiki/Condition_number",
                "/wiki/Positive-definite_matrix",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Cholesky_decomposition",
                "/wiki/Triangular_matrix",
                "/wiki/Moore%E2%80%93Penrose_pseudoinverse",
                "/wiki/Gramian_matrix",
                "/wiki/Numerical_smoothing_and_differentiation",
                "/wiki/Maxima_and_minima",
                "/wiki/Maxima_and_minima",
                "/wiki/Overdetermined_system",
                "/wiki/Least_squares",
                "/wiki/Maxima_and_minima",
                "/wiki/Statistical_model",
                "/wiki/Linear_regression",
                "/wiki/Regression_analysis",
                "/wiki/Ordinary_least_squares",
                "/wiki/Statistical_inference",
                "/wiki/Outline_of_regression_analysis",
                "/wiki/Overdetermined_system",
                "/wiki/Convex_function",
                "/wiki/Closed-form_expression",
                "/wiki/Non-linear_least_squares",
                "/wiki/Iterative_method",
                "/wiki/Minimum_mean_square_error",
                "/wiki/Statistics",
                "/wiki/Mathematics",
                "/wiki/Mathematical_model",
                "/wiki/Statistical_model",
                "/wiki/Data",
                "/wiki/Parameter",
                "/wiki/Descriptive_statistics",
                "/wiki/Prediction"
            ],
            "text": "Matrix calculations, like any other, are affected by rounding errors. An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson.[14]Fitting of linear models by least squares often, but not always, arise in the context of statistical analysis. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can beThe numerical methods for linear least squares are important because linear regression models are among the most important types of model, both as formal statistical models and for exploration of data-sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to round-off error.and the best fit can be found by solving the normal equations.so to minimize the functionIdeally, the model function fits the data exactly, soOften it is of interest to solve a linear least squares problem with an additional constraint on the solution. With constrained linear least squares, the original equationThese values can be used for a statistical criterion as to the goodness of fit. When unit weights are used, the numbers should be divided by the variance of an observation.The optimal value of the objective function, found by substituting in the optimal expression for the coefficient vector, can be written as (assuming unweighted observations)If experimental error follows a normal distribution, then, because of the linear relationship between residuals and observations, so should residuals,[9] but since the observations are only a sample of the population of all possible observations, the residuals should belong to a Student's t-distribution. Studentized residuals are useful in making a statistical test for an outlier when a particular residual appears to be excessively large.Thus, in the motivational example, above, the fact that the sum of residual values is equal to zero it is not accidental but is a consequence of the presence of the constant term, \u03b1, in the model.The sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by XT:Thus the residuals are correlated, even if the observations are not.and I is the identity matrix. The variance-covariance matrix of the residuals, Mr is given bywhere H is the idempotent matrix known as the hat matrix:The residuals are related to the observations byWhen the number of observations is relatively small, Chebychev's inequality can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively.The assumption is not unreasonable when m\u00a0>>\u00a0n. If the experimental errors are normally distributed the parameters will belong to a Student's t-distribution with m\u00a0\u2212\u00a0n degrees of freedom. When m\u00a0>>\u00a0n Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to sampling error.[8]where S is the minimum value of the (weighted) objective function:When W = M\u22121, this simplifies toTherefore, an expression for the residuals (i.e., the estimated errors in the parameters) can be obtained by error propagation from the errors in the observations. Let the variance-covariance matrix for the observations be denoted by M and that of the parameters by M\u03b2. ThenThe estimated parameter values are linear combinations of the observed valuesThis method is used in iteratively reweighted least squares.The weights should, ideally, be equal to the reciprocal of the variance of the measurement.[6][7] The normal equations are then:where wi > 0 is the weight of the ith observation, and W is the diagonal matrix of such weights.In some cases the observations may be weighted\u2014for example, they may not be equally reliable. In this case, one can minimize the weighted sum of squares:An assumption underlying the treatment given above is that the independent variable, x, is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares or more generally errors-in-variables models, or rigorous least squares, should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure.[4][5]These properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.However, in the case that the experimental errors do belong to a normal distribution, the least-squares estimator is also a maximum likelihood estimator.[3]For example, it is easy to show that the arithmetic mean of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss\u2013Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.The equation and solution of linear least squares are thus described as follows:The gradient equations at the minimum can be written asis a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, XTX, is very ill-conditioned (i.e. if its condition number multiplied by the machine's relative round-off error is appreciably large). In that case, including the smallest singular values in the inversion merely adds numerical noise to the solution. This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to factor analysis.and thus,An alternative decomposition of X is the singular value decomposition (SVD)[2]These equations are easily solved as R is upper triangular.Since v doesn't depend on \u03b2, the minimum value of s is attained when the upper block, u, is zero. Therefore, the parameters are found by solving:Because Q is orthogonal, the sum of squares of the residuals, s, may be written as:The residual vector is left-multiplied by QT.The matrix X is subjected to an orthogonal decomposition, e.g., the QR decomposition as follows.The residuals are written in matrix notation asOrthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more numerically stable because they avoid forming the product XTX.Both substitutions are facilitated by the triangular nature of R.The solution is obtained in two stages, a forward substitution step, solving for z:If the matrix XTX is well-conditioned and positive definite, implying that it has full rank, the normal equations can be solved directly by using the Cholesky decomposition RTR, where R is an upper triangular matrix, giving:where X+ is the Moore\u2013Penrose pseudoinverse of X. Although this equation is correct and can work in many applications, it is not computationally efficient to invert the normal-equations matrix (the Gramian matrix). An exception occurs in numerical smoothing and differentiation where an analytical expression is required.The algebraic solution of the normal equations with a full-rank matrix XTX can be written assimply becauseIn matrix form:and the derivatives change intoand therefore minimized exactly whencan be written asThe normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimizeThe normal equations are written in matrix notation asUpon rearrangement, we obtain the normal equations:Substitution of the expressions for the residuals and the derivatives into the gradient equations givesThe derivatives areGiven that S is convex, it is minimized when its gradient vector is zero (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further \u2013 see maxima and minima.) The elements of the gradient vector are the partial derivatives of S with respect to the parameters:where the objective function S is given bywhereConsider an overdetermined systemand solvedThe partial derivatives with respect to the parameters (this time there is only one) are again computed and set to 0:This results in a system of two equations in two unknowns, called the normal equations, which when solved giveThe \"error\", at each point, between the curve fit and the data is the difference between the right- and left-hand sides of the equations above. The least squares approach to solving this problem is to try to make the sum of the squares of these errors as small as possible; that is, to find the minimum of the functionof four equations in two unknowns in some \"best\" sense.In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See Outline of regression analysis for an outline of the topic.Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called linear least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator.In statistics and mathematics, linear least squares is an approach to fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system.",
            "title": "\nLinear least squares (mathematics)\n",
            "url": "https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)"
        },
        {
            "links": [
                "/wiki/Bootstrapping_(finance)",
                "/wiki/Yield_curve",
                "/wiki/Isomorphic",
                "/wiki/Abelian_group",
                "/wiki/M%C3%B6bius_transformation",
                "/wiki/Heisenberg_group",
                "/wiki/Parabolic_subgroup",
                "/wiki/Flag_(linear_algebra)",
                "/wiki/Borel_subgroup",
                "/wiki/Group_(mathematics)",
                "/wiki/Lie_group",
                "/wiki/General_linear_group",
                "/wiki/Trapezoid",
                "/wiki/Associative_algebra",
                "/wiki/Functional_analysis",
                "/wiki/Nest_algebra",
                "/wiki/Hilbert_space",
                "/wiki/Lie%27s_theorem",
                "/wiki/Solvable_Lie_algebra",
                "/wiki/Abelian_Lie_algebra",
                "/wiki/Schur_decomposition",
                "/wiki/Unitary_matrix",
                "/wiki/Jordan_normal_form",
                "/wiki/Field_(mathematics)",
                "/wiki/Algebraically_closed_field",
                "/wiki/Transpose",
                "/wiki/Normal_matrix",
                "/wiki/Off-diagonal_element",
                "/wiki/Engel%27s_theorem",
                "/wiki/Lie_group",
                "/wiki/Main_diagonal",
                "/wiki/Unipotent",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_norm",
                "/wiki/Identity_matrix",
                "/wiki/Subalgebra",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Lie_bracket",
                "/wiki/Commutator#Ring_theory",
                "/wiki/Solvable_Lie_algebra",
                "/wiki/Borel_subalgebra",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Numerical_analysis",
                "/wiki/LU_decomposition",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Main_diagonal",
                "/wiki/Main_diagonal",
                "/wiki/Diagonal_matrix"
            ],
            "text": "Forward substitution is used in financial bootstrapping to construct a yield curve.A matrix equation with an upper triangular matrix U can be solved in an analogous way, only working backwards.The resulting formulas are:The matrix equation Lx = b can be written as a system of linear equationsNotice that this does not require inverting the matrix.The group of 2 by 2 upper unitriangular matrices is isomorphic to the additive group of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic M\u00f6bius transformations; the 3 by 3 upper unitriangular matrices form the Heisenberg group.The stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are not all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called parabolic subgroups.The upper triangular matrices are precisely those that stabilize the standard flag. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are Borel subgroups. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.The set of invertible triangular matrices of a given kind (upper or lower) forms a group, indeed a Lie group, which is a subgroup of the general linear group of all invertible matrices. Note that a triangular matrix is invertible precisely when its diagonal entries are invertible (non-zero).A non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a trapezoid.Because the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an algebra. Algebras of upper triangular matrices have a natural generalization in functional analysis which yields nest algebras on Hilbert spaces.This is generalized by Lie's theorem, which shows that any representation of a solvable Lie algebra is simultaneously upper triangularizable, the case of commuting matrices being the abelian Lie algebra case, abelian being a fortiori solvable.In the case of complex matrices, it is possible to say more about triangularization, namely, that any square matrix A has a Schur decomposition. This means that A is unitarily equivalent (i.e. similar, using a unitary matrix as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.A more precise statement is given by the Jordan normal form theorem, which states that in this situation, A is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.[1][2]Any complex square matrix is triangularizable.[1] In fact, a matrix A over a field containing all of the eigenvalues of A (for example, any matrix over an algebraically closed field) is similar to a triangular matrix. This can be proven by using induction on the fact that A has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that A stabilises a flag, and is thus triangularizable with respect to a basis for that flag.The transpose of an upper triangular matrix is a lower triangular matrix and vice versa.A matrix which is simultaneously triangular and normal is also diagonal. This can be seen by looking at the diagonal entries of A*A and AA*, where A is a normal, triangular matrix.is atomic lower triangular. Its inverse isThe matrixi.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.The inverse of an atomic triangular matrix is again atomic triangular. Indeed, we haveAn atomic (upper or lower) triangular matrix is a special form of unitriangular matrix, where all of the off-diagonal elements are zero, except for the entries in a single column. Such a matrix is also called a Frobenius matrix, a Gauss matrix, or a Gauss transformation matrix. So an atomic lower triangular matrix is of the formIn fact, by Engel's theorem, any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable.The set of unitriangular matrices forms a Lie group.If the entries on the main diagonal of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) unitriangular. All unitriangular matrices are unipotent. Other names used for these matrices are unit (upper or lower) triangular (of which \"unitriangular\" might be a contraction), or very rarely normed (upper or lower) triangular. However a unit triangular matrix is not the same as the unit matrix, and a normed triangular matrix has nothing to do with the notion of matrix norm. The identity matrix is the only matrix which is both upper and lower unitriangular.is lower triangular.is upper triangular and this matrixThis matrixAll these results hold if \"upper triangular\" is replaced by \"lower triangular\" throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.Together these facts mean that the upper triangular matrices form a subalgebra of the associative algebra of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the Lie algebra of square matrices of a fixed size, where the Lie bracket [a,b] given by the commutator ab-ba. The Lie algebra of all upper triangular matrices is a solvable Lie algebra. It is often referred to as a Borel subalgebra of the Lie algebra of all square matrices.Many operations on upper triangular matrices preserve the shape:Matrices that are similar to triangular matrices are called triangularisable.is called an upper triangular matrix or right triangular matrix. The variable L (standing for lower or left) is commonly used to represent a lower triangular matrix, while the variable U (standing for upper) or R (standing for right) is commonly used for upper triangular matrix. A matrix that is both upper and lower triangular is diagonal.is called a lower triangular matrix or left triangular matrix, and analogously a matrix of the formA matrix of the formBecause matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix L and an upper triangular matrix U if and only if all its leading principal minors are non-zero.In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. A square matrix is called lower triangular if all the entries above the main diagonal are zero. Similarly, a square matrix is called upper triangular if all the entries below the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.",
            "title": "\nTriangular matrix\n",
            "url": "https://en.wikipedia.org/wiki/Triangular_form"
        },
        {
            "links": [],
            "text": "",
            "title": "\nNormal matrix\n",
            "url": "https://en.wikipedia.org/wiki/Normal_matrix"
        },
        {
            "links": [
                "/wiki/Adjoint_functor",
                "/wiki/Category_theory",
                "/wiki/Antilinear_map",
                "/wiki/Real_number",
                "/wiki/Vector_space",
                "/wiki/Observable",
                "/wiki/Quantum_mechanics",
                "/wiki/Self-adjoint_operator",
                "/wiki/Bounded_operator",
                "/wiki/Self-adjoint",
                "/wiki/Continuous_function",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Domain_of_a_function",
                "/wiki/Codomain",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Densely_defined_operator",
                "/wiki/Linear_subspace",
                "/wiki/C*-algebra",
                "/wiki/Operator_norm",
                "/wiki/Bounded_operator",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Charles_Hermite",
                "/wiki/Bra%E2%80%93ket_notation",
                "/wiki/Banach_space",
                "/wiki/Mathematics",
                "/wiki/Functional_analysis",
                "/wiki/Linear_operator",
                "/wiki/Hilbert_space",
                "/wiki/Conjugate_transpose",
                "/wiki/Square_matrices",
                "/wiki/Complex_conjugate"
            ],
            "text": "is formally similar to the defining properties of pairs of adjoint functors in category theory, and this is where adjoint functors got their name from.The equationFor an antilinear operator the definition of adjoint needs to be adjusted in order to compensate for the complex conjugation. An adjoint operator of the antilinear operator A on a complex Hilbert space H is an antilinear operator A\u2217\u00a0: H \u2192 H with the property:In some sense, these operators play the role of the real numbers (being equal to their own \"complex conjugate\") and form a real vector space. They serve as the model of real-valued observables in quantum mechanics. See the article on self-adjoint operators for a full treatment.which is equivalent toA bounded operator A\u00a0: H \u2192 H is called Hermitian or self-adjoint ifThe second equation follows from the first by taking the orthogonal complement on both sides. Note that in general, the image need not be closed, but the kernel of a continuous operator[7] always is.[clarification needed]Proof of the first equation:[6][clarification needed]The relationship between the image of A and the kernel of its adjoint is given by:Properties 1.\u20135. hold with appropriate clauses about domains and codomains.[clarification needed] For instance, the last property now states that (AB)\u2217 is an extension of B\u2217A\u2217 if A, B and AB are densely defined operators.[5]and A\u2217(y) is defined to be the z thus found.[4]A densely defined operator A from a complex Hilbert space H to itself is a linear operator whose domain D(A) is a dense linear subspace of H and whose values lies in H.[3] By definition, the domain D(A\u2217) of its adjoint A\u2217 is the set of all y \u2208 H for which there is a z \u2208 H satisfyingThe set of bounded linear operators on a complex Hilbert space H together with the adjoint operation and the operator norm form the prototype of a C*-algebra.One says that a norm that satisfies this condition behaves like a \"largest value\", extrapolating from the case of self-adjoint operators.Moreover,thenIf we define the operator norm of A byThe following properties of the Hermitian adjoint of bounded operators are immediate:[2]This can be seen as a generalization of the adjoint matrix of a square matrix which has a similar property involving the standard complex inner product.Existence and uniqueness of this operator follows from the Riesz representation theorem.[2]The fundamental defining identity is thusThe adjoint of an operator A may also be called the Hermitian adjoint, Hermitian conjugate or Hermitian transpose[1] (after Charles Hermite) of A and is denoted by A\u2217 or A\u2020 (the latter especially when used in conjunction with the bra\u2013ket notation).In a similar sense there can be defined an adjoint operator for linear (and possibly unbounded) operators between Banach spaces.In mathematics, specifically in functional analysis, each bounded linear operator on a complex Hilbert space has a corresponding adjoint operator. Adjoints of operators generalize conjugate transposes of square matrices to (possibly) infinite-dimensional situations. If one thinks of operators on a complex Hilbert space as \"generalized complex numbers\", then the adjoint of an operator plays the role of the complex conjugate of a complex number.",
            "title": "\nHermitian adjoint\n",
            "url": "https://en.wikipedia.org/wiki/Hermitian_conjugate"
        },
        {
            "links": [
                "/wiki/Operator_theory",
                "/wiki/Operator_algebra",
                "/wiki/C*-algebra",
                "/wiki/W*-algebra",
                "/wiki/Random_variable",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_space",
                "/wiki/Complex_numbers",
                "/wiki/Inner-product_space",
                "/wiki/Fidelity_of_quantum_states",
                "/wiki/Real_numbers",
                "/wiki/Continuous_function",
                "/wiki/Topology",
                "/wiki/Triangle_inequality",
                "/wiki/H%C3%B6lder_inequality",
                "/wiki/Square-integrable",
                "/wiki/Function_(mathematics)",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Viktor_Bunyakovsky",
                "/wiki/Hermann_Amandus_Schwarz",
                "/wiki/Mathematics",
                "/wiki/Inequality_(mathematics)",
                "/wiki/Linear_algebra",
                "/wiki/Mathematical_analysis",
                "/wiki/Probability_theory",
                "/wiki/Vector_algebra"
            ],
            "text": "The next two theorems are further examples in operator algebra.which extends verbatim to positive functionals on C*-algebras:Various generalizations of the Cauchy\u2013Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.then the Cauchy\u2013Schwarz inequality becomesAfter defining an inner product on the set of random variables using the expectation of their product,Let X, Y be random variables, then the covariance inequality[13][14] is given byThe Cauchy\u2013Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [\u22121,\u00a01] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[11][12] as is done when extracting a metric from quantum fidelity.The Cauchy\u2013Schwarz inequality allows one to extend the notion of \"angle between two vectors\" to any real inner-product space by defining:[9][10]The Cauchy\u2013Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[7][8]Taking square roots gives the triangle inequality.The triangle inequality for the standard norm is often shown as a consequence of the Cauchy\u2013Schwarz inequality, as follows: given vectors x and y:A generalization of this is the H\u00f6lder inequality.For the inner product space of square-integrable complex-valued functions, one haswhich yields the Cauchy\u2013Schwarz inequality.This establishes the theorem.which givesThen, by linearity of the inner product in its first argument, one hasLetorThe inequality for sums was published by Augustin-Louis Cauchy\u00a0(1821), while the corresponding inequality for integrals was first proved by Viktor Bunyakovsky\u00a0(1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz\u00a0(1888).[1]In mathematics, the Cauchy\u2013Schwarz inequality, also known as the Cauchy\u2013Bunyakovsky\u2013Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]",
            "title": "\nCauchy\u2013Schwarz inequality\n",
            "url": "https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality"
        },
        {
            "links": [
                "/wiki/Mathematician",
                "/wiki/Foundations_of_geometry",
                "/wiki/Physical_space",
                "/wiki/Boolean_algebra_(logic)",
                "/wiki/%C3%89variste_Galois",
                "/wiki/Abstract_algebra",
                "/wiki/Real_numbers",
                "/wiki/Isomorphism",
                "/wiki/Second-order_logic",
                "/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem",
                "/wiki/First-order_logic",
                "/wiki/Non-standard_analysis",
                "/wiki/Euclid%27s_postulates",
                "/wiki/Euclidean_geometry",
                "/wiki/Parallel_postulate",
                "/wiki/Angle",
                "/wiki/Triangle",
                "/wiki/Hyperbolic_geometry",
                "/wiki/Elliptic_geometry",
                "/wiki/Peano_axioms",
                "/wiki/First-order_arithmetic",
                "/wiki/Number_theory",
                "/wiki/G%C3%B6del%27s_second_incompleteness_theorem",
                "/wiki/Measure_theory",
                "/wiki/Ergodic_theory",
                "/wiki/Probability",
                "/wiki/Representation_theory",
                "/wiki/Differential_geometry",
                "/wiki/Point_set_topology",
                "/wiki/Algebraic_topology",
                "/wiki/Differential_topology",
                "/wiki/Homology_theory",
                "/wiki/Homotopy_theory",
                "/wiki/Group_theory",
                "/wiki/Ring_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Galois_theory",
                "/wiki/Arithmetic",
                "/wiki/Real_analysis",
                "/wiki/Complex_analysis",
                "/wiki/Zermelo%E2%80%93Fraenkel_set_theory",
                "/wiki/Axiomatic_set_theory",
                "/wiki/Von_Neumann%E2%80%93Bernays%E2%80%93G%C3%B6del_set_theory",
                "/wiki/Conservative_extension",
                "/wiki/Morse%E2%80%93Kelley_set_theory",
                "/wiki/Strongly_inaccessible_cardinal",
                "/wiki/Grothendieck_universe",
                "/wiki/Second-order_arithmetic",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Formal_system#Logical_system",
                "/wiki/Rules_of_inference",
                "/wiki/Deductive_system",
                "/wiki/Discourse",
                "/wiki/Commutative",
                "/wiki/Mathematical_theory",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Natural_number",
                "/wiki/Integer",
                "/wiki/Group_(algebra)",
                "/wiki/Tautology_(logic)",
                "/wiki/Axiom_scheme",
                "/wiki/Predicate_calculus",
                "/wiki/Formula_(mathematical_logic)",
                "/wiki/Formal_language",
                "/wiki/Tautology_(logic)",
                "/wiki/Satisfiability",
                "/wiki/Assignment_(mathematical_logic)",
                "/wiki/Tautology_(logic)",
                "/wiki/Predicate_logic",
                "/wiki/Logical_truth",
                "/wiki/Mathematical_logic",
                "/wiki/Falsifiability",
                "/wiki/EPR_paradox",
                "/wiki/Niels_Bohr",
                "/wiki/Quantum_mechanics",
                "/wiki/Probability_theory",
                "/wiki/Deterministic",
                "/wiki/John_Stewart_Bell",
                "/wiki/Bell_inequalities",
                "/wiki/Alain_Aspect",
                "/wiki/Copenhagen_interpretation",
                "/wiki/Albert_Einstein",
                "/wiki/Special_relativity",
                "/wiki/General_relativity",
                "/wiki/Theoretical_physics",
                "/wiki/Isaac_Newton",
                "/wiki/Euclid",
                "/wiki/Spacetime",
                "/wiki/Natural_number",
                "/wiki/Infinite_set",
                "/wiki/Zermelo%E2%80%93Fraenkel_axioms",
                "/wiki/Forcing_(mathematics)",
                "/wiki/Paul_Cohen",
                "/wiki/Continuum_hypothesis",
                "/wiki/Peano_arithmetic",
                "/wiki/Corollary",
                "/wiki/Peano_arithmetic",
                "/wiki/Georg_Cantor",
                "/wiki/Set_theory",
                "/wiki/Russell%27s_paradox",
                "/wiki/Na%C3%AFve_set_theory",
                "/wiki/Euclidean_geometry",
                "/wiki/Class_(set_theory)",
                "/wiki/Consistent",
                "/wiki/Logic",
                "/wiki/Gottlob_Frege",
                "/wiki/Bertrand_Russell",
                "/wiki/Henri_Poincar%C3%A9",
                "/wiki/David_Hilbert",
                "/wiki/Kurt_G%C3%B6del",
                "/wiki/Field_(mathematics)",
                "/wiki/Field_theory_(mathematics)",
                "/wiki/Group_(mathematics)",
                "/wiki/Topological_space",
                "/wiki/Linear_space",
                "/wiki/Hyperbolic_geometry",
                "/wiki/Propositional_logic",
                "/wiki/Primitive_notion",
                "/wiki/Alessandro_Padoa",
                "/wiki/Mario_Pieri",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclid%27s_Elements",
                "/wiki/Geometry",
                "/wiki/Science",
                "/wiki/Posterior_analytics",
                "/wiki/Syllogisms",
                "/wiki/Tautology_(logic)",
                "/wiki/Theorem",
                "/wiki/Aristotle",
                "/wiki/Euclid",
                "/wiki/Proclus",
                "/wiki/Geminus",
                "/wiki/Boethius",
                "/wiki/Euclid",
                "/wiki/Greek_language",
                "/wiki/Verbal_noun",
                "/wiki/Ancient_Greece",
                "/wiki/Philosopher",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Philosophy_of_mathematics",
                "/wiki/Mathematics",
                "/wiki/Arithmetic",
                "/wiki/Classic_philosophy",
                "/wiki/Self-evidence",
                "/wiki/Logic",
                "/wiki/Truth",
                "/wiki/Premise"
            ],
            "text": "Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.There is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms. The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.The objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The L\u00f6wenheim\u2013Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as \"4 + 1\" because for nearly two millennia the fifth (parallel) postulate (\"through a point outside a line there is exactly one parallel\") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate (\"a line can be extended indefinitely\") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.The Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed G\u00f6del to establish his famous second incompleteness theorem.[12]This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo\u2013Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann\u2013Bernays\u2013G\u00f6del set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse\u2013Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.[citation needed]This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.Non-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought[citation needed] that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not tautologies. Another name for a non-logical axiom is postulate.[11]Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.[10]Other axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed.[9]These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms at least some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.In the field of mathematical logic, a clear distinction is made between two notions of axioms: logical and non-logical (somewhat similar to the ancient distinction between \"axioms\" and \"postulates\" respectively).Regardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither \"proves\" nor \"disproves\" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.As a consequence, it is not necessary to explicitly cite Einstein's axioms, the more so since they concern subtle points on the \"reality\" and \"locality\" of experiments.Another paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of \"theorems\" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics \"hidden variables\" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein's axioms compared to using Bohr's axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr's axioms, not Einstein's. (Bohr's axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)In 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity.Axioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment.It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo\u2013Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo\u2013Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.The formalist project suffered a decisive setback, when in 1931 G\u00f6del showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, G\u00f6del proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of na\u00efve set theory raised the possibility that any such system could turn out to be inconsistent.It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms.In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincar\u00e9, Hilbert, and G\u00f6del are some of the key figures in this development.It is not correct to say that the axioms of field theory are \"propositions that are regarded as true without proof.\" Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without any particular application in mind. The distinction between an \"axiom\" and a \"postulate\" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like \"line\" and \"parallel\" with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience.A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.The classical approach is well-illustrated by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of \"common notions\" (very basic, self-evident assertions).At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a postulate. While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.[8]An \"axiom\", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion thatThe ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that, \"Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property.\"[7] Boethius translated 'postulate' as petitio and called the axioms notiones communes but in later manuscripts this usage was not always strictly kept.The root meaning of the word postulate is to \"demand\"; for instance, Euclid demands that one agree that some things can be done, e.g. any two points can be joined by a straight line, etc.[6]The word axiom comes from the Greek word \u1f00\u03be\u03af\u03c9\u03bc\u03b1 (ax\u00ed\u014dma), a verbal noun from the verb \u1f00\u03be\u03b9\u03cc\u03b5\u03b9\u03bd (axioein), meaning \"to deem worthy\", but also \"to require\", which in turn comes from \u1f04\u03be\u03b9\u03bf\u03c2 (\u00e1xios), meaning \"being in balance\", and hence \"having (the same) value (as)\", \"worthy\", \"proper\". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof.In both senses, an axiom is any mathematical statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom, or any mathematical statement, to be \"true\" is an open question[citation needed] in the philosophy of mathematics.[5]As used in mathematics, the term axiom is used in two related but distinguishable senses: \"logical axioms\" and \"non-logical axioms\". Logical axioms are usually statements that are taken to be true within the system of logic they define (e.g., (A and B) implies A), often shown in symbolic form, while non-logical axioms (e.g., a + b = b + a) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, \"axiom\", \"postulate\", and \"assumption\" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain.The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] As used in modern logic, an axiom is simply a premise or starting point for reasoning.[4]An axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek ax\u00ed\u014dma (\u1f00\u03be\u03af\u03c9\u03bc\u03b1) 'that which is thought worthy or fit' or 'that which commends itself as evident.'[1][2]",
            "title": "\nAxiom\n",
            "url": "https://en.wikipedia.org/wiki/Axiom"
        },
        {
            "links": [
                "/wiki/Geometric_algebra",
                "/wiki/Clifford_algebra",
                "/wiki/Interior_product",
                "/wiki/Exterior_product",
                "/wiki/Vector_field",
                "/wiki/Differential_form",
                "/wiki/Exterior_algebra",
                "/wiki/Simple_tensor",
                "/wiki/Nondegenerate_form",
                "/wiki/Outer_product",
                "/wiki/Nondegenerate_form",
                "/wiki/Differential_geometry",
                "/wiki/Riemannian_manifold",
                "/wiki/Pseudo-Riemannian_manifold",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Minkowski_space",
                "/wiki/Dimension_(mathematics)",
                "/wiki/Sign_(mathematics)",
                "/wiki/Sign_convention#Metric_signature",
                "/wiki/Gelfand%E2%80%93Naimark%E2%80%93Segal_construction",
                "/wiki/Mercer%27s_theorem",
                "/wiki/Semi-norm",
                "/wiki/Spectral_theorem",
                "/wiki/Normal_operator",
                "/wiki/Linear",
                "/wiki/Weierstrass_approximation_theorem",
                "/wiki/Fourier_series",
                "/wiki/Trigonometric_polynomial",
                "/wiki/Hilbert_space",
                "/wiki/Parseval%27s_identity",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Hilbert_space",
                "/wiki/Hausdorff_maximal_principle",
                "/wiki/Hilbert_space",
                "/wiki/Separable_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_space",
                "/wiki/Parallelogram_equality#Normed_vector_spaces_satisfying_the_parallelogram_law",
                "/wiki/Probability",
                "/wiki/Almost_surely",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Expected_value",
                "/wiki/Cauchy_sequence",
                "/wiki/Hilbert_space",
                "/wiki/Complete_space",
                "/wiki/Hermitian_matrix",
                "/wiki/Positive-definite_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Scaling_(geometry)",
                "/wiki/Scale_factor",
                "/wiki/Weight_function",
                "/wiki/Hermitian_form",
                "/wiki/Transpose",
                "/wiki/Real_coordinate_space",
                "/wiki/Dot_product",
                "/wiki/Euclidean_space",
                "/wiki/Real_numbers",
                "/wiki/Sesquilinear",
                "/wiki/Complex_conjugate",
                "/wiki/Basefield",
                "/wiki/Ordered_field",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Algebraic_number",
                "/wiki/Constructible_number#Transformation_into_algebra",
                "/wiki/Quantum_computation",
                "/wiki/Complete_metric_space",
                "/wiki/Hilbert_space",
                "/wiki/Physics",
                "/wiki/Matrix_algebra",
                "/wiki/Sesquilinear_form",
                "/wiki/Bra%E2%80%93ket_notation",
                "/wiki/Quantum_mechanics",
                "/wiki/Linear_functional",
                "/wiki/Dual_space",
                "/wiki/Axiom",
                "/wiki/Vector_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Field_(mathematics)",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_vector_space",
                "/wiki/Complete_space",
                "/wiki/Hilbert_space",
                "/wiki/Complete_space#Completion",
                "/wiki/Hilbert_space",
                "/wiki/Linear_algebra",
                "/wiki/Vector_space",
                "/wiki/Mathematical_structure",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Angle",
                "/wiki/Orthogonality",
                "/wiki/Euclidean_space",
                "/wiki/Dot_product",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Functional_analysis"
            ],
            "text": "As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) \u2013 the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) \u2013 and in this context the exterior product is usually called the \"outer (alternatively, wedge) product\". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.More abstractly, the outer product is the bilinear map W \u00d7 V\u2217 \u2192 Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V\u2217 \u00d7 V \u2192 F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.In a quip: \"inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out\".On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V \u2192 V\u2217) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.The term \"inner product\" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 \u00d7 n covector with an n \u00d7 1 vector, yielding a 1\u00a0\u00d7\u00a01 matrix (a scalar), while the outer product is the product of an m \u00d7 1 vector with a 1 \u00d7 n covector, yielding an m \u00d7 n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V \u2192 V\u2217) and thus hold more generally.Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that \u27e8x,y\u27e9 \u2260 0, though y need not equal x; in other words, the induced map to the dual space V \u2192 V\u2217 is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of \"+\" and \"\u2212\" to them differs depending on conventions).This construction is used in numerous contexts. The Gelfand\u2013Naimark\u2013Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x\u00a0: ||x|| = 0}. The sesquilinear form \u27e8\u00b7,\u00b7\u27e9 factors through W.If V is a vector space and \u27e8\u00b7,\u00b7\u00b7\u00b7\u27e9 a semi-definite sesquilinear form, then the function:Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.Several types of linear maps A from an inner product space V to an inner product space W are of relevance:Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [\u2212\u03c0,\u03c0] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.Orthogonality of the sequence {ek}k follows immediately from the fact that if k \u2260 j, thenis an isometric linear map with dense image.is an orthonormal basis of the space C[\u2212\u03c0,\u03c0] with the L2 inner product. The mappingTheorem. Let V be the inner product space C[\u2212\u03c0,\u03c0]. Then the sequence (indexed on set of all integers) of continuous functionsThis theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:is an isometric linear map V \u2192 l2 with a dense image.Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of\u00a0V. Then the mapParseval's identity leads immediately to the following theorem:The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]Theorem. Any complete inner product space V has an orthonormal basis.Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show thatTheorem. Any separable inner product space V has an orthonormal basis.Using an infinite-dimensional analog of the Gram-Schmidt process one may show:if \u03b1 \u2260 \u03b2 and \u27e8e\u03b1,e\u03b1\u27e9 = ||e\u03b1|| = 1 for all \u03b1, \u03b2 \u2208 A.is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis andThis definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collectionLet V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram\u2013Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if \u27e8ei,ej\u27e9 = 0 for every i \u2260 j and \u27e8ei,ei\u27e9 = ||ei|| = 1 for each i.This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]A linear space with a norm such as:is an inner product.For real matrices of the same size, \u27e8A,B\u27e9\u00a0:= tr(ABT) with transpose as conjugationis an inner product.[6][7][8] In this case, \u27e8X,X\u27e9 = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.For real random variables X and Y, the expected value of their productThis sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.This space is not complete; consider for example, for the interval [\u22121,1] the sequence of continuous \"step\" functions, {\u2009fk}k, defined by:The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product iswhere M is any Hermitian positive-definite matrix and y\u2020 is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.The general form of an inner product on Cn is known as the Hermitian form and is given bywhere xT is the transpose of x.More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.A simple example is the real numbers with the standard multiplication as the inner productis also known as additivity.The property of an inner product space V thatAssuming the underlying field to be R, the inner product becomes symmetric, and we obtainCombining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:From the linearity property it is derived that x = 0 implies \u27e8x,x\u27e9 = 0. while from the positive-definiteness axiom we obtain the converse, \u27e8x,x\u27e9 = 0 implies x = 0. Combining these two, we have the property that \u27e8x,x\u27e9 = 0 if and only if x = 0.In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.Conjugate symmetry and linearity in the first variable givesMoreover, sesquilinearity (see below) implies thatNotice that conjugate symmetry implies that \u27e8x,x\u27e9 is real for all x, since we have:When F = R, conjugate symmetry reduces to symmetry. That is, \u27e8x,y\u27e9 = \u27e8y,x\u27e9 for F = R; while for F = C, \u27e8x,y\u27e9 is equal to the complex conjugate.In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that \u27e8x,x\u27e9 is only required to be non-negative. We show how to treat these below.There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product \u27e8x,y\u27e9 as \u27e8y|x\u27e9 (the bra\u2013ket notation of quantum mechanics), respectively y\u2020x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V\u2217, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking \u27e8x,y\u27e9 to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both \u27e8\u00b7,\u00b7\u27e9 and \u27e8\u00b7|\u00b7\u27e9 as distinct notations differing only in which argument is conjugate linear.that satisfies the following three axioms for all vectors x, y, z \u2208 V and all scalars a \u2208 F:[2][3]Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a mapIn this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]",
            "title": "\nInner product space\n",
            "url": "https://en.wikipedia.org/wiki/Inner_product"
        },
        {
            "links": [
                "/wiki/Double_dual",
                "/wiki/Natural_pairing",
                "/wiki/Ring_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Dual_module",
                "/wiki/Symmetric_power",
                "/wiki/Exterior_power",
                "/wiki/Dual_space",
                "/wiki/Universal_property",
                "/wiki/Tensor_product",
                "/wiki/Symplectic_vector_space",
                "/wiki/Sesquilinear_form",
                "/wiki/Sesquilinear_form#Hermitian_form",
                "/wiki/Quaternions",
                "/wiki/Bilinear_mapping",
                "/wiki/Orthogonal_complement",
                "/wiki/Quadratic_form",
                "/wiki/If_and_only_if",
                "/wiki/Symmetric_matrix",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Finite-dimensional",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Determinant",
                "/wiki/Non-singular_matrix",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(algebra)",
                "/wiki/Transpose",
                "/wiki/Degenerate_form",
                "/wiki/Linear_functional",
                "/wiki/Currying",
                "/wiki/Dual_space",
                "/wiki/Complex_number",
                "/wiki/Sesquilinear_form",
                "/wiki/Conjugate_linear",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Linear_map",
                "/wiki/Module_homomorphism",
                "/wiki/Mathematics",
                "/wiki/Abstract_algebra",
                "/wiki/Linear_algebra",
                "/wiki/Vector_space",
                "/wiki/Bilinear_map",
                "/wiki/Field_(mathematics)",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Linear_map"
            ],
            "text": "A linear map S\u00a0: M\u2217 \u2192 M\u2217\u00a0: u \u21a6 S(u) induces the bilinear form B\u00a0: M\u2217 \u00d7 M \u2192 R\u00a0: (u, x) \u21a6 \u27e8S(u), x\u27e9, and a linear map T\u00a0: M \u2192 M\u00a0: x \u21a6 T(x) induces the bilinear form B\u00a0: M\u2217 \u00d7 M \u2192 R\u00a0: (u, x) \u21a6 \u27e8u, T(x))\u27e9. Conversely, a bilinear form B\u00a0: M\u2217 \u00d7 M \u2192 R induces the R-linear maps S\u00a0: M\u2217 \u2192 M\u2217\u00a0: u \u21a6 (x \u21a6 B(u, x)) and T\u2032\u00a0: M \u2192 M\u2217\u2217\u00a0: x \u21a6 (u \u21a6 B(u, x)). Here, M\u2217\u2217 denotes the double dual of M.The mapping \u27e8\u22c5,\u22c5\u27e9\u00a0: M\u2217 \u00d7 M \u2192 R\u00a0: (u, x) \u21a6 u(x) is known as the natural pairing, also called the canonical bilinear form on M\u2217 \u00d7 M.[7]for all u, v \u2208 M\u2217, x, y \u2208 M, \u03b1, \u03b2 \u2208 R.Given a ring R and a right R-module M and its dual module M\u2217, a mapping B\u00a0: M\u2217 \u00d7 M \u2192 R is called a bilinear form ifLikewise, symmetric bilinear forms may be thought of as elements of Sym2(V\u2217) (the second symmetric power of V\u2217), and alternating bilinear forms as elements of \u039b2V\u2217 (the second exterior power of V\u2217).The set of all linear maps V \u2297 V \u2192 K is the dual space of V \u2297 V, so bilinear forms may be thought of as elements ofBy the universal property of the tensor product, bilinear forms on V are in 1-to-1 correspondence with linear maps V \u2297 V \u2192 K. If B is a bilinear form on V the corresponding linear map is given byis called the real symmetric case and labeled R(p, q), where p + q = n. Then he articulates the connection to traditional terminology:[6]Terminology varies in coverage of bilinear forms. For example, F. Reese Harvey discusses \"eight types of inner product\".[5] To define them he uses diagonal matrices Aij having only +1 or \u22121 for non-zero elements. Some of the \"inner products\" are symplectic forms and some are sesquilinear forms or Hermitian forms. Rather than a general field K, the instances with real numbers R, complex numbers C, and quaternions H are spelled out. The bilinear formIn finite dimensions, this is equivalent to the pairing being nondegenerate (the spaces necessarily having the same dimensions). For modules (instead of vector spaces), just as how a nondegenerate form is weaker than a unimodular form, a nondegenerate pairing is a weaker notion than a perfect pairing. A pairing can be nondegenerate without being a perfect pairing, for instance Z \u00d7 Z \u2192 Z via (x,y) \u21a6 2xy is nondegenerate, but induces multiplication by 2 on the map Z \u2192 Z\u2217.Here we still have induced linear mappings from V to W\u2217, and from W to V\u2217. It may happen that these mappings are isomorphisms; assuming finite dimensions, if one is an isomorphism, the other must be. When this occurs, B is said to be a perfect pairing.Much of the theory is available for a bilinear mapping from two vector spaces over the same base field to that fieldFor a non-degenerate form on a finite dimensional space, the map V/W \u2192 W\u22a5 is bijective, and the dimension of W\u22a5 is dim(V) \u2212 dim(W).Suppose W is a subspace. Define the orthogonal complement[4]A bilinear form B is reflexive if and only if it is either symmetric or alternating.[3] In the absence of reflexivity we have to distinguish left and right orthogonality. In a reflexive space the left and right radicals agree and are termed the kernel or the radical of the bilinear form: the subspace of all vectors orthogonal with every other vector. A vector v, with matrix representation x, is in the radical of a bilinear form with matrix representation A, if and only if Ax = 0 \u21d4 xTA = 0. The radical is always a subspace of V. It is trivial if and only if the matrix A is nonsingular, and thus if and only if the bilinear form is nondegenerate.When char(K) = 2 and dim V > 1, this correspondence between quadratic forms and symmetric bilinear forms breaks down.When char(K) \u2260 2, the quadratic form Q is determined by the symmetric part of the bilinear form B and is independent of the antisymmetric part. In this case there is a one-to-one correspondence between the symmetric part of the bilinear form and the quadratic form, and it makes sense to speak of the symmetric bilinear form associated with a quadratic form.For any bilinear form B\u00a0: V \u00d7 V \u2192 K, there exists an associated quadratic form Q\u00a0: V \u2192 K defined by Q\u00a0: V \u2192 K\u00a0: v \u21a6 B(v, v).where tB is the transpose of B (defined above).A bilinear form is symmetric if and only if the maps B1, B2: V \u2192 V\u2217 are equal, and skew-symmetric if and only if they are negatives of one another. If char(K) \u2260 2 then one can decompose a bilinear form into a symmetric and a skew-symmetric part as followsA bilinear form is symmetric (resp. skew-symmetric) if and only if its coordinate matrix (relative to any basis) is symmetric (resp. skew-symmetric). A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero (which follows from skew-symmetry when char(K) \u2260 2).If the characteristic of K is not 2 then the converse is also true: every skew-symmetric form is alternating. If, however, char(K) = 2 then a skew-symmetric form is the same as a symmetric form and there exist symmetric/skew-symmetric forms that are not alternating.We define a bilinear form to beIf V is finite-dimensional then, relative to some basis for V, a bilinear form is degenerate if and only if the determinant of the associated matrix is zero. Likewise, a nondegenerate form is one for which the determinant of the associated matrix is non-zero (the matrix is non-singular). These statements are independent of the chosen basis. For a module over a commutative ring, a unimodular form is one for which the determinant of the associate matrix is a unit (for example 1), hence the term; note that a form whose matrix is non-zero but not a unit will be nondegenerate but not unimodular, for example B(x, y) = 2xy over the integers.This form will be nondegenerate if and only if A is an isomorphism.Given any linear map A\u00a0: V \u2192 V\u2217 one can obtain a bilinear form B on V viaIf V is finite-dimensional then the rank of B1 is equal to the rank of B2. If this number is equal to dim(V) then B1 and B2 are linear isomorphisms from V to V\u2217. In this case B is nondegenerate. By the rank\u2013nullity theorem, this is equivalent to the condition that the left and equivalently right radicals be trivial. For finite-dimensional spaces, this is often taken as the definition of nondegeneracy:The left radical and right radical of the form B are the kernels of B1 and B2 respectively;[1] they are the vectors orthogonal to the whole space on the left and on the right.[2]If V is finite-dimensional then one can identify V with its double dual V\u2217\u2217. One can then show that B2 is the transpose of the linear map B1 (if V is infinite-dimensional then B2 is the transpose of B1 restricted to the image of V in V\u2217\u2217). Given B one can define the transpose of B to be the bilinear form given byThe corresponding notion for a module over a commutative ring is that a bilinear form is unimodular if V \u2192 V\u2217 is an isomorphism. Given a finitely generated module over a commutative ring, the pairing may be injective (hence \"nondegenerate\" in the above sense) but not unimodular. For example, over the integers, the pairing B(x, y) = 2xy is nondegenerate but not unimodular, as the induced map from V = Z to V\u2217 = Z is multiplication by 2.For a finite-dimensional vector space V, if either of B1 or B2 is an isomorphism, then both are, and the bilinear form B is said to be nondegenerate. More concretely, for a finite-dimensional vector space, non-degenerate means that every non-zero element pairs non-trivially with some other element:where the dot ( \u22c5 ) indicates the slot into which the argument for the resulting linear functional is to be placed (see Currying).This is often denoted asEvery bilinear form B on V defines a pair of linear maps from V to its dual space V\u2217. Define B1, B2: V \u2192 V\u2217 byNow the new matrix representation for the bilinear form is given by: STAS.where S \u2208 GL(n, K).Suppose {f1, ..., fn} is another basis for V, such that:If the n\u2009\u00d7\u20091 matrix x represents a vector v with respect to this basis, and analogously, y represents w, then:Define the n\u2009\u00d7\u2009n matrix A by Aij = B(ei, ej).Let V \u2245 Kn be an n-dimensional vector space with basis {e1, ..., en}.When K is the field of complex numbers C, one is often more interested in sesquilinear forms, which are similar to bilinear forms but are conjugate linear in one argument.The definition of a bilinear form can be extended to include modules over a ring, with linear maps replaced by module homomorphisms.In mathematics, more specifically in abstract algebra and linear algebra, a bilinear form on a vector space V is a bilinear map V \u00d7 V \u2192 K, where K is the field of scalars. In other words, a bilinear form is a function B\u00a0: V \u00d7 V \u2192 K that is linear in each argument separately:",
            "title": "\nBilinear form\n",
            "url": "https://en.wikipedia.org/wiki/Bilinear_form"
        },
        {
            "links": [
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_chemistry",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hilbert_space",
                "/wiki/Variational_principle",
                "/wiki/Linear_recursive_sequences",
                "/wiki/Fibonacci_number",
                "/wiki/Matrix_exponential",
                "/wiki/Matrix_function",
                "/wiki/Invertible_matrix#Methods_of_matrix_inversion",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Nilpotent_matrix",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace#Algebraic_and_geometric_multiplicities",
                "/wiki/Rotation_matrix",
                "/wiki/Rotation_matrix#Independent_planes",
                "/wiki/Jordan_Normal_Form",
                "/wiki/Lie_theory",
                "/wiki/Toral_Lie_algebra",
                "/wiki/Normal_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Commuting_matrices",
                "/wiki/Eigenvalue_algorithm",
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Orthonormal_basis",
                "/wiki/Unitary_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Transpose",
                "/wiki/Right_eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Linearly_independent",
                "/wiki/Row_vector",
                "/wiki/Left_eigenvector",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Nilpotent",
                "/wiki/Jordan_form",
                "/wiki/Subset",
                "/wiki/Lebesgue_measure",
                "/wiki/Zariski_topology",
                "/wiki/Discriminant",
                "/wiki/Hypersurface",
                "/wiki/Norm_(mathematics)",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Elementary_divisor",
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "text": "In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schr\u00f6dinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schr\u00f6dinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.thereby explaining the above phenomenon.The preceding relations, expressed in matrix form, areSwitching back to the standard basis, we haveThus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have thatStraightforward calculations show thatwhere ei denotes the standard basis of Rn. The reverse change of basis is given byThe above phenomenon can be explained by diagonalizing M. To accomplish this, we need a basis of R2 consisting of eigenvectors of M. One such eigenvector basis is given byCalculating the various powers of M reveals a surprising pattern:For example, consider the following matrix:This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found thatThen P diagonalizes A, as a simple computation confirms, having calculated P \u22121 using any suitable method:Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]Now, let P be the matrix with these eigenvectors as its columns:The eigenvectors of A areThese eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.A is a 3\u00d73 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n\u00d7n matrix then this matrix is diagonalizable.This matrix has eigenvaluesConsider a matrixNote that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.then Q\u22121BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle \u03b8=3\u03c0/2The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q\u22121BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we takeSome real matrices are not diagonalizable over the reals. Consider for instance the matrixThis matrix is not diagonalizable: there is no matrix U such that U\u22121CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, considerIn general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to \"do the best one can\", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.are diagonalizable but not simultaneously diagonalizable because they do not commute.The set of all n\u00d7n diagonalisable matrices (over C) with n > 1 is not simultaneously diagonalisable. For instance, the matricesA set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P\u22121AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.[2]In practice, matrices are diagonalized numerically using computers. Many algorithms exist to accomplish this.When the matrix A is a Hermitian matrix (resp. symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of Cn (resp. Rn). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P\u22121 equals the conjugate transpose (resp. transpose) of P.So the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P\u22121 are the left eigenvectors of A.the above equation can be rewritten asthen:If a matrix A can be diagonalized, that is,The Jordan\u2013Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each \"block\" is a one-by-one matrix.As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n\u00d7n matrices that are not diagonalizable over C, considered as a subset of Cn\u00d7n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.The following sufficient (but not necessary) condition is often useful.Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)The fundamental fact about diagonalizable maps and matrices is expressed by the following:Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling)\u00a0\u2014 it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P\u22121AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T\u00a0: V \u2192 V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.",
            "title": "\nDiagonalizable matrix\n",
            "url": "https://en.wikipedia.org/wiki/Diagonalizable_matrix"
        },
        {
            "links": [
                "/wiki/Multiplication_operator",
                "/wiki/Operator_theory",
                "/wiki/PDEs",
                "/wiki/Separable_partial_differential_equation",
                "/wiki/Integral_transform",
                "/wiki/Eigenbasis",
                "/wiki/Eigenfunction",
                "/wiki/Fourier_transform",
                "/wiki/Heat_equation",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Spectral_theorem",
                "/wiki/Normal_matrix",
                "/wiki/Matrix_similarity",
                "/wiki/Unitary_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Similar_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linear_operator",
                "/wiki/Triangular_matrix",
                "/wiki/Triangular_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Zero_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Adjugate",
                "/wiki/Determinant",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvectors",
                "/wiki/Subring",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_multiplication",
                "/wiki/Center_of_an_algebra",
                "/wiki/Commute_(mathematics)",
                "/wiki/Identity_matrix",
                "/wiki/Vector_(mathematics_and_physics)",
                "/wiki/Scalar_multiplication",
                "/wiki/Real_numbers",
                "/wiki/Complex_numbers",
                "/wiki/Normal_matrix"
            ],
            "text": "Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function\u2013the values of the function at each point correspond to the diagonal entries of a matrix.In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates\u2013in the language of operators, an integral transform\u2013which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA\u2217 = A\u2217A then there exists a unitary matrix U such that UAU\u2217 is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV\u2217 is diagonal with positive entries.In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X\u22121AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.A symmetric diagonal matrix can be defined as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.Any square diagonal matrix is also a symmetric matrix.A square matrix is diagonal if and only if it is triangular and normal.The adjugate of a diagonal matrix is again diagonal.The determinant of diag(a1, ..., an) is the product a1...an.In other words, the eigenvalues of diag(\u03bb1, ..., \u03bbn) are \u03bb1, ..., \u03bbn with associated eigenvectors of e1, ..., en.Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i.In particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.The diagonal matrix diag(a1, ..., an) is invertible if and only if the entries a1, ..., an are all non-zero. In this case, we haveand for matrix multiplication,The operations of matrix addition and matrix multiplication are especially simple for symmetric diagonal matrices. Write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1, ..., an. Then, for addition, we haveThe scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.A square diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple \u03bbI of the identity matrix I. Its effect on a vector is scalar multiplication by \u03bb. For example, a 3\u00d73 scalar matrix has the form:In the remainder of this article we will consider only square matrices.If the entries are real numbers or complex numbers, then it is a normal matrix as well.The following matrix is a symmetric diagonal matrix:The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. For example:However, the main diagonal entries need not be zero.As stated above, the off-diagonal entries are zero. That is, the matrix D = (di,j) with n columns and n rows is diagonal if",
            "title": "\nDiagonal matrix\n",
            "url": "https://en.wikipedia.org/wiki/Diagonal_matrix"
        },
        {
            "links": [
                "/wiki/Square_root_of_a_matrix",
                "/wiki/Positive-definite_matrix",
                "/wiki/Idempotent_matrix",
                "/wiki/Full_rank",
                "/wiki/Linear_independence",
                "/wiki/Kronecker_delta",
                "/wiki/Diagonal_matrix",
                "/wiki/Unit_vector",
                "/wiki/Determinant",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Linear_transformation",
                "/wiki/Identity_function",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Identity_element",
                "/wiki/General_linear_group",
                "/wiki/Invertible_matrix",
                "/wiki/Involutory_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Quantum_mechanics"
            ],
            "text": "The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.[3]The identity matrix of a given size is the only idempotent matrix of that size having full rank. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.It can also be written using the Kronecker delta notation:Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:The ith column of an identity matrix is the unit vector ei. It follows that the determinant of the identity matrix is\u00a01 and the trace is\u00a0n.Where n\u00d7n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.In particular, the identity matrix serves as the unit of the ring of all n\u00d7n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n\u00d7n matrices. (The identity matrix itself is invertible, being its own inverse.)When A is m\u00d7n, it is a property of matrix multiplication thatIn linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\"[1] and the German word \"Einheitsmatrix\",[2] respectively.",
            "title": "\nIdentity matrix\n",
            "url": "https://en.wikipedia.org/wiki/Identity_matrix"
        },
        {
            "links": [
                "/wiki/Robert_Recorde",
                "/wiki/The_Whetstone_of_Witte",
                "/wiki/Michael_Stifel",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/The_Nine_Chapters_on_the_Mathematical_Art",
                "/wiki/Computational_complexity_theory",
                "/wiki/Polynomial_time",
                "/wiki/Algorithm",
                "/wiki/Characteristic_polynomial",
                "/wiki/Eigenvalue",
                "/wiki/Minimal_polynomial_(field_theory)",
                "/wiki/Algebraic_element",
                "/wiki/Chromatic_polynomial",
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Spline_(mathematics)",
                "/wiki/Irreducible_polynomial",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Unique_factorization_domain",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Computer_algebra_system",
                "/wiki/Eisenstein%27s_criterion",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Euclidean_domain",
                "/wiki/Field_(mathematics)",
                "/wiki/Commutative_algebra",
                "/wiki/Integral_domain",
                "/wiki/Polynomial_long_division",
                "/wiki/Unital_algebra",
                "/wiki/Associative_algebra",
                "/wiki/Substitution_(algebra)",
                "/wiki/Fermat%27s_little_theorem",
                "/wiki/Analysis_(mathematics)",
                "/wiki/Euclidean_division",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Finite_field",
                "/wiki/Prime_number",
                "/wiki/Modular_arithmetic",
                "/wiki/Commutative_ring",
                "/wiki/Algebra_(ring_theory)",
                "/wiki/Distributive_law",
                "/wiki/Abstract_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Calculus",
                "/wiki/Taylor%27s_theorem",
                "/wiki/Differentiable_function",
                "/wiki/Stone%E2%80%93Weierstrass_theorem",
                "/wiki/Continuous_function",
                "/wiki/Compact_space",
                "/wiki/Interval_(mathematics)",
                "/wiki/Formal_power_series",
                "/wiki/Irrational_number",
                "/wiki/Power_series",
                "/wiki/Rational_fraction",
                "/wiki/Quotient",
                "/wiki/Algebraic_fraction",
                "/wiki/Algebraic_expression",
                "/wiki/Rational_function",
                "/wiki/Laurent_polynomial",
                "/wiki/Matrix_ring",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_polynomial",
                "/wiki/Square_matrix",
                "/wiki/Trigonometric_interpolation",
                "/wiki/Interpolation",
                "/wiki/Periodic_function",
                "/wiki/Discrete_Fourier_transform",
                "/wiki/Complex_number",
                "/wiki/Fourier_series",
                "/wiki/List_of_trigonometric_identities#Multiple-angle_formulae",
                "/wiki/List_of_trigonometric_identities#Product-to-sum_and_sum-to-product_identities",
                "/wiki/Linear_combination",
                "/wiki/Function_(mathematics)",
                "/wiki/Natural_number",
                "/wiki/Integer",
                "/wiki/Diophantine_equation",
                "/wiki/Algorithm",
                "/wiki/Hilbert%27s_tenth_problem",
                "/wiki/Fermat%27s_Last_Theorem",
                "/wiki/System_of_linear_equations",
                "/wiki/System_of_linear_equations#Solving_a_linear_system",
                "/wiki/Gaussian_elimination",
                "/wiki/Algebraic_geometry",
                "/wiki/Algorithm",
                "/wiki/Complex_number",
                "/wiki/System_of_polynomial_equations",
                "/wiki/Numerical_approximation",
                "/wiki/Continuous_function",
                "/wiki/Algorithm",
                "/wiki/Computer",
                "/wiki/Root-finding_algorithm",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Vieta%27s_formulas",
                "/wiki/Complex_number",
                "/wiki/Multiplicity_(mathematics)",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Algebra",
                "/wiki/Quadratic_formula",
                "/wiki/Cubic_function",
                "/wiki/Quartic_equations",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Root-finding_algorithm",
                "/wiki/Numerical_approximation",
                "/wiki/Variable_(mathematics)",
                "/wiki/Identity_(mathematics)",
                "/wiki/Algebraic_equation",
                "/wiki/Equation",
                "/wiki/Infinity#Calculus",
                "/wiki/Absolute_value",
                "/wiki/Asymptote",
                "/wiki/Parabolic_branch",
                "/wiki/Graph_of_a_function",
                "/wiki/Continuous_function",
                "/wiki/Smooth_function",
                "/wiki/Entire_function",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Restriction_of_a_function",
                "/wiki/Expression_(mathematics)",
                "/wiki/Argument_of_a_function",
                "/wiki/Algebraic_fraction",
                "/wiki/Rational_function",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Algorithm",
                "/wiki/Computer_algebra_system",
                "/wiki/Unique_factorization_domain",
                "/wiki/Field_(mathematics)",
                "/wiki/Irreducible_polynomial",
                "/wiki/Complex_number",
                "/wiki/Real_number",
                "/wiki/Rational_number",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Euclidean_division",
                "/wiki/Polynomial_long_division",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Polynomial_remainder_theorem",
                "/wiki/Associative_law",
                "/wiki/Horner%27s_method",
                "/wiki/Univariate",
                "/wiki/Real_number",
                "/wiki/S-plane",
                "/wiki/Laplace_transform",
                "/wiki/Integer",
                "/wiki/Complex_number",
                "/wiki/Distributive_law",
                "/wiki/Monomial",
                "/wiki/Binomial_(polynomial)",
                "/wiki/Commutative_law",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Root_of_a_function",
                "/wiki/Constant_term",
                "/wiki/Term_(mathematics)",
                "/wiki/Coefficient",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Summation#Capital-sigma_notation",
                "/wiki/Expression_(mathematics)",
                "/wiki/Constant_(mathematics)",
                "/wiki/Addition",
                "/wiki/Multiplication",
                "/wiki/Exponentiation",
                "/wiki/Non-negative_integer",
                "/wiki/Commutative_property",
                "/wiki/Associative_property",
                "/wiki/Distributive_property",
                "/wiki/Ring_(mathematics)",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Function_(mathematics)",
                "/wiki/Hybrid_word",
                "/wiki/Binomial_(polynomial)",
                "/wiki/Polynomial_equation",
                "/wiki/Word_problem_(mathematics_education)",
                "/wiki/Chemistry",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Social_science",
                "/wiki/Calculus",
                "/wiki/Numerical_analysis",
                "/wiki/Polynomial_ring",
                "/wiki/Algebraic_variety",
                "/wiki/Algebra",
                "/wiki/Algebraic_geometry",
                "/wiki/Mathematics",
                "/wiki/Expression_(mathematics)",
                "/wiki/Variable_(mathematics)",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Coefficient",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Integer",
                "/wiki/Exponentiation"
            ],
            "text": "The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, \u2212 for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. Ren\u00e9 Descartes, in La g\u00e9ometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.[23]Determining the roots of polynomials, or \"solving algebraic equations\", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins \"Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou.\" We would write 3x\u00a0+\u00a02y\u00a0+\u00a0z =\u00a029.The term \"polynomial\", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.Polynomials serve to approximate other functions,[22] such as the use of splines.Analogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, \"non-constant\" must be replaced by \"non-constant or non-unit\" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.and such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[x] is a Euclidean domain.If F is a field and f and g are polynomials in F[x] with g \u2260 0, then there exist unique polynomials q and r in F[x] withIn commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[x], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[x] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[x] and r is an element of R such that f(r) = 0, then the polynomial (x \u2212 r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.[20][21]If R is commutative, then one can associate to every polynomial P in R[x], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for x.Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[x] over the real numbers by factoring out the ideal of multiples of the polynomial x2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).One can think of the ring R[x] as arising from R by adding one new element x to R, and extending in a minimal way to a ring in which x satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is xr = rx). To do this, one must add all powers of x and their linear combinations as well.Thus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[x]. The map from R to R[x] sending r to rx0 is an injective homomorphism of rings, by which R is viewed as a subring of R[x]. If R is commutative, then R[x] is an algebra over R.where n is a natural number, the coefficients a0, . . ., an are elements of R, and x is a formal symbol, whose powers xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aixi is interpreted as a polynomial that has zero coefficients at all powers of x other than xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the ruleIn abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate x over a ring R is defined as a formal expression of the formand the indefinite integral isthe derivative with respect to x isCalculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial functionThe simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone\u2013Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).where I is the identity matrix.[19]this polynomial evaluated at a matrix A isA matrix polynomial is a polynomial with square matrices as variables.[18] Given an ordinary, scalar-valued polynomialTrigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.For complex coefficients, there is no difference between such a function and a finite Fourier series.If sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.A trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers.[17] The coefficients may be taken as real numbers, for real-valued functions. There are several generalizations of the concept of polynomials.A polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.For polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of \"roots\". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.When there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions.[16] There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).Some polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x \u2212 a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree\u00a01; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.A number a is a root of a polynomial P if and only if the linear polynomial x \u2212 a divides P, that is if there is another polynomial Q such that P = (x \u2013 a) Q. It may happen that x \u2212 a divides P more than once: if (x \u2212 a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x \u2212 a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P.[15] The relation between the coefficients of a polynomial and its roots is described by Vieta's formulas.The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.In elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel\u2013Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.When considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x \u2212 y) = x2 \u2212 y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.is a polynomial equation.For example,A polynomial equation, also called algebraic equation, is an equation of the form[14]Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.A non-constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).A polynomial function in one real variable can be represented by a graph.Every polynomial function is continuous, smooth, and entire.is a polynomial function of one variable. Polynomial functions of multiple variables are similarly defined, using polynomials in multiple indeterminates, as inFor example, the function f, defined byGenerally, unless otherwise specified, polynomial functions have real or complex coefficients and have real or complex arguments and values. In particular, a polynomial with real coefficients defines a function from the complex numbers to the complex numbers, whose restriction to the reals maps reals to reals.for all arguments x, where n is a non-negative integer and a0, a1, a2, ..., an are constant coefficients.A polynomial function is a function that can be defined by evaluating a polynomial. A function f of one argument is thus a polynomial function if it satisfies.Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a \"rational expression\" or \"rational fraction\" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.The computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.over the complex numbers.over the integers and the reals andisAll polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree.[13] For example, the factored form ofAs for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b \u2260 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.[12]Polynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of f(x) by (x \u2212 a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.which can be simplified tothenTo work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.[8] For example, ifwhich can be simplified tothenPolynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.[8][10] For example, ifThe evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:A polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply \"polynomials in x, y, and z\", listing the indeterminates allowed.A real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms. A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.Two terms with the same indeterminates raised to the same powers are called \"similar terms\" or \"like terms\", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.[8] Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,[9] a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term \"quadrinomial\" is occasionally used for a four-term polynomial.The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in \"descending powers of x\", with the term of largest degree first, or in \"ascending powers of x\". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is \u22125. The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.[7]In the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its non-zero terms have degree n. The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.[6] For example, x3y2 + 7x2y3 \u2212 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either \u22121 or \u2212\u221e).[5] These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, f(x) = 0, is the X-axis.Polynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:is a term. The coefficient is \u22125, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.For example:A term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.[3] The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).[4]That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number\u2014called the coefficient of the term[2]\u2014and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because x = x1, the degree of an indeterminate without a written exponent is one.This can be expressed more concisely by using summation notation:A polynomial in a single indeterminate x can always be written (or rewritten) in the formA polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.This equality allows writing \"let P(x) be a polynomial\" as a shorthand for \"let P be a polynomial in the indeterminate x\". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.Frequently, when using this function, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (any ring). In particular, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,which is the polynomial function associated to P.Normally, the name of the polynomial is P, not P(x). However, if a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting x by a in P. Thus, the polynomial P defines the functionIt may be confusing that a polynomial P in the indeterminate x may appear in the formulas either as P or as P(x).[citation needed]It is a common convention to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.[citation needed]The x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is \"indeterminate\"). It is thus more correct to call it an \"indeterminate\".[citation needed] However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a \"variable\". Many authors use these two words interchangeably.The word polynomial joins two diverse roots: the Greek poly, meaning \"many,\" and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.[1] Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate x is x2 \u2212 4x + 7. An example in three variables is x3 + 2xyz2 \u2212 yz + 1.",
            "title": "\nPolynomial\n",
            "url": "https://en.wikipedia.org/wiki/Polynomial"
        }
    ]
}
