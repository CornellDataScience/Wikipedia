Linear algebra
b'There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to.'b'Algebraic geometry considers the solutions of systems of polynomial equations.'b'Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.'b'Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.'b'If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \xc3\x97 V \xe2\x86\x92 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).'b'In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V\xe2\x88\x97 consisting of linear maps f: V \xe2\x86\x92 F where F is the field of scalars. Multilinear maps T: Vn \xe2\x86\x92 F can be described via tensor products of elements of V\xe2\x88\x97.'b'Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.'b'The set of points of a linear functional that map to zero define the kernel of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.[25][27]'b'Notice that if h is a solution to this homogeneous equation, then t h is also a solution.'b'The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,'b'For convenience the free parameter x has been relabeled t.'b'Solve for y and obtain the inverse image as the set of points,'b'In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation'b'Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c.'b'The set of points in the plane E that map to the same image in R under the linear functional \xce\xbb define a line in E. This line is the image of the inverse map, \xce\xbb\xe2\x88\x921: R\xe2\x86\x92E. This inverse image is the set of the points x=(x, y) that solve the equation,'b'Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.[25][27]'b'These equations can be assembled into the single matrix equation,'b'These coordinate functionals have the properties,'b'which can be written in matrix form as'b'The functionals \xcf\x83 and \xcf\x84 compute the components of x along the basis vectors v and w, respectively, that is,'b'To solve this equation for \xce\xb1, \xce\xb2, we compute the linear coordinate functionals \xcf\x83 and \xcf\x84 for the basis v, w, which are given by,[26]'b'This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E. Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is to find the real numbers \xce\xb1, \xce\xb2, so that x=\xce\xb1v+\xce\xb2w, that is'b'where Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form as'b'This is true for any pair of vectors used to define coordinates in E. Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E. This means a vector x has coordinates (\xce\xb1,\xce\xb2), such that x=\xce\xb1v+\xce\xb2w. Then, we have the linear functional'b'Thus, the columns of the matrix A are the image of the basis vectors of E in R.'b'Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj. It is now possible to see that'b'This shows that the sum of vectors in E map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation.[25] For this case, where the image space is a real number the map is called a linear functional.[27]'b'This transformation has the important property that if Ay=d, then'b'or'b'Another way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in E and consider the linear function \xce\xbb: E\xe2\x86\x92R, given by'b'Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E. For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.'b'which in homogeneous form yields,'b'It is interesting to consider the case of three lines, \xce\xbb1, \xce\xbb2 and \xce\xbb3, which yield the matrix equation,'b"if the rows of B are linearly independent (i.e., \xce\xbb1 and \xce\xbb2 represent distinct lines). Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns.[27] Notice that this yields a point in the z = 1 plane only when the 2\xe2\x80\x89\xc3\x97\xe2\x80\x892 submatrix associated with x3 has a non-zero determinant."b'The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates, the solutions are multiples of the following solution:[26]'b'or using homogeneous coordinates,'b'which forms a system of linear equations. The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,'b'Now consider the equations of the two lines \xce\xbb1 and \xce\xbb2,'b'The linear equation, \xce\xbb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \xce\xb1x1 + \xce\xb2x2 is also on the line, for any real \xce\xb1 and \xce\xb2.'b'Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space. The x\xe2\x88\x92y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1).'b'where x = (x, y, 1) is the 3\xe2\x80\x89\xc3\x97\xe2\x80\x891 set of homogeneous coordinates associated with the point (x, y).[26]'b'or'b'where a, b and c are not all zero. Then,'b'Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]'b'Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E. When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.'b'The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \xe2\x89\xa5 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient ak, we take the inner product with hk:'b'The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product'b'This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.'b'Fourier series are a representation of a function f: [\xe2\x88\x92\xcf\x80, \xcf\x80] \xe2\x86\x92 R as a trigonometric series:'b'The least squares method is used to determine the best-fit line for a set of data.[24] This line will minimize the sum of the squares of the residuals.'b'We can, in general, write any system of linear equations as a matrix equation:'b'The system is solved.'b'Next, z and y can be substituted into L1, which can be solved to obtain'b'Then, z can be substituted into L2, which can then be solved to obtain'b'The last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that'b'This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.'b'The result is:'b'Now y is eliminated from L3 by adding \xe2\x88\x924L2 to L3:'b'The result is:'b'In the example, x is eliminated from L2 by adding (3/2)L1 to L2. x is then eliminated from L3 by adding L1 to L3. Formally:'b'The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.'b'Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:'b'Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.'b'If T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.'b'The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying'b'and so we can call this quantity the cosine of the angle between the two vectors.'b'In particular, the quantity'b'and we can prove the Cauchy\xe2\x80\x93Schwarz inequality:'b'We can define the length of a vector v in V by'b'Note that in R, it is symmetric.'b'that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]'b'Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map'b'Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).'b'where I is the identity matrix. For there to be nontrivial solutions to that equation, det(T \xe2\x88\x92 \xce\xbb I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ..., vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbn, and if v = a1v1 + ... + an vn, then,'b'To find an eigenvector or an eigenvalue, we note that'b'In general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation T is to find "characteristic lines" that are invariant sets under T. If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector. The scalar \xce\xbb such that Tv = \xce\xbbv is called a characteristic value or eigenvalue of T.'b"One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20]). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method."b'There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V. While Rn has a standard basis {e1, e2, ..., en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V).'b'The condition that v1, v2, ..., vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ..., vn assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to v). In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn. Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn. Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \xe2\x86\x92 W may be encoded by an m \xc3\x97 n matrix A with entries in the field F, called the matrix of T with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.'b'A particular basis {v1, v2, ..., vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ..., an) is the linear combination'b'One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism.'b'Any two bases of a vector space V have the same cardinality, which is called the dimension of V. The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of V has finite number of elements, V is called a finite-dimensional vector space. If V is finite-dimensional and U is a subspace of V, then dim U \xe2\x89\xa4 dim V. If U1 and U2 are subspaces of V, then'b'A linear combination of any system of vectors with all zero coefficients is the zero vector of V. If this is the only way to express the zero vector as a linear combination of v1, v2, ..., vk then these vectors are linearly independent. Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V. Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis.[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.'b'where a1, a2, ..., ak are scalars. The set of all linear combinations of vectors v1, v2, ..., vk is called their span, which forms a subspace.'b'Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ..., vk:'b'Linear transformations have geometric significance. For example, 2 \xc3\x97 2 real matrices denote standard planar mappings that preserve the origin.'b'When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.'b'Additionally for any vectors u, v \xe2\x88\x88 V and scalars a, b \xe2\x88\x88 F:'b'for any vectors u,v \xe2\x88\x88 V and a scalar a \xe2\x88\x88 F.'b'that is compatible with addition and scalar multiplication:'b'Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map'b'The first four axioms are those of V being an abelian group under vector addition. Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.'b'The main structures of linear algebra are vector spaces. A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The operations of addition and multiplication in a vector space must satisfy the following axioms.[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.'b'Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s.[7] Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s.[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum.[9] In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as \'so many other disciplines use it\'.[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination.[13][14]'b'The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.'b'In 1882, H\xc3\xbcseyin Tevfik Pasha wrote the book titled "Linear Algebra".[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[4]'b'The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".[4]'b"The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations. Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear system, now called Cramer's Rule. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.[4]"b''b''b'Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation.'b'and their representations through matrices and vector spaces.[1][2][3]'b'linear functions such as'b'Linear algebra is the branch of mathematics concerning linear equations such as'
Algebraic geometry
b'Algebraic geometry now finds applications in statistics,[8] control theory,[9][10] robotics,[11] error-correcting codes,[12] phylogenetics[13] and geometric modelling.[14] There are also connections to string theory,[15] game theory,[16] graph matchings,[17] solitons[18] and integer programming.[19]'b'Modern analytic geometry is essentially equivalent to real and complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper GAGA, the name of which is French for Algebraic geometry and analytic geometry. Nevertheless, the two fields remain distinct, as the methods of proof are quite different and algebraic geometry includes also geometry in finite characteristic.'b'An analytic variety is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the included concept of real or complex algebraic variety. Any complex manifold is an analytic variety. Since analytic varieties may have singular points, not all analytic varieties are manifolds.'b'See also: derived algebraic geometry.'b'In parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gr\xc3\xb6bner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.'b"An important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's last theorem and are also used in elliptic curve cryptography."b'In the 1950s and 1960s Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely led by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities and moduli.'b'B. L. van der Waerden, Oscar Zariski and Andr\xc3\xa9 Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.'b"In the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connexion between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.[7]"b'The second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.'b'It took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of homogeneous polynomial forms, and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.'b'During the same period, Blaise Pascal and G\xc3\xa9rard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek ruler and compass construction. Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the calculus of infinitesimals of Lagrange and Euler.'b'Such techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccol\xc3\xb2 Fontana "Tartaglia" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry.[6] The French mathematicians Franciscus Vieta and later Ren\xc3\xa9 Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of algebraic curves, such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).'b'Some of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length x so that the cube of side x contained the same volume as the rectangular box a2b for given sides a and b. Menaechmus (circa 350 BC) considered the problem geometrically by intersecting the pair of plane conics ay\xc2\xa0=\xc2\xa0x2 and xy\xc2\xa0=\xc2\xa0ab.[1] The later work, in the 3rd century BC, of Archimedes and Apollonius studied more systematically problems on conic sections,[2] and also involved the use of coordinates.[1] The Arab mathematicians were able to solve by purely algebraic means certain cubic equations, and then to interpret the results geometrically. This was done, for instance, by Ibn al-Haytham in the 10th century AD.[3] Subsequently, Persian mathematician Omar Khayy\xc3\xa1m (born 1048 A.D.) discovered a method for solving cubic equations by intersecting a parabola with a circle.[4] A few years after Omar Khayy\xc3\xa1m, Sharaf al-Din al-Tusi\'s "Treatise on equations" has been described as inaugurating the beginning of algebraic geometry.[5]'b'Algebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the derived algebraic geometry, introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand To\xc3\xabn, Gabrielle Vezzosi, Michel Vaqui\xc3\xa9 and others; and developed further by Jacob Lurie, Bertrand To\xc3\xabn, and Gabrielle Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from early 1990s by Maxim Kontsevich and followers.'b'The language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.'b'Another formal generalization is possible to universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term variety of algebras should not be confused with algebraic variety.'b"Sometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup."b"Most remarkably, in late 1950s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field k, and the category of finitely generated reduced k-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the \xc3\xa9tale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne-Mumford stacks, both often called algebraic stacks."b'The modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.'b"Among these algorithms which solve a sub problem of the problems solved by Gr\xc3\xb6bner bases, one may cite testing if an affine variety is empty and solving nonhomogeneous polynomial systems which have a finite number of solutions. Such algorithms are rarely implemented because, on most entries Faug\xc3\xa8re's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity (probably because the evaluation of the complexity of Gr\xc3\xb6bner basis algorithms on a particular class of entries is a difficult task which has been done only in a few special cases)."b'As an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand, CAD is yet, in practice, the best algorithm to count the number of connected components.'b'Since 1973, most of the research on this subject is devoted either to improve CAD or to find alternate algorithms in special cases of general interest.'b'While Gr\xc3\xb6bner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.'b'The complexity of CAD is doubly exponential in the number of variables. This means that CAD allows, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.'b"This theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators and (\xe2\x88\xa7), or (\xe2\x88\xa8), not (\xc2\xac), for all (\xe2\x88\x80) and exists (\xe2\x88\x83). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (\xe2\x88\x80, \xe2\x88\x83)."b'CAD is an algorithm which was introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski\xe2\x80\x93Seidenberg theorem on quantifier elimination over the real numbers.'b"Gr\xc3\xb6bner bases are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faug\xc3\xa8re F5 algorithm realizes this complexity, as it may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow one to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gr\xc3\xb6bner basis is strongly related to the intrinsic difficulty of the problem."b'Gr\xc3\xb6bner basis computations do not allow one to compute directly the primary decomposition of I nor the prime ideals defining the irreducible components of V, but most algorithms for this involve Gr\xc3\xb6bner basis computation. The algorithms which are not based on Gr\xc3\xb6bner bases use regular chains but may need Gr\xc3\xb6bner bases in some exceptional situations.'b'Given an ideal I defining an algebraic set V:'b'A Gr\xc3\xb6bner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.'b'A body of mathematical theory complementary to symbolic methods called numerical algebraic geometry has been developed over the last several decades. The main computational method is homotopy continuation. This supports, for example, a model of floating point computation for solving problems of algebraic geometry.'b'Since then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.'b"One may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France in June 1979. At this meeting,"b"One of the challenging problems of real algebraic geometry is the unsolved Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8."b'Real algebraic geometry is the study of the real points of algebraic geometry.'b'The only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand, the field of the rational functions or function field is a useful notion, which, similarly to the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.'b'A polynomial in n + 1 variables vanishes at all points of a line passing through the origin if and only if it is homogeneous. In this case, one says that the polynomial vanishes at the corresponding point of Pn. This allows us to define a projective algebraic set in Pn as the set V(f1, ..., fk), where a finite set of homogeneous polynomials {f1, ..., fk} vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The projective varieties are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the projective coordinates ring being defined as the quotient of the graded ring or the polynomials in n + 1 variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.'b'Nowadays, the projective space Pn of dimension n is usually defined as the set of the lines passing through a point, considered as the origin, in the affine space of dimension n + 1, or equivalently to the set of the vector lines in a vector space of dimension n + 1. When a coordinate system has been chosen in the space of dimension n + 1, all the points of a line have the same set of coordinates, up to the multiplication by an element of k. This defines the homogeneous coordinates of a point of Pn as a sequence of n + 1 elements of the base field k, defined up to the multiplication by a nonzero element of k (the same for the whole sequence).'b'Thus many of the properties of algebraic varieties, including birational equivalence and all the topological properties, depend on the behavior "at infinity" and so it is natural to study the varieties in projective space. Furthermore, the introduction of projective techniques made many theorems in algebraic geometry simpler and sharper: For example, B\xc3\xa9zout\'s theorem on the number of intersection points between two varieties can be stated in its sharpest form only in projective space. For these reasons, projective space plays a fundamental role in algebraic geometry.'b'The consideration of the projective completion of the two curves, which is their prolongation "at infinity" in the projective plane, allows us to quantify this difference: the point at infinity of the parabola is a regular point, whose tangent is the line at infinity, while the point at infinity of the cubic curve is a cusp. Also, both curves are rational, as they are parameterized by x, and the Riemann-Roch theorem implies that the cubic curve must have a singularity, which must be at infinity, as all its points in the affine space are regular.'b'Compare this to the variety V(y\xc2\xa0\xe2\x88\x92\xc2\xa0x3). This is a cubic curve. As x goes to positive infinity, the slope of the line from the origin to the point (x,\xc2\xa0x3) goes to positive infinity just as before. But unlike before, as x goes to negative infinity, the slope of the same line goes to positive infinity as well; the exact opposite of the parabola. So the behavior "at infinity" of V(y\xc2\xa0\xe2\x88\x92\xc2\xa0x3) is different from the behavior "at infinity" of V(y\xc2\xa0\xe2\x88\x92\xc2\xa0x2).'b'To see how this might come about, consider the variety V(y \xe2\x88\x92 x2). If we draw it, we get a parabola. As x goes to positive infinity, the slope of the line from the origin to the point (x,\xc2\xa0x2) also goes to positive infinity. As x goes to negative infinity, the slope of the same line goes to negative infinity.'b'Just as the formulas for the roots of second, third, and fourth degree polynomials suggest extending real numbers to the more algebraically complete setting of the complex numbers, many properties of algebraic varieties suggest extending affine space to a more geometrically complete projective space. Whereas the complex numbers are obtained by adding the number i, a root of the polynomial x2 + 1, projective space is obtained by adding in appropriate points "at infinity", points where parallel lines may meet.'b'The problem of resolution of singularities is to know if every algebraic variety is birationally equivalent to a variety whose projective completion is nonsingular (see also smooth completion). It was solved in the affirmative in characteristic 0 by Heisuke Hironaka in 1964 and is yet unsolved in finite characteristic.'b'which may also be viewed as a rational map from the line to the circle.'b'Two affine varieties are birationally equivalent if there are two rational functions between them which are inverse one to the other in the regions where both are defined. Equivalently, they are birationally equivalent if their function fields are isomorphic.'b"As with regular maps, one may define a rational map from a variety V to a variety V'. As with the regular maps, the rational maps from V to V' may be identified to the field homomorphisms from k(V') to k(V)."b'If V is an affine variety, its coordinate ring is an integral domain and has thus a field of fractions which is denoted k(V) and called the field of the rational functions on V or, shortly, the function field of V. Its elements are the restrictions to V of the rational functions over the affine space containing V. The domain of a rational function f is not V but the complement of the subvariety (a hypersurface) where the denominator of f vanishes.'b'In contrast to the preceding sections, this section concerns only varieties and not algebraic sets. On the other hand, the definitions extend naturally to projective varieties (next section), as an affine variety and its projective completion have the same field of functions.'b'Given a regular map g from V to V\xe2\x80\xb2 and a regular function f of k[V\xe2\x80\xb2], then f \xe2\x88\x98 g \xe2\x88\x88 k[V]. The map f \xe2\x86\x92 f \xe2\x88\x98 g is a ring homomorphism from k[V\xe2\x80\xb2] to k[V]. Conversely, every ring homomorphism from k[V\xe2\x80\xb2] to k[V] defines a regular map from V to V\xe2\x80\xb2. This defines an equivalence of categories between the category of algebraic sets and the opposite category of the finitely generated reduced k-algebras. This equivalence is one of the starting points of scheme theory.'b'The definition of the regular maps apply also to algebraic sets. The regular maps are also called morphisms, as they make the collection of all affine algebraic sets into a category, where the objects are the affine algebraic sets and the morphisms are the regular maps. The affine varieties is a subcategory of the category of the algebraic sets.'b'If V\xe2\x80\xb2 is a variety contained in Am, we say that f is a regular map from V to V\xe2\x80\xb2 if the range of f is contained in V\xe2\x80\xb2.'b'Using regular functions from an affine variety to A1, we can define regular maps from one affine variety to another. First we will define a regular map from a variety into affine space: Let V be a variety contained in An. Choose m regular functions on V, and call them f1, ..., fm. We define a regular map f from V to Am by letting f = (f1, ..., fm). In other words, each fi determines one coordinate of the range of f.'b'Since regular functions on V come from regular functions on An, there is a relationship between the coordinate rings. Specifically, if a regular function on V is the restriction of two functions f and g in k[An], then f\xc2\xa0\xe2\x88\x92\xc2\xa0g is a polynomial function which is null on V and thus belongs to I(V). Thus k[V] may be identified with k[An]/I(V).'b'Just as with the regular functions on affine space, the regular functions on V form a ring, which we denote by k[V]. This ring is called the coordinate ring of V.'b'It may seem unnaturally restrictive to require that a regular function always extend to the ambient space, but it is very similar to the situation in a normal topological space, where the Tietze extension theorem guarantees that a continuous function on a closed subset always extends to the ambient topological space.'b'Just as continuous functions are the natural maps on topological spaces and smooth functions are the natural maps on differentiable manifolds, there is a natural class of functions on an algebraic set, called regular functions or polynomial functions. A regular function on an algebraic set V contained in An is the restriction to V of a regular function on An. For an algebraic set defined on the field of the complex numbers, the regular functions are smooth and even analytic.'b'Some authors do not make a clear distinction between algebraic sets and varieties and use irreducible variety to make the distinction when needed.'b'An algebraic set is called irreducible if it cannot be written as the union of two smaller algebraic sets. Any algebraic set is a finite union of irreducible algebraic sets and this decomposition is unique. Thus its elements are called the irreducible components of the algebraic set. An irreducible algebraic set is also called a variety. It turns out that an algebraic set is a variety if and only if it may be defined as the vanishing set of a prime ideal of the polynomial ring.'b"For various reasons we may not always want to work with the entire ideal corresponding to an algebraic set U. Hilbert's basis theorem implies that ideals in k[An] are always finitely generated."b"The answer to the first question is provided by introducing the Zariski topology, a topology on An whose closed sets are the algebraic sets, and which directly reflects the algebraic structure of k[An]. Then U = V(I(U)) if and only if U is an algebraic set or equivalently a Zariski-closed set. The answer to the second question is given by Hilbert's Nullstellensatz. In one of its forms, it says that I(V(S)) is the radical of the ideal generated by S. In more abstract language, there is a Galois connection, giving rise to two closure operators; they can be identified, and naturally play a basic role in the theory; the example is elaborated at Galois connection."b'Two natural questions to ask are:'b'Given a subset U of An, can one recover the set of polynomials which generate it? If U is any subset of An, define I(U) to be the set of all polynomials whose vanishing set contains U. The I stands for ideal: if two polynomials f and g both vanish on U, then f+g vanishes on U, and if h is any polynomial, then hf vanishes on U, so I(U) is always an ideal of the polynomial ring k[An].'b'A subset of An which is V(S), for some S, is called an algebraic set. The V stands for variety (a specific type of algebraic set to be defined below).'b'We say that a polynomial vanishes at a point if evaluating it at that point gives zero. Let S be a set of polynomials in k[An]. The vanishing set of S (or vanishing locus or zero set) is the set V(S) of all points in An where every polynomial in S vanishes. Symbolically,'b'When a coordinate system is chosen, the regular functions on the affine n-space may be identified with the ring of polynomial functions in n variables over k. Therefore, the set of the regular functions on An is a ring, which is denoted k[An].'b'A function f\xc2\xa0: An \xe2\x86\x92 A1 is said to be polynomial (or regular) if it can be written as a polynomial, that is, if there is a polynomial p in k[x1,...,xn] such that f(M) = p(t1,...,tn) for every point M with coordinates (t1,...,tn) in An. The property of a function to be polynomial (or regular) does not depend on the choice of a coordinate system in An.'b'First we start with a field k. In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that k is algebraically closed. We consider the affine space of dimension n over k, denoted An(k) (or more simply An, when k is clear from the context). When one fixes a coordinate system, one may identify An(k) with kn. The purpose of not working with kn is to emphasize that one "forgets" the vector space structure that kn carries.'b'A "slanted" circle in R3 can be defined as the set of all points (x,y,z) which satisfy the two polynomial equations'b'In classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere of radius 1 in three-dimensional Euclidean space R3 could be defined as the set of all points (x,y,z) with'b''b''b'Much of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on "intrinsic" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck\'s scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert\'s Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles\'s proof of the longstanding conjecture called Fermat\'s last theorem is an example of the power of this approach.'b'In the 20th century, algebraic geometry split into several subareas.'b'Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.'b'The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.'b'Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving geometrical problems about these sets of zeros.'
System of polynomial equations
b'While the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm[17] is based on the concept of a regular semi-algebraic system.'b'The command RegularChains[RealTriangularize] is part of the Maple library RegularChains, written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command RegularChains[Triangularize] for computing the solutions whose coordinates are in the algebraic closure of the coefficient field. The underlying algorithms are based on the notion of a regular chain.'b'The fourth solver is the Maple command RegularChains[RealTriangularize]. For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.'b'The third solver is Bertini,[15][16] written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision. In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.'b'The second solver is PHCpack,[11][14] written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.'b'To extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.'b'The rational univariate representation may be computed with Maple function Groebner[RationalUnivariateRepresentation].'b'Internally, this solver, designed by F. Rouillier computes first a Gr\xc3\xb6bner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.'b'The Maple function RootFinding[Isolate] takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.'b'There are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.'b'The roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.'b'To deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.'b'Then a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the system'b'This method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore, it is computed by, at least, four different methods and the best value, say N, is kept.'b'This is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades.[11]'b'Nevertheless, two methods deserve to be mentioned here.'b'The general numerical algorithms which are designed for any system of nonlinear equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow one to find all solutions. In particular, when a general method does not find any solution, this is usually not an indication that there is no solution.'b'Contrarily to triangular decompositions and equiprojectable decompositions, the RUR is not defined in positive dimension.'b'Moreover, the univariate polynomial h(x0) of the RUR may be factorized, and this gives a RUR for every irreducible factor. This provides the prime decomposition of the given ideal (that is the primary decomposition of the radical of the ideal). In practice, this provides an output with much smaller coefficients, especially in the case of systems with high multiplicities.'b'For zero-dimensional systems, the RUR allows retrieval of the numeric values of the solutions by solving a single univariate polynomial and substituting them in rational functions. This allows production of certified approximations of the solutions to any given precision.'b'The RUR is uniquely defined for a given separating variable, independently of any algorithm, and it preserves the multiplicities of the roots. This is a notable difference with triangular decompositions (even the equiprojectable decomposition), which, in general, do not preserve multiplicities. The RUR shares with equiprojectable decomposition the property of producing an output with coefficients of relatively small size.'b'For example, for the system in the previous section, every linear combination of the variable, except the multiples of x, y and x + y, is a separating variable. If one chooses t = x \xe2\x80\x93 y/2 as a separating variable, then the RUR is'b'Given a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.'b'where h is a univariate polynomial in x0 of degree D and g0, ..., gn are univariate polynomials in x0 of degree less than D.'b'A RUR of a zero-dimensional system consists in a linear combination x0 of the variables, called separating variable, and a system of equations[9]'b'The rational univariate representation or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by F. Rouillier.[8]'b'The second issue is generally solved by outputting regular chains of a special form, sometimes called shape lemma, for which all di but the first one are equal to 1. For getting such regular chains, one may have to add a further variable, called separating variable, which is given the index 0. The rational univariate representation, described below, allows computing such a special regular chain, satisfying Dahan\xe2\x80\x93Schost bound, by starting from either a regular chain or a Gr\xc3\xb6bner basis.'b'The first issue has been solved by Dahan and Schost:[5][6] Among the sets of regular chains that represent a given set of solutions, there is a set for which the coefficients are explicitly bounded in terms of the size of the input system, with a nearly optimal bound. This set, called equiprojectable decomposition, depends only on the choice of the coordinates. This allows the use of modular methods for computing efficiently the equiprojectable decomposition.[7]'b'This representation of the solutions are fully convenient for coefficients in a finite field. However, for rational coefficients, two aspects have to be taken care of:'b'There is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gr\xc3\xb6bner basis for the graded reverse lexicographic order (grevlex), then deducing the lexicographical Gr\xc3\xb6bner basis by FGLM algorithm[3] and finally applying the Lextriangular algorithm.[4]'b'There are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional)[2] into regular chains (or regular semi-algebraic systems).'b'Every zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.'b'The solutions of this system are obtained by solving the first univariate equation, substituting the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from fi has degree di and thus that the system has d1 ... dn solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).'b'To such a regular chain is associated a triangular system of equations'b'The usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists of a sequence of polynomials f1(x1), f2(x1, x2), ..., fn(x1, ..., xn) such that, for every i such that 1 \xe2\x89\xa4 i \xe2\x89\xa4 n'b'The other way to represent the solutions is said to be algebraic. It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field k of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation involving the solving of only one univariate polynomial for each solution is preferable: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.'b'For zero-dimensional systems, solving consists of computing all the solutions. There are two different ways of outputting the solutions. The most common, possible for real or complex solutions, consists of outputting numeric approximations of the solutions. Such a solution is called numeric. A solution is certified if it is provided with a bound on the error of the approximations which separates the different solutions.'b'A natural example of an open question about solving positive-dimensional systems is the following: decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them. The only published algorithm which allows one to solve this question is cylindrical algebraic decomposition, which is not efficient enough, in practice, to be used for this.'b'If the system is positive-dimensional, it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean "finding a description of the solutions from which the relevant properties of the solutions are easy to extract". There is no commonly accepted such description. In fact there are many different "relevant properties", which involve almost every subfield of algebraic geometry.'b'The first thing to do in solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gr\xc3\xb6bner basis of the left-hand sides of the equations. The system is inconsistent if this Gr\xc3\xb6bner basis is reduced to 1. The system is zero-dimensional if, for every variable there is a leading monomial of some element of the Gr\xc3\xb6bner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).'b"This exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with B\xc3\xa9zout's bound higher than, say, 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound)."b"A zero-dimensional system with as many equations as variables is said to be well-behaved.[1] B\xc3\xa9zout's theorem asserts that a well-behaved system whose equations have degrees d1, ..., dn has at most d1...dn solutions. This bound is sharp. If all the degrees are equal to d, this bound becomes dn and is exponential in the number of variables."b'A system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension K of\xc2\xa0k. This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be positive-dimensional.'b'A system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension K of\xc2\xa0k.'b"A system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems, when constructed with random coefficients, are inconsistent. For example, the system \xc2\xa0x3\xc2\xa0\xe2\x88\x92\xc2\xa01 =\xc2\xa00, x2\xc2\xa0\xe2\x88\x92\xc2\xa01\xc2\xa0=\xc2\xa00 is overdetermined (having two equations but only one unknown), but it is not inconsistent since it has the solution x =1."b'In the case of a finite field, the same transformation allows always to suppose that the field k has a prime order.'b'The elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.'b'When solving a system over a finite field k with q elements, one is primarily interested in the solutions in k. As the elements of k are exactly the solutions of the equation xq\xc2\xa0\xe2\x88\x92\xc2\xa0x\xc2\xa0=\xc2\xa00, it suffices, for restricting the solutions to k, to add the equation xiq\xc2\xa0\xe2\x88\x92\xc2\xa0xi\xc2\xa0=\xc2\xa00 for each variable\xc2\xa0xi.'b'is equivalent to the polynomial system'b'For example, the equation'b'A trigonometric equation is an equation g = 0 where g is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing sin(x) and cos(x) by two new variables s and c and adding the new equation s2\xc2\xa0+\xc2\xa0c2\xc2\xa0\xe2\x88\x92\xc2\xa01\xc2\xa0=\xc2\xa00.'b''b''b'A solution is a set of the values for the xi which make all of the equations true and which belong to some algebraically closed field extension K of k. When k is the field of rational numbers, K is the field of complex numbers.'b'Usually, the field k is either the field of rational numbers or a finite field, although most of the theory applies to any field.'b'A system of polynomial equations is a set of simultaneous equations f1 = 0, ..., fh = 0 where the fi are polynomials in several variables, say x1, ..., xn, over some field k.'
Representation theory
b'One special case has had a significant impact on representation theory, namely the representation theory of quivers.[11] A quiver is simply a directed graph (with loops and multiple arrows allowed), but it can be made into a category (and also an algebra) by considering paths in the graph. Representations of such categories/algebras have illuminated several aspects of representation theory, for instance by allowing non-semisimple representation theory questions about a group to be reduced in some cases to semisimple representation theory questions about a quiver.'b'More generally, one can relax the assumption that the category being represented has only one object. In full generality, this is simply the theory of functors between categories, and little can be said.'b'Since groups are categories, one can also consider representation of other categories. The simplest generalization is to monoids, which are categories with one object. Groups are monoids for which every morphism is invertible. General monoids have representations in any category. In the category of sets, these are monoid actions, but monoid representations on vector spaces and other objects can be studied.'b'Two types of representations closely related to linear representations are:'b'For another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from G to the homeomorphism group of a topological space X.'b'In the case where C is VectF, the category of vector spaces over a field F, this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of G in the category of sets.'b'Every group G can be viewed as a category with a single object; morphisms in this category are just the elements of G. Given an arbitrary category C, a representation of G in C is a functor from G to C. Such a functor selects an object X in C and a group homomorphism from G to Aut(X), the automorphism group of X.'b'This condition and the axioms for a group imply that \xcf\x81(g) is a bijection (or permutation) for all g in G. Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group SX of X.'b'A set-theoretic representation (also known as a group action or permutation representation) of a group G on a set X is given by a function \xcf\x81 from G to XX, the set of functions from X to X, such that for all g1, g2 in G and all x in X:'b'The Hopf algebras associated to groups have a commutative algebra structure, and so general Hopf algebras are known as quantum groups, although this term is often restricted to certain Hopf algebras arising as deformations of groups or their universal enveloping algebras. The representation theory of quantum groups has added surprising insights to the representation theory of Lie groups and Lie algebras, for instance through the crystal basis of Kashiwara.'b'Hopf algebras provide a way to improve the representation theory of associative algebras, while retaining the representation theory of groups and Lie algebras as special cases. In particular, the tensor product of two representations is a representation, as is the dual vector space.'b'When considering representations of an associative algebra, one can forget the underlying field, and simply regard the associative algebra as a ring, and its representations as modules. This approach is surprisingly fruitful: many results in representation theory can be interpreted as special cases of results about modules over a ring.'b'In one sense, associative algebra representations generalize both representations of groups and Lie algebras. A representation of a group induces a representation of a corresponding group ring or group algebra, while representations of a Lie algebra correspond bijectively to representations of its universal enveloping algebra. However, the representation theory of general associative algebras does not have all of the nice properties of the representation theory of groups and Lie algebras.'b'Before the development of the general theory, many important special cases were worked out in detail, including the Hilbert modular forms and Siegel modular forms. Important results in the theory include the Selberg trace formula and the realization by Robert Langlands that the Riemann-Roch theorem could be applied to calculate the dimension of the space of automorphic forms. The subsequent notion of "automorphic representation" has proved of great technical value for dealing with the case that G is an algebraic group, treated as an adelic algebraic group. As a result, an entire philosophy, the Langlands program has developed around the relation between representation and number theoretic properties of automorphic forms.[40]'b'Automorphic forms are a generalization of modular forms to more general analytic functions, perhaps of several complex variables, with similar transformation properties.[39] The generalization involves replacing the modular group PSL2 (R) and a chosen congruence subgroup by a semisimple Lie group G and a discrete subgroup \xce\x93. Just as modular forms can be viewed as differential forms on a quotient of the upper half space H = PSL2 (R)/SO(2), automorphic forms can be viewed as differential forms (or similar objects) on \xce\x93\\G/K, where K is (typically) a maximal compact subgroup of G. Some care is required, however, as the quotient typically has singularities. The quotient of a semisimple Lie group by a compact subgroup is a symmetric space and so the theory of automorphic forms is intimately related to harmonic analysis on symmetric spaces.'b"The representation theory of semisimple Lie groups has its roots in invariant theory[30] and the strong links between representation theory and algebraic geometry have many parallels in differential geometry, beginning with Felix Klein's Erlangen program and \xc3\x89lie Cartan's connections, which place groups and symmetry at the heart of geometry.[38] Modern developments link representation theory and invariant theory to areas as diverse as holonomy, differential operators and the theory of several complex variables."b'Invariant theory of infinite groups is inextricably linked with the development of linear algebra, especially, the theories of quadratic forms and determinants. Another subject with strong mutual influence is projective geometry, where invariant theory can be used to organize the subject, and during the 1960s, new life was breathed into the subject by David Mumford in the form of his geometric invariant theory.[37]'b'Invariant theory studies actions on algebraic varieties from the point of view of their effect on functions, which form representations of the group. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. The modern approach analyses the decomposition of these representations into irreducibles.[36]'b'Linear algebraic groups (or more generally, affine group schemes) are analogues in algebraic geometry of Lie groups, but over more general fields than just R or C. In particular, over finite fields, they give rise to finite groups of Lie type. Although linear algebraic groups have a classification that is very similar to that of Lie groups, their representation theory is rather different (and much less well understood) and requires different techniques, since the Zariski topology is relatively weak, and techniques from analysis are no longer available.[35]'b'Lie superalgebras are generalizations of Lie algebras in which the underlying vector space has a Z2-grading, and skew-symmetry and Jacobi identity properties of the Lie bracket are modified by signs. Their representation theory is similar to the representation theory of Lie algebras.[34]'b'Affine Lie algebras are a special case of Kac\xe2\x80\x93Moody algebras, which have particular importance in mathematics and theoretical physics, especially conformal field theory and the theory of exactly solvable models. Kac discovered an elegant proof of certain combinatorial identities, Macdonald identities, which is based on the representation theory of affine Kac\xe2\x80\x93Moody algebras.'b'There are many classes of infinite-dimensional Lie algebras whose representations have been studied. Among these, an important class are the Kac\xe2\x80\x93Moody algebras.[33] They are named after Victor Kac and Robert Moody, who independently discovered them. These algebras form a generalization of finite-dimensional semisimple Lie algebras, and share many of their combinatorial properties. This means that they have a class of representations that can be understood in the same way as representations of semisimple Lie algebras.'b'Lie algebras, like Lie groups, have a Levi decomposition into semisimple and solvable parts, with the representation theory of solvable Lie algebras being intractable in general. In contrast, the finite-dimensional representations of semisimple Lie algebras are completely understood, after work of \xc3\x89lie Cartan. A representation of a semisimple Lie algebra g is analysed by choosing a Cartan subalgebra, which is essentially a generic maximal subalgebra h of g on which the Lie bracket is zero ("abelian"). The representation of g can be decomposed into weight spaces that are eigenspaces for the action of h and the infinitesimal analogue of characters. The structure of semisimple Lie algebras then reduces the analysis of representations to easily understood combinatorics of the possible weights that can occur.[31]'b'A Lie algebra over a field F is a vector space over F equipped with a skew-symmetric bilinear operation called the Lie bracket, which satisfies the Jacobi identity. Lie algebras arise in particular as tangent spaces to Lie groups at the identity element, leading to their interpretation as "infinitesimal symmetries".[31] An important approach to the representation theory of Lie groups is to study the corresponding representation theory of Lie algebras, but representations of Lie algebras also have an intrinsic interest.[32]'b"A general Lie group is a semidirect product of a solvable Lie group and a semisimple Lie group (the Levi decomposition).[31] The classification of representations of solvable Lie groups is intractable in general, but often easy in practical cases. Representations of semidirect products can then be analysed by means of general results called Mackey theory, which is a generalization of the methods used in Wigner's classification of representations of the Poincar\xc3\xa9 group."b"The representation theory of Lie groups can be developed first by considering the compact groups, to which results of compact representation theory apply.[26] This theory can be extended to finite-dimensional representations of semisimple Lie groups using Weyl's unitary trick: each semisimple real Lie group G has a complexification, which is a complex Lie group Gc, and this complex Lie group has a maximal compact subgroup K. The finite-dimensional representations of G closely correspond to those of K."b'A Lie group is a group that is also a smooth manifold. Many classical groups of matrices over the real or complex numbers are Lie groups.[30] Many of the groups important in physics and chemistry are Lie groups, and their representation theory is crucial to the application of group theory in those fields.[5]'b'Harmonic analysis has also been extended from the analysis of functions on a group G to functions on homogeneous spaces for G. The theory is particularly well developed for symmetric spaces and provides a theory of automorphic forms (discussed below).'b'If the group is neither abelian nor compact, no general theory is known with an analogue of the Plancherel theorem or Fourier inversion, although Alexander Grothendieck extended Tannaka\xe2\x80\x93Krein duality to a relationship between linear algebraic groups and tannakian categories.'b'Another approach involves considering all unitary representations, not just the irreducible ones. These form a category, and Tannaka\xe2\x80\x93Krein duality provides a way to recover a compact group from its category of unitary representations.'b'A major goal is to provide a general form of the Fourier transform and the Plancherel theorem. This is done by constructing a measure on the unitary dual and an isomorphism between the regular representation of G on the space L2(G) of square integrable functions on G and its representation on the space of L2 functions on the unitary dual. Pontrjagin duality and the Peter\xe2\x80\x93Weyl theorem achieve this for abelian and compact G respectively.[27][29]'b'The duality between the circle group S1 and the integers Z, or more generally, between a torus Tn and Zn is well known in analysis as the theory of Fourier series, and the Fourier transform similarly expresses the fact that the space of characters on a real vector space is the dual vector space. Thus unitary representation theory and harmonic analysis are intimately related, and abstract harmonic analysis exploits this relationship, by developing the analysis of functions on locally compact topological groups and related spaces.[7]'b'For non-compact G, the question of which representations are unitary is a subtle one. Although irreducible unitary representations must be "admissible" (as Harish-Chandra modules) and it is easy to detect which admissible representations have a nondegenerate invariant sesquilinear form, it is hard to determine when this form is positive definite. An effective description of the unitary dual, even for relatively well-behaved groups such as real reductive Lie groups (discussed below), remains an important open problem in representation theory. It has been solved for many particular groups, such as SL(2,R) and the Lorentz group.[28]'b'A major goal is to describe the "unitary dual", the space of irreducible unitary representations of G.[26] The theory is most well-developed in the case that G is a locally compact (Hausdorff) topological group and the representations are strongly continuous.[7] For G abelian, the unitary dual is just the space of characters, while for G compact, the Peter\xe2\x80\x93Weyl theorem shows that the irreducible unitary representations are finite-dimensional and the unitary dual is discrete.[27] For example, if G is the circle group S1, then the characters are given by integers, and the unitary dual is Z.'b'A unitary representation of a group G is a linear representation \xcf\x86 of G on a real or (usually) complex Hilbert space V such that \xcf\x86(g) is a unitary operator for every g \xe2\x88\x88 G. Such representations have been widely applied in quantum mechanics since the 1920s, thanks in particular to the influence of Hermann Weyl,[23] and this has inspired the development of the theory, most notably through the analysis of representations of the Poincar\xc3\xa9 group by Eugene Wigner.[24] One of the pioneers in constructing a general theory of unitary representations (for any group G rather than just for particular groups useful in applications) was George Mackey, and an extensive theory was developed by Harish-Chandra and others in the 1950s and 1960s.[25]'b'As well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory, combinatorics and number theory.'b'Modular representations of a finite group G are representations over a field whose characteristic is not coprime to |G|, so that Maschke\'s theorem no longer holds (because |G| is not invertible in F and so one cannot divide by it).[21] Nevertheless, Richard Brauer extended much of character theory to modular representations, and this theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were "too small".[22]'b'Representations of a finite group G are also linked directly to algebra representations via the group algebra F[G], which is a vector space over F with the elements of G as a basis, equipped with the multiplication operation defined by the group operation, linearity, and the requirement that the group operation and scalar multiplication commute.'b'Over arbitrary fields, another class of finite groups that have a good representation theory are the finite groups of Lie type. Important examples are linear algebraic groups over finite fields. The representation theory of linear algebraic groups and Lie groups extends these examples to infinite-dimensional groups, the latter being intimately related to Lie algebra representations. The importance of character theory for finite groups has an analogue in the theory of weights for representations of Lie groups and Lie algebras.'b"Results such as Maschke's theorem and the unitary property that rely on averaging can be generalized to more general groups by replacing the average with an integral, provided that a suitable notion of integral can be defined. This can be done for compact topological groups (including compact Lie groups), using Haar measure, and the resulting theory is known as abstract harmonic analysis."b"Unitary representations are automatically semisimple, since Maschke's result can be proven by taking the orthogonal complement of a subrepresentation. When studying representations of groups that are not finite, the unitary representations provide a good generalization of the real and complex representations of a finite group."b'for all g in G and v, w in W. Hence any G-representation is unitary.'b"Maschke's theorem holds more generally for fields of positive characteristic p, such as the finite fields, as long as the prime p is coprime to the order of G. When p and |G| have a common factor, there are G-representations that are not semisimple, which are studied in a subbranch called modular representation theory."b'The finite-dimensional G-representations can be understood using character theory: the character of a representation \xcf\x86: G \xe2\x86\x92 GL(V) is the class function \xcf\x87\xcf\x86: G \xe2\x86\x92 F defined by'b'\xcf\x80G is equivariant, and its kernel is the required complement.'b"Over a field of characteristic zero, the representation of a finite group G has a number of convenient properties. First, the representations of G are semisimple (completely reducible). This is a consequence of Maschke's theorem, which states that any subrepresentation V of a G-representation W has a G-invariant complement. One proof is to choose any projection \xcf\x80 from W to V and replace it by its average \xcf\x80G defined by"b'Group representations are a very important tool in the study of finite groups.[19] They also arise in the applications of finite group theory to geometry and crystallography.[20] Representations of finite groups exhibit many of the features of the general theory and point the way to other branches and topics in representation theory.'b'Representation theory is notable for the number of branches it has, and the diversity of the approaches to studying representations of groups and algebras. Although, all the theories have in common the basic concepts discussed already, they differ considerably in detail. The differences are at least 3-fold:'b'In general, the tensor product of irreducible representations is not irreducible; the process of decomposing a tensor product as a direct sum of irreducible representations is known as Clebsch\xe2\x80\x93Gordan theory.'b'In cases where complete reducibility does not hold, one must understand how indecomposable representations can be built from irreducible representations as extensions of a quotient by a subrepresentation.'b'In favorable circumstances, every finite-dimensional representation is a direct sum of irreducible representations: such representations are said to be semisimple. In this case, it suffices to understand only the irreducible representations. Examples where this "complete reducibility" phenomenon occur include finite and compact groups, and semisimple Lie algebras.'b'The direct sum of two representations carries no more information about the group G than the two representations do individually. If a representation is the direct sum of two proper nontrivial subrepresentations, it is said to be decomposable. Otherwise, it is said to be indecomposable.'b'If (V,\xcf\x86) and (W,\xcf\x88) are representations of (say) a group G, then the direct sum of V and W is a representation, in a canonical way, via the equation'b'Irreducible representations are the building blocks of representation theory: if a representation V is not irreducible then it is built from a subrepresentation and a quotient that are both "simpler" in some sense; for instance, if V is finite-dimensional, then both the subrepresentation and the quotient have smaller dimension.'b"The definition of an irreducible representation implies Schur's lemma: an equivariant map \xce\xb1: V \xe2\x86\x92 W between irreducible representations is either the zero map or an isomorphism, since its kernel and image are subrepresentations. In particular, when V = W, this shows that the equivariant endomorphisms of V form an associative division algebra over the underlying field F. If F is algebraically closed, the only equivariant endomorphisms of an irreducible representation are the scalar multiples of the identity."b'If V has exactly two subrepresentations, namely the trivial subspace {0} and V itself, then the representation is said to be irreducible; if V has a proper nontrivial subrepresentation, the representation is said to be reducible.[15]'b'If (V,\xcf\x88) is a representation of (say) a group G, and W is a linear subspace of V that is preserved by the action of G in the sense that g \xc2\xb7 w \xe2\x88\x88 W for all w \xe2\x88\x88 W (Serre [14] calls these W stable under G), then W is called a subrepresentation: by defining \xcf\x86(g) to be the restriction of \xcf\x88(g) to W, (W, \xcf\x86) is a representation of G and the inclusion of W into V is an equivariant map. The quotient space V/W can also be made into a representation of G.'b'Isomorphic representations are, for practical purposes, "the same"; they provide the same information about the group or algebra being represented. Representation theory therefore seeks to classify representations up to isomorphism.'b'Equivariant maps for representations of an associative or Lie algebra are defined similarly. If \xce\xb1 is invertible, then it is said to be an isomorphism, in which case V and W (or, more precisely, \xcf\x86 and \xcf\x88) are isomorphic representations, also phrased as equivalent representations. An equivariant map is often called an intertwining map of representations. Also, in the case of a group G, it is on occasion called a G-map.'b'for all g in G, i.e. the following diagram commutes:'b'for all g in G and v in V. In terms of \xcf\x86: G \xe2\x86\x92 GL(V) and \xcf\x88: G \xe2\x86\x92 GL(W), this means'b'If V and W are vector spaces over F, equipped with representations \xcf\x86 and \xcf\x88 of a group G, then an equivariant map from V to W is a linear map \xce\xb1: V \xe2\x86\x92 W such that'b'An effective or faithful representation is a representation (V,\xcf\x86) for which the homomorphism \xcf\x86 is injective.'b'When V is of finite dimension n, one can choose a basis for V to identify V with Fn and hence recover a matrix representation with entries in the field F.'b'The vector space V is called the representation space of \xcf\x86 and its dimension (if finite) is called the dimension of the representation (sometimes degree, as in [14]). It is also common practice to refer to V itself as the representation when the homomorphism \xcf\x86 is clear from the context; otherwise the notation (V,\xcf\x86) can be used to denote a representation.'b'and similarly in the other cases. This approach is both more concise and more abstract. From this point of view:'b'The second way to define a representation focuses on the map \xcf\x86 sending g in G to a linear map \xcf\x86(g): V \xe2\x86\x92 V, which satisfies'b'where [x1, x2] is the Lie bracket, which generalizes the matrix commutator MN \xe2\x88\x92 NM.'b"where e is the identity element of G and g1g2 is the product in G. The requirement for associative algebras is analogous, except that associative algebras do not always have an identity element, in which case equation (1) is ignored. Equation (2) is an abstract expression of the associativity of matrix multiplication. This doesn't hold for the matrix commutator and also there is no identity element for the commutator. Hence for Lie algebras, the only requirement is that for any x1, x2 in A and v in V:"b'with two properties. First, for any g in G (or a in A), the map'b'There are two ways to say what a representation is.[13] The first uses the idea of an action, generalizing the way that matrices act on column vectors by matrix multiplication. A representation of a group G or (associative or Lie) algebra A on a vector space V is a map'b'This generalizes to any field F and any vector space V over F, with linear maps replacing matrices and composition replacing matrix multiplication: there is a group GL(V,F) of automorphisms of V, an associative algebra EndF(V) of all endomorphisms of V, and a corresponding Lie algebra gl(V,F).'b'There are three main sorts of algebraic objects for which this can be done: groups, associative algebras and Lie algebras.[12]'b'Let V be a vector space over a field F.[3] For instance, suppose V is Rn or Cn, the standard n-dimensional space of column vectors over the real or complex numbers respectively. In this case, the idea of representation theory is to do abstract algebra concretely by using n \xc3\x97 n matrices of real or complex numbers.'b''b''b'The success of representation theory has led to numerous generalizations. One of the most general is in category theory.[11] The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces. This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.'b'Secondly, there are diverse approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.[10]'b'Representation theory is pervasive across fields of mathematics, for two reasons. First, the applications of representation theory are diverse:[6] in addition to its impact on algebra, representation theory:'b'Representation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood.[3] Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups.[4] Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.[5]'b'Representation theory is a branch of mathematics that studies abstract algebraic structures by representing their elements as linear transformations of vector spaces, and studies modules over these abstract algebraic structures.[1] In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations in terms of matrix addition and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.[2]'
Functional analysis
b'Functional analysis in its present form[update] includes the following tendencies:'b"Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn\xe2\x80\x93Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice."b'List of functional analysis topics.'b'The closed graph theorem states the following: If X is a topological space and Y is a compact Hausdorff space, then the graph of a linear map T from X to Y is closed if and only if T is continuous.[3]'b'The proof uses the Baire category theorem, and completeness of both X and Y is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if X and Y are taken to be Fr\xc3\xa9chet spaces.'b'The open mapping theorem, also known as the Banach\xe2\x80\x93Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:[2]'b'then there exists a linear extension \xcf\x88\xc2\xa0: V \xe2\x86\x92 R of \xcf\x86 to the whole space V, i.e., there exists a linear functional \xcf\x88 such that'b'Hahn\xe2\x80\x93Banach theorem:[2] If p\xc2\xa0: V \xe2\x86\x92 R is a sublinear function, and \xcf\x86\xc2\xa0: U \xe2\x86\x92 R is a linear functional on a linear subspace U \xe2\x8a\x86 V which is dominated by p on U, i.e.'b'The Hahn\xe2\x80\x93Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are "enough" continuous linear functionals defined on every normed vector space to make the study of the dual space "interesting".'b'This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.'b'where T is the multiplication operator:'b'Theorem:[1] Let A be a bounded self-adjoint operator on a Hilbert space H. Then there is a measure space (X, \xce\xa3, \xce\xbc) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H \xe2\x86\x92 L2\xce\xbc(X) such that'b'There are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let A be the operator of multiplication by t on L2[0, 1], that is'b'The theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.'b'The uniform boundedness principle or Banach\xe2\x80\x93Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn\xe2\x80\x93Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.'b'Important results of functional analysis include:'b'Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fr\xc3\xa9chet derivative article.'b'In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.'b'General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.'b'An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.'b'More generally, functional analysis includes the study of Fr\xc3\xa9chet spaces and other topological vector spaces not endowed with a norm.'b'The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.'b''b''b'In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.'b"The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra.[citation needed] The theory of nonlinear functionals was continued by students of Hadamard, in particular Fr\xc3\xa9chet and L\xc3\xa9vy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach."b'Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.'
Mathematical analysis
b'Techniques from analysis are used in many areas of mathematics, including:'b'When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal. A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[24]'b'Functional analysis is also a major factor in quantum mechanics.'b"The vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schr\xc3\xb6dinger equation, and the Einstein field equations."b'Techniques from analysis are also found in other areas such as:'b'Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st\xc2\xa0century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.'b'Modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.'b'Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).[22]'b"Differential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly."b'A differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders.[18][19][20] Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.'b'Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense.[16][17] The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.'b"Complex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics."b'Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers.[15] It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.'b'Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable.[13][14] In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.'b'One of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n \xe2\x86\x92 \xe2\x88\x9e, denoted'b'A sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.'b'Much of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).'b'In mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.'b'Also, "monsters" (nowhere continuous functions, continuous but nowhere differentiable functions, space-filling curves) began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue solved the problem of measure, and Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.'b'In the middle of the 19th century Riemann introduced his theory of integration. The last third of the century saw the arithmetization of analysis by Weierstrass, who thought that geometric reasoning was inherently misleading, and introduced the "epsilon-delta" definition of limit. Then, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the "gaps" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the "size" of the set of discontinuities of real functions.'b"In the 18th century, Euler introduced the notion of mathematical function.[11] Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816,[12] but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler. Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals. Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y. He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis. The contributions of these mathematicians and others, such as Weierstrass, developed the (\xce\xb5, \xce\xb4)-definition of limit approach, thus founding the modern field of mathematical analysis."b'The modern foundations of mathematical analysis were established in 17th century Europe.[3] Descartes and Fermat independently developed analytic geometry, and a few decades later Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18th\xc2\xa0century, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.'b'In the 14th century, Madhava of Sangamagrama developed infinite series expansions, like the power series and the Taylor series, of functions such as sine, cosine, tangent and arctangent.[10] Alongside his development of the Taylor series of the trigonometric functions, he also estimated the magnitude of the error terms created by truncating these series and gave a rational approximation of an infinite series. His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.'b"Mathematical analysis formally developed in the 17th century during the Scientific Revolution,[3] but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy.[4] Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids.[5] The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century.[6] In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century AD to find the area of a circle.[7] Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century.[8] The Indian mathematician Bh\xc4\x81skara II gave examples of the derivative and used what is now known as Rolle's theorem in the 12th century.[9]"b''b''b'These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis. Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects (a metric space).'b'Mathematical analysis is the branch of mathematics dealing with limits and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2]'
Lp space
b'One may also define spaces Lp(M) on a manifold, called the intrinsic Lp spaces of the manifold, using densities.'b"As Lp-spaces, the weighted spaces have nothing special, since Lp(S, w\xe2\x80\x89d\xce\xbc) is equal to Lp(S, d\xce\xbd). But they are the natural framework for several results in harmonic analysis (Grafakos 2004); they appear for example in the Muckenhoupt theorem: for 1 < p < \xe2\x88\x9e, the classical Hilbert transform is defined on Lp(T, \xce\xbb) where T denotes the unit circle and \xce\xbb the Lebesgue measure; the (nonlinear) Hardy\xe2\x80\x93Littlewood maximal operator is bounded on Lp(Rn, \xce\xbb). Muckenhoupt's theorem describes weights w such that the Hilbert transform remains bounded on Lp(T, w\xe2\x80\x89d\xce\xbb) and the maximal operator on Lp(Rn, w\xe2\x80\x89d\xce\xbb)."b'or, in terms of the Radon\xe2\x80\x93Nikodym derivative, w = d\xce\xbd/d\xce\xbc\xe2\x80\x89 the norm for Lp(S, w\xe2\x80\x89d\xce\xbc) is explicitly'b'As before, consider a measure space (S, \xce\xa3, \xce\xbc). Let w\xc2\xa0: S \xe2\x86\x92 [0, \xe2\x88\x9e) be a measurable function. The w-weighted Lp space is defined as Lp(S, w\xe2\x80\x89d\xce\xbc), where w\xe2\x80\x89d\xce\xbc means the measure \xce\xbd defined by'b'A major result that uses the Lp,w-spaces is the Marcinkiewicz interpolation theorem, which has broad applications to harmonic analysis and the study of singular integrals.'b'is comparable to the Lp,w-norm. Further in the case p\xc2\xa0> 1, this expression defines a norm if r\xc2\xa0= 1. Hence for p\xc2\xa0> 1 the weak Lp spaces are Banach spaces (Grafakos 2004).'b'For any 0\xc2\xa0< r\xc2\xa0< p the expression'b'and in particular Lp(S, \xce\xbc)\xc2\xa0\xe2\x8a\x82 Lp,w(S, \xce\xbc). Under the convention that two functions are equal if they are equal \xce\xbc almost everywhere, then the spaces Lp,w are complete (Grafakos 2004).'b'The Lp,w-norm is not a true norm, since the triangle inequality fails to hold. Nevertheless, for f in Lp(S, \xce\xbc),'b'The weak Lp coincide with the Lorentz spaces Lp,\xe2\x88\x9e, so this notation is also used to denote them.'b'The best constant C for this inequality is the Lp,w-norm of f, and is denoted by'b'A function f is said to be in the space weak Lp(S, \xce\xbc), or Lp,w(S, \xce\xbc), if there is a constant C\xc2\xa0> 0 such that, for all t\xc2\xa0> 0,'b"If f is in Lp(S, \xce\xbc) for some p with 1\xc2\xa0\xe2\x89\xa4 p\xc2\xa0< \xe2\x88\x9e, then by Markov's inequality,"b'Let (S, \xce\xa3, \xce\xbc) be a measure space, and f a measurable function with real or complex values on S. The distribution function of f is defined for t\xc2\xa0> 0 by'b'The resulting space L0(Rn, \xce\xbb) coincides as topological vector space with L0(Rn, g(x)\xe2\x80\x89d\xce\xbb(x)), for any positive \xce\xbb\xe2\x80\x93integrable density g.'b'For the infinite Lebesgue measure \xce\xbb on Rn, the definition of the fundamental system of neighborhoods could be modified as follows'b'where \xcf\x86 is bounded continuous concave and non-decreasing on [0, \xe2\x88\x9e), with \xcf\x86(0) = 0 and \xcf\x86(t) > 0 when t > 0 (for example, \xcf\x86(t) = min(t, 1)). Such a metric is called L\xc3\xa9vy-metric for L0. Under this metric the space L0 is complete (it is again an F-space). The space L0 is in general not locally bounded, and not locally convex.'b'The topology can be defined by any metric d of the form'b'The description is easier when \xce\xbc is finite. If \xce\xbc is a finite measure on (S, \xce\xa3), the 0 function admits for the convergence in measure the following fundamental system of neighborhoods'b'The vector space of (equivalence classes of) measurable functions on (S, \xce\xa3, \xce\xbc) is denoted L0(S, \xce\xa3, \xce\xbc) (Kalton, Peck & Roberts 1984). By definition, it contains all the Lp, and is equipped with the topology of convergence in measure. When \xce\xbc is a probability measure (i.e., \xce\xbc(S) = 1), this mode of convergence is named convergence in probability.'b'The situation of having no linear functionals is highly undesirable for the purposes of doing analysis. In the case of the Lebesgue measure on Rn, rather than work with Lp for 0 < p < 1, it is common to work with the Hardy space H\xe2\x80\x89p whenever possible, as this has quite a few linear functionals: enough to distinguish points from one another. However, the Hahn\xe2\x80\x93Banach theorem still fails in H\xe2\x80\x89p for p < 1 (Duren 1970, \xc2\xa77.5).'b'The only nonempty convex open set in Lp([0, 1]) is the entire space (Rudin 1991, \xc2\xa71.47). As a particular consequence, there are no nonzero linear functionals on Lp([0, 1]): the dual space is the zero space. In the case of the counting measure on the natural numbers (producing the sequence space Lp(\xce\xbc) = \xe2\x84\x93\xe2\x80\x89p), the bounded linear functionals on \xe2\x84\x93\xe2\x80\x89p are exactly those that are bounded on \xe2\x84\x93\xe2\x80\x891, namely those given by sequences in \xe2\x84\x93\xe2\x80\x89\xe2\x88\x9e. Although \xe2\x84\x93\xe2\x80\x89p does contain non-trivial convex open sets, it fails to have enough of them to give a base for the topology.'b'The space Lp for 0 < p < 1 is an F-space: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p \xe2\x89\xa5 1. It is the prototypical example of an F-space that, for most reasonable measure spaces, is not locally convex: in \xe2\x84\x93\xe2\x80\x89p or Lp([0, 1]), every open convex set containing the 0 function is unbounded for the p-quasi-norm; therefore, the 0 vector does not possess a fundamental system of convex neighborhoods. Specifically, this is true if the measure space S contains an infinite family of disjoint measurable sets of finite positive measure.'b"This result may be used to prove Clarkson's inequalities, which are in turn used to establish the uniform convexity of the spaces Lp for 1 < p < \xe2\x88\x9e (Adams & Fournier 2003)."b'In this setting Lp satisfies a reverse Minkowski inequality, that is for u, v in Lp'b'is a metric on Lp(\xce\xbc). The resulting metric space is complete; the verification is similar to the familiar case when p \xe2\x89\xa5 1.'b'and so the function'b'As before, we may introduce the p-norm ||\xe2\x80\x89f\xe2\x80\x89||p = Np(\xe2\x80\x89f\xe2\x80\x89)1/p, but || \xc2\xb7 ||\xe2\x80\x89p does not satisfy the triangle inequality in this case, and defines only a quasi-norm. The inequality (a + b)\xe2\x80\x89p \xe2\x89\xa4 a\xe2\x80\x89p + b\xe2\x80\x89p, valid for a, b \xe2\x89\xa5 0 implies that (Rudin 1991, \xc2\xa71.47)'b'Let (S, \xce\xa3, \xce\xbc) be a measure space. If 0 < p < 1, then Lp(\xce\xbc) can be defined as above: it is the vector space of those measurable functions \xe2\x80\x89f\xe2\x80\x89 such that'b'where'b'Several properties of general functions in Lp(Rd) are first proved for continuous and compactly supported functions (sometimes for step functions), then extended by density to all functions. For example, it is proved this way that translations are continuous on Lp(Rd), in the following sense:'b'This applies in particular when S = Rd and when \xce\xbc is the Lebesgue measure. The space of continuous and compactly supported functions is dense in Lp(Rd). Similarly, the space of integrable step functions is dense in Lp(Rd); this space is the linear span of indicator functions of bounded intervals when d = 1, of bounded rectangles when d = 2 and more generally of products of bounded intervals.'b'If S can be covered by an increasing sequence (Vn) of open sets that have finite measure, then the space of p\xe2\x80\x93integrable continuous functions is dense in Lp(S, \xce\xa3, \xce\xbc). More precisely, one can use bounded continuous functions that vanish outside one of the open sets Vn.'b'It follows that there exists \xcf\x86 continuous on S such that'b'Suppose V \xe2\x8a\x82 S is an open set with \xce\xbc(V) < \xe2\x88\x9e. It can be proved that for every Borel set A \xe2\x88\x88 \xce\xa3 contained in V, and for every \xce\xb5 > 0, there exist a closed set F and an open set U such that'b'More can be said when S is a metrizable topological space and \xce\xa3 its Borel \xcf\x83\xe2\x80\x93algebra, i.e., the smallest \xcf\x83\xe2\x80\x93algebra of subsets of S containing the open sets.'b'Let (S, \xce\xa3, \xce\xbc) be a measure space. An integrable simple function \xe2\x80\x89f\xe2\x80\x89 on S is one of the form'b'Throughout this section we assume that: 1 \xe2\x89\xa4 p < \xe2\x88\x9e.'b'the case of equality being achieved exactly when \xe2\x80\x89f\xe2\x80\x89 = 1 \xce\xbc-a.e.'b'The constant appearing in the above inequality is optimal, in the sense that the operator norm of the identity I\xc2\xa0: Lq(S, \xce\xbc) \xe2\x86\x92 Lp(S, \xce\xbc) is precisely'b'leading to'b"Neither condition holds for the real line with the Lebesgue measure. In both cases the embedding is continuous, in that the identity operator is a bounded linear map from Lq to Lp in the first case, and Lp to Lq in the second. (This is a consequence of the closed graph theorem and properties of Lp spaces.) Indeed, if the domain S has finite measure, one can make the following explicit calculation using H\xc3\xb6lder's inequality"b'Colloquially, if 1 \xe2\x89\xa4 p < q \xe2\x89\xa4 \xe2\x88\x9e, then Lp(S, \xce\xbc) contains functions that are more locally singular, while elements of Lq(S, \xce\xbc) can be more spread out. Consider the Lebesgue measure on the half line (0, \xe2\x88\x9e). A continuous function in L1 might blow up near 0 but must decay sufficiently fast toward infinity. On the other hand, continuous functions in L\xe2\x88\x9e need not decay at all but no blow-up is allowed. The precise technical result is the following.[6] Suppose that 0 < p < q \xe2\x89\xa4 \xe2\x88\x9e. Then:'b'The dual of L\xe2\x88\x9e is subtler. Elements of L\xe2\x88\x9e(\xce\xbc)\xe2\x88\x97 can be identified with bounded signed finitely additive measures on S that are absolutely continuous with respect to \xce\xbc. See ba space for more details. If we assume the axiom of choice, this space is much bigger than L1(\xce\xbc) except in some trivial cases. However, Saharon Shelah proved that there are relatively consistent extensions of Zermelo\xe2\x80\x93Fraenkel set theory (ZF + DC + "Every subset of the real numbers has the Baire property") in which the dual of \xe2\x84\x93\xe2\x88\x9e is \xe2\x84\x931.[5]'b'If the measure \xce\xbc on S is sigma-finite, then the dual of L1(\xce\xbc) is isometrically isomorphic to L\xe2\x88\x9e(\xce\xbc) (more precisely, the map \xce\xba1 corresponding to p = 1 is an isometry from L\xe2\x88\x9e(\xce\xbc) onto L1(\xce\xbc)\xe2\x88\x97).'b'This map coincides with the canonical embedding J of Lp(\xce\xbc) into its bidual. Moreover, the map jp is onto, as composition of two onto isometries, and this proves reflexivity.'b'For 1 < p < \xe2\x88\x9e, the space Lp(\xce\xbc) is reflexive. Let \xce\xbap be as above and let \xce\xbaq\xc2\xa0: Lp(\xce\xbc) \xe2\x86\x92 Lq(\xce\xbc)\xe2\x88\x97 be the corresponding linear isometry. Consider the map from Lp(\xce\xbc) to Lp(\xce\xbc)\xe2\x88\x97\xe2\x88\x97, obtained by composing \xce\xbaq with the transpose (or adjoint) of the inverse of \xce\xbap:'b"The fact that \xce\xbap(g) is well defined and continuous follows from H\xc3\xb6lder's inequality. \xce\xbap\xc2\xa0: Lq(\xce\xbc) \xe2\x86\x92 Lp(\xce\xbc)\xe2\x88\x97 is a linear mapping which is an isometry by the extremal case of H\xc3\xb6lder's inequality. It is also possible to show (for example with the Radon\xe2\x80\x93Nikodym theorem, see[4]) that any G \xe2\x88\x88 Lp(\xce\xbc)\xe2\x88\x97 can be expressed this way: i.e., that \xce\xbap is onto. Since \xce\xbap is onto and isometric, it is an isomorphism of Banach spaces. With this (isometric) isomorphism in mind, it is usual to say simply that Lq is the dual Banach space of Lp."b'For 1 \xe2\x89\xa4 p \xe2\x89\xa4 \xe2\x88\x9e the \xe2\x84\x93p spaces are a special case of Lp spaces, when S = N, and \xce\xbc is the counting measure on N. More generally, if one considers any set S with the counting measure, the resulting Lp space is denoted \xe2\x84\x93p(S). For example, the space \xe2\x84\x93p(Z) is the space of all sequences indexed by the integers, and when defining the p-norm on such a space, one sums over all the integers. The space \xe2\x84\x93p(n), where n is the set with n elements, is Rn with its p-norm as defined above. As any Hilbert space, every space L2 is linearly isometric to a suitable \xe2\x84\x932(I), where the cardinality of the set I is the cardinality of an arbitrary Hilbertian basis for this particular L2.'b'If we use complex-valued functions, the space L\xe2\x88\x9e is a commutative C*-algebra with pointwise multiplication and conjugation. For many measure spaces, including all sigma-finite ones, it is in fact a commutative von Neumann algebra. An element of L\xe2\x88\x9e defines a bounded operator on any Lp space by multiplication.'b'The additional inner product structure allows for a richer theory, with applications to, for instance, Fourier series and quantum mechanics. Functions in L2 are sometimes called quadratically integrable functions, square-integrable functions or square-summable functions, but sometimes these terms are reserved for functions that are square-integrable in some other sense, such as in the sense of a Riemann integral (Titchmarsh 1976).'b'Similar to the \xe2\x84\x93p spaces, L2 is the only Hilbert space among Lp spaces. In the complex case, the inner product on L2 is defined by'b'When the underlying measure space S is understood, Lp(S, \xce\xbc) is often abbreviated Lp(\xce\xbc), or just Lp. The above definitions generalize to Bochner spaces.'b'For 1 \xe2\x89\xa4 p \xe2\x89\xa4 \xe2\x88\x9e, Lp(S, \xce\xbc) is a Banach space. The fact that Lp is complete is often referred to as the Riesz-Fischer theorem. Completeness can be checked using the convergence theorems for Lebesgue integrals.'b'As before, if there exists q < \xe2\x88\x9e such that \xe2\x80\x89f\xe2\x80\x89 \xe2\x88\x88 L\xe2\x88\x9e(S, \xce\xbc) \xe2\x88\xa9 Lq(S, \xce\xbc), then'b'For p = \xe2\x88\x9e, the space L\xe2\x88\x9e(S, \xce\xbc) is defined as follows. We start with the set of all measurable functions from S to C or R which are bounded. Again two such functions are identified if they are equal almost everywhere. Denote this set by L\xe2\x88\x9e(S, \xce\xbc). For a function \xe2\x80\x89f\xe2\x80\x89 in this set, the essential supremum of its absolute value serves as an appropriate norm:'b'In the quotient space, two functions \xe2\x80\x89f\xe2\x80\x89 and g are identified if \xe2\x80\x89f\xe2\x80\x89 = g almost everywhere. The resulting normed vector space is, by definition,'b' This can be made into a normed vector space in a standard way; one simply takes the quotient space with respect to the kernel of || \xc2\xb7 ||p. Since for any measurable function \xe2\x80\x89f\xe2\x80\x89, we have that ||\xe2\x80\x89f\xe2\x80\x89||p = 0 if and only if \xe2\x80\x89f\xe2\x80\x89 = 0 almost everywhere, the kernel of || \xc2\xb7 ||p does not depend upon p,'b'That the sum of two p-th power integrable functions is again p-th power integrable follows from the inequality'b'for every scalar \xce\xbb.'b'The set of such functions forms a vector space, with the following natural operations:'b'An Lp space may be defined as a space of functions for which the p-th power of the absolute value is Lebesgue integrable,[3] where functions which agree almost everywhere are identified. More generally, let 1 \xe2\x89\xa4 p < \xe2\x88\x9e and (S, \xce\xa3, \xce\xbc) be a measure space. Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, that'b'The p-norm thus defined on \xe2\x84\x93\xe2\x80\x89p is indeed a norm, and \xe2\x84\x93\xe2\x80\x89p together with this norm is a Banach space. The fully general Lp space is obtained\xe2\x80\x94as seen below \xe2\x80\x94 by considering vectors, not only with finitely or countably-infinitely many components, but with "arbitrarily many components"; in other words, functions. An integral instead of a sum is used to define the p-norm.'b'if the right-hand side is finite, or the left-hand side is infinite. Thus, we will consider \xe2\x84\x93\xe2\x80\x89p spaces for 1 \xe2\x89\xa4 p \xe2\x89\xa4 \xe2\x88\x9e.'b'and the corresponding space \xe2\x84\x93\xe2\x80\x89\xe2\x88\x9e of all bounded sequences. It turns out that[2]'b'One also defines the \xe2\x88\x9e-norm using the supremum:'b'diverges for p = 1 (the harmonic series), but is convergent for p > 1.'b'is not in \xe2\x84\x93\xe2\x80\x891, but it is in \xe2\x84\x93\xe2\x80\x89p for p > 1, as the series'b'One can check that as p increases, the set \xe2\x84\x93\xe2\x80\x89p grows larger. For example, the sequence'b'Here, a complication arises, namely that the series on the right is not always convergent, so for example, the sequence made up of only ones, (1, 1, 1, ...), will have an infinite p-norm for 1 \xe2\x89\xa4 p < \xe2\x88\x9e. The space \xe2\x84\x93\xe2\x80\x89p is then defined as the set of all infinite sequences of real (or complex) numbers such that the p-norm is finite.'b'Define the p-norm:'b'The space of sequences has a natural vector space structure by applying addition and scalar multiplication coordinate by coordinate. Explicitly, the vector sum and the scalar action for infinite sequences of real (or complex) numbers are given by:'b'The p-norm can be extended to vectors that have an infinite number of components, which yields the space \xe2\x84\x93\xe2\x80\x89p. This contains as special cases:'b'This is not a norm because it is not homogeneous. Despite these defects as a mathematical norm, the non-zero counting "norm" has uses in scientific computing, information theory, and statistics\xe2\x80\x93notably in compressed sensing in signal processing and computational harmonic analysis.'b'Another function was called the \xe2\x84\x930 "norm" by David Donoho\xe2\x80\x94whose quotation marks warn that this function is not a proper norm\xe2\x80\x94is the number of non-zero entries of the vector x. Many authors abuse terminology by omitting the quotation marks. Defining 00 = 0, the zero "norm" of x is equal to'b'which is discussed by Stefan Rolewicz in Metric Linear Spaces.[1] The \xe2\x84\x930-normed space is studied in functional analysis, probability theory, and harmonic analysis.'b"The mathematical definition of the \xe2\x84\x930 norm was established by Banach's Theory of Linear Operations. The space of sequences has a complete metric topology provided by the F-norm"b'There is one \xe2\x84\x930 norm and another function called the \xe2\x84\x930 "norm" (with quotation marks).'b'shows that the infinite-dimensional sequence space \xe2\x84\x93p defined below, is no longer locally convex.[citation needed]'b'Although the p-unit ball Bnp around the origin in this metric is "concave", the topology defined on Rn by the metric dp is the usual vector space topology of Rn, hence \xe2\x84\x93np is a locally convex topological vector space. Beyond this qualitative statement, a quantitative way to measure the lack of convexity of \xe2\x84\x93np is to denote by Cp(n) the smallest constant C such that the multiple C\xc2\xa0Bnp of the p-unit ball contains the convex hull of Bnp, equal to Bn1. The fact that for fixed p < 1 we have'b'defines a metric. The metric space (Rn, dp) is denoted by \xe2\x84\x93np.'b'Hence, the function'b'defines a subadditive function at the cost of losing absolute homogeneity. It does define an F-norm, though, which is homogeneous of degree p.'b'defines an absolutely homogeneous function for 0 < p < 1; however, the resulting function does not define a norm, because it is not subadditive. On the other hand, the formula'b'In Rn for n > 1, the formula'b'In general, for vectors in Cn where 0 < r < p:'b'This inequality depends on the dimension n of the underlying vector space and follows directly from the Cauchy\xe2\x80\x93Schwarz inequality.'b'For the opposite direction, the following relation between the 1-norm and the 2-norm is known:'b'This fact generalizes to p-norms in that the p-norm ||x||p of any given vector x does not grow with p:'b'The grid distance or rectilinear distance (sometimes called the "Manhattan distance") between two points is never shorter than the length of the line segment between them (the Euclidean or "as the crow flies" distance). Formally, this means that the Euclidean norm of any vector is bounded by its 1-norm:'b'Abstractly speaking, this means that Rn together with the p-norm is a Banach space. This Banach space is the Lp-space over Rn.'b'For all p \xe2\x89\xa5 1, the p-norms and maximum norm as defined above indeed satisfy the properties of a "length function" (or norm), which are that:'b'See L-infinity.'b'The L\xe2\x88\x9e-norm or maximum norm (or uniform norm) is the limit of the Lp-norms for p \xe2\x86\x92 \xe2\x88\x9e. It turns out that this limit is equivalent to the following definition:'b'The Euclidean norm from above falls into this class and is the 2-norm, and the 1-norm is the norm that corresponds to the rectilinear distance.'b'Of course the absolute value bars are unnecessary when p is a rational number and, in reduced form, has an even numerator.'b'For a real number p \xe2\x89\xa5 1, the p-norm or Lp-norm of x is defined by'b'The Euclidean distance between two points x and y is the length ||x \xe2\x88\x92 y||2 of the straight line between the two points. In many situations, the Euclidean distance is insufficient for capturing the actual distances in a given space. An analogy to this is suggested by taxi drivers in a grid street plan who should measure distance not in terms of the length of the straight line to their destination, but in terms of the rectilinear distance, which takes into account that streets are either orthogonal or parallel to each other. The class of p-norms generalizes these two examples and has an abundance of applications in many parts of mathematics, physics, and computer science.'b'\nThe length of a vector x = (x1, x2, ..., xn) in the n-dimensional real vector space Rn is usually given by the Euclidean norm:'b'Hilbert spaces are central to many applications, from quantum mechanics to stochastic calculus. The spaces L2 and \xe2\x84\x932 are both Hilbert spaces. In fact, by choosing a Hilbert basis (i.e., a maximal orthonormal subset of L2 or any Hilbert space), one sees that all Hilbert spaces are isometric to \xe2\x84\x932(E), where E is a set with an appropriate cardinality.'b'By contrast, if p\xc2\xa0> 2, the Fourier transform does not map into Lq.'b'The Fourier transform for the real line (or, for periodic functions, see Fourier series), maps Lp(R) to Lq(R) (or Lp(T) to \xe2\x84\x93q) respectively, where 1\xc2\xa0\xe2\x89\xa4 p\xc2\xa0\xe2\x89\xa4 2 and 1/p\xc2\xa0+ 1/q\xc2\xa0= 1. This is a consequence of the Riesz\xe2\x80\x93Thorin interpolation theorem, and is made precise with the Hausdorff\xe2\x80\x93Young inequality.'b"In penalized regression, 'L1 penalty' and 'L2 penalty' refer to penalizing either the L1 norm of a solution's vector of parameter values (i.e. the sum of its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector."b'In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as solutions to variational problems.'b''b''b'In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910). Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces. Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, finance, engineering, and other disciplines.'
Algebra over a field
b'In some areas of mathematics, such as commutative algebra, it is common to consider the more general concept of an algebra over a ring, where a commutative unital ring R replaces the field K. The only part of the definition that changes is that A is assumed to be an R-module (instead of a vector space over K).'b'The fourth algebra is non-commutative, others are commutative.'b'There exist five three-dimensional algebras. Each algebra consists of linear combinations of three basis elements, 1 (the identity element), a and b. Taking into account the definition of an identity element, it is sufficient to specify'b'It remains to specify'b'There exist two two-dimensional algebras. Each algebra consists of linear combinations (with complex coefficients) of two basis elements, 1 (the identity element) and a. According to the definition of an identity element,'b'Two-dimensional, three-dimensional and four-dimensional unital associative algebras over the field of complex numbers were completely classified up to isomorphism by Eduard Study.[4]'b"If K is only a commutative ring and not a field, then the same process works if A is a free module over K. If it isn't, then the multiplication is still completely determined by its action on a set that spans A; however, the structure constants can't be specified arbitrarily in this case, and knowing only the structure constants does not specify the algebra up to isomorphism."b'If you apply this to vectors written in index notation, then this becomes'b'When the algebra can be endowed with a metric, then the structure coefficients are generally written with upper and lower indices, so as to distinguish their transformation properties under coordinate transformations. Specifically, lower indices are covariant indices, and transform via pullbacks, while upper indices are contravariant, transforming under pushforwards. Thus, in mathematical physics, the structure coefficients are often written ci,jk, and their defining rule is written using the Einstein notation as'b'Note however that several different sets of structure coefficients can give rise to isomorphic algebras.'b'where e1,...,en form a basis of A.'b'Thus, given the field K, any finite-dimensional algebra can be specified up to isomorphism by giving its dimension (say n), and specifying n3 structure coefficients ci,j,k, which are scalars. These structure coefficients determine the multiplication in A via the following rule:'b'For algebras over a field, the bilinear multiplication from A \xc3\x97 A to A is completely determined by the multiplication of basis elements of A. Conversely, once a basis for A has been chosen, the products of basis elements can be set arbitrarily, and then extended in a unique way to a bilinear operator on A, i.e., so the resulting multiplication satisfies the algebra laws.'b'Given two such associative unital K-algebras A and B, a unital K-algebra morphism f: A \xe2\x86\x92 B is a ring morphism that commutes with the scalar multiplication defined by \xce\xb7, which one may write as'b'given by'b'where Z(A) is the center of A. Since \xce\xb7 is a ring morphism, then one must have either that A is the zero ring, or that \xce\xb7 is injective. This definition is equivalent to that above, with scalar multiplication'b'The definition of an associative K-algebra with unit is also frequently given in an alternative way. In this case, an algebra over a field K is a ring A together with a ring homomorphism'b'Examples detailed in the main article include:'b'These unital zero algebras may be more generally useful, as they allow to translate any general property of the algebras to properties of vector spaces or modules. For example, the theory of Gr\xc3\xb6bner bases was introduced by Bruno Buchberger for ideals in a polynomial ring R = K[x1, ..., xn] over a field. The construction of the unital zero algebra over a free R-module allows extending this theory as a Gr\xc3\xb6bner basis theory for sub modules of a free module. This extension allows, for computing a Gr\xc3\xb6bner basis of a submodule, to use, without any modification, any algorithm and any software for computing Gr\xc3\xb6bner bases of ideals.'b'An example of unital zero algebra is the algebra of dual numbers, the unital zero R-algebra built from a one dimensional real vector space.'b'One may define a unital zero algebra by taking the direct sum of modules of a field (or more generally a ring) K and a K-vector space (or module) V, and defining the product of every pair of elements of V to be zero. That is, if \xce\xbb, \xce\xbc \xe2\x88\x88 k and u, v \xe2\x88\x88 V, then (\xce\xbb + u) (\xce\xbc + v) = \xce\xbb\xce\xbc + (\xce\xbbv + \xce\xbcu). If e1, ... ed is a basis of V, the unital zero algebra is the quotient of the polynomial ring K[E1, ..., En] by the ideal generated by the EiEj for every pair (i, j).'b'An algebra is called zero algebra if uv = 0 for all u, v in the algebra,[2] not to be confused with the algebra with one element. It is inherently non-unital (except in the case of only one element), associative and commutative.'b'An algebra is unital or unitary if it has a unit or identity element I with Ix = x = xI for all x in the algebra.'b'Algebras over fields come in many different types. These types are specified by insisting on some further axioms, such as commutativity or associativity of the multiplication operation, which are not required in the broad definition of an algebra. The theories corresponding to the different types of algebras are often very different.'b'It is important to notice that this definition is different from the definition of an ideal of a ring, in that here we require the condition (2). Of course if the algebra is unital, then condition (3) implies condition (2).'b'If (3) were replaced with x \xc2\xb7 z is in L, then this would define a right ideal. A two-sided ideal is a subset that is both a left and a right ideal. The term ideal on its own is usually taken to mean a two-sided ideal. Of course when the algebra is commutative, then all of these notions of ideal are equivalent. Notice that conditions (1) and (2) together are equivalent to L being a linear subspace of A. It follows from condition (3) that every left or right ideal is a subalgebra.'b'A left ideal of a K-algebra is a linear subspace that has the property that any element of the subspace multiplied on the left by any element of the algebra produces an element of the subspace. In symbols, we say that a subset L of a K-algebra A is a left ideal if for every x and y in L, z in A and c in K, we have the following three statements.'b'In the above example of the complex numbers viewed as a two-dimensional algebra over the real numbers, the one-dimensional real line is a subalgebra.'b'A subalgebra of an algebra over a field K is a linear subspace that has the property that the product of any two of its elements is again in the subspace. In other words, a subalgebra of an algebra is a subset of elements that is closed under addition, multiplication, and scalar multiplication. In symbols, we say that a subset L of a K-algebra A is a subalgebra if for every x, y in L and c in K, we have that x \xc2\xb7 y, x + y, and cx are all in L.'b'A K-algebra isomorphism is a bijective K-algebra homomorphism. For all practical purposes, isomorphic algebras differ only by notation.'b'Given K-algebras A and B, a K-algebra homomorphism is a K-linear map f: A \xe2\x86\x92 B such that f(xy) = f(x) f(y) for all x,y in A. The space of all K-algebra homomorphisms between A and B is frequently written as'b'Previous examples are associative algebras. An example of a non-associative algebra is a three dimensional vector space equipped with the cross product. This is a simple example of a class of nonassociative algebras, which is widely used in mathematics and physics, the Lie algebras.'b'The quaternions were soon followed by several other hypercomplex number systems, which were the early examples of algebras over a field.'b'The real numbers may be viewed as a one-dimensional vector space with a compatible multiplication, and hence a one-dimensional algebra over itself. Likewise, as we saw above, the complex numbers form a two-dimensional vector space over the field of real numbers, and hence form a two dimensional algebra over the reals. In both these examples, every non-zero vector has an inverse, making them both division algebras. Although there are no division algebras in 3 dimensions, in 1843, the quaternions were defined and provided the now famous 4-dimensional example of an algebra over the real numbers, where one can not only multiply vectors, but also divide. Any quaternion may be written as (a, b, c, d) = a + bi + cj + dk. Unlike the complex numbers, the quaternions are an example of a non-commutative algebra: for instance, (0,1,0,0) \xc2\xb7 (0,0,1,0) = (0,0,0,1) but (0,0,1,0) \xc2\xb7 (0,1,0,0) = (0,0,0,\xe2\x88\x921).'b'Notice that when a binary operation on a vector space is commutative, as in the above example of the complex numbers, it is left distributive exactly when it is right distributive. But in general, for non-commutative operations (such as the next example of the quaternions), they are not equivalent, and therefore require separate axioms.'b'These three axioms are another way of saying that the binary operation is bilinear. An algebra over K is sometimes also called a K-algebra, and K is called the base field of A. The binary operation is often referred to as multiplication in A. The convention adopted in this article is that multiplication of elements of an algebra is not necessarily associative, although some authors use the term algebra to refer to an associative algebra.'b'Let K be a field, and let A be a vector space over K equipped with an additional binary operation from A \xc3\x97 A to A, denoted here by \xc2\xb7 (i.e. if x and y are any two elements of A, x \xc2\xb7 y is the product of x and y). Then A is an algebra over K if the following identities hold for all elements x, y, and z of A, and all elements (often called scalars) a and b of K:'b'This example fits into the following definition by taking the field K to be the real numbers, and the vector space A to be the complex numbers.'b'The following statements are basic properties of the complex numbers. If x, y, z are complex numbers and a, b are real numbers, then'b'Any complex number may be written a + bi, where a and b are real numbers and i is the imaginary unit. In other words, a complex number is represented by the vector (a, b) over the field of real numbers. So the complex numbers form a two-dimensional real vector space, where addition is given by (a, b) + (c, d) = (a + c, b + d) and scalar multiplication is given by c(a, b) = (ca, cb), where all of a, b, c and d are real numbers. We use the symbol \xc2\xb7 to multiply two vectors together, which we use complex multiplication to define: (a, b) \xc2\xb7 (c, d) = (ac \xe2\x88\x92 bd, ad + bc).'b''b''b'Replacing the field of scalars by a commutative ring leads to the more general notion of an algebra over a ring. Algebras are not to be confused with vector spaces equipped with a bilinear form, like inner product spaces, as, for such a space, the result of a product is not in the space, but rather in the field of coefficients.'b'Many authors use the term algebra to mean associative algebra, or unital associative algebra, or in some subjects such as algebraic geometry, unital associative commutative algebra.'b'An algebra is unital or unitary if it has an identity element with respect to the multiplication. The ring of real square matrices of order n forms a unital algebra since the identity matrix of order n is the identity element with respect to matrix multiplication. It is an example of a unital associative algebra, a (unital) ring that is also a vector space.'b'The multiplication operation in an algebra may or may not be associative, leading to the notions of associative algebras and nonassociative algebras. Given an integer n, the ring of real square matrices of order n is an example of an associative algebra over the field of real numbers under matrix addition and matrix multiplication since matrix multiplication is associative. Three-dimensional Euclidean space with multiplication given by the vector cross product is an example of a nonassociative algebra over the field of real numbers since the vector cross product is nonassociative, satisfying the Jacobi identity instead.'b'In mathematics, an algebra over a field (often simply called an algebra) is a vector space equipped with a bilinear product. Thus, an algebra is an algebraic structure, which consists of a set, together with operations of multiplication, addition, and scalar multiplication by elements of the underlying field, and satisfies the axioms implied by "vector space" and "bilinear".[1]'
Multilinear algebra

Dual space
b'be continuous for the chosen topology on V\xe2\x80\xb2. Further, there is still a choice of a topology on V\xe2\x80\xb2\xe2\x80\xb2, and continuity of \xce\xa8 depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.'b'Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V\xe2\x80\xb2, so that the continuous double dual V\xe2\x80\xb2\xe2\x80\xb2 is not uniquely defined as a set. Saying that \xce\xa8 maps from V to V\xe2\x80\xb2\xe2\x80\xb2, or in other words, that \xce\xa8(x) is continuous on V\xe2\x80\xb2 for every x \xe2\x88\x88 V, is a reasonable minimal requirement on the topology of V\xe2\x80\xb2, namely that the evaluation mappings'b'When V is a topological vector space, one can still define \xce\xa8(x) by the same formula, for every x \xe2\x88\x88 V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map \xce\xa8 trivial. However, if V is Hausdorff and locally convex, the map \xce\xa8 is injective from V to the algebraic dual V\xe2\x80\xb2\xe2\x88\x97 of the continuous dual, again as a consequence of the Hahn\xe2\x80\x93Banach theorem.[15]'b'As a consequence of the Hahn\xe2\x80\x93Banach theorem, this map is in fact an isometry, meaning \xe2\x80\x96 \xce\xa8(x) \xe2\x80\x96 = \xe2\x80\x96 x \xe2\x80\x96 for all x in\xc2\xa0V. Normed spaces for which the map \xce\xa8 is a bijection are called reflexive.'b'In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator \xce\xa8\xc2\xa0: V \xe2\x86\x92 V\xe2\x80\xb2\xe2\x80\xb2 from a normed space V into its continuous double dual V\xe2\x80\xb2\xe2\x80\xb2, defined by'b'The topology of V and the topology of real or complex numbers can be used to induce on V\xe2\x80\xb2 a dual space topology.'b'If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space \xe2\x84\x93\xe2\x80\x891 is separable, but its dual \xe2\x84\x93\xe2\x80\x89\xe2\x88\x9e is not.'b'and it follows from the Hahn\xe2\x80\x93Banach theorem that j\xe2\x80\xb2 induces an isometric isomorphism V\xe2\x80\xb2\xe2\x80\x89/\xe2\x80\x89W\xe2\x8a\xa5 \xe2\x86\x92 W\xe2\x80\xb2.'b'Then, the dual of the quotient V\xe2\x80\x89/\xe2\x80\x89W\xe2\x80\x89 can be identified with W\xe2\x8a\xa5, and the dual of W can be identified with the quotient V\xe2\x80\xb2\xe2\x80\x89/\xe2\x80\x89W\xe2\x8a\xa5.[14] Indeed, let P denote the canonical surjection from V onto the quotient V\xe2\x80\x89/\xe2\x80\x89W\xe2\x80\x89; then, the transpose P\xe2\x80\xb2 is an isometric isomorphism from (V\xe2\x80\x89/\xe2\x80\x89W\xe2\x80\x89)\xe2\x80\xb2 into V\xe2\x80\xb2, with range equal to W\xe2\x8a\xa5. If j denotes the injection map from W into V, then the kernel of the transpose j\xe2\x80\xb2 is the annihilator of W:'b'Assume that W is a closed linear subspace of a normed space\xc2\xa0V, and consider the annihilator of W in V\xe2\x80\xb2,'b'When T is a continuous linear map between two topological vector spaces V and W, then the transpose T\xe2\x80\xb2 is continuous when W\xe2\x80\xb2 and V\xe2\x80\xb2 are equipped with"compatible" topologies: for example when, for X = V and X = W, both duals X\xe2\x80\xb2 have the strong topology \xce\xb2(X\xe2\x80\xb2, X) of uniform convergence on bounded sets of X, or both have the weak-\xe2\x88\x97 topology \xcf\x83(X\xe2\x80\xb2, X) of pointwise convergence on\xc2\xa0X. The transpose T\xe2\x80\xb2 is continuous from \xce\xb2(W\xe2\x80\xb2, W) to \xce\xb2(V\xe2\x80\xb2, V), or from \xcf\x83(W\xe2\x80\xb2, W) to \xcf\x83(V\xe2\x80\xb2, V).'b'When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V\xe2\x80\xb2. For every bounded linear map T on V, the transpose and the adjoint operators are linked by'b'When T is a compact linear map between two Banach spaces V and W, then the transpose T\xe2\x80\xb2 is compact. This can be proved using the Arzel\xc3\xa0\xe2\x80\x93Ascoli theorem.'b'When V and W are normed spaces, the norm of the transpose in L(W\xe2\x80\xb2, V\xe2\x80\xb2) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn\xe2\x80\x93Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T\xe2\x80\xb2 is injective.'b'The resulting functional T\xe2\x80\xb2(\xcf\x86) is in V\xe2\x80\xb2. The assignment T \xe2\x86\x92 T\xe2\x80\xb2 produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W\xe2\x80\xb2 to V\xe2\x80\xb2. When T and U are composable continuous linear maps, then'b'If T\xc2\xa0: V \xe2\x86\x92 W is a continuous linear map between two topological vector spaces, then the (continuous) transpose T\xe2\x80\xb2\xc2\xa0: W\xe2\x80\xb2 \xe2\x86\x92 V\xe2\x80\xb2 is defined by the same formula as before:'b'By the Riesz\xe2\x80\x93Markov\xe2\x80\x93Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.'b'By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra\xe2\x80\x93ket notation used by physicists in the mathematical formulation of quantum mechanics.'b'In a similar manner, the continuous dual of \xe2\x84\x93\xe2\x80\x891 is naturally identified with \xe2\x84\x93\xe2\x80\x89\xe2\x88\x9e (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with \xe2\x84\x93\xe2\x80\x891.'b"is finite. Define the number q by 1/p + 1/q = 1. Then the continuous dual of \xe2\x84\x93\xe2\x80\x89p is naturally identified with \xe2\x84\x93\xe2\x80\x89q: given an element \xcf\x86 \xe2\x88\x88 (\xe2\x84\x93\xe2\x80\x89p)\xe2\x80\xb2, the corresponding element of \xe2\x84\x93\xe2\x80\x89q is the sequence (\xcf\x86(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) \xe2\x88\x88 \xe2\x84\x93\xe2\x80\x89q, the corresponding continuous linear functional \xcf\x86 on \xe2\x84\x93\xe2\x80\x89p is defined by \xcf\x86(b) = \xe2\x88\x91n anbn for all b = (bn) \xe2\x88\x88 \xe2\x84\x93\xe2\x80\x89p (see H\xc3\xb6lder's inequality)."b'Let 1 < p < \xe2\x88\x9e be a real number and consider the Banach space \xe2\x84\x93\xe2\x80\x89p of all sequences a = (an) for which'b'Here are the three most important special cases.'b'form its local base.'b'As a particular consequence, if V is a direct sum of two subspaces A and B, then V\xe2\x88\x97 is a direct sum of A0 and B0.'b'If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional f\xc2\xa0: V \xe2\x86\x92 F factors through V/W if and only if W is in the kernel of f. There is thus an isomorphism'b'after identifying W with its image in the second dual space under the double duality isomorphism V \xe2\x89\x88 V\xe2\x88\x97\xe2\x88\x97. Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.'b'If V is finite-dimensional, and W is a vector subspace, then'b'In particular if A and B are subspaces of V, it follows that'b'and equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, then'b'Moreover, if A and B are two subsets of V, then'b'The annihilator of a subset is itself a vector space. In particular, \xe2\x88\x850 = V\xe2\x88\x97 is all of V\xe2\x88\x97 (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S \xe2\x8a\x82 T \xe2\x8a\x82 V, then'b'Let S be a subset of V. The annihilator of S in V\xe2\x88\x97, denoted here S0, is the collection of linear functionals f \xe2\x88\x88 V\xe2\x88\x97 such that [f, s] = 0 for all s \xe2\x88\x88 S. That is, S0 consists of all linear functionals f\xc2\xa0: V \xe2\x86\x92 F such that the restriction to S vanishes: f|S = 0.'b'If the linear map f is represented by the matrix A with respect to two bases of V and W, then f\xe2\x88\x97 is represented by the transpose matrix AT with respect to the dual bases of W\xe2\x88\x97 and V\xe2\x88\x97, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f\xe2\x88\x97 is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.'b'The assignment f \xe2\x86\xa6 f\xe2\x88\x97 produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W\xe2\x88\x97 to V\xe2\x88\x97; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)\xe2\x88\x97 = g\xe2\x88\x97f\xe2\x88\x97. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify (f\xe2\x88\x97)\xe2\x88\x97 with f using the natural injection into the double dual.'b'where the bracket [\xc2\xb7,\xc2\xb7] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual. This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.'b'The following identity holds for all \xcf\x86 \xe2\x88\x88 W\xe2\x88\x97 and v \xe2\x88\x88 V:'b'for every \xcf\x86 \xe2\x88\x88 W\xe2\x88\x97. The resulting functional f\xe2\x88\x97(\xcf\x86) in V\xe2\x88\x97 is called the pullback of \xcf\x86 along f.'b'If f\xc2\xa0: V \xe2\x86\x92 W is a linear map, then the transpose (or dual) f\xe2\x88\x97\xc2\xa0: W\xe2\x88\x97 \xe2\x86\x92 V\xe2\x88\x97 is defined by'b'The conjugate space V\xe2\x88\x97 can be identified with the set of all additive complex-valued functionals f: V \xe2\x86\x92 C such that'b'If the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9 determines an isomorphism of V with the complex conjugate of the dual space'b'Thus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V\xe2\x88\x97 and nondegenerate bilinear forms on V.'b'defined by'b'where the right hand side is defined as the functional on V taking each w \xe2\x88\x88 V to \xe2\x9f\xa8v,w\xe2\x9f\xa9. In other words, the bilinear form determines a linear mapping'b'If V is finite-dimensional, then V is isomorphic to V\xe2\x88\x97. But there is in general no natural isomorphism between these two spaces.[7] Any bilinear form \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9 on V gives a mapping of V into its dual space via'b'Thus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.'b'is a special case of a general result relating direct sums (of modules) to direct products.'b'On the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identification'b'Note that (FA)0 may be identified (essentially by definition) with the direct sum of infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphisms'b'Again the sum is finite because f\xce\xb1 is nonzero for only finitely many \xce\xb1.'b'The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values \xce\xb8\xce\xb1 = T(e\xce\xb1) it takes on the basis of V, and any function \xce\xb8\xc2\xa0: A \xe2\x86\x92 F (with \xce\xb8(\xce\xb1) = \xce\xb8\xce\xb1) defines a linear functional T on V by'b'in V (the sum is finite by the assumption on f, and any v \xe2\x88\x88 V may be written in this way by the definition of the basis).'b'This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {e\xce\xb1\xc2\xa0: \xce\xb1 \xe2\x88\x88 A} identifies V with the space (FA)0 of functions f\xc2\xa0: A \xe2\x86\x92 F such that f\xce\xb1 = f(\xce\xb1) is nonzero for only finitely many \xce\xb1 \xe2\x88\x88 A, where such a function f is identified with the vector'b'Consider, for instance, the space R\xe2\x88\x9e, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i \xe2\x88\x88 N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R\xe2\x88\x9e is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R\xe2\x88\x9e to give the number \xe2\x88\x91anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R\xe2\x88\x9e is countably infinite, whereas RN does not have a countable basis.'b'If V is not finite-dimensional but has a basis[6] e\xce\xb1 indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements e\xce\xb1 (\xce\xb1 \xe2\x88\x88 A) of the dual space, but they will not form a basis.'b'If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V\xe2\x88\x97 form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V\xe2\x88\x97 can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V\xe2\x88\x97 are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]'b'In particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n \xc3\x97 1 matrix and a 1 \xc3\x97 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 \xc3\x97 n matrix, i.e., M must be a row vector.'b'for any choice of coefficients ci \xe2\x88\x88 F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equations'b'If V is finite-dimensional, then V\xe2\x88\x97 has the same dimension as V. Given a basis {e1, ..., en} in V, it is possible to construct a specific basis in V\xe2\x88\x97, called the dual basis. This dual basis is a set {e1, ..., en} of linear functionals on V, defined by the relation'b'The pairing of a functional \xcf\x86 in the dual space V\xe2\x88\x97 and an element x of V is sometimes denoted by a bracket: \xcf\x86(x) = [x,\xcf\x86] [2] or \xcf\x86(x) = \xe2\x9f\xa8\xcf\x86,x\xe2\x9f\xa9.[3] This pairing defines a nondegenerate bilinear mapping[4] \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9\xc2\xa0: V\xe2\x88\x97 \xc3\x97 V \xe2\x86\x92 F called the natural pairing.'b'for all \xcf\x86 and \xcf\x88 \xe2\x88\x88 V\xe2\x88\x97, x \xe2\x88\x88 V, and a \xe2\x88\x88 F. Elements of the algebraic dual space V\xe2\x88\x97 are sometimes called covectors or one-forms.'b''b''b'Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.'b'The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space. When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.'b'In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.'
Tensor product
b'However, these kinds of notation are not universally present in array languages. Other array languages may require explicit treatment of indices (for example, MATLAB), and/or may not support higher-order functions such as the Jacobian derivative (for example, Fortran/APL).'b"Note that J's treatment also allows the representation of some tensor fields, as a and b may be functions instead of constants. This product of two functions is a derived function, and if a and b are differentiable, then a */ b is differentiable."b'Array programming languages may have this pattern built in. For example, in APL the tensor product is expressed as \xe2\x97\x8b.\xc3\x97 (for example A \xe2\x97\x8b.\xc3\x97 B or A \xe2\x97\x8b.\xc3\x97 B \xe2\x97\x8b.\xc3\x97 C). In J the tensor product is the dyadic form of */ (for example a */ b or a */ b */ c).'b'That is, in the symmetric algebra two adjacent vectors (and therefore all of them) can be interchanged. The resulting objects are called symmetric tensors.'b'The symmetric algebra is constructed in a similar manner:'b'Note that when the underlying field of V does not have characteristic 2, then this definition is equivalent to'b'is defined as'b'Two notable constructions in linear algebra can be constructed as quotients of the tensor product: the exterior algebra and the symmetric algebra. For example, given a vector space V, the exterior product'b'A general context for tensor product is that of a monoidal category.'b'It should be mentioned that, though called "tensor product", this is not a tensor product of graphs in the above sense; actually it is the category-theoretic product in the category of graphs and graph homomorphisms. However it is actually the Kronecker tensor product of the adjacency matrices of the graphs. Compare also the section Tensor product of linear maps above.'b'This is a special case of the product of tensors if they are seen as multilinear maps (see also tensors as multilinear maps). Thus the components of the tensor product of multilinear forms can be computed by the Kronecker product.'b'is isomorphic (as an A-algebra) to the Adeg(f).'b'where now f is interpreted as the same polynomial, but with its coefficients regarded as elements of B. In the larger field B, the polynomial may become reducible, which brings in Galois theory. For example, if A = B is a Galois extension of R, then'b'A particular example is when A and B are fields containing a common subfield R. The tensor product of fields is closely related to Galois theory: if, say, A = R[x] / f(x), where f is some irreducible polynomial with coefficients in R, the tensor product can be calculated as'b'For example,'b'Let R be a commutative ring. The tensor product of R-modules applies, in particular, if A and B are R-algebras. In this case, the tensor product A \xe2\x8a\x97R B is an R-algebra itself by putting'b'is not usually injective. For example, tensoring the (injective) map given by multiplication with n, n\xc2\xa0: Z \xe2\x86\x92 Z with Z/nZ yields the zero map 0\xc2\xa0: Z/nZ \xe2\x86\x92 Z/nZ, which is not injective. Higher Tor functors measure the defect of the tensor product being not left exact. All higher Tor functors are assembled in the derived tensor product.'b'Here NJ\xc2\xa0:= \xe2\xa8\x81j \xe2\x88\x88 J N and the map is determined by sending some n \xe2\x88\x88 N in the jth copy of NJ to ajin (in NI). Colloquially, this may be rephrased by saying that a presentation of M gives rise to a presentation of M \xe2\x8a\x97R N. This is referred to by saying that the tensor product is a right exact functor. It is not in general left exact, that is, given an injective map of R-modules M1 \xe2\x86\x92 M2, the tensor product'b'For vector spaces, the tensor product V \xe2\x8a\x97 W is quickly computed since bases of V of W immediately determine a basis of V \xe2\x8a\x97 W, as was mentioned above. For modules over a general (commutative) ring, not every module is free. For example, Z/nZ is not a free abelian group (= Z-module). The tensor product with Z/nZ is given by'b'Let A be a right R-module and B be a left R-module B. Then the tensor product of A and B is an abelian group defined by'b'The universal property also carries over, slightly modified: the map \xcf\x86\xc2\xa0: A \xc3\x97 B \xe2\x86\x92 A \xe2\x8a\x97R B defined by (a, b) \xe2\x86\xa6 a \xe2\x8a\x97 b is a middle linear map (referred to as "the canonical middle linear map".[13]); that is,[14] it satisfies:'b'is imposed. If R is non-commutative, this is no longer an R-module, but just an abelian group.'b'More generally, the tensor product can be defined even if the ring is non-commutative (ab \xe2\x89\xa0 ba). In this case A has to be a right-R-module and B is a left-R-module, and instead of the last two relations above, the relation'b'where now F(A \xc3\x97 B) is the free R-module generated by the cartesian product and G is the R-module generated by the same relations as above.'b'The tensor product of two modules A and B over a commutative ring R is defined in exactly the same way as the tensor product of vector spaces over a field:'b'where u\xe2\x88\x97 in End(V\xe2\x88\x97) is the transpose of u, that is, in terms of the obvious pairing on V \xe2\x8a\x97 V\xe2\x88\x97,'b'Here Hom(-,-) denotes the K-vector space of all linear maps. This is an example of adjoint functors: the tensor product is "left adjoint" to Hom.'b'Furthermore, given three vector spaces U, V, W the tensor product is linked to the vector space of all linear maps, as follows:'b'This result implies'b'Given two finite dimensional vector spaces U, V, denote the dual space of U as U*, we have the following relation:'b'The interplay of evaluation and coevaluation map can be used to characterize finite-dimensional vector spaces without referring to bases.[12]'b'where v1, ..., vn is any basis of V, and vi\xe2\x88\x97 is its dual basis. Surprisingly, this map does not depend on our choice of basis.[11]'b'On the other hand, if V is finite-dimensional, there is a canonical map in the other direction (called the coevaluation map)'b'is called tensor contraction (for r, s > 0).'b'The resulting map'b'which on elementary tensors is defined by'b'A particular example is the tensor product of some vector space V with its dual vector space V\xe2\x88\x97 (which consists of all linear maps f from V to the ground field K). In this case, there is a canonical evaluation map'b'and'b'[10] Thus, the components of the tensor product of two tensors are the ordinary product of the components of each tensor. Another example: let U be a tensor of type (1, 1) with components U\xce\xb1\xce\xb2, and let V be a tensor of type (1, 0) with components V\xe2\x80\x89\xce\xb3. Then'b'Picking a basis of V and the corresponding dual basis of V\xe2\x88\x97 naturally induces a basis for Tr\ns(V) (this basis is described in the article on Kronecker products). In terms of these bases, the components of a (tensor) product of two (or more) tensors can be computed. For example, if F and G are two covariant tensors of rank m and n respectively (i.e. F \xe2\x88\x88 T\xe2\x80\x890\nm, and G \xe2\x88\x88 T\xe2\x80\x890\nn), then the components of their tensor product are given by'b'It is defined by grouping all occurring "factors" V together: writing vi for an element of V and fi for elements of the dual space,'b'There is a product map, called the (tensor) product of tensors[9]'b'Here V\xe2\x88\x97 is the dual vector space (which consists of all linear maps f from V to the ground field K).'b'For non-negative integers r and s a type (r,s) tensor on a vector space V is an element of'b'The isomorphism \xcf\x84\xcf\x83 is called the braiding map associated to the permutation \xcf\x83.'b'such that'b'be the natural multilinear embedding of the Cartesian power of V into the tensor power of V. Then, by the universal property, there is a unique isomorphism'b'Let'b'A permutation \xcf\x83 of the set {1, 2, ..., n} determines a mapping of the nth Cartesian power of V as follows:'b'Let n be a non-negative integer. The nth tensor power of the vector space V is the n-fold tensor product of V with itself. That is'b'The universal-property definition of a tensor product is valid in more categories that just the category of vector spaces. Instead of using multilinear (bilinear) maps, the general tensor product definition uses multimorphisms.[8]'b'The category of vector spaces with tensor product is an example of a symmetric monoidal category.'b'Similar reasoning can be used to show that the tensor product is associative, that is, there are natural isomorphisms'b'This characterization can simplify proofs about the tensor product. For example, the tensor product is symmetric, meaning there is a canonical isomorphism:'b'A dyadic product is the special case of the tensor product between two vectors of the same dimension.'b'The resultant rank is at most 4, and thus the resultant dimension is 4. Here rank denotes the tensor rank (number of requisite indices), while the matrix rank counts the number of degrees of freedom in the resulting array.'b'respectively, then the tensor product of these two matrices is'b'By choosing bases of all vector spaces involved, the linear maps S and T can be represented by matrices. Then, the matrix describing the tensor product S \xe2\x8a\x97 T is the Kronecker product of the two matrices. For example, if V, X, W, and Y above are all two-dimensional and bases have been fixed for all of them, and S and T are given by the matrices'b'If S and T are both injective, surjective, or continuous then S \xe2\x8a\x97 T is, respectively, injective, surjective, continuous.'b'In this way, the tensor product becomes a bifunctor from the category of vector spaces to itself, covariant in both arguments.[7]'b'defined by'b'The tensor product also operates on linear maps between vector spaces. Specifically, given two linear maps S\xc2\xa0: V \xe2\x86\x92 X and T\xc2\xa0: W \xe2\x86\x92 Y between vector spaces, the tensor product of the two linear maps S and T is a linear map'b'Given bases {vi} and {wj} for V and W respectively, the tensors {vi \xe2\x8a\x97 wj} form a basis for V \xe2\x8a\x97 W. Therefore, if V and W are finite-dimensional, the dimension of the tensor product is the product of dimensions of the original spaces; for instance Rm \xe2\x8a\x97 Rn is isomorphic to Rmn.'b'Elements of V \xe2\x8a\x97 W are often referred to as tensors, although this term refers to many other related concepts as well.[5] If v belongs to V and w belongs to W, then the equivalence class of (v, w) is denoted by v \xe2\x8a\x97 w, which is called the tensor product of v with w. In physics and engineering, this use of the "\xe2\x8a\x97" symbol refers specifically to the outer product operation; the result of the outer product v \xe2\x8a\x97 w is one of the standard ways of representing the equivalence class v \xe2\x8a\x97 w.[6] An element of V \xe2\x8a\x97 W that can be written in the form v \xe2\x8a\x97 w is called a pure or simple tensor. In general, an element of the tensor product space is not a pure tensor, but rather a finite linear combination of pure tensors. For example, if v1 and v2 are linearly independent, and w1 and w2 are also linearly independent, then v1 \xe2\x8a\x97 w1 + v2 \xe2\x8a\x97 w2 cannot be written as a pure tensor. The number of simple tensors required to express an element of a tensor product is called the tensor rank (not to be confused with tensor order, which is the number of spaces one has taken the product of, in this case 2; in notation, the number of indices), and for linear operators or matrices, thought of as (1, 1) tensors (elements of the space V \xe2\x8a\x97 V\xe2\x88\x97), it agrees with matrix rank.'b'all hold (unlike in F(V \xc3\x97 W)), which is exactly what is desired. In these latter expressions, the (v1, w), etc., are images in the quotient of vectors in the free product under the quotient map. Usually, some other notation is employed for them, see below.'b'In the quotient, where N is mapped to the zero vector, the following equalities,'b'The following expression explicitly gives the subspace N:[4]'b'The result can be proven to be independent of which representatives of the involved classes have been chosen. In other words, the operations are well-defined.'b'in the involved equivalence classes outputting the one equivalence class of the result.'b'The operations of V \xe2\x8a\x97 W, i.e. the map of vector addition +\xc2\xa0: U \xc3\x97 U \xe2\x86\x92 U and scalar multiplication \xe2\x8b\x85\xc2\xa0: K \xc3\x97 U \xe2\x86\x92 U are defined to be the respective operations +F and \xe2\x8b\x85F from F(V \xc3\x97 W), acting on any representatives'b'From the Cartesian product V \xc3\x97 W, the free vector space F(V \xc3\x97 W) over K is formed. The vectors of V \xe2\x8a\x97 W are then defined to be the equivalence classes of the congruence generated by the following relations on F(V \xc3\x97 W):'b'In general, given two vector spaces V and W over a field K, the tensor product U of V and W, denoted as U = V \xe2\x8a\x97 W is defined as the vector space whose elements and operations are constructed as follows:'b'Let us first consider a special case: let us say V, W are free vector spaces for the sets S, T respectively. That is, V = F(S), W = F(T). In this special case, the tensor product is defined as F(S) \xe2\x8a\x97 F(T) = F(S \xc3\x97 T). In most typical cases, any vector space can be immediately understood as the free vector space for some set, so this definition suffices. However, there is also an explicit way of constructing the tensor product directly from V, W, without appeal to S, T.'b'By construction, the (possibly infinite) dimension of the vector space F(S) equals the cardinality of the set S.'b'Then {\xce\xb4s | s \xe2\x88\x88 S} is a basis for F(S), since each element g of F(S) can be uniquely written as a linear combination of \xce\xb4s, and because of the restriction that g has finite support, this linear combination consists of finitely many terms. Because of this explicit expression, an element of F(S) is often called a formal sum of symbols in S.'b'The definition of \xe2\x8a\x97 requires the notion of the free vector space F(S) on some set S, a vector space whose basis is indexed by S. F(S) is defined as the set of all functions g from S to a given field K that have finite support; i.e., g is identically zero outside some finite subset of S. It is a vector space over K with the usual addition and scalar multiplication of functions. It has a basis parameterized by S. Indeed, for each s in S we define[1]'b'The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below). Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over V \xc3\x97 W. This approach is described below.'b'The tensor product of two vector spaces V and W over a field K is another vector space over K. It is denoted V \xe2\x8a\x97K W, or V \xe2\x8a\x97 W when the underlying field K is understood.'b''b''b'More generally, the tensor product can be extended to other categories of mathematical objects in addition to vector spaces, such as to matrices, tensors, algebras, topological vector spaces, and modules. In each such case the tensor product is characterized by a similar universal property: it is the freest bilinear operation. The general concept of a "tensor product" is captured by monoidal categories; that is, the class of all things that have a tensor product is a monoidal category.'b'In particular, this distinguishes the tensor product from the direct sum vector space, whose dimension is the sum of the dimensions of the two summands:'b'The tensor product of (finite dimensional) vector spaces has dimension equal to the product of the dimensions of the two factors:'b'In mathematics, the tensor product V \xe2\x8a\x97 W of two vector spaces V and W (over the same field) is itself a vector space, together with an operation of bilinear composition, denoted by \xe2\x8a\x97, from ordered pairs in the Cartesian product V \xc3\x97 W into V \xe2\x8a\x97 W, in a way that generalizes the outer product. The tensor product of V and W is the vector space generated by the symbols v \xe2\x8a\x97 w, with v \xe2\x88\x88 V and w \xe2\x88\x88 W, in which the relations of bilinearity are imposed for the product operation \xe2\x8a\x97, and no other relations are assumed to hold. The tensor product space is thus the "freest" (or most general) such vector space, in the sense of having the fewest constraints.'
Module (mathematics)
b'Over near-rings, one can consider near-ring modules, a nonabelian generalization of modules.[citation needed]'b'One can also consider modules over a semiring. Modules over rings are abelian groups, but modules over semirings are only commutative monoids. Most applications of modules are still possible. In particular, for any semiring S the matrices over S form a semiring over which the tuples of elements from S are a module (in this generalized sense only). This allows a further generalization of the concept of vector space incorporating the semirings from theoretical computer science.'b'Modules over commutative rings can be generalized in a different direction: take a ringed space (X, OX) and consider the sheaves of OX-modules; see sheaf of modules for more. These form a category OX-Mod, and play an important role in modern algebraic geometry. If X has only a single point, then this is a module category in the old sense over the commutative ring OX(X).'b'Any ring R can be viewed as a preadditive category with a single object. With this understanding, a left R-module is nothing but a (covariant) additive functor from R to the category Ab of abelian groups. Right R-modules are contravariant additive functors. This suggests that, if C is any preadditive category, a covariant additive functor from C to Ab should be considered a generalized left module over C; these functors form a functor category C-Mod which is the natural generalization of the module category R-Mod.'b'A representation is called faithful if and only if the map R \xe2\x86\x92 EndZ(M) is injective. In terms of modules, this means that if r is an element of R such that rx = 0 for all x in M, then r = 0. Every abelian group is a faithful module over the integers or over some modular arithmetic Z/nZ.'b'Such a ring homomorphism R \xe2\x86\x92 EndZ(M) is called a representation of R over the abelian group M; an alternative and equivalent way of defining left R-modules is to say that a left R-module is an abelian group M together with a representation of R over it.'b'If M is a left R-module, then the action of an element r in R is defined to be the map M \xe2\x86\x92 M that sends each x to rx (or xr in the case of a right module), and is necessarily a group endomorphism of the abelian group (M, +). The set of all group endomorphisms of M is denoted EndZ(M) and forms a ring under addition and composition, and sending a ring element r of R to its action actually defines a ring homomorphism from R to EndZ(M).'b'Uniform. A uniform module is a module in which all pairs of nonzero submodules have nonzero intersection.'b'Graded. A graded module is a module with a decomposition as a direct sum M = \xe2\xa8\x81x Mx over a graded ring R = \xe2\xa8\x81x Rx such that RxMy \xe2\x8a\x82 Mx+y for all x and y.'b'Artinian. An Artinian module is a module which satisfies the descending chain condition on submodules, that is, every decreasing chain of submodules becomes stationary after finitely many steps.'b'Noetherian. A Noetherian module is a module which satisfies the ascending chain condition on submodules, that is, every increasing chain of submodules becomes stationary after finitely many steps. Equivalently, every submodule is finitely generated.'b'Torsion-free. A torsion-free module is a module over a ring such that 0 is the only element annihilated by a regular element (non zero-divisor) of the ring.'b'Faithful. A faithful module M is one where the action of each r \xe2\x89\xa0 0 in R on M is nontrivial (i.e. r \xe2\x8b\x85 x \xe2\x89\xa0 0 for some x in M). Equivalently, the annihilator of M is the zero ideal.'b'Indecomposable. An indecomposable module is a non-zero module that cannot be written as a direct sum of two non-zero submodules. Every simple module is indecomposable, but there are indecomposable modules which are not simple (e.g. uniform modules).'b'Semisimple. A semisimple module is a direct sum (finite or not) of simple modules. Historically these modules are also called completely reducible.'b'Simple. A simple module S is a module that is not {0} and whose only submodules are {0} and S. Simple modules are sometimes called irreducible.[3]'b'Torsionless module. A module is called torsionless if it embeds into its algebraic dual.'b'Flat. A module is called flat if taking the tensor product of it with any exact sequence of R-modules preserves exactness.'b'Injective. Injective modules are defined dually to projective modules.'b'Projective. Projective modules are direct summands of free modules and share many of their desirable properties.'b'Free. A free R-module is a module that has a basis, or equivalently, one that is isomorphic to a direct sum of copies of the ring R. These are the modules that behave very much like vector spaces.'b'Cyclic. A module is called a cyclic module if it is generated by one element.'b'Finitely generated. An R-module M is finitely generated if there exist finitely many elements x1, ..., xn in M such that every element of M is a linear combination of those elements with coefficients from the ring R.'b'The left R-modules, together with their module homomorphisms, form a category, written as R-Mod (see category of modules for more.) This is an abelian category.'b'The kernel of a module homomorphism f\xc2\xa0: M \xe2\x86\x92 N is the submodule of M consisting of all elements that are sent to zero by f. The isomorphism theorems familiar from groups and vector spaces are also valid for R-modules.'b'A bijective module homomorphism is an isomorphism of modules, and the two modules are called isomorphic. Two isomorphic modules are identical for all practical purposes, differing solely in the notation for their elements.'b'This, like any homomorphism of mathematical objects, is just a mapping which preserves the structure of the objects. Another name for a homomorphism of modules over R is an R-linear map.'b'If M and N are left R-modules, then a map f\xc2\xa0: M \xe2\x86\x92 N is a homomorphism of R-modules if, for any m, n in M and r, s in R,'b'The set of submodules of a given module M, together with the two binary operations + and \xe2\x88\xa9, forms a lattice which satisfies the modular law: Given submodules U, N1, N2 of M such that N1 \xe2\x8a\x82 N2, then the following two submodules are equal: (N1 + U) \xe2\x88\xa9 N2 = N1 + (U \xe2\x88\xa9 N2).'b'Suppose M is a left R-module and N is a subgroup of M. Then N is a submodule (or R-submodule, to be more explicit) if, for any n in N and any r in R, the product r \xe2\x8b\x85 n is in N (or n \xe2\x8b\x85 r for a right module).'b'If R is commutative, then left R-modules are the same as right R-modules and are simply called R-modules.'b'A bimodule is a module that is a left module and a right module such that the two multiplications are compatible.'b'If one writes the scalar action as fr so that fr(x) = r \xe2\x8b\x85 x, and f for the map that takes each r to its corresponding map fr\xc2\xa0, then the first axiom states that every fr is a group endomorphism of M, and the other three axioms assert that the map f\xc2\xa0: R \xe2\x86\x92 End(M) given by r \xe2\x86\xa6 fr is a ring homomorphism from R to the endomorphism ring End(M).[2] Thus a module is a ring action on an abelian group (cf. group action. Also consider monoid action of multiplicative structure of R). In this sense, module theory generalizes representation theory, which deals with group actions on vector spaces, or equivalently group ring actions.'b'Authors who do not require rings to be unital omit condition 4 above in the definition of an R-module, and so would call the structures defined above "unital left R-modules". In this article, consistent with the glossary of ring theory, all rings and modules are assumed to be unital.[1]'b'The operation of the ring on M is called scalar multiplication, and is usually written by juxtaposition, i.e. as rx for r in R and x in M, though here it is denoted as r \xe2\x8b\x85 x to distinguish it from the ring multiplication operation, denoted here by juxtaposition. The notation RM indicates a left R-module M. A right R-module M or MR is defined similarly, except that the ring acts on the right; i.e., scalar multiplication takes the form \xe2\x8b\x85\xc2\xa0: M \xc3\x97 R \xe2\x86\x92 M, and the above axioms are written with scalars r and s on the right of x and y.'b'Suppose that R is a ring and 1R is its multiplicative identity. A left R-module M consists of an abelian group (M, +) and an operation \xe2\x8b\x85\xc2\xa0: R \xc3\x97 M \xe2\x86\x92 M such that for all r, s in R and x, y in M, we have:'b'Much of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a "well-behaved" ring, such as a principal ideal domain. However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis, and even those that do, free modules, need not have a unique rank if the underlying ring does not satisfy the invariant basis number condition, unlike vector spaces, which always have a (possibly infinite) basis whose cardinality is then unique. (These last two assertions require the axiom of choice in general, but not in the case of finite-dimensional spaces, or certain well-behaved infinite-dimensional spaces such as Lp spaces.)'b'In a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law. In a module, the scalars need only be a ring, so the module concept represents a significant generalization. In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules. In non-commutative algebra the distinction between left ideals, ideals, and modules becomes more pronounced, though some ring-theoretic conditions can be expressed either about left ideals or left modules.'b''b''b'Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.'b'Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is compatible with the ring multiplication.'b'In mathematics, a module is one of the fundamental algebraic structures used in abstract algebra. A module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary given ring (with identity) and a multiplication (on the left and/or on the right) is defined between elements of the ring and elements of the module.'
Field (mathematics)

Free module
b'See local ring, perfect ring and Dedekind ring.'b'Many statements about free modules, which are wrong for general modules over rings, are still true for certain generalisations of free modules. Projective modules are direct summands of free modules, so one can choose an injection in a free module and use the basis of this one to prove something for the projective module. Even weaker generalisations are flat modules, which still have the property that tensoring with them preserves exact sequences, and torsion-free modules. If the ring has special properties, this hierarchy may collapse, e.g., for any perfect local Dedekind ring, every torsion-free module is flat, projective and free as well. A finitely generated torsion-free module of a commutative PID is free. A finitely generated Z-module is free if and only if it is flat.'b'and the scalar multiplication by: for r in R and x in E,'b'We equip it with a structure of a left module such that the addition is defined by: for x in E,'b'Given a ring R and a set E, first as a set we let'b'The free module R(E) may also be constructed in the following equivalent way.'b'A similar argument shows that every free left (resp. right) R-module is isomorphic to a direct sum of copies of R as left (resp. right) module.'b'Given a set E and ring R, there is a free R-module that has E as a basis: namely, the direct sum of copies of R indexed by E'b'Let R be a ring.'b'An immediate consequence of the second half of the definition is that the coefficients in the first half are unique for each element of M.'b'A free module is a module with a basis.[2]'b''b''b'A free abelian group is precisely a free module over the ring Z of integers.'b'Given any set S and ring R, there is a free R-module with basis S, which is called free module on S or module of formal linear combinations of the elements of S.'b'In mathematics, a free module is a module that has a basis \xe2\x80\x93 that is, a generating set consisting of linearly independent elements. Every vector space is a free module,[1] but, if the ring of the coefficients is not a division ring (not a field in the commutative case), then there exist non-free modules.'
Linear map
b'Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.'b'A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.'b'An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).'b'A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.'b'Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.'b'Therefore, the matrix in the new basis is A\xe2\x80\xb2 = B\xe2\x88\x921AB, being B the matrix of the given basis.'b'hence'b'Substituting this in the first expression'b"Given a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v']."b'Let V and W denote vector spaces over a field, F. Let T: V \xe2\x86\x92 W be a linear map.'b'No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.'b'The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \xe2\x86\x92 V \xe2\x86\x92 W \xe2\x86\x92 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\xe2\x80\x93Singer index theorem.[8]'b'For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \xe2\x88\x92 dim(W), by rank\xe2\x80\x93nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.'b'namely the degrees of freedom minus the number of constraints.'b'For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:'b'The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.'b'These can be interpreted thus: given a linear equation f(v) = w to solve,'b'This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequence'b'The number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \xcf\x81(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \xce\xbd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.'b'ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\xe2\x80\x93nullity theorem:'b'If f\xc2\xa0: V \xe2\x86\x92 W is linear, we define the kernel and the image or range of f by'b'If V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \xc3\x97 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \xc3\x97 n invertible matrices with entries in K.'b'An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).'b'A linear transformation f: V \xe2\x86\x92 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \xe2\x86\x92 V.'b'Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.'b'Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.'b'If f\xc2\xa0: V \xe2\x86\x92 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.'b'If f1\xc2\xa0: V \xe2\x86\x92 W and f2\xc2\xa0: V \xe2\x86\x92 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).'b'The inverse of a linear map, when defined, is again a linear map.'b'The composition of linear maps is linear: if f\xc2\xa0: V \xe2\x86\x92 W and g\xc2\xa0: W \xe2\x86\x92 Z are linear, then so is their composition g \xe2\x88\x98 f\xc2\xa0: V \xe2\x86\x92 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.'b'In two-dimensional space R2 linear maps are described by 2 \xc3\x97 2 real matrices. These are some examples:'b'The matrices of a linear transformation can be represented visually:'b'where M is the matrix of f. The symbol \xe2\x88\x97 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.'b'corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),'b'Thus, the function f is entirely determined by the values of aij. If we put these values into an m \xc3\x97 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vector'b'which implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) as'b'If f\xc2\xa0: V \xe2\x86\x92 W is a linear map,'b'Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:'b'If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \xc3\x97 n matrix, then f(x) = Ax describes a linear map Rn \xe2\x86\x92 Rm (see Euclidean space).'b'Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.'b''b''b'In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.'b'A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.'b'An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\xc2\xa0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.'b'In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \xe2\x86\x92 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.'
Linear form
b'In higher dimensions, this generalizes as follows'b'So each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.'b'due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.\xc2\xa0 Then'b'where \xce\xb4 is the Kronecker delta.\xc2\xa0 Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.'b'Or, more succinctly,'b'In an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.\xc2\xa0 There is a mapping V \xe2\x86\x92 V\xe2\x88\x97 into the continuous dual space V\xe2\x88\x97.\xc2\xa0 However, this mapping is antilinear rather than linear.'b'The above defined vector v\xe2\x88\x97 \xe2\x88\x88 V\xe2\x88\x97 is said to be the dual vector of v \xe2\x88\x88 V.'b'The inverse isomorphism is V\xe2\x88\x97 \xe2\x86\x92 V\xc2\xa0: v\xe2\x88\x97 \xe2\x86\xa6 v, where v is the unique element of V such that'b'where the bilinear form on V is denoted \xe2\x9f\xa8 , \xe2\x9f\xa9 (for instance, in Euclidean space \xe2\x9f\xa8v, w\xe2\x9f\xa9 = v \xe2\x8b\x85 w is the dot product of v and w).'b'Every non-degenerate bilinear form on a finite-dimensional vector space V induces an isomorphism V \xe2\x86\x92 V\xe2\x88\x97\xc2\xa0: v \xe2\x86\xa6 v\xe2\x88\x97 such that'b'In finite dimensions, a linear functional can be visualized in terms of its level sets.\xc2\xa0 In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.\xc2\xa0 This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as Gravitation by Misner, Thorne & Wheeler (1973).'b'In the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.'b'Linear functionals are particularly important in quantum mechanics.\xc2\xa0 Quantum mechanical systems are represented by Hilbert spaces, which are anti\xe2\x80\x93isomorphic to their own dual spaces.\xc2\xa0 A state of a quantum mechanical system can be identified with a linear functional.\xc2\xa0 For more information see bra\xe2\x80\x93ket notation.'b'This follows from the fact that the linear functionals evxi\xc2\xa0: f \xe2\x86\x92 f(xi) defined above form a basis of the dual space of Pn.[1]'b'for all f \xe2\x88\x88 Pn. This forms the foundation of the theory of numerical quadrature.'b'The integration functional I defined above defines a linear functional on the subspace Pn of polynomials of degree \xe2\x89\xa4 n. If x0, ..., xn are n + 1 distinct points in [a, b], then there are coefficients a0, ..., an for which'b'If x0, ..., xn are n + 1 distinct points in [a, b], then the evaluation functionals evxi, i = 0, 1, ..., n form a basis of the dual space of Pn.\xc2\xa0 (Lax (1996) proves this last fact using Lagrange interpolation.)'b'The mapping f\xc2\xa0\xe2\x86\x92\xc2\xa0f(c) is linear since'b'Let Pn denote the vector space of real-valued polynomial functions of degree \xe2\x89\xa4n defined on an interval [a,\xc2\xa0b].\xc2\xa0 If c\xc2\xa0\xe2\x88\x88\xc2\xa0[a,\xc2\xa0b], then let evc\xc2\xa0: Pn \xe2\x86\x92 R be the evaluation functional'b'is a linear functional from the vector space C[a,\xc2\xa0b] of continuous functions on the interval [a,\xc2\xa0b] to the real numbers. The linearity of I follows from the standard facts about the integral:'b'Linear functionals first appeared in functional analysis, the study of vector spaces of functions.\xc2\xa0 A typical example of a linear functional is integration: the linear transformation defined by the Riemann integral'b'and each linear functional can be expressed in this form.'b'For each row vector [a1 \xe2\x80\xa6 an] there is a linear functional f defined by'b'Suppose that vectors in the real coordinate space Rn are represented as column vectors'b'If V is a topological vector space, the space of continuous linear functionals \xe2\x80\x94 the continuous dual \xe2\x80\x94 is often simply called the dual space.\xc2\xa0 If V is a Banach space, then so is its (continuous) dual.\xc2\xa0 To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the algebraic dual space.\xc2\xa0 In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.'b''b''b'The set of all linear functionals from V to k, Homk(V,k), forms a vector space over k with the addition of the operations of addition and scalar multiplication (defined pointwise).\xc2\xa0 This space is called the dual space of V, or sometimes the algebraic dual space, to distinguish it from the continuous dual space.\xc2\xa0 It is often written V\xe2\x88\x97, V\xe2\x80\xb2, or V\xe1\x90\xaf when the field k is understood.'b'In linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In \xe2\x84\x9dn, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.\xc2\xa0 In general, if V is a vector space over a field k, then a linear functional f is a function from V to k that is linear:'
Cramer's rule

System of linear equations
b'This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.'b'Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.'b'Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described as'b'There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:'b'These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.'b'Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) \xe2\x89\xa0 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:'b'where A is an m \xc3\x97 n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.'b'A homogeneous system is equivalent to a matrix equation of the form'b'A system of linear equations is homogeneous if all of the constant terms are zero:'b'There is also a quantum algorithm for linear systems of equations.[3]'b'A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.'b'If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.'b'While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.'b"Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]"b'For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.'b'is given by'b"Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the system"b'The last matrix is in reduced row echelon form, and represents the system x = \xe2\x88\x9215, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.'b'There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:'b'Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.'b'This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:'b'In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:'b'Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = \xe2\x88\x9215. Therefore, the solution set is the single point (x, y, z) = (\xe2\x88\x9215, 8, 2).'b'Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:'b'Solving the first equation for x gives x = 5 + 2z \xe2\x88\x92 3y, and plugging this into the second and third equation yields'b'For example, consider the following system:'b'The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:'b'Here x is the free variable, and y and z are dependent.'b'Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:'b'Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.'b'Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.'b'The solution set to this system can be described by the following equations:'b'For example, consider the following system:'b'To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.'b'There are several algorithms for solving a system of linear equations.'b'Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.'b'Putting it another way, according to the Rouch\xc3\xa9\xe2\x80\x93Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.'b'In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.'b'are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.'b'It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equations'b'are inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.'b'For example, the equations'b'A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.'b'are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.'b'For a more complicated example, the equations'b'are not independent \xe2\x80\x94 they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.'b'For example, the equations'b'The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.'b'A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.'b'It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).'b'The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.'b'The following pictures illustrate this trichotomy in the case of two variables:'b'In the first case, the dimension of the solution set is, in general, equal to n \xe2\x88\x92 m, where n is the number of variables and m is the number of equations.'b'In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.'b'For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.'b'For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]'b'For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.'b'A linear system may behave in any one of three possible ways:'b'A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.'b'The number of vectors in a basis for the span is now expressed as the rank of the matrix.'b'where A is an m\xc3\x97n matrix, x is a column vector with n entries, and b is a column vector with m entries.'b'The vector equation is equivalent to a matrix equation of the form'b'This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.'b'One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.'b'Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.'b'A general system of m linear equations with n unknowns can be written as'b'Now substitute this expression for x into the bottom equation:'b'The simplest kind of linear system involves two equations and two variables:'b''b''b'Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gr\xc3\xb6bner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.'b'In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.'b'since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.'b'is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by'b'In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,'
Homogeneous coordinates

Linear equation
b'If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n\xc2\xa0\xe2\x80\x93\xc2\xa01)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).'b'In other words, if ai \xe2\x89\xa0 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.'b'If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 \xe2\x89\xa0 0, and rewrite the equation'b'If all the coefficients are zero, then either b \xe2\x89\xa0 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.'b'where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.'b'A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewritten'b'An everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point\xe2\x80\x93slope form or slope\xe2\x80\x93intercept form.'b'where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.'b'and'b'A linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:'b'This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).'b'This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.'b'Ergo,'b'Thus,'b'One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.'b'In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.'b'and'b'These are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:'b'and'b'Since this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.'b'becomes:'b'Further, this representation extends to systems of linear equations.'b'one can rewrite the equation in matrix form:'b'Using the order of the standard form'b'where a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.'b'Using a determinant, one gets a determinant form, easy to remember:'b'Expanding the products and regrouping the terms leads to the general form:'b'Multiplying both sides of this equation by (x2\xc2\xa0\xe2\x88\x92\xc2\xa0x1) yields a form of the line generally referred to as the symmetric form:'b'where (x1,\xc2\xa0y1) and (x2,\xc2\xa0y2) are two points on the line with x2 \xe2\x89\xa0 x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2\xc2\xa0\xe2\x88\x92\xc2\xa0y1)/(x2\xc2\xa0\xe2\x88\x92\xc2\xa0x1).'b'The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y\xc2\xa0\xe2\x88\x92\xc2\xa0y1) is proportional to the difference in the x coordinate (that is, x\xc2\xa0\xe2\x88\x92\xc2\xa0x1). The proportionality constant is m (the slope of the line).'b'where m is the slope of the line and (x1,y1) is any point on the line.'b'where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.'b'where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.'b'where A and B are not both equal to zero. The equation is usually written so that A \xe2\x89\xa5 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is \xe2\x88\x92A/B. The general form is sometimes written as:'b'In the general (or standard[1]) form the linear equation is written as:'b'Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and \xce\xb8 are variables; other letters represent constants (fixed numbers).'b'Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.'b'where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.'b'A common form of a linear equation in the two variables x and y is'b'If a = 0, then, when b = 0 every number is a solution of the equation, but if b \xe2\x89\xa0 0 there are no solutions (and the equation is said to be inconsistent.)'b'If a \xe2\x89\xa0 0, there is a unique solution'b'A linear equation in one unknown x may always be rewritten'b''b''b'This article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.'b'Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a \xe2\x89\xa0 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.'b'Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.'b'A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a \xe2\x89\xa0 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).'
Inner product space
b'As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) \xe2\x80\x93 the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) \xe2\x80\x93 and in this context the exterior product is usually called the "outer (alternatively, wedge) product". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).'b'The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.'b'More abstractly, the outer product is the bilinear map W \xc3\x97 V\xe2\x88\x97 \xe2\x86\x92 Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V\xe2\x88\x97 \xc3\x97 V \xe2\x86\x92 F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.'b'In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".'b'On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V \xe2\x86\x92 V\xe2\x88\x97) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.'b'The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 \xc3\x97 n covector with an n \xc3\x97 1 vector, yielding a 1\xc2\xa0\xc3\x97\xc2\xa01 matrix (a scalar), while the outer product is the product of an m \xc3\x97 1 vector with a 1 \xc3\x97 n covector, yielding an m \xc3\x97 n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).'b'Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V \xe2\x86\x92 V\xe2\x88\x97) and thus hold more generally.'b'Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that \xe2\x9f\xa8x,y\xe2\x9f\xa9 \xe2\x89\xa0 0, though y need not equal x; in other words, the induced map to the dual space V \xe2\x86\x92 V\xe2\x88\x97 is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester\'s law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "\xe2\x88\x92" to them differs depending on conventions).'b'This construction is used in numerous contexts. The Gelfand\xe2\x80\x93Naimark\xe2\x80\x93Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.'b'makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x\xc2\xa0: ||x|| = 0}. The sesquilinear form \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9 factors through W.'b'If V is a vector space and \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xc2\xb7\xc2\xb7\xe2\x9f\xa9 a semi-definite sesquilinear form, then the function:'b'Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.'b'From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.'b'Several types of linear maps A from an inner product space V to an inner product space W are of relevance:'b'Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [\xe2\x88\x92\xcf\x80,\xcf\x80] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.'b'Orthogonality of the sequence {ek}k follows immediately from the fact that if k \xe2\x89\xa0 j, then'b'is an isometric linear map with dense image.'b'is an orthonormal basis of the space C[\xe2\x88\x92\xcf\x80,\xcf\x80] with the L2 inner product. The mapping'b'Theorem. Let V be the inner product space C[\xe2\x88\x92\xcf\x80,\xcf\x80]. Then the sequence (indexed on set of all integers) of continuous functions'b'This theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:'b'is an isometric linear map V \xe2\x86\x92 l2 with a dense image.'b'Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of\xc2\xa0V. Then the map'b"Parseval's identity leads immediately to the following theorem:"b"The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]"b'Theorem. Any complete inner product space V has an orthonormal basis.'b'Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show that'b'Theorem. Any separable inner product space V has an orthonormal basis.'b'Using an infinite-dimensional analog of the Gram-Schmidt process one may show:'b'if \xce\xb1 \xe2\x89\xa0 \xce\xb2 and \xe2\x9f\xa8e\xce\xb1,e\xce\xb1\xe2\x9f\xa9 = ||e\xce\xb1|| = 1 for all \xce\xb1, \xce\xb2 \xe2\x88\x88 A.'b'is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis and'b'This definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collection'b'Let V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram\xe2\x80\x93Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if \xe2\x9f\xa8ei,ej\xe2\x9f\xa9 = 0 for every i \xe2\x89\xa0 j and \xe2\x9f\xa8ei,ei\xe2\x9f\xa9 = ||ei|| = 1 for each i.'b'This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:'b'However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:'b'is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]'b'A linear space with a norm such as:'b'is an inner product.'b'For real matrices of the same size, \xe2\x9f\xa8A,B\xe2\x9f\xa9\xc2\xa0:= tr(ABT) with transpose as conjugation'b'is an inner product.[6][7][8] In this case, \xe2\x9f\xa8X,X\xe2\x9f\xa9 = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.'b'For real random variables X and Y, the expected value of their product'b'This sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.'b'This space is not complete; consider for example, for the interval [\xe2\x88\x921,1] the sequence of continuous "step" functions, {\xe2\x80\x89fk}k, defined by:'b'The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product is'b'where M is any Hermitian positive-definite matrix and y\xe2\x80\xa0 is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.'b'The general form of an inner product on Cn is known as the Hermitian form and is given by'b'where xT is the transpose of x.'b'More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.'b'A simple example is the real numbers with the standard multiplication as the inner product'b'is also known as additivity.'b'The property of an inner product space V that'b'Assuming the underlying field to be R, the inner product becomes symmetric, and we obtain'b'Combining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:'b'From the linearity property it is derived that x = 0 implies \xe2\x9f\xa8x,x\xe2\x9f\xa9 = 0. while from the positive-definiteness axiom we obtain the converse, \xe2\x9f\xa8x,x\xe2\x9f\xa9 = 0 implies x = 0. Combining these two, we have the property that \xe2\x9f\xa8x,x\xe2\x9f\xa9 = 0 if and only if x = 0.'b'In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.'b'so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.'b'Conjugate symmetry and linearity in the first variable gives'b'Moreover, sesquilinearity (see below) implies that'b'Notice that conjugate symmetry implies that \xe2\x9f\xa8x,x\xe2\x9f\xa9 is real for all x, since we have:'b'When F = R, conjugate symmetry reduces to symmetry. That is, \xe2\x9f\xa8x,y\xe2\x9f\xa9 = \xe2\x9f\xa8y,x\xe2\x9f\xa9 for F = R; while for F = C, \xe2\x9f\xa8x,y\xe2\x9f\xa9 is equal to the complex conjugate.'b'In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that \xe2\x9f\xa8x,x\xe2\x9f\xa9 is only required to be non-negative. We show how to treat these below.'b'There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.'b'Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product \xe2\x9f\xa8x,y\xe2\x9f\xa9 as \xe2\x9f\xa8y|x\xe2\x9f\xa9 (the bra\xe2\x80\x93ket notation of quantum mechanics), respectively y\xe2\x80\xa0x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V\xe2\x88\x97, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking \xe2\x9f\xa8x,y\xe2\x9f\xa9 to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9 and \xe2\x9f\xa8\xc2\xb7|\xc2\xb7\xe2\x9f\xa9 as distinct notations differing only in which argument is conjugate linear.'b'that satisfies the following three axioms for all vectors x, y, z \xe2\x88\x88 V and all scalars a \xe2\x88\x88 F:[2][3]'b'Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a map'b'In this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.'b''b''b'An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.'b'In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]'
Partial differential equation
b'Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. "Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods conserve mass by design.'b'Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.'b"The finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge\xe2\x80\x93Kutta, etc."b'The three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM), as well other kind of methods called Meshfree methods, which were made to solve problems where the before mentioned methods are limited. The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other hybrid versions of FEM and Meshfree methods include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.'b"The adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality."b'Symmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.'b"A general approach to solving PDE's uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, B\xc3\xa4cklund transform and finally finding exact analytic solutions to the PDE."b"From 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact."b'In some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.'b'The method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.'b'Nevertheless, some techniques can be used for several types of equations. The h-principle is the most powerful method to solve underdetermined equations. The Riquier\xe2\x80\x93Janet theory is an effective method for obtaining information about many analytic overdetermined systems.'b'There are no generally applicable methods to solve nonlinear PDEs. Still, existence and uniqueness results (such as the Cauchy\xe2\x80\x93Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schr\xc3\xb6dinger equation.'b'This is analogous in signal processing to understanding a filter by its impulse response.'b'Inhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.'b'by the change of variables (for complete details see Solution of the Black Scholes Equation at the Wayback Machine (archived April 11, 2008))'b'is reducible to the heat equation'b'Often a PDE can be reduced to a simpler form with a known solution by a suitable change of variables. For example, the Black\xe2\x80\x93Scholes PDE'b'If the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example of the use of a Fourier integral.'b'An important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.'b'An integral transform may transform the PDE to a simpler one, in particular, a separable PDE. This corresponds to diagonalizing an operator.'b'More generally, one may find characteristic surfaces.'b'In special cases, one can find characteristic curves on which the equation reduces to an ODE \xe2\x80\x93 changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.'b'This generalizes to the method of characteristics, and is also used in integral transforms.'b'This is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices \xe2\x80\x93 thinking of "the value for fixed x" as a coordinate, each coordinate can be understood separately.'b'In the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable \xe2\x80\x93 these are in turn easier to solve.'b'Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can find any solution that solves the equation and satisfies the boundary conditions, then it is the solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.[2]'b"In the phase space formulation of quantum mechanics, one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of \xc4\xa7. The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function."b'which is called elliptic-hyperbolic because it is elliptic in the region x < 0, hyperbolic in the region x > 0, and degenerate parabolic on the line x = 0.'b'If a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler\xe2\x80\x93Tricomi equation'b'The geometric interpretation of this condition is as follows: if data for u are prescribed on the surface S, then it may be possible to determine the normal derivative of u on S from the differential equation. If the data on S and the differential equation determine the normal derivative of u on S, then S is non-characteristic. If the data on S and the differential equation do not determine the normal derivative of u on S, then the surface is characteristic, and the differential equation restricts the data on S: the differential equation is internal to S.'b'where \xcf\x86 has a non-zero gradient, then S is a characteristic surface for the operator L at a given point if the characteristic form vanishes:'b'where the coefficient matrices A\xce\xbd and the vector B may depend upon x and u. If a hypersurface S is given in the implicit form'b'The classification of partial differential equations can be extended to systems of first-order equations, where the unknown u is now a vector with m components, and the coefficient matrices A\xce\xbd are m by m matrices for \xce\xbd = 1, ..., n. The partial differential equation takes the form'b'The classification depends upon the signature of the eigenvalues of the coefficient matrix ai,j..'b'If there are n independent variables x1, x2 , ..., xn, a general linear partial differential equation of second order has the form'b'More precisely, replacing \xe2\x88\x82x by X, and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.'b'Some linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others such as the Euler\xe2\x80\x93Tricomi equation have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions, and to the smoothness of the solutions.'b'where \xce\x94 is the Laplace operator.'b'or'b'In PDEs, it is common to denote partial derivatives using subscripts. That is:'b'This solution approaches infinity if nx is not an integer multiple of \xcf\x80 for any non-zero value of y. The Cauchy problem for the Laplace equation is called ill-posed or not well-posed, since the solution does not continuously depend on the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.'b'where n is an integer. The derivative of u with respect to y approaches 0 uniformly in x as n increases, but the solution is'b'with boundary conditions'b'An example of pathological behavior is the sequence (depending upon n) of Cauchy problems for the Laplace equation'b'Although the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard\xe2\x80\x93Lindel\xc3\xb6f theorem, that is far from the case for partial differential equations. The Cauchy\xe2\x80\x93Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties. The mathematical study of these questions is usually in the more powerful context of weak solutions.'b'where c is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function f(y) can be determined if u is specified on the line x = 0.'b'which has the solution'b'where f is an arbitrary function of y. The analogous ordinary differential equation is'b"This relation implies that the function u(x,y) is independent of x. However, the equation gives no information on the function's dependence on the variable y. Hence the general solution of this equation is"b'A relatively simple PDE is'b"If f is a linear function of u and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein\xe2\x80\x93Gordon equation, and Poisson's equation."b'Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six parameters[1], but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the fluid occur in an infinite-dimensional configuration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again, there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid dynamics, electrodynamics, and heat transfer.'b''b''b'PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.'b'In mathematics, a partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.'
Dirichlet conditions

Fourier series
b'In 1922, Andrey Kolmogorov published an article titled "Une s\xc3\xa9rie de Fourier-Lebesgue divergente presque partout" in which he gave an example of a Lebesgue-integrable function whose Fourier series diverges almost everywhere. He later constructed an example of an integrable function whose Fourier series diverges everywhere (Katznelson 1976).'b'Since Fourier series have such good convergence properties, many are often surprised by some of the negative results. For example, the Fourier series of a continuous T-periodic function need not converge pointwise. The uniform boundedness principle yields a simple non-constructive proof of this fact.'b'These theorems, and informal variations of them that don\'t specify the convergence conditions, are sometimes referred to generically as "Fourier\'s theorem" or "the Fourier theorem".[14][15][16][17]'b"Many other results concerning the convergence of Fourier series are known, ranging from the moderately simple result that the series converges at x if f is differentiable at x, to Lennart Carleson's much more sophisticated result that the Fourier series of an L2 function actually converges almost everywhere."b'Because of the least squares property, and because of the completeness of the Fourier basis, we obtain an elementary convergence result.'b"Note that fN is a trigonometric polynomial of degree N. Parseval's theorem implies that"b'We say that p is a trigonometric polynomial of degree N when it is of the form'b'This is called a partial sum. We would like to know, in which sense does fN(x) converge to f(x) as N \xe2\x86\x92 \xe2\x88\x9e.'b'This generalizes the Fourier transform to L1(G) or L2(G), where G is an LCA group. If G is compact, one also obtains a Fourier series, which converges similarly to the [\xe2\x88\x92\xcf\x80,\xc2\xa0\xcf\x80] case, but if G is noncompact, one obtains instead a Fourier integral. This generalization yields the usual Fourier transform when the underlying locally compact Abelian group is R.'b'The generalization to compact groups discussed above does not generalize to noncompact, nonabelian groups. However, there is a straightforward generalization to Locally Compact Abelian (LCA) groups.'b'If the domain is not a group, then there is no intrinsically defined convolution. However, if X is a compact Riemannian manifold, it has a Laplace\xe2\x80\x93Beltrami operator. The Laplace\xe2\x80\x93Beltrami operator is the differential operator that corresponds to Laplace operator for the Riemannian manifold X. Then, by analogy, one can consider heat equations on X. Since Fourier arrived at his basis by attempting to solve the heat equation, the natural generalization is to use the eigensolutions of the Laplace\xe2\x80\x93Beltrami operator as a basis. This generalizes Fourier series to spaces of the type L2(X), where X is a Riemannian manifold. The Fourier series converges in ways similar to the [\xe2\x88\x92\xcf\x80,\xc2\xa0\xcf\x80] case. A typical example is to take X to be the sphere with the usual metric, in which case the Fourier basis consists of spherical harmonics.'b'An alternative extension to compact groups is the Peter\xe2\x80\x93Weyl theorem, which proves results about representations of compact groups analogous to those about finite groups.'b'One of the interesting properties of the Fourier transform which we have mentioned, is that it carries convolutions to pointwise products. If that is the property which we seek to preserve, one can produce Fourier series on any compact group. Typical examples include those classical groups that are compact. This generalizes the Fourier transform to all spaces of the form L2(G), where G is a compact group, in such a way that the Fourier transform carries convolutions to pointwise products. The Fourier series exists and converges in similar ways to the [\xe2\x88\x92\xcf\x80,\xcf\x80] case.'b'furthermore, the sines and cosines are orthogonal to the constant function 1. An orthonormal basis for L2([\xe2\x88\x92\xcf\x80,\xcf\x80]) consisting of real functions is formed by the functions 1 and \xe2\x88\x9a2\xc2\xa0cos(nx),\xe2\x80\x89 \xe2\x88\x9a2\xc2\xa0sin(nx) with n\xc2\xa0= 1,\xc2\xa02,...\xc2\xa0 The density of their span is a consequence of the Stone\xe2\x80\x93Weierstrass theorem, but follows also from the properties of classical kernels like the Fej\xc3\xa9r kernel.'b'(where \xce\xb4mn is the Kronecker delta), and'b'This corresponds exactly to the complex exponential formulation given above. The version with sines and cosines is also justified with the Hilbert space interpretation. Indeed, the sines and cosines form an orthogonal set:'b'The basic Fourier series result for Hilbert spaces can be written as'b'We can write now h(K) as an integral with the traditional coordinate system over the volume of the primitive cell, instead of with the x1, x2 and x3 variables:'b'(it may be advantageous for the sake of simplifying calculations, to work in such a cartesian coordinate system, in which it just so happens that a1 is parallel to the x axis, a2 lies in the x-y plane, and a3 has components of all three axes). The denominator is exactly the volume of the primitive unit cell which is enclosed by the three primitive-vectors a1, a2 and a3. In particular, we now know that'b'which after some calculation and applying some non-trivial cross-product identities can be shown to be equal to:'b'we can solve this system of three linear equations for x, y, and z in terms of x1, x2 and x3 in order to calculate the volume element in the original cartesian coordinate system. Once we have x, y, and z in terms of x1, x2 and x3, we can calculate the Jacobian determinant:'b'Assuming'b'where'b'And so it is clear that in our expansion, the sum is actually over reciprocal lattice vectors:'b'Re-arranging:'b'We write g as:'b'Finally applying the same for the third coordinate, we define:'b'We can write g once again as:'b'Further defining:'b'And then we can write:'b'If we write a series for g on the interval [0, a1] for x1, we can define the following:'b'Thus we can define a new function,'b'where ai = |ai|.'b'where ni are integers and ai are three linearly independent vectors. Assuming we have some function, f(r), such that it obeys the following condition for any Bravais lattice vector R: f(r) = f(r\xc2\xa0+\xc2\xa0R), we could make a Fourier series of it. This kind of function can be, for example, the effective potential that one electron "feels" inside a periodic crystal. It is useful to make a Fourier series of the potential then when applying Bloch\'s theorem. First, we may write any arbitrary vector r in the coordinate-system of the lattice:'b'The three-dimensional Bravais lattice is defined as the set of vectors of the form:'b'Aside from being useful for solving partial differential equations such as the heat equation, one notable application of Fourier series on the square is in image compression. In particular, the jpeg image compression standard uses the two-dimensional discrete cosine transform, which is a Fourier transform using the cosine basis functions.'b'We can also define the Fourier series for functions of two variables x and y in the square [\xe2\x88\x92\xcf\x80,\xc2\xa0\xcf\x80]\xc2\xa0\xc3\x97\xc2\xa0[\xe2\x88\x92\xcf\x80,\xc2\xa0\xcf\x80]:'b'Many other Fourier-related transforms have since been defined, extending the initial idea to other applications. This general area of inquiry is now sometimes called harmonic analysis. A Fourier series, however, can be used only for periodic functions, or for functions on a bounded (compact) interval.'b"Since Fourier's time, many different approaches to defining and understanding the concept of Fourier series have been discovered, all of which are consistent with one another, but each of which emphasizes different aspects of the topic. Some of the more powerful and elegant approaches are based on mathematical ideas and tools that were not available at the time Fourier completed his original work. Fourier originally defined the Fourier series for real-valued functions of real arguments, and using the sine and cosine functions as the basis set for the decomposition."b'When Fourier submitted a later competition essay in 1811, the committee (which included Lagrange, Laplace, Malus and Legendre, among others) concluded: ...the manner in which the author arrives at these equations is not exempt of difficulties and...his analysis to integrate them still leaves something to be desired on the score of generality and even rigour.[citation needed]'b"In these few lines, which are close to the modern formalism used in Fourier series, Fourier revolutionized both mathematics and physics. Although similar trigonometric series were previously used by Euler, d'Alembert, Daniel Bernoulli and Gauss, Fourier believed that such trigonometric series could represent any arbitrary function. In what sense that is actually true is a somewhat subtle issue and the attempts over many years to clarify this idea have led to important discoveries in the theories of convergence, function spaces, and harmonic analysis."b'This immediately gives any coefficient ak of the trigonometrical series for \xcf\x86(y) for any function which has such an expansion. It works because if \xcf\x86 has such an expansion, then (under suitable convergence assumptions) the integral'b'The constructed function S(f) is therefore commonly referred to as a Fourier transform, even though the Fourier integral of a periodic function is not convergent at the harmonic frequencies.[nb 2]'b'Another commonly used frequency domain representation uses the Fourier series coefficients to modulate a Dirac comb:'b'In engineering, particularly when the variable x represents time, the coefficient sequence is called a frequency domain representation. Square brackets are often used to emphasize that the domain of this function is a discrete set of frequencies.'b"Another application of this Fourier series is to solve the Basel problem by using Parseval's theorem. The example generalizes and one may compute \xce\xb6(2n), for any positive integer\xc2\xa0n."b"Here, sinh is the hyperbolic sine function. This solution of the heat equation is obtained by multiplying each term of \xc2\xa0Eq.1 by sinh(ny)/sinh(n\xcf\x80). While our example function s(x) seems to have a needlessly complicated Fourier series, the heat distribution T(x,\xc2\xa0y) is nontrivial. The function T cannot be written as a closed-form expression. This method of solving the heat problem was made possible by Fourier's work."b"The Fourier series expansion of our function in Example 1 looks more complicated than the simple formula s(x) = x/\xcf\x80, so it is not immediately apparent why one would need the Fourier series. While there are many applications, Fourier's motivation was in solving the heat equation. For example, consider a metal plate in the shape of a square whose side measures \xcf\x80 meters, with coordinates (x,\xc2\xa0y) \xe2\x88\x88 [0,\xc2\xa0\xcf\x80]\xc2\xa0\xc3\x97\xc2\xa0[0,\xc2\xa0\xcf\x80]. If there is no heat source within the plate, and if three of the four sides are held at 0 degrees Celsius, while the fourth side, given by y\xc2\xa0=\xc2\xa0\xcf\x80, is maintained at the temperature gradient T(x,\xc2\xa0\xcf\x80) = x degrees Celsius, for x in (0,\xc2\xa0\xcf\x80), then one can show that the stationary heat distribution (or the heat distribution after a long period of time has elapsed) is given by"b'This example leads us to a solution to the Basel problem.'b'When x\xc2\xa0= \xcf\x80, the Fourier series converges to 0, which is the half-sum of the left- and right-limit of s at x\xc2\xa0= \xcf\x80. This is a particular instance of the Dirichlet theorem for Fourier series.'b'It can be proven that Fourier series converges to s(x) at every point x where s is differentiable, and therefore:'b'In this case, the Fourier coefficients are given by'b'We now use the formula above to give a Fourier series expansion of a very simple function. Consider a sawtooth wave'b"In engineering applications, the Fourier series is generally presumed to converge everywhere except at discontinuities, since the functions encountered in engineering are more well behaved than the ones that mathematicians can provide as counter-examples to this presumption. In particular, if s is continuous and the derivative of s(x) (which may not exist everywhere) is square integrable, then the Fourier series of s converges absolutely and uniformly to s(x).[11]\xc2\xa0 If a function is square-integrable on the interval [x0, x0+P], then the Fourier series converges to the function at almost every point. Convergence of Fourier series also depends on the finite number of maxima and minima in a function which is popularly known as one of the Dirichlet's condition for Fourier series. See Convergence of Fourier series. It is possible to define Fourier coefficients for more general functions or distributions, in such cases convergence in norm or weak convergence is usually of interest."b'This is the same formula as before except cn and c\xe2\x88\x92n are no longer complex conjugates. The formula for cn is also unchanged:'b'Both components of a complex-valued function are real-valued functions that can be represented by a Fourier series. The two sets of coefficients and the partial sum are given by:'b'When the coefficients (known as Fourier coefficients) are computed as follows:[10]'b'The inverse relationships between the coefficients are:'b'where:'b'we can also write the function in these equivalent forms:'b'In this section, s(x) denotes a function of the real variable x, and s is integrable on an interval [x0,\xc2\xa0x0\xc2\xa0+\xc2\xa0P], for real numbers x0 and\xc2\xa0P. We will attempt to represent \xc2\xa0s\xc2\xa0 in that interval as an infinite sum, or series, of harmonically related sinusoidal functions. Outside the interval, the series is periodic with period\xc2\xa0P (frequency\xc2\xa01/P). It follows that if s also has that property, the approximation is valid on the entire real line. We can begin with a finite summation (or partial sum):'b'Although the original motivation was to solve the heat equation, it later became obvious that the same techniques could be applied to a wide array of mathematical and physical problems, and especially those involving linear differential equations with constant coefficients, for which the eigensolutions are sinusoids. The Fourier series has many such applications in electrical engineering, vibration analysis, acoustics, optics, signal processing, image processing, quantum mechanics, econometrics,[8] thin-walled shell theory,[9] etc.'b"From a modern point of view, Fourier's results are somewhat informal, due to the lack of a precise notion of function and integral in the early nineteenth century. Later, Peter Gustav Lejeune Dirichlet[4] and Bernhard Riemann[5][6][7] expressed Fourier's results with greater precision and formality."b"The heat equation is a partial differential equation. Prior to Fourier's work, no solution to the heat equation was known in the general case, although particular solutions were known if the heat source behaved in a simple way, in particular, if the heat source was a sine or cosine wave. These simple solutions are now sometimes called eigensolutions. Fourier's idea was to model a complicated heat source as a superposition (or linear combination) of simple sine and cosine waves, and to write the solution as a superposition of the corresponding eigensolutions. This superposition or linear combination is called the Fourier series."b"The Fourier series is named in honour of Jean-Baptiste Joseph Fourier (1768\xe2\x80\x931830), who made important contributions to the study of trigonometric series, after preliminary investigations by Leonhard Euler, Jean le Rond d'Alembert, and Daniel Bernoulli.[nb 1] Fourier introduced the series for the purpose of solving the heat equation in a metal plate, publishing his initial results in his 1807 M\xc3\xa9moire sur la propagation de la chaleur dans les corps solides (Treatise on the propagation of heat in solid bodies), and publishing his Th\xc3\xa9orie analytique de la chaleur (Analytical theory of heat) in 1822. The M\xc3\xa9moire introduced Fourier analysis, specifically Fourier series. Through Fourier's research the fact was established that an arbitrary (continuous)[2] function can be represented by a trigonometric series. The first announcement of this great discovery was made by Fourier in 1807, before the French Academy.[3] Early ideas of decomposing a periodic function into the sum of simple oscillating functions date back to the 3rd century BC, when ancient astronomers proposed an empiric model of planetary motions, based on deferents and epicycles."b''b''b'In mathematics, a Fourier series (English: /\xcb\x88f\xca\x8a\xc9\x99ri\xcb\x8ce\xc9\xaa/)[1] is a way to represent a function as the sum of simple sine waves. More formally, it decomposes any periodic function or periodic signal into the sum of a (possibly infinite) set of simple oscillating functions, namely sines and cosines (or, equivalently, complex exponentials). The discrete-time Fourier transform is a periodic function, often defined in terms of a Fourier series. The Z-transform, another example of application, reduces to a Fourier series for the important case |z|=1. Fourier series are also central to the original proof of the Nyquist\xe2\x80\x93Shannon sampling theorem. The study of Fourier series is a branch of Fourier analysis.'
Linear least squares (mathematics)
b'Matrix calculations, like any other, are affected by rounding errors. An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson.[14]'b'Fitting of linear models by least squares often, but not always, arise in the context of statistical analysis. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.'b'Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can be'b'The numerical methods for linear least squares are important because linear regression models are among the most important types of model, both as formal statistical models and for exploration of data-sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to round-off error.'b'and the best fit can be found by solving the normal equations.'b'so to minimize the function'b'Ideally, the model function fits the data exactly, so'b'Often it is of interest to solve a linear least squares problem with an additional constraint on the solution. With constrained linear least squares, the original equation'b'These values can be used for a statistical criterion as to the goodness of fit. When unit weights are used, the numbers should be divided by the variance of an observation.'b'The optimal value of the objective function, found by substituting in the optimal expression for the coefficient vector, can be written as (assuming unweighted observations)'b"If experimental error follows a normal distribution, then, because of the linear relationship between residuals and observations, so should residuals,[9] but since the observations are only a sample of the population of all possible observations, the residuals should belong to a Student's t-distribution. Studentized residuals are useful in making a statistical test for an outlier when a particular residual appears to be excessively large."b'Thus, in the motivational example, above, the fact that the sum of residual values is equal to zero it is not accidental but is a consequence of the presence of the constant term, \xce\xb1, in the model.'b'The sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by XT:'b'Thus the residuals are correlated, even if the observations are not.'b'and I is the identity matrix. The variance-covariance matrix of the residuals, Mr is given by'b'where H is the idempotent matrix known as the hat matrix:'b'The residuals are related to the observations by'b"When the number of observations is relatively small, Chebychev's inequality can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively."b"The assumption is not unreasonable when m\xc2\xa0>>\xc2\xa0n. If the experimental errors are normally distributed the parameters will belong to a Student's t-distribution with m\xc2\xa0\xe2\x88\x92\xc2\xa0n degrees of freedom. When m\xc2\xa0>>\xc2\xa0n Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to sampling error.[8]"b'where S is the minimum value of the (weighted) objective function:'b'When W = M\xe2\x88\x921, this simplifies to'b'Therefore, an expression for the residuals (i.e., the estimated errors in the parameters) can be obtained by error propagation from the errors in the observations. Let the variance-covariance matrix for the observations be denoted by M and that of the parameters by M\xce\xb2. Then'b'The estimated parameter values are linear combinations of the observed values'b'This method is used in iteratively reweighted least squares.'b'The weights should, ideally, be equal to the reciprocal of the variance of the measurement.[6][7] The normal equations are then:'b'where wi > 0 is the weight of the ith observation, and W is the diagonal matrix of such weights.'b'In some cases the observations may be weighted\xe2\x80\x94for example, they may not be equally reliable. In this case, one can minimize the weighted sum of squares:'b'An assumption underlying the treatment given above is that the independent variable, x, is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares or more generally errors-in-variables models, or rigorous least squares, should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure.[4][5]'b'These properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.'b'However, in the case that the experimental errors do belong to a normal distribution, the least-squares estimator is also a maximum likelihood estimator.[3]'b'For example, it is easy to show that the arithmetic mean of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss\xe2\x80\x93Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.'b'The equation and solution of linear least squares are thus described as follows:'b'The gradient equations at the minimum can be written as'b"is a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, XTX, is very ill-conditioned (i.e. if its condition number multiplied by the machine's relative round-off error is appreciably large). In that case, including the smallest singular values in the inversion merely adds numerical noise to the solution. This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to factor analysis."b'and thus,'b'An alternative decomposition of X is the singular value decomposition (SVD)[2]'b'These equations are easily solved as R is upper triangular.'b"Since v doesn't depend on \xce\xb2, the minimum value of s is attained when the upper block, u, is zero. Therefore, the parameters are found by solving:"b'Because Q is orthogonal, the sum of squares of the residuals, s, may be written as:'b'The residual vector is left-multiplied by QT.'b'The matrix X is subjected to an orthogonal decomposition, e.g., the QR decomposition as follows.'b'The residuals are written in matrix notation as'b'Orthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more numerically stable because they avoid forming the product XTX.'b'Both substitutions are facilitated by the triangular nature of R.'b'The solution is obtained in two stages, a forward substitution step, solving for z:'b'If the matrix XTX is well-conditioned and positive definite, implying that it has full rank, the normal equations can be solved directly by using the Cholesky decomposition RTR, where R is an upper triangular matrix, giving:'b'where X+ is the Moore\xe2\x80\x93Penrose pseudoinverse of X. Although this equation is correct and can work in many applications, it is not computationally efficient to invert the normal-equations matrix (the Gramian matrix). An exception occurs in numerical smoothing and differentiation where an analytical expression is required.'b'The algebraic solution of the normal equations with a full-rank matrix XTX can be written as'b'simply because'b'In matrix form:'b'and the derivatives change into'b'and therefore minimized exactly when'b'can be written as'b'The normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimize'b'The normal equations are written in matrix notation as'b'Upon rearrangement, we obtain the normal equations:'b'Substitution of the expressions for the residuals and the derivatives into the gradient equations gives'b'The derivatives are'b'Given that S is convex, it is minimized when its gradient vector is zero (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further \xe2\x80\x93 see maxima and minima.) The elements of the gradient vector are the partial derivatives of S with respect to the parameters:'b'where the objective function S is given by'b'where'b'Consider an overdetermined system'b'and solved'b'The partial derivatives with respect to the parameters (this time there is only one) are again computed and set to 0:'b'This results in a system of two equations in two unknowns, called the normal equations, which when solved give'b'The "error", at each point, between the curve fit and the data is the difference between the right- and left-hand sides of the equations above. The least squares approach to solving this problem is to try to make the sum of the squares of these errors as small as possible; that is, to find the minimum of the function'b'of four equations in two unknowns in some "best" sense.'b''b''b'In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See Outline of regression analysis for an outline of the topic.'b'Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called linear least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator.'b'In statistics and mathematics, linear least squares is an approach to fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system.'
Triangular matrix
b'Forward substitution is used in financial bootstrapping to construct a yield curve.'b'A matrix equation with an upper triangular matrix U can be solved in an analogous way, only working backwards.'b'The resulting formulas are:'b'The matrix equation Lx = b can be written as a system of linear equations'b'Notice that this does not require inverting the matrix.'b'The group of 2 by 2 upper unitriangular matrices is isomorphic to the additive group of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic M\xc3\xb6bius transformations; the 3 by 3 upper unitriangular matrices form the Heisenberg group.'b'The stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are not all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called parabolic subgroups.'b'The upper triangular matrices are precisely those that stabilize the standard flag. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are Borel subgroups. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.'b'The set of invertible triangular matrices of a given kind (upper or lower) forms a group, indeed a Lie group, which is a subgroup of the general linear group of all invertible matrices. Note that a triangular matrix is invertible precisely when its diagonal entries are invertible (non-zero).'b'A non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a trapezoid.'b'Because the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an algebra. Algebras of upper triangular matrices have a natural generalization in functional analysis which yields nest algebras on Hilbert spaces.'b"This is generalized by Lie's theorem, which shows that any representation of a solvable Lie algebra is simultaneously upper triangularizable, the case of commuting matrices being the abelian Lie algebra case, abelian being a fortiori solvable."b'In the case of complex matrices, it is possible to say more about triangularization, namely, that any square matrix A has a Schur decomposition. This means that A is unitarily equivalent (i.e. similar, using a unitary matrix as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.'b'A more precise statement is given by the Jordan normal form theorem, which states that in this situation, A is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.[1][2]'b'Any complex square matrix is triangularizable.[1] In fact, a matrix A over a field containing all of the eigenvalues of A (for example, any matrix over an algebraically closed field) is similar to a triangular matrix. This can be proven by using induction on the fact that A has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that A stabilises a flag, and is thus triangularizable with respect to a basis for that flag.'b'The transpose of an upper triangular matrix is a lower triangular matrix and vice versa.'b'A matrix which is simultaneously triangular and normal is also diagonal. This can be seen by looking at the diagonal entries of A*A and AA*, where A is a normal, triangular matrix.'b'is atomic lower triangular. Its inverse is'b'The matrix'b'i.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.'b'The inverse of an atomic triangular matrix is again atomic triangular. Indeed, we have'b'An atomic (upper or lower) triangular matrix is a special form of unitriangular matrix, where all of the off-diagonal elements are zero, except for the entries in a single column. Such a matrix is also called a Frobenius matrix, a Gauss matrix, or a Gauss transformation matrix. So an atomic lower triangular matrix is of the form'b"In fact, by Engel's theorem, any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable."b'The set of unitriangular matrices forms a Lie group.'b'If the entries on the main diagonal of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) unitriangular. All unitriangular matrices are unipotent. Other names used for these matrices are unit (upper or lower) triangular (of which "unitriangular" might be a contraction), or very rarely normed (upper or lower) triangular. However a unit triangular matrix is not the same as the unit matrix, and a normed triangular matrix has nothing to do with the notion of matrix norm. The identity matrix is the only matrix which is both upper and lower unitriangular.'b'is lower triangular.'b'is upper triangular and this matrix'b'This matrix'b'All these results hold if "upper triangular" is replaced by "lower triangular" throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.'b'Together these facts mean that the upper triangular matrices form a subalgebra of the associative algebra of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the Lie algebra of square matrices of a fixed size, where the Lie bracket [a,b] given by the commutator ab-ba. The Lie algebra of all upper triangular matrices is a solvable Lie algebra. It is often referred to as a Borel subalgebra of the Lie algebra of all square matrices.'b'Many operations on upper triangular matrices preserve the shape:'b'Matrices that are similar to triangular matrices are called triangularisable.'b'is called an upper triangular matrix or right triangular matrix. The variable L (standing for lower or left) is commonly used to represent a lower triangular matrix, while the variable U (standing for upper) or R (standing for right) is commonly used for upper triangular matrix. A matrix that is both upper and lower triangular is diagonal.'b'is called a lower triangular matrix or left triangular matrix, and analogously a matrix of the form'b'A matrix of the form'b''b''b'Because matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix L and an upper triangular matrix U if and only if all its leading principal minors are non-zero.'b'In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. A square matrix is called lower triangular if all the entries above the main diagonal are zero. Similarly, a square matrix is called upper triangular if all the entries below the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.'
Normal matrix

Hermitian adjoint
b'is formally similar to the defining properties of pairs of adjoint functors in category theory, and this is where adjoint functors got their name from.'b'The equation'b'For an antilinear operator the definition of adjoint needs to be adjusted in order to compensate for the complex conjugation. An adjoint operator of the antilinear operator A on a complex Hilbert space H is an antilinear operator A\xe2\x88\x97\xc2\xa0: H \xe2\x86\x92 H with the property:'b'In some sense, these operators play the role of the real numbers (being equal to their own "complex conjugate") and form a real vector space. They serve as the model of real-valued observables in quantum mechanics. See the article on self-adjoint operators for a full treatment.'b'which is equivalent to'b'A bounded operator A\xc2\xa0: H \xe2\x86\x92 H is called Hermitian or self-adjoint if'b'The second equation follows from the first by taking the orthogonal complement on both sides. Note that in general, the image need not be closed, but the kernel of a continuous operator[7] always is.[clarification needed]'b'Proof of the first equation:[6][clarification needed]'b'The relationship between the image of A and the kernel of its adjoint is given by:'b'Properties 1.\xe2\x80\x935. hold with appropriate clauses about domains and codomains.[clarification needed] For instance, the last property now states that (AB)\xe2\x88\x97 is an extension of B\xe2\x88\x97A\xe2\x88\x97 if A, B and AB are densely defined operators.[5]'b'and A\xe2\x88\x97(y) is defined to be the z thus found.[4]'b'A densely defined operator A from a complex Hilbert space H to itself is a linear operator whose domain D(A) is a dense linear subspace of H and whose values lies in H.[3] By definition, the domain D(A\xe2\x88\x97) of its adjoint A\xe2\x88\x97 is the set of all y \xe2\x88\x88 H for which there is a z \xe2\x88\x88 H satisfying'b'The set of bounded linear operators on a complex Hilbert space H together with the adjoint operation and the operator norm form the prototype of a C*-algebra.'b'One says that a norm that satisfies this condition behaves like a "largest value", extrapolating from the case of self-adjoint operators.'b'Moreover,'b'then'b'If we define the operator norm of A by'b'The following properties of the Hermitian adjoint of bounded operators are immediate:[2]'b'This can be seen as a generalization of the adjoint matrix of a square matrix which has a similar property involving the standard complex inner product.'b'Existence and uniqueness of this operator follows from the Riesz representation theorem.[2]'b'The fundamental defining identity is thus'b''b''b'The adjoint of an operator A may also be called the Hermitian adjoint, Hermitian conjugate or Hermitian transpose[1] (after Charles Hermite) of A and is denoted by A\xe2\x88\x97 or A\xe2\x80\xa0 (the latter especially when used in conjunction with the bra\xe2\x80\x93ket notation).'b'In a similar sense there can be defined an adjoint operator for linear (and possibly unbounded) operators between Banach spaces.'b'In mathematics, specifically in functional analysis, each bounded linear operator on a complex Hilbert space has a corresponding adjoint operator. Adjoints of operators generalize conjugate transposes of square matrices to (possibly) infinite-dimensional situations. If one thinks of operators on a complex Hilbert space as "generalized complex numbers", then the adjoint of an operator plays the role of the complex conjugate of a complex number.'
CauchySchwarz inequality
b'The next two theorems are further examples in operator algebra.'b'which extends verbatim to positive functionals on C*-algebras:'b'Various generalizations of the Cauchy\xe2\x80\x93Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.'b'then the Cauchy\xe2\x80\x93Schwarz inequality becomes'b'After defining an inner product on the set of random variables using the expectation of their product,'b'Let X, Y be random variables, then the covariance inequality[13][14] is given by'b'The Cauchy\xe2\x80\x93Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [\xe2\x88\x921,\xc2\xa01] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[11][12] as is done when extracting a metric from quantum fidelity.'b'The Cauchy\xe2\x80\x93Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner-product space by defining:[9][10]'b'The Cauchy\xe2\x80\x93Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[7][8]'b'Taking square roots gives the triangle inequality.'b'The triangle inequality for the standard norm is often shown as a consequence of the Cauchy\xe2\x80\x93Schwarz inequality, as follows: given vectors x and y:'b'A generalization of this is the H\xc3\xb6lder inequality.'b'For the inner product space of square-integrable complex-valued functions, one has'b'which yields the Cauchy\xe2\x80\x93Schwarz inequality.'b'This establishes the theorem.'b'which gives'b'Then, by linearity of the inner product in its first argument, one has'b'Let'b'or'b''b''b'The inequality for sums was published by Augustin-Louis Cauchy\xc2\xa0(1821), while the corresponding inequality for integrals was first proved by Viktor Bunyakovsky\xc2\xa0(1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz\xc2\xa0(1888).[1]'b'In mathematics, the Cauchy\xe2\x80\x93Schwarz inequality, also known as the Cauchy\xe2\x80\x93Bunyakovsky\xe2\x80\x93Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]'
Axiom
b'Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.'b'There is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms. The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.'b'The objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The L\xc3\xb6wenheim\xe2\x80\x93Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.'b'Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid\'s postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.'b'The Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed G\xc3\xb6del to establish his famous second incompleteness theorem.[12]'b'This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.'b'The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.'b'Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo\xe2\x80\x93Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann\xe2\x80\x93Bernays\xe2\x80\x93G\xc3\xb6del set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse\xe2\x80\x93Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.[citation needed]'b'This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.'b'Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.'b'Non-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.'b'Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought[citation needed] that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.'b'Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not tautologies. Another name for a non-logical axiom is postulate.[11]'b'Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:'b'These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.[10]'b'Other axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed.[9]'b'These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms at least some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.'b'In the field of mathematical logic, a clear distinction is made between two notions of axioms: logical and non-logical (somewhat similar to the ancient distinction between "axioms" and "postulates" respectively).'b'Regardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither "proves" nor "disproves" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.'b'As a consequence, it is not necessary to explicitly cite Einstein\'s axioms, the more so since they concern subtle points on the "reality" and "locality" of experiments.'b'Another paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of "theorems" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics "hidden variables" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein\'s axioms compared to using Bohr\'s axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr\'s axioms, not Einstein\'s. (Bohr\'s axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)'b"In 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity."b"Axioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment."b'It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo\xe2\x80\x93Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo\xe2\x80\x93Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.'b"The formalist project suffered a decisive setback, when in 1931 G\xc3\xb6del showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, G\xc3\xb6del proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory."b"In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of na\xc3\xafve set theory raised the possibility that any such system could turn out to be inconsistent."b"It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms."b'In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.'b'Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincar\xc3\xa9, Hilbert, and G\xc3\xb6del are some of the key figures in this development.'b'It is not correct to say that the axioms of field theory are "propositions that are regarded as true without proof." Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.'b'When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.'b'Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without any particular application in mind. The distinction between an "axiom" and a "postulate" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid\'s fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like "line" and "parallel" with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience.'b'A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.'b'The classical approach is well-illustrated by Euclid\'s Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).'b'At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a postulate. While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.[8]'b'An "axiom", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that'b"The ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view."b'The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.'b'Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid\'s books, Proclus remarks that, "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property."[7] Boethius translated \'postulate\' as petitio and called the axioms notiones communes but in later manuscripts this usage was not always strictly kept.'b'The root meaning of the word postulate is to "demand"; for instance, Euclid demands that one agree that some things can be done, e.g. any two points can be joined by a straight line, etc.[6]'b'The word axiom comes from the Greek word \xe1\xbc\x80\xce\xbe\xce\xaf\xcf\x89\xce\xbc\xce\xb1 (ax\xc3\xad\xc5\x8dma), a verbal noun from the verb \xe1\xbc\x80\xce\xbe\xce\xb9\xcf\x8c\xce\xb5\xce\xb9\xce\xbd (axioein), meaning "to deem worthy", but also "to require", which in turn comes from \xe1\xbc\x84\xce\xbe\xce\xb9\xce\xbf\xcf\x82 (\xc3\xa1xios), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof.'b''b''b'In both senses, an axiom is any mathematical statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom, or any mathematical statement, to be "true" is an open question[citation needed] in the philosophy of mathematics.[5]'b'As used in mathematics, the term axiom is used in two related but distinguishable senses: "logical axioms" and "non-logical axioms". Logical axioms are usually statements that are taken to be true within the system of logic they define (e.g., (A and B) implies A), often shown in symbolic form, while non-logical axioms (e.g., a + b = b + a) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, "axiom", "postulate", and "assumption" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain.'b'The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] As used in modern logic, an axiom is simply a premise or starting point for reasoning.[4]'b"An axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek ax\xc3\xad\xc5\x8dma (\xe1\xbc\x80\xce\xbe\xce\xaf\xcf\x89\xce\xbc\xce\xb1) 'that which is thought worthy or fit' or 'that which commends itself as evident.'[1][2]"
Inner product space
b'As a further complication, in geometric algebra the inner product and the exterior (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) \xe2\x80\x93 the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) \xe2\x80\x93 and in this context the exterior product is usually called the "outer (alternatively, wedge) product". The inner product is more correctly called a scalar product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).'b'The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.'b'More abstractly, the outer product is the bilinear map W \xc3\x97 V\xe2\x88\x97 \xe2\x86\x92 Hom(V,W) sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map V\xe2\x88\x97 \xc3\x97 V \xe2\x86\x92 F given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.'b'In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".'b'On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism V \xe2\x86\x92 V\xe2\x88\x97) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.'b'The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1 \xc3\x97 n covector with an n \xc3\x97 1 vector, yielding a 1\xc2\xa0\xc3\x97\xc2\xa01 matrix (a scalar), while the outer product is the product of an m \xc3\x97 1 vector with a 1 \xc3\x97 n covector, yielding an m \xc3\x97 n matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the trace of the outer product (trace only being properly defined for square matrices).'b'Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism V \xe2\x86\x92 V\xe2\x88\x97) and thus hold more generally.'b'Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero x there exists some y such that \xe2\x9f\xa8x,y\xe2\x9f\xa9 \xe2\x89\xa0 0, though y need not equal x; in other words, the induced map to the dual space V \xe2\x86\x92 V\xe2\x88\x97 is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester\'s law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with nonzero weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "\xe2\x88\x92" to them differs depending on conventions).'b'This construction is used in numerous contexts. The Gelfand\xe2\x80\x93Naimark\xe2\x80\x93Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.'b'makes sense and satisfies all the properties of norm except that ||x|| = 0 does not imply x = 0 (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient W = V/{x\xc2\xa0: ||x|| = 0}. The sesquilinear form \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9 factors through W.'b'If V is a vector space and \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xc2\xb7\xc2\xb7\xe2\x9f\xa9 a semi-definite sesquilinear form, then the function:'b'Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.'b'From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.'b'Several types of linear maps A from an inner product space V to an inner product space W are of relevance:'b'Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the inner product norm, follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on [\xe2\x88\x92\xcf\x80,\xcf\x80] with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.'b'Orthogonality of the sequence {ek}k follows immediately from the fact that if k \xe2\x89\xa0 j, then'b'is an isometric linear map with dense image.'b'is an orthonormal basis of the space C[\xe2\x88\x92\xcf\x80,\xcf\x80] with the L2 inner product. The mapping'b'Theorem. Let V be the inner product space C[\xe2\x88\x92\xcf\x80,\xcf\x80]. Then the sequence (indexed on set of all integers) of continuous functions'b'This theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided l2 is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:'b'is an isometric linear map V \xe2\x86\x92 l2 with a dense image.'b'Theorem. Let V be a separable inner product space and {ek}k an orthonormal basis of\xc2\xa0V. Then the map'b"Parseval's identity leads immediately to the following theorem:"b"The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).[citation needed]"b'Theorem. Any complete inner product space V has an orthonormal basis.'b'Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show that'b'Theorem. Any separable inner product space V has an orthonormal basis.'b'Using an infinite-dimensional analog of the Gram-Schmidt process one may show:'b'if \xce\xb1 \xe2\x89\xa0 \xce\xb2 and \xe2\x9f\xa8e\xce\xb1,e\xce\xb1\xe2\x9f\xa9 = ||e\xce\xb1|| = 1 for all \xce\xb1, \xce\xb2 \xe2\x88\x88 A.'b'is a basis for V if the subspace of V generated by finite linear combinations of elements of E is dense in V (in the norm induced by the inner product). We say that E is an orthonormal basis for V if it is a basis and'b'This definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let V be any inner product space. Then a collection'b'Let V be a finite dimensional inner product space of dimension n. Recall that every basis of V consists of exactly n linearly independent vectors. Using the Gram\xe2\x80\x93Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis {e1, ..., en} is orthonormal if \xe2\x9f\xa8ei,ej\xe2\x9f\xa9 = 0 for every i \xe2\x89\xa0 j and \xe2\x9f\xa8ei,ei\xe2\x9f\xa9 = ||ei|| = 1 for each i.'b'This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector x. Directly from the axioms, we can prove the following:'b'However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:'b'is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.[9][10]'b'A linear space with a norm such as:'b'is an inner product.'b'For real matrices of the same size, \xe2\x9f\xa8A,B\xe2\x9f\xa9\xc2\xa0:= tr(ABT) with transpose as conjugation'b'is an inner product.[6][7][8] In this case, \xe2\x9f\xa8X,X\xe2\x9f\xa9 = 0 if and only if Pr(X = 0) = 1 (i.e., X = 0 almost surely). This definition of expectation as inner product can be extended to random vectors as well.'b'For real random variables X and Y, the expected value of their product'b'This sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a continuous function.'b'This space is not complete; consider for example, for the interval [\xe2\x88\x921,1] the sequence of continuous "step" functions, {\xe2\x80\x89fk}k, defined by:'b'The article on Hilbert space has several examples of inner product spaces wherein the metric induced by the inner product yields a complete metric space. An example of an inner product which induces an incomplete metric occurs with the space C([a,b]) of continuous complex valued functions on the interval [a,b]. The inner product is'b'where M is any Hermitian positive-definite matrix and y\xe2\x80\xa0 is the conjugate transpose of y. For the real case this corresponds to the dot product of the results of directionally different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. Up to an orthogonal transformation it is a weighted-sum version of the dot product, with positive weights.'b'The general form of an inner product on Cn is known as the Hermitian form and is given by'b'where xT is the transpose of x.'b'More generally, the real n-space Rn with the dot product is an inner product space, an example of a Euclidean n-space.'b'A simple example is the real numbers with the standard multiplication as the inner product'b'is also known as additivity.'b'The property of an inner product space V that'b'Assuming the underlying field to be R, the inner product becomes symmetric, and we obtain'b'Combining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:'b'From the linearity property it is derived that x = 0 implies \xe2\x9f\xa8x,x\xe2\x9f\xa9 = 0. while from the positive-definiteness axiom we obtain the converse, \xe2\x9f\xa8x,x\xe2\x9f\xa9 = 0 implies x = 0. Combining these two, we have the property that \xe2\x9f\xa8x,x\xe2\x9f\xa9 = 0 if and only if x = 0.'b'In the case of F = R, conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.'b'so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.'b'Conjugate symmetry and linearity in the first variable gives'b'Moreover, sesquilinearity (see below) implies that'b'Notice that conjugate symmetry implies that \xe2\x9f\xa8x,x\xe2\x9f\xa9 is real for all x, since we have:'b'When F = R, conjugate symmetry reduces to symmetry. That is, \xe2\x9f\xa8x,y\xe2\x9f\xa9 = \xe2\x9f\xa8y,x\xe2\x9f\xa9 for F = R; while for F = C, \xe2\x9f\xa8x,y\xe2\x9f\xa9 is equal to the complex conjugate.'b'In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that \xe2\x9f\xa8x,x\xe2\x9f\xa9 is only required to be non-negative. We show how to treat these below.'b'There are various technical reasons why it is necessary to restrict the basefield to R and C in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense,[5] and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of R or C will suffice for this purpose, e.g., the algebraic numbers or the constructible numbers. However in these cases when it is a proper subfield (i.e., neither R nor C) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over R or C, such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.'b'Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product \xe2\x9f\xa8x,y\xe2\x9f\xa9 as \xe2\x9f\xa8y|x\xe2\x9f\xa9 (the bra\xe2\x80\x93ket notation of quantum mechanics), respectively y\xe2\x80\xa0x (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the linear functionals (covectors) of the dual space V\xe2\x88\x97, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature,[4] taking \xe2\x9f\xa8x,y\xe2\x9f\xa9 to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both \xe2\x9f\xa8\xc2\xb7,\xc2\xb7\xe2\x9f\xa9 and \xe2\x9f\xa8\xc2\xb7|\xc2\xb7\xe2\x9f\xa9 as distinct notations differing only in which argument is conjugate linear.'b'that satisfies the following three axioms for all vectors x, y, z \xe2\x88\x88 V and all scalars a \xe2\x88\x88 F:[2][3]'b'Formally, an inner product space is a vector space V over the field F together with an inner product, i.e., with a map'b'In this article, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C.'b''b''b'An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.'b'In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Peano, in 1898.[1]'
Bilinear form
b'A linear map S\xc2\xa0: M\xe2\x88\x97 \xe2\x86\x92 M\xe2\x88\x97\xc2\xa0: u \xe2\x86\xa6 S(u) induces the bilinear form B\xc2\xa0: M\xe2\x88\x97 \xc3\x97 M \xe2\x86\x92 R\xc2\xa0: (u, x) \xe2\x86\xa6 \xe2\x9f\xa8S(u), x\xe2\x9f\xa9, and a linear map T\xc2\xa0: M \xe2\x86\x92 M\xc2\xa0: x \xe2\x86\xa6 T(x) induces the bilinear form B\xc2\xa0: M\xe2\x88\x97 \xc3\x97 M \xe2\x86\x92 R\xc2\xa0: (u, x) \xe2\x86\xa6 \xe2\x9f\xa8u, T(x))\xe2\x9f\xa9. Conversely, a bilinear form B\xc2\xa0: M\xe2\x88\x97 \xc3\x97 M \xe2\x86\x92 R induces the R-linear maps S\xc2\xa0: M\xe2\x88\x97 \xe2\x86\x92 M\xe2\x88\x97\xc2\xa0: u \xe2\x86\xa6 (x \xe2\x86\xa6 B(u, x)) and T\xe2\x80\xb2\xc2\xa0: M \xe2\x86\x92 M\xe2\x88\x97\xe2\x88\x97\xc2\xa0: x \xe2\x86\xa6 (u \xe2\x86\xa6 B(u, x)). Here, M\xe2\x88\x97\xe2\x88\x97 denotes the double dual of M.'b'The mapping \xe2\x9f\xa8\xe2\x8b\x85,\xe2\x8b\x85\xe2\x9f\xa9\xc2\xa0: M\xe2\x88\x97 \xc3\x97 M \xe2\x86\x92 R\xc2\xa0: (u, x) \xe2\x86\xa6 u(x) is known as the natural pairing, also called the canonical bilinear form on M\xe2\x88\x97 \xc3\x97 M.[7]'b'for all u, v \xe2\x88\x88 M\xe2\x88\x97, x, y \xe2\x88\x88 M, \xce\xb1, \xce\xb2 \xe2\x88\x88 R.'b'Given a ring R and a right R-module M and its dual module M\xe2\x88\x97, a mapping B\xc2\xa0: M\xe2\x88\x97 \xc3\x97 M \xe2\x86\x92 R is called a bilinear form if'b'Likewise, symmetric bilinear forms may be thought of as elements of Sym2(V\xe2\x88\x97) (the second symmetric power of V\xe2\x88\x97), and alternating bilinear forms as elements of \xce\x9b2V\xe2\x88\x97 (the second exterior power of V\xe2\x88\x97).'b'The set of all linear maps V \xe2\x8a\x97 V \xe2\x86\x92 K is the dual space of V \xe2\x8a\x97 V, so bilinear forms may be thought of as elements of'b'By the universal property of the tensor product, bilinear forms on V are in 1-to-1 correspondence with linear maps V \xe2\x8a\x97 V \xe2\x86\x92 K. If B is a bilinear form on V the corresponding linear map is given by'b'is called the real symmetric case and labeled R(p, q), where p + q = n. Then he articulates the connection to traditional terminology:[6]'b'Terminology varies in coverage of bilinear forms. For example, F. Reese Harvey discusses "eight types of inner product".[5] To define them he uses diagonal matrices Aij having only +1 or \xe2\x88\x921 for non-zero elements. Some of the "inner products" are symplectic forms and some are sesquilinear forms or Hermitian forms. Rather than a general field K, the instances with real numbers R, complex numbers C, and quaternions H are spelled out. The bilinear form'b'In finite dimensions, this is equivalent to the pairing being nondegenerate (the spaces necessarily having the same dimensions). For modules (instead of vector spaces), just as how a nondegenerate form is weaker than a unimodular form, a nondegenerate pairing is a weaker notion than a perfect pairing. A pairing can be nondegenerate without being a perfect pairing, for instance Z \xc3\x97 Z \xe2\x86\x92 Z via (x,y) \xe2\x86\xa6 2xy is nondegenerate, but induces multiplication by 2 on the map Z \xe2\x86\x92 Z\xe2\x88\x97.'b'Here we still have induced linear mappings from V to W\xe2\x88\x97, and from W to V\xe2\x88\x97. It may happen that these mappings are isomorphisms; assuming finite dimensions, if one is an isomorphism, the other must be. When this occurs, B is said to be a perfect pairing.'b'Much of the theory is available for a bilinear mapping from two vector spaces over the same base field to that field'b'For a non-degenerate form on a finite dimensional space, the map V/W \xe2\x86\x92 W\xe2\x8a\xa5 is bijective, and the dimension of W\xe2\x8a\xa5 is dim(V) \xe2\x88\x92 dim(W).'b'Suppose W is a subspace. Define the orthogonal complement[4]'b'A bilinear form B is reflexive if and only if it is either symmetric or alternating.[3] In the absence of reflexivity we have to distinguish left and right orthogonality. In a reflexive space the left and right radicals agree and are termed the kernel or the radical of the bilinear form: the subspace of all vectors orthogonal with every other vector. A vector v, with matrix representation x, is in the radical of a bilinear form with matrix representation A, if and only if Ax = 0 \xe2\x87\x94 xTA = 0. The radical is always a subspace of V. It is trivial if and only if the matrix A is nonsingular, and thus if and only if the bilinear form is nondegenerate.'b'When char(K) = 2 and dim V > 1, this correspondence between quadratic forms and symmetric bilinear forms breaks down.'b'When char(K) \xe2\x89\xa0 2, the quadratic form Q is determined by the symmetric part of the bilinear form B and is independent of the antisymmetric part. In this case there is a one-to-one correspondence between the symmetric part of the bilinear form and the quadratic form, and it makes sense to speak of the symmetric bilinear form associated with a quadratic form.'b'For any bilinear form B\xc2\xa0: V \xc3\x97 V \xe2\x86\x92 K, there exists an associated quadratic form Q\xc2\xa0: V \xe2\x86\x92 K defined by Q\xc2\xa0: V \xe2\x86\x92 K\xc2\xa0: v \xe2\x86\xa6 B(v, v).'b'where tB is the transpose of B (defined above).'b'A bilinear form is symmetric if and only if the maps B1, B2: V \xe2\x86\x92 V\xe2\x88\x97 are equal, and skew-symmetric if and only if they are negatives of one another. If char(K) \xe2\x89\xa0 2 then one can decompose a bilinear form into a symmetric and a skew-symmetric part as follows'b'A bilinear form is symmetric (resp. skew-symmetric) if and only if its coordinate matrix (relative to any basis) is symmetric (resp. skew-symmetric). A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero (which follows from skew-symmetry when char(K) \xe2\x89\xa0 2).'b'If the characteristic of K is not 2 then the converse is also true: every skew-symmetric form is alternating. If, however, char(K) = 2 then a skew-symmetric form is the same as a symmetric form and there exist symmetric/skew-symmetric forms that are not alternating.'b'We define a bilinear form to be'b'If V is finite-dimensional then, relative to some basis for V, a bilinear form is degenerate if and only if the determinant of the associated matrix is zero. Likewise, a nondegenerate form is one for which the determinant of the associated matrix is non-zero (the matrix is non-singular). These statements are independent of the chosen basis. For a module over a commutative ring, a unimodular form is one for which the determinant of the associate matrix is a unit (for example 1), hence the term; note that a form whose matrix is non-zero but not a unit will be nondegenerate but not unimodular, for example B(x, y) = 2xy over the integers.'b'This form will be nondegenerate if and only if A is an isomorphism.'b'Given any linear map A\xc2\xa0: V \xe2\x86\x92 V\xe2\x88\x97 one can obtain a bilinear form B on V via'b'If V is finite-dimensional then the rank of B1 is equal to the rank of B2. If this number is equal to dim(V) then B1 and B2 are linear isomorphisms from V to V\xe2\x88\x97. In this case B is nondegenerate. By the rank\xe2\x80\x93nullity theorem, this is equivalent to the condition that the left and equivalently right radicals be trivial. For finite-dimensional spaces, this is often taken as the definition of nondegeneracy:'b'The left radical and right radical of the form B are the kernels of B1 and B2 respectively;[1] they are the vectors orthogonal to the whole space on the left and on the right.[2]'b'If V is finite-dimensional then one can identify V with its double dual V\xe2\x88\x97\xe2\x88\x97. One can then show that B2 is the transpose of the linear map B1 (if V is infinite-dimensional then B2 is the transpose of B1 restricted to the image of V in V\xe2\x88\x97\xe2\x88\x97). Given B one can define the transpose of B to be the bilinear form given by'b'The corresponding notion for a module over a commutative ring is that a bilinear form is unimodular if V \xe2\x86\x92 V\xe2\x88\x97 is an isomorphism. Given a finitely generated module over a commutative ring, the pairing may be injective (hence "nondegenerate" in the above sense) but not unimodular. For example, over the integers, the pairing B(x, y) = 2xy is nondegenerate but not unimodular, as the induced map from V = Z to V\xe2\x88\x97 = Z is multiplication by 2.'b'For a finite-dimensional vector space V, if either of B1 or B2 is an isomorphism, then both are, and the bilinear form B is said to be nondegenerate. More concretely, for a finite-dimensional vector space, non-degenerate means that every non-zero element pairs non-trivially with some other element:'b'where the dot ( \xe2\x8b\x85 ) indicates the slot into which the argument for the resulting linear functional is to be placed (see Currying).'b'This is often denoted as'b'Every bilinear form B on V defines a pair of linear maps from V to its dual space V\xe2\x88\x97. Define B1, B2: V \xe2\x86\x92 V\xe2\x88\x97 by'b'Now the new matrix representation for the bilinear form is given by: STAS.'b'where S \xe2\x88\x88 GL(n, K).'b'Suppose {f1, ..., fn} is another basis for V, such that:'b'If the n\xe2\x80\x89\xc3\x97\xe2\x80\x891 matrix x represents a vector v with respect to this basis, and analogously, y represents w, then:'b'Define the n\xe2\x80\x89\xc3\x97\xe2\x80\x89n matrix A by Aij = B(ei, ej).'b'Let V \xe2\x89\x85 Kn be an n-dimensional vector space with basis {e1, ..., en}.'b''b''b'When K is the field of complex numbers C, one is often more interested in sesquilinear forms, which are similar to bilinear forms but are conjugate linear in one argument.'b'The definition of a bilinear form can be extended to include modules over a ring, with linear maps replaced by module homomorphisms.'b'In mathematics, more specifically in abstract algebra and linear algebra, a bilinear form on a vector space V is a bilinear map V \xc3\x97 V \xe2\x86\x92 K, where K is the field of scalars. In other words, a bilinear form is a function B\xc2\xa0: V \xc3\x97 V \xe2\x86\x92 K that is linear in each argument separately:'
Diagonalizable matrix
b'In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schr\xc3\xb6dinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schr\xc3\xb6dinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.'b'thereby explaining the above phenomenon.'b'The preceding relations, expressed in matrix form, are'b'Switching back to the standard basis, we have'b'Thus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have that'b'Straightforward calculations show that'b'where ei denotes the standard basis of Rn. The reverse change of basis is given by'b'The above phenomenon can be explained by diagonalizing M. To accomplish this, we need a basis of R2 consisting of eigenvectors of M. One such eigenvector basis is given by'b'Calculating the various powers of M reveals a surprising pattern:'b'For example, consider the following matrix:'b'This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.'b'and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.'b'Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found that'b'Then P diagonalizes A, as a simple computation confirms, having calculated P \xe2\x88\x921 using any suitable method:'b'Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]'b'Now, let P be the matrix with these eigenvectors as its columns:'b'The eigenvectors of A are'b'These eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.'b'A is a 3\xc3\x973 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n\xc3\x97n matrix then this matrix is diagonalizable.'b'This matrix has eigenvalues'b'Consider a matrix'b'Note that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.'b'then Q\xe2\x88\x921BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle \xce\xb8=3\xcf\x80/2'b'The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q\xe2\x88\x921BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we take'b'Some real matrices are not diagonalizable over the reals. Consider for instance the matrix'b'This matrix is not diagonalizable: there is no matrix U such that U\xe2\x88\x921CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.'b'Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, consider'b'In general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to "do the best one can", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.'b'In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.'b'A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.'b'are diagonalizable but not simultaneously diagonalizable because they do not commute.'b'The set of all n\xc3\x97n diagonalisable matrices (over C) with n > 1 is not simultaneously diagonalisable. For instance, the matrices'b'A set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P\xe2\x88\x921AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.[2]'b'In practice, matrices are diagonalized numerically using computers. Many algorithms exist to accomplish this.'b'When the matrix A is a Hermitian matrix (resp. symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of Cn (resp. Rn). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P\xe2\x88\x921 equals the conjugate transpose (resp. transpose) of P.'b'So the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P\xe2\x88\x921 are the left eigenvectors of A.'b'the above equation can be rewritten as'b'then:'b'If a matrix A can be diagonalized, that is,'b'The Jordan\xe2\x80\x93Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each "block" is a one-by-one matrix.'b'As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n\xc3\x97n matrices that are not diagonalizable over C, considered as a subset of Cn\xc3\x97n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.'b'The following sufficient (but not necessary) condition is often useful.'b'Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)'b'The fundamental fact about diagonalizable maps and matrices is expressed by the following:'b''b''b'Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling)\xc2\xa0\xe2\x80\x94 it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).'b'In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P\xe2\x88\x921AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T\xc2\xa0: V \xe2\x86\x92 V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.'
Diagonal matrix
b'Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function\xe2\x80\x93the values of the function at each point correspond to the diagonal entries of a matrix.'b'In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates\xe2\x80\x93in the language of operators, an integral transform\xe2\x80\x93which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.'b'Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA\xe2\x88\x97 = A\xe2\x88\x97A then there exists a unitary matrix U such that UAU\xe2\x88\x97 is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV\xe2\x88\x97 is diagonal with positive entries.'b'In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X\xe2\x88\x921AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.'b'Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.'b'A symmetric diagonal matrix can be defined as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.'b'Any square diagonal matrix is also a symmetric matrix.'b'A square matrix is diagonal if and only if it is triangular and normal.'b'The adjugate of a diagonal matrix is again diagonal.'b'The determinant of diag(a1, ..., an) is the product a1...an.'b'In other words, the eigenvalues of diag(\xce\xbb1, ..., \xce\xbbn) are \xce\xbb1, ..., \xce\xbbn with associated eigenvectors of e1, ..., en.'b'Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i.'b'In particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.'b'The diagonal matrix diag(a1, ..., an) is invertible if and only if the entries a1, ..., an are all non-zero. In this case, we have'b'and for matrix multiplication,'b'The operations of matrix addition and matrix multiplication are especially simple for symmetric diagonal matrices. Write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1, ..., an. Then, for addition, we have'b'The scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.'b'A square diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple \xce\xbbI of the identity matrix I. Its effect on a vector is scalar multiplication by \xce\xbb. For example, a 3\xc3\x973 scalar matrix has the form:'b'In the remainder of this article we will consider only square matrices.'b'If the entries are real numbers or complex numbers, then it is a normal matrix as well.'b'The following matrix is a symmetric diagonal matrix:'b'The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. For example:'b'However, the main diagonal entries need not be zero.'b'As stated above, the off-diagonal entries are zero. That is, the matrix D = (di,j) with n columns and n rows is diagonal if'b''b''
Identity matrix
b'The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.[3]'b'The identity matrix of a given size is the only idempotent matrix of that size having full rank. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.'b'The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.'b'It can also be written using the Kronecker delta notation:'b'Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:'b'The ith column of an identity matrix is the unit vector ei. It follows that the determinant of the identity matrix is\xc2\xa01 and the trace is\xc2\xa0n.'b'Where n\xc3\x97n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.'b'In particular, the identity matrix serves as the unit of the ring of all n\xc3\x97n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n\xc3\x97n matrices. (The identity matrix itself is invertible, being its own inverse.)'b'When A is m\xc3\x97n, it is a property of matrix multiplication that'b'In linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n \xc3\x97 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning "unit matrix"[1] and the German word "Einheitsmatrix",[2] respectively.'
Polynomial
b"The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, \xe2\x88\x92 for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. Ren\xc3\xa9 Descartes, in La g\xc3\xa9ometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.[23]"b'Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write 3x\xc2\xa0+\xc2\xa02y\xc2\xa0+\xc2\xa0z =\xc2\xa029.'b'The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.'b"Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph."b'Polynomials serve to approximate other functions,[22] such as the use of splines.'b'Analogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, "non-constant" must be replaced by "non-constant or non-unit" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein\'s criterion can also be used in some cases to determine irreducibility.'b'and such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[x] is a Euclidean domain.'b'If F is a field and f and g are polynomials in F[x] with g \xe2\x89\xa0 0, then there exist unique polynomials q and r in F[x] with'b'In commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[x], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[x] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[x] and r is an element of R such that f(r) = 0, then the polynomial (x \xe2\x88\x92 r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.[20][21]'b"If R is commutative, then one can associate to every polynomial P in R[x], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for x."b'Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[x] over the real numbers by factoring out the ideal of multiples of the polynomial x2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).'b'One can think of the ring R[x] as arising from R by adding one new element x to R, and extending in a minimal way to a ring in which x satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is xr = rx). To do this, one must add all powers of x and their linear combinations as well.'b'Thus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[x]. The map from R to R[x] sending r to rx0 is an injective homomorphism of rings, by which R is viewed as a subring of R[x]. If R is commutative, then R[x] is an algebra over R.'b'where n is a natural number, the coefficients a0, . . ., an are elements of R, and x is a formal symbol, whose powers xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aixi is interpreted as a polynomial that has zero coefficients at all powers of x other than xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the rule'b'In abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate x over a ring R is defined as a formal expression of the form'b'and the indefinite integral is'b'the derivative with respect to x is'b'Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function'b"The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone\xe2\x80\x93Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function."b'Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.'b'The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.'b'While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.'b'A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.'b'Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.'b'A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).'b'where I is the identity matrix.[19]'b'this polynomial evaluated at a matrix A is'b'A matrix polynomial is a polynomial with square matrices as variables.[18] Given an ordinary, scalar-valued polynomial'b'Trigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.'b'For complex coefficients, there is no difference between such a function and a finite Fourier series.'b'If sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.'b'A trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers.[17] The coefficients may be taken as real numbers, for real-valued functions.'b' There are several generalizations of the concept of polynomials.'b"A polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem."b'The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.'b'For polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of "roots". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.'b'When there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions.[16] There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).'b'Some polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x \xe2\x88\x92 a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree\xc2\xa01; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.'b"A number a is a root of a polynomial P if and only if the linear polynomial x \xe2\x88\x92 a divides P, that is if there is another polynomial Q such that P = (x \xe2\x80\x93 a) Q. It may happen that x \xe2\x88\x92 a divides P more than once: if (x \xe2\x88\x92 a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x \xe2\x88\x92 a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P.[15] The relation between the coefficients of a polynomial and its roots is described by Vieta's formulas."b'The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.'b'In elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel\xe2\x80\x93Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.'b'When considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x \xe2\x88\x92 y) = x2 \xe2\x88\x92 y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.'b'is a polynomial equation.'b'For example,'b'A polynomial equation, also called algebraic equation, is an equation of the form[14]'b'Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.'b'A non constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).'b'A polynomial function in one real variable can be represented by a graph.'b'Every polynomial function is continuous, smooth, and entire.'b'is a polynomial function of one variable. Polynomial functions of multiple variables are similarly defined, using polynomials in multiple indeterminates, as in'b'For example, the function f, defined by'b'Generally, unless otherwise specified, polynomial functions have real or complex coefficients and have real or complex arguments and values. In particular, a polynomial with real coefficients defines a function from the complex numbers to the complex numbers, whose restriction to the reals maps reals to reals.'b'for all arguments x, where n is a non-negative integer and a0, a1, a2, ..., an are constant coefficients.'b'A polynomial function is a function that can be defined by evaluating a polynomial. A function f of one argument is thus a polynomial function if it satisfies.'b'Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.'b'A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.'b'The computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.'b'over the complex numbers.'b'over the integers and the reals and'b'is'b'All polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree.[13] For example, the factored form of'b'As for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b \xe2\x89\xa0 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.[12]'b'Polynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of f(x) by (x \xe2\x88\x92 a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.'b'which can be simplified to'b'then'b'To work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.[8] For example, if'b'which can be simplified to'b'then'b'Polynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.[8][10] For example, if'b"The evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:"b'A polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in x, y, and z", listing the indeterminates allowed.'b'A real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms. A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.'b'Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.[8] Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,[9] a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term "quadrinomial" is occasionally used for a four-term polynomial.'b'The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of x", with the term of largest degree first, or in "ascending powers of x". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is \xe2\x88\x925. The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.[7]'b'In the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its non-zero terms have degree n. The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.[6] For example, x3y2 + 7x2y3 \xe2\x88\x92 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.'b'The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either \xe2\x88\x921 or \xe2\x88\x92\xe2\x88\x9e).[5] These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, f(x) = 0, is the X-axis.'b'Polynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.'b'It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.'b'Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:'b'is a term. The coefficient is \xe2\x88\x925, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.'b'For example:'b'A term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.[3] The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).[4]'b'That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number\xe2\x80\x94called the coefficient of the term[2]\xe2\x80\x94and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because x = x1, the degree of an indeterminate without a written exponent is one.'b'This can be expressed more concisely by using summation notation:'b'A polynomial in a single indeterminate x can always be written (or rewritten) in the form'b'A polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.'b'This equality allows writing "let P(x) be a polynomial" as a shorthand for "let P be a polynomial in the indeterminate x". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.'b'Frequently, when using this function, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (any ring). In particular, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,'b'which is the polynomial function associated to P.'b'Normally, the name of the polynomial is P, not P(x). However, if a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting x by a in P. Thus, the polynomial P defines the function'b'It may be confusing that a polynomial P in the indeterminate x may appear in the formulas either as P or as P(x).[citation needed]'b'It is a common convention to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.[citation needed]'b'The x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is "indeterminate"). It is thus more correct to call it an "indeterminate".[citation needed] However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a "variable". Many authors use these two words interchangeably.'b'The word polynomial joins two diverse roots: the Greek poly, meaning "many," and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.[1] 'b''b''b'Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.'b'In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate x is x2 \xe2\x88\x92 4x + 7. An example in three variables is x3 + 2xyz2 \xe2\x88\x92 yz + 1.'
Algebraically closed field

Complex number
b'The fields R and Qp and their finite field extensions, including C, are local fields.'b'Hypercomplex numbers also generalize R, C, H, and O. For example, this notion contains the split-complex numbers, which are elements of the ring R[x]/(x2 \xe2\x88\x92 1) (as opposed to R[x]/(x2 + 1)). In this ring, the equation a2 = 1 has four solutions.'b'is also isomorphic to the field C, and gives an alternative complex structure on R2. This is generalized by the notion of a linear complex structure.'b'has the property that its square is the negative of the identity matrix: J2 = \xe2\x88\x92I. Then'b'i.e., the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of C in the 2 \xc3\x97 2 real matrices, it is not the only one. Any matrix'b'for some fixed complex number w can be represented by a 2\xe2\x80\x89\xc3\x97\xe2\x80\x892 matrix (once a basis has been chosen). With respect to the basis (1,\xe2\x80\x89i), this matrix is'b'The Cayley\xe2\x80\x93Dickson construction is closely related to the regular representation of C, thought of as an R-algebra (an R-vector space with a multiplication), with respect to the basis (1,\xe2\x80\x89i). This means the following: the R-linear map'b"Reals, complex numbers, quaternions and octonions are all normed division algebras over R. However, by Hurwitz's theorem they are the only ones. The next step in the Cayley\xe2\x80\x93Dickson construction, the sedenions, in fact fails to have this structure."b'However, just as applying the construction to reals loses the property of ordering, more properties familiar from real and complex numbers vanish with increasing dimension. The quaternions are only a skew field, i.e. for some x,\xe2\x80\x89y: x\xc2\xb7y \xe2\x89\xa0 y\xc2\xb7x for two quaternions, the multiplication of octonions fails (in addition to not being commutative) to be associative: for some x,\xe2\x80\x89y,\xe2\x80\x89z: (x\xc2\xb7y)\xc2\xb7z \xe2\x89\xa0 x\xc2\xb7(y\xc2\xb7z).'b'The process of extending the field R of reals to C is known as the Cayley\xe2\x80\x93Dickson construction. It can be carried further to higher dimensions, yielding the quaternions H and octonions O which (as a real vector space) are of dimension\xc2\xa04 and 8, respectively. In this context the complex numbers have been called the binarions.[37]'b'Later classical writers on the general theory include Richard Dedekind, Otto H\xc3\xb6lder, Felix Klein, Henri Poincar\xc3\xa9, Hermann Schwarz, Karl Weierstrass and many others.'b"The English mathematician G. H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise.[36] Augustin Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case."b'Wessel\'s memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Carl Friedrich Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about "the true metaphysics of the square root of \xe2\x88\x921". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology. In the beginning of the 19th century, other mathematicians discovered independently the geometrical representation of the complex numbers: Bu\xc3\xa9e, Mourey, Warren, Fran\xc3\xa7ais and his brother, Bellavitis.[35]'b"The idea of a complex number as a point in the complex plane (above) was first described by Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's De Algebra tractatus."b'by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.'b"In 1748 Leonhard Euler went further and obtained Euler's formula of complex analysis:"b"In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:"b'The term "imaginary" for these quantities was coined by Ren\xc3\xa9 Descartes in 1637, although he was at pains to stress their imaginary nature[34]'b'Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function \xce\xb6(s) is related to the distribution of prime numbers.'b'Another example are Gaussian integers, that is, numbers of the form x + iy, where x and y are integers, which can be used to classify sums of squares.'b'As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in C. A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers \xe2\x80\x93 they are a principal object of study in algebraic number theory. Compared to Q, the algebraic closure of Q, which also contains all algebraic numbers, C has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge \xe2\x80\x93 a purely geometric problem.'b'Certain fractals are plotted in the complex plane, e.g. the Mandelbrot set and Julia sets.'b'In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.'b"The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics\xe2\x80\x94the Schr\xc3\xb6dinger equation and Heisenberg's matrix mechanics\xe2\x80\x94make use of complex numbers."b'Another example, relevant to the two side bands of amplitude modulation of AM radio, is:'b'This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.'b'where \xcf\x89 represents the angular frequency and the complex number A encodes the phase and amplitude as explained above.'b'and'b'If Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex valued functions of the form'b'Complex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value |\xe2\x80\x89z\xe2\x80\x89| of the corresponding z is the amplitude and the argument arg(z) is the phase.'b'To obtain the measurable quantity, the real part is taken:'b'Since the voltage in an AC circuit is oscillating, it can be represented as'b'In electrical engineering, the imaginary unit is denoted by j, to avoid confusion with I, which is generally in use to denote electric current, or, more particularly, i, which is generally in use to denote instantaneous electric current.'b'In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.'b'In differential equations, it is common to first find all complex roots r of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form f(t) = ert. Likewise, in difference equations, the complex roots r of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form f(t) = rt.'b'In fluid dynamics, complex functions are used to describe potential flow in two dimensions.'b'In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.'b'If a system has zeros in the right half plane, it is a nonminimum phase system.'b'In the root locus method, it is important whether zeros and poles are in the left or right half planes, i.e. have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that are'b"In control theory, systems are often transformed from the time domain to the frequency domain using the Laplace transform. The system's zeros and poles are then analyzed in the complex plane. The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane."b'Complex numbers have essential concrete applications in a variety of scientific and related areas such as signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some applications of complex numbers are:'b'Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions f and g that agree on an arbitrarily small open subset of C necessarily agree everywhere. Meromorphic functions, functions that can locally be written as f(z)/(z \xe2\x88\x92 z0)n with a holomorphic function f, still share some of the features of holomorphic functions. Other functions have essential singularities, such as sin(1/z) at z = 0.'b'A function f\xe2\x80\x89: C \xe2\x86\x92 C is called holomorphic if it satisfies the Cauchy\xe2\x80\x93Riemann equations. For example, any R-linear map C \xe2\x86\x92 C can be written in the form'b'Both sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.'b'Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when na\xc3\xafvely treated as single-valued functions; see failure of power and logarithm identities. For example, they do not satisfy'b'Complex exponentiation z\xcf\x89 is defined as'b'where arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2\xcf\x80, log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval (\xe2\x88\x92\xcf\x80,\xcf\x80].'b'for any complex number w \xe2\x89\xa0 0. It can be shown that any such solution z\xe2\x80\x94called complex logarithm of w\xe2\x80\x94satisfies'b'Unlike in the situation of real numbers, there is an infinitude of complex solutions z of the equation'b'for any real number \xcf\x86, in particular'b"Euler's identity states:"b'The series defining the real trigonometric functions sine and cosine, as well as the hyperbolic functions sinh and cosh, also carry over to complex arguments without change. For the other trigonometric and hyperbolic functions, such as tangent, things are slightly more complicated, as the defining series do not converge for all complex values. Therefore, one must define them either in terms of sine, cosine and exponential, or, equivalently, by using the method of analytic continuation.'b'Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the exponential function exp(z), also written ez, is defined as the infinite series'b'for any two complex numbers z1 and z2.'b'is a complete metric space, which notably includes the triangle inequality'b'The notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (\xce\xb5, \xce\xb4)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, C, endowed with the metric'b"The study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane."b'The geometric description of the multiplication of complex numbers can also be expressed in terms of rotation matrices by using this correspondence between complex numbers and such matrices. Moreover, the square of the absolute value of a complex number expressed as a matrix is equal to the determinant of that matrix:'b'Here the entries a and b are real numbers. The sum and product of two such matrices is again of this form, and the sum and product of complex numbers corresponds to the sum and product of such matrices, the product being:'b'Complex numbers a + bi can also be represented by 2\xe2\x80\x89\xc3\x97\xe2\x80\x892 matrices that have the following form:'b'Accepting that C is algebraically closed, since it is an algebraic extension of R in this approach, C is therefore the algebraic closure of R.'b'The formulas for addition and multiplication in the ring R[X], modulo the relation (X2 = 1 correspond to the formulas for addition and multiplication of complex numbers defined as ordered pairs. So the two definitions of the field C are isomorphic (as fields).'b'The set of complex numbers is defined as the quotient ring R[X]/(X 2 + 1).[28] This extension field contains two square roots of \xe2\x88\x921, namely (the cosets of) X and \xe2\x88\x92X, respectively. (The cosets of) 1 and X form a basis of R[X]/(X 2 + 1) as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs (a,\xe2\x80\x89b) of real numbers. The quotient ring is a field, because the (X2 + 1) is a prime ideal in R[X], a principal ideal domain, and therefore is a maximal ideal.'b'where the a0, ...,\xe2\x80\x89an are real numbers. The usual addition and multiplication of polynomials endows the set R[X] of all such polynomials with a ring structure. This ring is called the polynomial ring over the real numbers.'b'must hold for any three elements x, y and z of a field. The set R of real numbers does form a field. A polynomial p(X) with real coefficients is an expression of the form'b'Though this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of C more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive law'b'It is then just a matter of notation to express (a,\xe2\x80\x89b) as a + bi.'b'The set C of complex numbers can be defined as the set R2 of ordered pairs (a,\xe2\x80\x89b) of real numbers, in which the following rules for addition and multiplication are imposed:[27]'b'The only connected locally compact topological fields are R and C. This gives another characterization of C as a topological field, since C can be distinguished from R because the nonzero complex numbers are connected, while the nonzero real numbers are not.[26]'b'Any field F with these properties can be endowed with a topology by taking the sets B(x,\xe2\x80\x89p) = {\xe2\x80\x89y | p \xe2\x88\x92 (y \xe2\x88\x92 x)(y \xe2\x88\x92 x)* \xe2\x88\x88 P\xe2\x80\x89}\xe2\x80\x89 as a base, where x ranges over the field and p ranges over P. With this topology F is isomorphic as a topological field to C.'b'Moreover, C has a nontrivial involutive automorphism x \xe2\x86\xa6 x* (namely the complex conjugation), such that x\xe2\x80\x89x* is in P for any nonzero x in C.'b'The preceding characterization of C describes only the algebraic aspects of C. That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of C as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. C contains a subset P (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:'b'The field C has the following three properties: first, it has characteristic 0. This means that 1 + 1 + \xe2\x8b\xaf + 1 \xe2\x89\xa0 0 for any number of summands (all of which equal one). Second, its transcendence degree over Q, the prime field of C, is the cardinality of the continuum. Third, it is algebraically closed (see above). It can be shown that any field having these properties is isomorphic (as a field) to C. For example, the algebraic closure of Qp also satisfies these three properties, so these two fields are isomorphic (as fields, but not as topological fields).[25] Also, C is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that C contains many proper subfields that are isomorphic to C.'b'Because of this fact, theorems that hold for any algebraically closed field, apply to C. For example, any non-empty complex square matrix has at least one (complex) eigenvalue.'b"There are various proofs of this theorem, either by analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of odd degree has at least one real root."b"has at least one complex solution z, provided that at least one of the higher coefficients a1,\xe2\x80\x89\xe2\x80\xa6,\xe2\x80\x89an is nonzero.[24] This is the statement of the fundamental theorem of algebra, of Carl Friedrich Gauss and Jean le Rond d'Alembert. Because of this fact, C is called an algebraically closed field. This property does not hold for the field of rational numbers Q (the polynomial x2 \xe2\x88\x92 2 does not have a rational root, since \xe2\x88\x9a2 is not a rational number) nor the real numbers R (the polynomial x2 + a does not have a real root for a > 0, since the square of x is positive for any real number x)."b'Given any complex numbers (called coefficients) a0,\xe2\x80\x89\xe2\x80\xa6,\xe2\x80\x89an, the equation'b"When the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra."b'Unlike the reals, C is not an ordered field, that is to say, it is not possible to define a relation z1 < z2 that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so i2 = \xe2\x88\x921 precludes the existence of an ordering on C.[23]'b'These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.'b'The set C of complex numbers is a field.[22] Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number z, its additive inverse \xe2\x88\x92z is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers z1 and z2:'b'(which holds for positive real numbers), do in general not hold for complex numbers.'b'for any integer k satisfying 0 \xe2\x89\xa4 k \xe2\x89\xa4 n \xe2\x88\x92 1. Here n\xe2\x88\x9ar is the usual (positive) nth root of the positive real number r. While the nth root of a positive real number r is chosen to be the positive real number c satisfying cn = r there is no natural way of distinguishing one particular complex nth root of a complex number. Therefore, the nth root of z is considered as a multivalued function (in z), as opposed to a usual function f, for which f(z) is a uniquely defined number. Formulas such as'b'The nth roots of z are given by'b"When n is an integer, this simplifies to de Moivre's formula:"b'to define complex exponentiation, which is likewise multi-valued:'b'We may use the identity'b'Alternatively, a branch cut can be used to define a single-valued "branch" of the complex logarithm.'b'To deal with the existence of more the one possible value for a given input, the complex logarithm may be considered a multi-valued function, with'b'where r is a non-negative real number, one possible value for the complex logarithm of z is'b"It follows from Euler's formula that, for any complex number z written in polar form,"b'The rearrangement of terms is justified because each series is absolutely convergent.'b'and so on, and by considering the Taylor series expansions of eix, cos x and sin x:'b'where e is the base of the natural logarithm. This can be proved through induction by observing that'b"Euler's formula states that, for any real number x,"b'Similarly, division is given by'b'holds. As the arctan function can be approximated highly efficiently, formulas like this\xe2\x80\x94known as Machin-like formulas\xe2\x80\x94are used for high-precision approximations of \xcf\x80.'b'Since the real and imaginary part of 5 + 5i are equal, the argument of that number is 45 degrees, or \xcf\x80/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formula'b'In other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by i corresponds to a quarter-turn counter-clockwise, which gives back i2 = \xe2\x88\x921. The picture at the right illustrates the multiplication of'b'we may derive'b'Formulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers z1 = r1(cos\xe2\x80\x8a\xcf\x861 + i\xe2\x80\x89sin\xe2\x80\x8a\xcf\x861) and z2 = r2(cos\xe2\x80\x8a\xcf\x862 + i\xe2\x80\x89sin\xe2\x80\x8a\xcf\x862), because of the well-known trigonometric identities'b'In angle notation, often used in electronics to represent a phasor with amplitude r and phase \xcf\x86, it is written as[21]'b'Using the cis function, this is sometimes abbreviated to'b"Using Euler's formula this can be written as"b'Together, r and \xcf\x86 give another way of representing complex numbers, the polar form, as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called trigonometric form'b'The value of \xcf\x86 equals the result of atan2:'b'Normally, as given above, the principal value in the interval (\xe2\x88\x92\xcf\x80,\xcf\x80] is chosen. Values in the range [0,2\xcf\x80) are obtained by adding 2\xcf\x80 if the value is negative. The value of \xcf\x86 is expressed in radians in this article. It can increase by any integer multiple of 2\xcf\x80 and still give the same angle. Hence, the arg function is sometimes considered as multivalued. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the angle\xc2\xa00 is common.'b'The square of the absolute value is'b"By Pythagoras' theorem, the absolute value of complex number is the distance to the origin of the point representing the complex number in the complex plane."b'If z is a real number (that is, if y = 0), then r = |\xe2\x80\x89x\xe2\x80\x89|. That is, the absolute value of a real number equals its absolute value as a complex number.'b'The absolute value (or modulus or magnitude) of a complex number z = x + yi is[19]'b'An alternative way of defining a point P in the complex plane, other than using the x- and y-coordinates, is to use the distance of the point from O, the point whose coordinates are (0,\xe2\x80\x890) (the origin), together with the angle subtended between the positive real axis and the line segment OP in a counterclockwise direction. This idea leads to the polar form of complex numbers.'b'and'b'This formula can be used to compute the multiplicative inverse of a complex number if it is given in rectangular coordinates. Inversive geometry, a branch of geometry studying reflections more general than ones about a line, can also be expressed in terms of complex numbers. In the network analysis of electrical circuits, the complex conjugate is used in finding the equivalent impedance when the maximum power transfer theorem is used.'b'The reciprocal of a nonzero complex number z = x + yi is given by'b'As shown earlier, c \xe2\x88\x92 di is the complex conjugate of the denominator c + di. At least one of the real part c and the imaginary part d of the denominator must be nonzero for division to be defined. This is called "rationalization" of the denominator (although the denominator in the final expression might be an irrational real number).'b'Division can be defined in this way because of the following observation:'b'The division of two complex numbers is defined in terms of complex multiplication, which is described above, and real division. When at least one of c and d is non-zero, we have'b'The preceding definition of multiplication of general complex numbers follows naturally from this fundamental property of i. Indeed, if i is treated as a number so that di means d times i, the above multiplication rule is identical to the usual rule for multiplying two sums of two terms.'b'In particular, the square of i is \xe2\x88\x921:'b'The multiplication of two complex numbers is defined by the following formula:'b'Using the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers A and B, interpreted as points of the complex plane, is the point X obtained by building a parallelogram, three of whose vertices are O, A and B. Equivalently, X is the point such that the triangles with vertices O, A, B, and X, B, A, are congruent.'b'Similarly, subtraction is defined by'b'Complex numbers are added by separately adding the real and imaginary parts of the summands. That is to say:'b'Conjugation distributes over the standard arithmetic operations:'b'Moreover, a complex number is real if and only if it equals its own conjugate.'b'The real and imaginary parts of a complex number z can be extracted using the conjugate:'b'Because complex numbers are naturally thought of as existing on a two-dimensional plane, there is no natural linear ordering on the set of complex numbers. Furthermore, there is no linear ordering on the complex numbers that is compatible with addition and multiplication \xe2\x80\x93 the complex numbers cannot have the structure of an ordered field. This is because any square in an ordered field is at least 0, but i2 = \xe2\x88\x921.'b'Because it is a polynomial in the indeterminate i, a + ib may be written instead of a + bi, which is often expedient when b is a radical.[13] In some disciplines, in particular electromagnetism and electrical engineering, j is used instead of i,[14] since i is frequently used for electric current. In these cases complex numbers are written as a + bj or a + jb.'b'Many mathematicians contributed to the full development of complex numbers. The rules for addition, subtraction, multiplication, and division of complex numbers were developed by the Italian mathematician Rafael Bombelli.[12] A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.'b'Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.'b'The solution in radicals (without trigonometric functions) of a general cubic equation contains the square roots of negative numbers when all three roots are real numbers, a situation that cannot be rectified by factoring aided by the rational root test if the cubic is irreducible (the so-called casus irreducibilis). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545,[11] though his understanding was rudimentary.'b"A position vector may also be defined in terms of its magnitude and direction relative to the origin. These are emphasized in a complex number's polar form. Using the polar form of the complex number in calculations may lead to a more intuitive interpretation of mathematical results. Notably, the operations of addition and multiplication take on a very natural geometric character when complex numbers are viewed as position vectors: addition corresponds to vector addition while multiplication corresponds to multiplying their magnitudes and adding their arguments (i.e. the angles they make with the x axis). Viewed in this way the multiplication of a complex number by i corresponds to rotating the position vector counterclockwise by a quarter turn (90\xc2\xb0) about the origin: (a+bi)i = ai+bi2 = -b+ai."b'A complex number can be viewed as a point or position vector in a two-dimensional Cartesian coordinate system called the complex plane or Argand diagram (see Pedoe 1988 and Solomentsev 2001), named after Jean-Robert Argand. The numbers are conventionally plotted using the real part as the horizontal component, and imaginary part as vertical (see Figure 1). These two values used to identify a given complex number are therefore called its Cartesian, rectangular, or algebraic form.'b'A complex number can thus be identified with an ordered pair (Re(z),Im(z)) in the Cartesian plane, an identification sometimes known as the Cartesian form of z. In fact, a complex number can be defined as an ordered pair (a,b), but then rules for addition and multiplication must also be included as part of the definition (see below).[9] William Rowan Hamilton introduced this approach to define the complex number system.[10]'b'A real number a can be regarded as a complex number a + 0i whose imaginary part is 0. A purely imaginary number bi is a complex number 0 + bi whose real part is zero. It is common to write a for a + 0i and bi for 0 + bi. Moreover, when the imaginary part is negative, it is common to write a \xe2\x88\x92 bi with b > 0 instead of a + (\xe2\x88\x92b)i, for example 3 \xe2\x88\x92 4i instead of 3 + (\xe2\x88\x924)i.'b'The real number a is called the real part of the complex number a + bi; the real number b is called the imaginary part of a + bi. By this convention, the imaginary part does not include a factor of i: hence b, not bi, is the imaginary part.[7][8] The real part of a complex number z is denoted by Re(z) or \xe2\x84\x9c(z); the imaginary part of a complex number z is denoted by Im(z) or \xe2\x84\x91(z). For example,'b'A complex number is a number of the form a + bi, where a and b are real numbers and i is an indeterminate satisfying i2 = \xe2\x88\x921. For example, 2 + 3i is a complex number.[5]'b'According to the fundamental theorem of algebra, all polynomial equations with real or complex coefficients in a single variable have a solution in complex numbers.'b'has no real solution, since the square of a real number cannot be negative. Complex numbers provide a solution to this problem. The idea is to extend the real numbers with an indeterminate i (sometimes called the imaginary unit) that is taken to satisfy the relation i2 = \xe2\x88\x921, so that solutions to equations like the preceding one can be found. In this case the solutions are \xe2\x88\x921 + 3i and \xe2\x88\x921 \xe2\x88\x92 3i, as can be verified using the fact that i2 = \xe2\x88\x921:'b'Complex numbers allow solutions to certain equations that have no solutions in real numbers. For example, the equation'b''b''b'Geometrically, complex numbers extend the concept of the one-dimensional number line to the two-dimensional complex plane by using the horizontal axis for the real part and the vertical axis for the imaginary part. The complex number a + bi can be identified with the point (a,\xe2\x80\x89b) in the complex plane. A complex number whose real part is zero is said to be purely imaginary; the points for these numbers lie on the vertical axis of the complex plane. A complex number whose imaginary part is zero can be viewed as a real number; its point lies on the horizontal axis of the complex plane. Complex numbers can also be represented in polar form, which associates each complex number with its distance from the origin (its magnitude) and with a particular angle known as the argument of this complex number.'b'Most importantly the complex numbers give rise to the fundamental theorem of algebra: every non-constant polynomial equation with complex coefficients has a complex solution. This property is true of the complex numbers, but not the reals. The 16th century Italian mathematician Gerolamo Cardano is credited with introducing complex numbers in his attempts to find solutions to cubic equations.[4]'b'The complex number system can be defined as the algebraic extension of the ordinary real numbers by an imaginary number i.[3] This means that complex numbers can be added, subtracted, and multiplied, as polynomials in the variable i, with the rule i2 = \xe2\x88\x921 imposed. Furthermore, complex numbers can also be divided by nonzero complex numbers. Overall, the complex number system is a field.'b'A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers, and i is a solution of the equation x2 = \xe2\x88\x921, which is called an imaginary number because there is no real number that satisfies this equation. For the complex number a + bi, a is called the real part, and b is called the imaginary part. Despite the historical nomenclature "imaginary", complex numbers are regarded in the mathematical sciences as just as "real" as the real numbers, and are fundamental in many aspects of the scientific description of the natural world.[1][2]'
Invariant (mathematics)
b'Thirdly, if one is studying an object which varies in a family, as is common in algebraic geometry and differential geometry, one may ask if the property is unchanged under perturbation \xe2\x80\x93 if an object is constant on families or invariant under change of metric, for instance.'b'The most common examples are:'b'Secondly, a function may be defined in terms of some presentation or decomposition of a mathematical object; for instance, the Euler characteristic of a cell complex is defined as the alternating sum of the number of cells in each dimension. One may forget the cell complex structure and look only at the underlying topological space (the manifold) \xe2\x80\x93 as different cell complexes give the same underlying manifold, one may ask if the function is independent of choice of presentation, in which case it is an intrinsically defined invariant. This is the case for the Euler characteristic, and a general method for defining and computing invariants is to define them for a given presentation and then show that they are independent of the choice of presentation. Note that there is no notion of a group action in this sense.'b'These are connected as follows: invariants are constant on coinvariants (for example, congruent triangles have the same perimeter), while two objects which agree in the value of one invariant may or may not be congruent (two triangles with the same perimeter need not be congruent). In classification problems, one seeks to find a complete set of invariants, such that if two objects have the same values for this set of invariants, they are congruent. For example, triangles such that all three sides are equal are congruent, via SSS congruence, and thus the lengths of all three sides form a complete set of invariants for triangles.'b'Dual to the notion of invariants are coinvariants, also known as orbits, which formalizes the notion of congruence: objects which can be taken to each other by a group action. For example, under the group of rigid motions of the plane, the perimeter of a triangle is an invariant, while the set of triangles congruent to a given triangle is a coinvariant.'b'More importantly, one may define a function on a set, such as "radius of a circle in the plane" and then ask if this function is invariant under a group action, such as rigid motions.'b'Very frequently one will have a group acting on a set X and ask which objects in an associated set F(X) are invariant. For example, rotation in the plane about a point leaves the point about which it rotates invariant, while translation in the plane does not leave any points invariant, but does leave all lines parallel to the direction of translation invariant as lines. Formally, define the set of lines in the plane P as L(P); then a rigid motion of the plane takes lines to lines \xe2\x80\x93 the group of rigid motions acts on the set of lines \xe2\x80\x93 and one may ask which lines are unchanged by an action.'b'Firstly, if one has a group G acting on a mathematical object (or set of objects) X, then one may ask which points x are unchanged, "invariant" under the group action, or under an element g of the group.'b'The notion of invariance is formalized in three different ways in mathematics: via group actions, presentations, and deformation.'b'When T is a screw displacement, the screw axis is an invariant line, though if the pitch is non-zero, T has no fixed points.'b'An invariant set of an operation T is also said to be stable under T. For example, the normal subgroups that are so important in group theory are those subgroups that are stable under the inner automorphisms of the ambient group.[6][7][8] Other examples occur in linear algebra. Suppose a linear transformation T has an eigenvector v. Then the line through 0 and v is an invariant set under T. The eigenvectors span an invariant subspace which is stable under T.'b'Some more complicated examples:'b'Angles and ratios of distances are invariant under scalings, rotations, translations and reflections. These transformations produce similar shapes, which is the basis of trigonometry. For example, no matter how a triangle is transformed, the sum of its interior angles is always 180\xc2\xb0. As another example, all circles are similar: they can be transformed into each other and the ratio of the circumference to the diameter is invariant (denoted by the Greek letter pi).'b'The distance between two points on a number line is not changed by adding the same quantity to both numbers. On the other hand, multiplication does not have this same property as addition, so distance is not invariant under multiplication.'b'An identity is an equation that remains true for all values of its variables. There are also inequalities that remain true when the values of their variables change.'b'A simple example of invariance is expressed in our ability to count. For a finite set of objects of any kind, there is a number to which we always arrive, regardless of the order in which we count the objects in the set. The quantity\xe2\x80\x94a cardinal number\xe2\x80\x94is associated with the set, and is invariant under the process of counting.'b''b''b'Invariants are used in diverse areas of mathematics such as geometry, topology and algebra. Some important classes of transformations are defined by an invariant they leave unchanged, for example conformal maps are defined as transformations of the plane that preserve angles. The discovery of invariants is an important step in the process of classifying mathematical objects.'b'In mathematics, an invariant is a property, held by a class of mathematical objects, which remains unchanged when transformations of a certain type are applied to the objects. The particular class of objects and type of transformations are usually indicated by the context in which the term is used. For example, the area of a triangle is an invariant with respect to isometries of the Euclidean plane. The phrases "invariant under" and "invariant to" a transformation are both used. More generally, an invariant with respect to an equivalence relation is a property that is constant on each equivalence class.'
Eigenvalues and eigenvectors
b"The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering."b'In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.'b'In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.'b'Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.'b'In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.'b'The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.'b'This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.'b'leads to a so-called quadratic eigenvalue problem,'b'or'b'Eigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed by'b"Principal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling."b'The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.'b"In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]"b"In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree\xe2\x80\x93Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree\xe2\x80\x93Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations."b'A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.'b'The following table presents some example transformations in the plane along with their 2\xc3\x972 matrices, eigenvalues, and eigenvectors.'b'Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.'b'This matrix equation is equivalent to two linear equations'b'Once the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrix'b'Efficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]'b"In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]"b'A similar procedure is used for solving a differential equation of the form'b'The solution of this equation for x in terms of t is found by using its characteristic equation'b'The simplest difference equations have the form'b'The representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.'b'One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation \xe2\x80\x93 an associative algebra acting on a module. The study of such actions is the field of representation theory.'b'For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars \xce\xbb for which the operator (T \xe2\x88\x92 \xce\xbbI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.'b'If \xce\xbb is an eigenvalue of T, then the operator (T \xe2\x88\x92 \xce\xbbI) is not one-to-one, and therefore its inverse (T \xe2\x88\x92 \xce\xbbI)\xe2\x88\x921 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T \xe2\x88\x92 \xce\xbbI) may not have an inverse even if \xce\xbb is not an eigenvalue.'b'Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar \xce\xbb \xe2\x88\x88 K such that there exists a non-zero vector v \xe2\x88\x88 V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue \xce\xbb to be any vector that, given \xce\xbb, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.'b'While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]'b'Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.'b'The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]'b'The geometric multiplicity \xce\xb3T(\xce\xbb) of an eigenvalue \xce\xbb is the dimension of the eigenspace associated with \xce\xbb, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, \xce\xb3T(\xce\xbb) \xe2\x89\xa5 1 because every eigenvalue has at least one eigenvector.'b'So, both u + v and \xce\xb1v are either zero or eigenvectors of T associated with \xce\xbb, namely (u+v,\xce\xb1v) \xe2\x88\x88 E, and E is closed under addition and scalar multiplication. The eigenspace E associated with \xce\xbb is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]'b'for (x,y) \xe2\x88\x88 V and \xce\xb1 \xe2\x88\x88 K. Therefore, if u and v are eigenvectors of T associated with eigenvalue \xce\xbb, namely (u,v) \xe2\x88\x88 E, then'b'By definition of a linear transformation,'b'which is the union of the zero vector with the set of all eigenvectors associated with\xc2\xa0\xce\xbb. E is called the eigenspace or characteristic space of T associated with\xc2\xa0\xce\xbb.'b'Given an eigenvalue \xce\xbb, consider the set'b'This equation is called the eigenvalue equation for T, and the scalar \xce\xbb is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while \xce\xbbv is the product of the scalar \xce\xbb with v.[33]'b'We say that a non-zero vector v \xe2\x88\x88 V is an eigenvector of T if and only if there exists a scalar \xce\xbb \xe2\x88\x88 K such that'b'The concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,'b'The main eigenfunction article gives other examples.'b'is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for \xce\xbb = 0 the eigenfunction f(t) is a constant.'b'This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential function'b'The functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.'b'The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C\xe2\x88\x9e of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equation'b'On the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 \xe2\x88\x921 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity \xce\xb3A is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.'b'The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is \xce\xbcA = 4 = n, the order of the characteristic polynomial and the dimension of A.'b'has a characteristic polynomial that is the product of its diagonal elements,'b'As in the previous example, the lower triangular matrix'b'respectively, as well as scalar multiples of these vectors.'b'These eigenvalues correspond to the eigenvectors,'b'which has the roots \xce\xbb1 = 1, \xce\xbb2 = 2, and \xce\xbb3 = 3. These roots are the diagonal elements as well as the eigenvalues of\xc2\xa0A.'b'The characteristic polynomial of A is'b'Consider the lower triangular matrix,'b'A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.'b'respectively, as well as scalar multiples of these vectors.'b'Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,'b'which has the roots \xce\xbb1 = 1, \xce\xbb2 = 2, and \xce\xbb3 = 3. These roots are the diagonal elements as well as the eigenvalues of\xc2\xa0A.'b'The characteristic polynomial of A is'b'Matrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrix'b'and'b'Then'b'For the complex conjugate pair of imaginary eigenvalues, note that'b'For the real eigenvalue \xce\xbb1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,'b'This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1\xc2\xa0\xe2\x88\x92\xc2\xa0\xce\xbb3, whose roots are'b'Consider the cyclic permutation matrix'b'The characteristic polynomial of A is'b'Consider the matrix'b'Thus, the vectors v\xce\xbb=1 and v\xce\xbb=3 are eigenvectors of A associated with the eigenvalues \xce\xbb = 1 and \xce\xbb = 3, respectively.'b'is an eigenvector of A corresponding to \xce\xbb = 3, as is any scalar multiple of this vector.'b'Any non-zero vector with v1 = v2 solves this equation. Therefore,'b'For \xce\xbb = 3, Equation (2) becomes'b'is an eigenvector of A corresponding to \xce\xbb = 1, as is any scalar multiple of this vector.'b'Any non-zero vector with v1 = \xe2\x88\x92v2 solves this equation. Therefore,'b'For \xce\xbb = 1, Equation (2) becomes,'b'Setting the characteristic polynomial equal to zero, it has roots at \xce\xbb = 1 and \xce\xbb = 3, which are the two eigenvalues of A.'b'Taking the determinant to find characteristic polynomial of A,'b'The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of \xce\xbb for which the determinant of the matrix (A\xc2\xa0\xe2\x88\x92\xc2\xa0\xce\xbbI) equals zero are the eigenvalues.'b'Consider the matrix'b'A matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.'b'Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P\xe2\x88\x921AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.'b'A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix \xce\x9b or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and \xce\x9b represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as\xc2\xa0\xce\x9b.'b'or by instead left multiplying both sides by Q\xe2\x88\x921,'b'Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q\xe2\x88\x921,'b'With this in mind, define a diagonal matrix \xce\x9b where each diagonal element \xce\x9bii is the eigenvalue associated with the ith column of Q. Then'b'Since each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,'b'Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,'b'Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.'b'where \xce\xba is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and \xce\xba is its associated eigenvalue. Taking the transpose of this equation,'b'The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation is'b'Many disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiples the n by n matrix A in the defining equation, Equation (1),'b"Let A be an arbitrary n by n matrix of complex numbers with eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbn. Each eigenvalue appears \xce\xbcA(\xce\xbbi) times in this list, where \xce\xbcA(\xce\xbbi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:"b"is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If \xce\xb3A = n, then"b'Suppose A has d \xe2\x89\xa4 n distinct eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbd, where the geometric multiplicity of \xce\xbbi is \xce\xb3A(\xce\xbbi). The total geometric multiplicity of A,'b'The condition that \xce\xb3A(\xce\xbb) \xe2\x89\xa4 \xce\xbcA(\xce\xbb) can be proven by considering a particular eigenvalue \xce\xbe of A and diagonalizing the first \xce\xb3A(\xce\xbe) columns of A with respect to the eigenvectors of \xce\xbe, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix \xce\xbeI\xce\xb3A(\xce\xbe). As a result, the characteristic polynomial of B will have a factor of (\xce\xbe\xc2\xa0\xe2\x88\x92\xc2\xa0\xce\xbb)\xce\xb3A(\xce\xbe). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of \xce\xbe as an eigenvalue of B is no less than the geometric multiplicity of \xce\xbe as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.'b"Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n."b"The dimension of the eigenspace E associated with \xce\xbb, or equivalently the maximum number of linearly independent eigenvectors associated with \xce\xbb, is referred to as the eigenvalue's geometric multiplicity \xce\xb3A(\xce\xbb). Because E is also the nullspace of (A \xe2\x88\x92 \xce\xbbI), the geometric multiplicity of \xce\xbb is the dimension of the nullspace of (A \xe2\x88\x92 \xce\xbbI), also called the nullity of (A \xe2\x88\x92 \xce\xbbI), which relates to the dimension and rank of (A \xe2\x88\x92 \xce\xbbI) as"b'Because the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) \xe2\x88\x88 E, then (u + v) \xe2\x88\x88 E or equivalently A(u + v) = \xce\xbb(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v \xe2\x88\x88 E and \xce\xb1 is a complex number, (\xce\xb1v) \xe2\x88\x88 E or equivalently A(\xce\xb1v) = \xce\xbb(\xce\xb1v). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and \xce\xb1v are not zero, they are also eigenvectors of A associated with \xce\xbb.'b'On one hand, this set is precisely the kernel or nullspace of the matrix (A \xe2\x88\x92 \xce\xbbI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with \xce\xbb. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with \xce\xbb, and E equals the nullspace of (A \xe2\x88\x92 \xce\xbbI). E is called the eigenspace or characteristic space of A associated with \xce\xbb.[7][8] In general \xce\xbb is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of \xe2\x84\x82n.'b'Given a particular eigenvalue \xce\xbb of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),'b'If \xce\xbcA(\xce\xbbi) = 1, then \xce\xbbi is said to be a simple eigenvalue.[27] If \xce\xbcA(\xce\xbbi) equals the geometric multiplicity of \xce\xbbi, \xce\xb3A(\xce\xbbi), defined in the next section, then \xce\xbbi is said to be a semisimple eigenvalue.'b"If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n as"b'Suppose a matrix A has dimension n and d \xe2\x89\xa4 n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,'b'Let \xce\xbbi be an eigenvalue of an n by n matrix A. The algebraic multiplicity \xce\xbcA(\xce\xbbi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (\xce\xbb \xe2\x88\x92 \xce\xbbi)k divides evenly that polynomial.[8][26][27]'b'The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.'b'If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.'b'Setting the characteristic polynomial equal to zero, it has roots at \xce\xbb = 1 and \xce\xbb = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = \xce\xbbv. In this example, the eigenvectors are any non-zero scalar multiples of'b'Taking the determinant of (M \xe2\x88\x92 \xce\xbbI), the characteristic polynomial of M is'b'As a brief example, which is described in more detail in the examples section later, consider the matrix'b'where each \xce\xbbi may be real but in general is a complex number. The numbers \xce\xbb1, \xce\xbb2, ... \xce\xbbn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.'b'The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,'b"Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable \xce\xbb and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (\xe2\x88\x921)n\xce\xbbn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A."b'Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A \xe2\x88\x92 \xce\xbbI) is zero. Therefore, the eigenvalues of A are values of \xce\xbb that satisfy the equation'b'where I is the n by n identity matrix.'b'Equation (1) can be stated equivalently as'b'then v is an eigenvector of the linear transformation A and the scale factor \xce\xbb is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.'b'If it occurs that v and w are scalar multiples, that is if'b'where, for each row,'b'or'b'Now consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,'b'In this case \xce\xbb = \xe2\x88\x921/20.'b'These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar \xce\xbb such that'b'Consider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectors'b'Eigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]'b'The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]'b'At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]'b"In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm\xe2\x80\x93Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincar\xc3\xa9 studied Poisson's equation a few years later.[16]"b"Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Th\xc3\xa9orie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]"b'In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caract\xc3\xa9ristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]'b'Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.'b'Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:'b'where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.'b'Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplication'b'The Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.'b'referred to as the eigenvalue equation or eigenequation. In general, \xce\xbb may be any scalar. For example, \xce\xbb may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.'b'In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value \xce\xbb, called an eigenvalue. This condition can be written as the equation'b'Eigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.'b''b''b'Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]'b'There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]'b'If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equation'b'where \xce\xbb is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.'b'In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation'
Eigenvalues and eigenvectors
b"The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering."b'In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.'b'In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.'b'Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.'b'In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.'b'The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.'b'This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.'b'leads to a so-called quadratic eigenvalue problem,'b'or'b'Eigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed by'b"Principal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling."b'The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.'b"In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]"b"In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree\xe2\x80\x93Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree\xe2\x80\x93Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations."b'A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.'b'The following table presents some example transformations in the plane along with their 2\xc3\x972 matrices, eigenvalues, and eigenvectors.'b'Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.'b'This matrix equation is equivalent to two linear equations'b'Once the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrix'b'Efficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]'b"In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]"b'A similar procedure is used for solving a differential equation of the form'b'The solution of this equation for x in terms of t is found by using its characteristic equation'b'The simplest difference equations have the form'b'The representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.'b'One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation \xe2\x80\x93 an associative algebra acting on a module. The study of such actions is the field of representation theory.'b'For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars \xce\xbb for which the operator (T \xe2\x88\x92 \xce\xbbI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.'b'If \xce\xbb is an eigenvalue of T, then the operator (T \xe2\x88\x92 \xce\xbbI) is not one-to-one, and therefore its inverse (T \xe2\x88\x92 \xce\xbbI)\xe2\x88\x921 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T \xe2\x88\x92 \xce\xbbI) may not have an inverse even if \xce\xbb is not an eigenvalue.'b'Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar \xce\xbb \xe2\x88\x88 K such that there exists a non-zero vector v \xe2\x88\x88 V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue \xce\xbb to be any vector that, given \xce\xbb, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.'b'While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]'b'Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.'b'The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]'b'The geometric multiplicity \xce\xb3T(\xce\xbb) of an eigenvalue \xce\xbb is the dimension of the eigenspace associated with \xce\xbb, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, \xce\xb3T(\xce\xbb) \xe2\x89\xa5 1 because every eigenvalue has at least one eigenvector.'b'So, both u + v and \xce\xb1v are either zero or eigenvectors of T associated with \xce\xbb, namely (u+v,\xce\xb1v) \xe2\x88\x88 E, and E is closed under addition and scalar multiplication. The eigenspace E associated with \xce\xbb is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]'b'for (x,y) \xe2\x88\x88 V and \xce\xb1 \xe2\x88\x88 K. Therefore, if u and v are eigenvectors of T associated with eigenvalue \xce\xbb, namely (u,v) \xe2\x88\x88 E, then'b'By definition of a linear transformation,'b'which is the union of the zero vector with the set of all eigenvectors associated with\xc2\xa0\xce\xbb. E is called the eigenspace or characteristic space of T associated with\xc2\xa0\xce\xbb.'b'Given an eigenvalue \xce\xbb, consider the set'b'This equation is called the eigenvalue equation for T, and the scalar \xce\xbb is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while \xce\xbbv is the product of the scalar \xce\xbb with v.[33]'b'We say that a non-zero vector v \xe2\x88\x88 V is an eigenvector of T if and only if there exists a scalar \xce\xbb \xe2\x88\x88 K such that'b'The concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,'b'The main eigenfunction article gives other examples.'b'is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for \xce\xbb = 0 the eigenfunction f(t) is a constant.'b'This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential function'b'The functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.'b'The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C\xe2\x88\x9e of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equation'b'On the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 \xe2\x88\x921 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity \xce\xb3A is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.'b'The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is \xce\xbcA = 4 = n, the order of the characteristic polynomial and the dimension of A.'b'has a characteristic polynomial that is the product of its diagonal elements,'b'As in the previous example, the lower triangular matrix'b'respectively, as well as scalar multiples of these vectors.'b'These eigenvalues correspond to the eigenvectors,'b'which has the roots \xce\xbb1 = 1, \xce\xbb2 = 2, and \xce\xbb3 = 3. These roots are the diagonal elements as well as the eigenvalues of\xc2\xa0A.'b'The characteristic polynomial of A is'b'Consider the lower triangular matrix,'b'A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.'b'respectively, as well as scalar multiples of these vectors.'b'Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,'b'which has the roots \xce\xbb1 = 1, \xce\xbb2 = 2, and \xce\xbb3 = 3. These roots are the diagonal elements as well as the eigenvalues of\xc2\xa0A.'b'The characteristic polynomial of A is'b'Matrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrix'b'and'b'Then'b'For the complex conjugate pair of imaginary eigenvalues, note that'b'For the real eigenvalue \xce\xbb1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,'b'This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1\xc2\xa0\xe2\x88\x92\xc2\xa0\xce\xbb3, whose roots are'b'Consider the cyclic permutation matrix'b'The characteristic polynomial of A is'b'Consider the matrix'b'Thus, the vectors v\xce\xbb=1 and v\xce\xbb=3 are eigenvectors of A associated with the eigenvalues \xce\xbb = 1 and \xce\xbb = 3, respectively.'b'is an eigenvector of A corresponding to \xce\xbb = 3, as is any scalar multiple of this vector.'b'Any non-zero vector with v1 = v2 solves this equation. Therefore,'b'For \xce\xbb = 3, Equation (2) becomes'b'is an eigenvector of A corresponding to \xce\xbb = 1, as is any scalar multiple of this vector.'b'Any non-zero vector with v1 = \xe2\x88\x92v2 solves this equation. Therefore,'b'For \xce\xbb = 1, Equation (2) becomes,'b'Setting the characteristic polynomial equal to zero, it has roots at \xce\xbb = 1 and \xce\xbb = 3, which are the two eigenvalues of A.'b'Taking the determinant to find characteristic polynomial of A,'b'The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of \xce\xbb for which the determinant of the matrix (A\xc2\xa0\xe2\x88\x92\xc2\xa0\xce\xbbI) equals zero are the eigenvalues.'b'Consider the matrix'b'A matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.'b'Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P\xe2\x88\x921AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.'b'A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix \xce\x9b or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and \xce\x9b represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as\xc2\xa0\xce\x9b.'b'or by instead left multiplying both sides by Q\xe2\x88\x921,'b'Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q\xe2\x88\x921,'b'With this in mind, define a diagonal matrix \xce\x9b where each diagonal element \xce\x9bii is the eigenvalue associated with the ith column of Q. Then'b'Since each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,'b'Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,'b'Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.'b'where \xce\xba is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and \xce\xba is its associated eigenvalue. Taking the transpose of this equation,'b'The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation is'b'Many disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiples the n by n matrix A in the defining equation, Equation (1),'b"Let A be an arbitrary n by n matrix of complex numbers with eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbn. Each eigenvalue appears \xce\xbcA(\xce\xbbi) times in this list, where \xce\xbcA(\xce\xbbi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:"b"is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If \xce\xb3A = n, then"b'Suppose A has d \xe2\x89\xa4 n distinct eigenvalues \xce\xbb1, \xce\xbb2, ..., \xce\xbbd, where the geometric multiplicity of \xce\xbbi is \xce\xb3A(\xce\xbbi). The total geometric multiplicity of A,'b'The condition that \xce\xb3A(\xce\xbb) \xe2\x89\xa4 \xce\xbcA(\xce\xbb) can be proven by considering a particular eigenvalue \xce\xbe of A and diagonalizing the first \xce\xb3A(\xce\xbe) columns of A with respect to the eigenvectors of \xce\xbe, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix \xce\xbeI\xce\xb3A(\xce\xbe). As a result, the characteristic polynomial of B will have a factor of (\xce\xbe\xc2\xa0\xe2\x88\x92\xc2\xa0\xce\xbb)\xce\xb3A(\xce\xbe). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of \xce\xbe as an eigenvalue of B is no less than the geometric multiplicity of \xce\xbe as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.'b"Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n."b"The dimension of the eigenspace E associated with \xce\xbb, or equivalently the maximum number of linearly independent eigenvectors associated with \xce\xbb, is referred to as the eigenvalue's geometric multiplicity \xce\xb3A(\xce\xbb). Because E is also the nullspace of (A \xe2\x88\x92 \xce\xbbI), the geometric multiplicity of \xce\xbb is the dimension of the nullspace of (A \xe2\x88\x92 \xce\xbbI), also called the nullity of (A \xe2\x88\x92 \xce\xbbI), which relates to the dimension and rank of (A \xe2\x88\x92 \xce\xbbI) as"b'Because the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) \xe2\x88\x88 E, then (u + v) \xe2\x88\x88 E or equivalently A(u + v) = \xce\xbb(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v \xe2\x88\x88 E and \xce\xb1 is a complex number, (\xce\xb1v) \xe2\x88\x88 E or equivalently A(\xce\xb1v) = \xce\xbb(\xce\xb1v). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and \xce\xb1v are not zero, they are also eigenvectors of A associated with \xce\xbb.'b'On one hand, this set is precisely the kernel or nullspace of the matrix (A \xe2\x88\x92 \xce\xbbI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with \xce\xbb. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with \xce\xbb, and E equals the nullspace of (A \xe2\x88\x92 \xce\xbbI). E is called the eigenspace or characteristic space of A associated with \xce\xbb.[7][8] In general \xce\xbb is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of \xe2\x84\x82n.'b'Given a particular eigenvalue \xce\xbb of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),'b'If \xce\xbcA(\xce\xbbi) = 1, then \xce\xbbi is said to be a simple eigenvalue.[27] If \xce\xbcA(\xce\xbbi) equals the geometric multiplicity of \xce\xbbi, \xce\xb3A(\xce\xbbi), defined in the next section, then \xce\xbbi is said to be a semisimple eigenvalue.'b"If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n as"b'Suppose a matrix A has dimension n and d \xe2\x89\xa4 n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,'b'Let \xce\xbbi be an eigenvalue of an n by n matrix A. The algebraic multiplicity \xce\xbcA(\xce\xbbi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (\xce\xbb \xe2\x88\x92 \xce\xbbi)k divides evenly that polynomial.[8][26][27]'b'The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.'b'If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.'b'Setting the characteristic polynomial equal to zero, it has roots at \xce\xbb = 1 and \xce\xbb = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = \xce\xbbv. In this example, the eigenvectors are any non-zero scalar multiples of'b'Taking the determinant of (M \xe2\x88\x92 \xce\xbbI), the characteristic polynomial of M is'b'As a brief example, which is described in more detail in the examples section later, consider the matrix'b'where each \xce\xbbi may be real but in general is a complex number. The numbers \xce\xbb1, \xce\xbb2, ... \xce\xbbn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.'b'The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,'b"Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable \xce\xbb and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (\xe2\x88\x921)n\xce\xbbn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A."b'Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A \xe2\x88\x92 \xce\xbbI) is zero. Therefore, the eigenvalues of A are values of \xce\xbb that satisfy the equation'b'where I is the n by n identity matrix.'b'Equation (1) can be stated equivalently as'b'then v is an eigenvector of the linear transformation A and the scale factor \xce\xbb is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.'b'If it occurs that v and w are scalar multiples, that is if'b'where, for each row,'b'or'b'Now consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,'b'In this case \xce\xbb = \xe2\x88\x921/20.'b'These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar \xce\xbb such that'b'Consider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectors'b'Eigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]'b'The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]'b'At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]'b"In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm\xe2\x80\x93Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincar\xc3\xa9 studied Poisson's equation a few years later.[16]"b"Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Th\xc3\xa9orie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]"b'In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caract\xc3\xa9ristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]'b'Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.'b'Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:'b'where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.'b'Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplication'b'The Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.'b'referred to as the eigenvalue equation or eigenequation. In general, \xce\xbb may be any scalar. For example, \xce\xbb may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.'b'In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value \xce\xbb, called an eigenvalue. This condition can be written as the equation'b'Eigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.'b''b''b'Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]'b'There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]'b'If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equation'b'where \xce\xbb is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.'b'In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation'
Determinant
b'where \xcf\x89j is an nth root of 1.'b'where \xcf\x89 and \xcf\x892 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]'b'Third order'b'Second order'b'where the right-hand side is the continued product of all the differences that can be formed from the n(n\xe2\x88\x921)/2 pairs of numbers taken from x1, x2, \xe2\x80\xa6, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.'b'In general, the nth-order Vandermonde determinant is[33]'b'The third order Vandermonde determinant is'b'The Jacobian also occurs in the inverse function theorem.'b'Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function \xcf\x86: Rn \xe2\x86\x92 Rm is given by'b'the Jacobian matrix is the n \xc3\x97 n matrix whose entries are given by'b'For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. For'b'By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)\xc2\xb7|det(a \xe2\x88\x92 b, b \xe2\x88\x92 c, c \xe2\x88\x92 d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.'b'As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn \xe2\x86\x92 Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn \xe2\x86\x92 Rm is represented by the m \xc3\x97 n matrix A, then the n-dimensional volume of f(S) is given by:'b'More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 \xc3\x97 2 or 3 \xc3\x97 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.'b'The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is \xe2\x88\x921, the basis has the opposite orientation.'b'It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n\xe2\x88\x921 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.'b'As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 \xc3\x97 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), \xe2\x80\xa6, fn(x) (supposed to be n \xe2\x88\x92 1 times differentiable), the Wronskian is defined to be'b"The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises."b"The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]"b"The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy\xe2\x80\x93Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality."b'Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.'b'It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.'b'In Japan, Seki Takakazu (\xe9\x96\xa2 \xe5\xad\x9d\xe5\x92\x8c) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by B\xc3\xa9zout (1764).'b'Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (\xe4\xb9\x9d\xe7\xab\xa0\xe7\xae\x97\xe8\xa1\x93, Chinese scholars, around the 3rd century BCE). In Europe, 2 \xc3\x97 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]'b"Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]"b"Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form."b'If two matrices of order n can be multiplied in time M(n), where M(n) \xe2\x89\xa5 na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith\xe2\x80\x93Winograd algorithm.'b'Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.'b'If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.'b'(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant\xc2\xa01, in which case the formula further simplifies to'b'The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:'b'Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)'b"Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n \xc3\x97 n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants."b'Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.'b"The permanent of a matrix is defined as the determinant, except that the factors sgn(\xcf\x83) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule."b'Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]'b'For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonn\xc3\xa9 determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.'b'Another infinite-dimensional notion of determinant is the functional determinant.'b'The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula'b'For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.'b'For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo\xc2\xa0m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo\xc2\xa0m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (\xe2\x8b\x85)\xc3\x97 (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,'b'holds. In other words, the following diagram commutes:'b'between the group of invertible n \xc3\x97 n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R \xe2\x86\x92 S, there is a map GLn(f): GLn(R) \xe2\x86\x92 GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identity'b'The determinant defines a mapping'b'This definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or \xe2\x88\x921. Such a matrix is called unimodular.'b'This fact also implies that every other n-linear alternating function F: Mn(K) \xe2\x86\x92 K satisfies'b'for any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.'b'from the set of all n \xc3\x97 n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that is'b'The determinant can also be characterized as the unique function'b'The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T\xe2\x80\xb2 on W, where for each w in W we define (T\xe2\x80\xb2w)(x1, \xe2\x80\xa6, xn) = w(Tx1, \xe2\x80\xa6, Txn). As a linear transformation on a one-dimensional space, T\xe2\x80\xb2 is equivalent to a scalar multiple. We call this scalar the determinant of T.'b'For this reason, the highest non-zero exterior power \xce\x9bn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms \xce\x9bkV with k < n.'b'This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 \xe2\x88\xa7 v2 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn to v2 \xe2\x88\xa7 v1 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn, say, also changes its sign.'b'As \xce\x9bnV is one-dimensional, the map \xce\x9bnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to say'b'The determinant of a linear transformation A\xc2\xa0: V \xe2\x86\x92 V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power \xce\x9bnV of V. A induces a linear map'b'for some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.'b'The determinant is therefore also called a similarity invariant. The determinant of a linear transformation'b'The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X\xe2\x88\x921BX. Indeed, repeatedly applying the above identities yields'b'This identity is used in describing the tangent space of certain matrix Lie groups.'b'Yet another equivalent formulation is'b'Expressed in terms of the entries of A, these are'b'where adj(A) denotes the adjugate of A. In particular, if A is invertible, we have'b"By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn \xc3\x97 n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]"b'When D is a 1\xc3\x971 matrix, B is a column vector, and C is a row vector then'b'When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)'b'Generally, if all pairs of n \xc3\x97 n matrices of the np \xc3\x97 np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p \xc3\x97 p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.'b'When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 \xc3\x97 2 matrix holds:[12]'b'as can be seen by employing the decomposition'b'When A is invertible, one has'b'This can be seen from the Leibniz formula, or from a decomposition like (for the former case)'b'Suppose A, B, C, and D are matrices of dimension n \xc3\x97 n, n \xc3\x97 m, m \xc3\x97 n, and m \xc3\x97 m, respectively. Then'b"It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition."b'where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.'b"the solution is given by Cramer's rule:"b'For a matrix equation'b'These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.'b'Also,'b'with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.'b'For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinant'b'is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).'b'where I is the identity matrix. More generally, if'b'An important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,'b'This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way as'b'The formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l \xe2\x80\x93 1)! tr(Al) as'b'where the sum is taken over the set of all integers kl \xe2\x89\xa5 0 satisfying the equation'b'In the general case, this may also be obtained from[9]'b"cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev\xe2\x80\x93LeVerrier algorithm. That is, for generic n, detA = (\xe2\x88\x92)nc0 the signed constant term of the characteristic polynomial, determined recursively from"b'For example, for n = 2, n = 3, and n = 4, respectively,'b'the determinant of A is given by'b'Here exp(A) denotes the matrix exponential of A, because every eigenvalue \xce\xbb of A corresponds to the eigenvalue exp(\xce\xbb) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfying'b'or, for real matrices A,'b'The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,'b'being positive, for all k between 1 and n.'b"A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices"b'where I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).'b'Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equation'b'The product of all non-zero eigenvalues is referred to as pseudo-determinant.'b'From this general result several consequences follow.'b'where Im and In are the m \xc3\x97 m and n \xc3\x97 n identity matrices, respectively.'b"Sylvester's determinant theorem states that for A, an m \xc3\x97 n matrix, and B, an n \xc3\x97 m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):"b"In terms of the adjugate matrix, Laplace's expansion can be written as[7]"b'The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,'b'However, Laplace expansion is efficient for small matrices only.'b'along the second column (j = 2 and the sum runs over i) is given by,'b'Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 \xc3\x97 3 matrix'b"Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n\xe2\x88\x921) \xc3\x97 (n\xe2\x88\x921)-matrix that results from A by removing the i-th row and the j-th column. The expression (\xe2\x88\x921)i+jMi,j is known as a cofactor. The determinant of A is given by"b'In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.'b'The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by'b'Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value\xc2\xa01 on the identity matrix, since the function Mn(K) \xe2\x86\x92 K that maps M \xe2\x86\xa6 det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy\xe2\x80\x93Binet formula, which also provides an independent proof of the multiplicative property.'b'The determinant of a matrix product of square matrices equals the product of their determinants:'b'Here, B is obtained from A by adding \xe2\x88\x921/2\xc3\x97the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = \xe2\x88\x92det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (\xe2\x88\x922) \xc2\xb7 2 \xc2\xb7 4.5 = \xe2\x88\x9218. Therefore, det(A) = \xe2\x88\x92det(D) = +18.'b'can be computed using the following matrices:'b'For example, the determinant of'b'Property 5 says that the determinant on n \xc3\x97 n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property\xc2\xa06; this is essentially the method of Gaussian elimination.'b'Property 2 above implies that properties for columns have their counterparts in terms of rows:'b'Properties 1, 8 and 10 \xe2\x80\x94 which all follow from the Leibniz formula \xe2\x80\x94 completely characterize the determinant; in other words the determinant is the unique function from n \xc3\x97 n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property\xc2\xa09) or else \xc2\xb11 (by properties 1 and\xc2\xa012 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n \xe2\x89\xa5 2,[6] so there is no good definition of the determinant in this setting.'b'A number of additional properties relate to the effects on the determinant of changing particular rows or columns:'b'This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.'b'The determinant has many properties. Some basic properties of determinants are'b'where now each ir and each jr should be summed over 1, \xe2\x80\xa6, n.'b'or using two epsilon symbols as'b'For example, the determinant of a 3 \xc3\x97 3 matrix A (n = 3) is'b'is notation for the product of the entries at positions (i, \xcf\x83i), where i ranges from 1 to n:'b'Here the sum is computed over all permutations \xcf\x83 of the set {1, 2, \xe2\x80\xa6, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering \xcf\x83 is denoted by \xcf\x83i. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to \xcf\x83 = [2, 3, 1], with \xcf\x831 = 2, \xcf\x832 = 3, and \xcf\x833 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation \xcf\x83, sgn(\xcf\x83) denotes the signature of \xcf\x83, a value that is +1 whenever the reordering given by \xcf\x83 can be achieved by successively interchanging two entries an even number of times, and \xe2\x88\x921 whenever it can be achieved by an odd number of such interchanges.'b'The Leibniz formula for the determinant of an n \xc3\x97 n matrix A is'b'The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.'b'The rule of Sarrus is a mnemonic for the 3 \xc3\x97 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 \xc3\x97 3 matrix does not carry over into higher dimensions.'b'which is the Leibniz formula for the determinant of a 3 \xc3\x97 3 matrix.'b'this can be expanded out to give'b'The Laplace formula for the determinant of a 3 \xc3\x97 3 matrix is'b'The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) \xe2\x88\xa7 (c, d)) is the signed area, which is also the determinant ad \xe2\x88\x92 bc.[3]'b'Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.'b"To show that ad \xe2\x88\x92 bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sin\xce\xb8 for the angle \xce\xb8 between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a\xe2\x8a\xa5 = (-b, a), such that |a\xe2\x8a\xa5||b|cos\xce\xb8' , which can be determined by the pattern of the scalar product to be equal to ad \xe2\x88\x92 bc:"b'The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).'b'The absolute value of ad \xe2\x88\x92 bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)'b'If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.'b'The Leibniz formula for the determinant of a 2 \xc3\x97 2 matrix is'b'The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:'b'The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.'b'Assume A is a square matrix with n rows and n columns, so that it can be written as'b'Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is \xe2\x88\x921 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n \xc3\x97 n matrix contributes n! terms), so it will first be given explicitly for the case of 2 \xc3\x97 2 matrices and 3 \xc3\x97 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.'b'where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]'b'Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n \xc3\x97 n matrix A in terms of its column vectors'b'There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 \xc3\x97 4 matrix:'b''b''b'When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.'b'Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.'b'Each determinant of a 2 \xc3\x97 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 \xc3\x97 4 matrix, the determinant of a 5 \xc3\x97 5 matrix, and so forth.'b'Similarly, suppose we have a 3 \xc3\x97 3 matrix A, and we want the specific formula for its determinant |A|:'b'In the case of a 2 \xc3\x97 2 matrix the specific formula for the determinant is:'b'In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.'
Invertible matrix
b'Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.'b'Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.'b'Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.'b'Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.'b'For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.'b'Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore\xe2\x80\x93Penrose inverse), which can be defined for any m-by-n matrix.'b'Therefore,'b'then,'b'More generally, if'b'Suppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given by'b'If it is also the case that A \xe2\x88\x92 X has rank 1 then this simplifies to'b'then A is nonsingular and its inverse is'b'More generally, if A is "near" the invertible matrix X in the sense that'b'Truncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies'b'then A is nonsingular and its inverse may be expressed by a Neumann series:[11]'b'If a matrix A has the property that'b'Since a blockwise inversion of an n \xc3\x97 n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is \xce\xa9(n2 log n).[10]'b'where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.'b'Equating Equations (1) and (2) leads to'b'The inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A \xe2\x88\x92 BD\xe2\x88\x921C are nonsingular,[8] the result is'b'The nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.'b'This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.'b'where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D \xe2\x88\x92 CA\xe2\x88\x921B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D \xe2\x88\x92 CA\xe2\x88\x921B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.'b'Matrices can also be inverted blockwise by using the following analytic inversion formula:'b'With increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley\xe2\x80\x93Hamilton method leads to an expression that is still tractable:'b'The Cayley\xe2\x80\x93Hamilton decomposition gives'b'The determinant of A can be computed by applying the rule of Sarrus as follows:'b'(where the scalar A is not to be confused with the matrix A). If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given by'b'A computationally efficient 3 \xc3\x97 3 matrix inversion is given by'b'The Cayley\xe2\x80\x93Hamilton method gives'b'This is possible because 1/(ad \xe2\x88\x92 bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.'b'The cofactor equation listed above yields the following result for 2 \xc3\x97 2 matrices. Inversion of these matrices can be done as follows:[6]'b'where |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.'b'so that'b'Writing the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:'b'where L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.'b'If matrix A is positive definite, then its inverse can be obtained as'b'If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given by'b'The formula can be rewritten in terms of complete Bell polynomials of arguments tl = - (l - 1)! tr(Al) as'b'where n is dimension of A, and tr(A) is the trace of matrix A given by the sum of the main diagonal. The sum is taken over s and the sets of all kl \xe2\x89\xa5 0 satisfying the linear Diophantine equation'b'The Cayley\xe2\x80\x93Hamilton theorem allows the inverse of A to be expressed in terms of det(A), traces and powers of A [5]'b'Newton\'s method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton\'s method is also useful for "touch up" corrections to the Gauss\xe2\x80\x93Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.'b'Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]'b"A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:"b'Gauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.'b'As an example of a non-invertible, or singular, matrix, consider the matrix'b'Consider the following 2-by-2 matrix:'b'In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.'b'Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices. Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.'b'Over the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn\xc3\x97n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.'b'for finite square matrices A and B, then also'b'It follows from the theory of matrices that if'b'A matrix that is its own inverse, i.e. such that A = A\xe2\x88\x921 and A2 = I, is called an involutory matrix.'b'Furthermore, the following properties hold for an invertible matrix A:'b'Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:'b''b''b'The set of n \xc3\x97 n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.'b'While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined. The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.'b'Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.'b'Non-square matrices (m-by-n matrices for which m \xe2\x89\xa0 n) do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse. If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In. If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.'b'A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.'b'where In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A\xe2\x88\x921.'b'In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that'
Inverse element
b'which is a singular matrix, and cannot be inverted.'b"The left inverse doesn't exist, because"b'As an example of matrix inverses, consider:'b'No rank deficient matrix has any (even one-sided) inverse. However, the Moore\xe2\x80\x93Penrose inverse exists for all matrices, and coincides with the left or right (or true) inverse when it exists.'b'Non-square matrices of full rank have several one-sided inverses:[3]'b'The lower and upper adjoints in a (monotone) Galois connection, L and G are quasi-inverses of each other, i.e. LGL = L and GLG = G and one uniquely determines the other. They are not left or right inverses of each other however.'b'All examples in this section involve associative operators, thus we shall use the terms left/right inverse for the unital magma-based definition, and quasi-inverse for its more general version.'b'Clearly a group is both an I-semigroup and a *-semigroup. A class of semigroups important in semigroup theory are completely regular semigroups; these are I-semigroups in which one additionally has aa\xc2\xb0 = a\xc2\xb0a; in other words every element has commuting pseudoinverse a\xc2\xb0. There are few concrete examples of such semigroups however; most are completely simple semigroups. In contrast, a subclass of *-semigroups, the *-regular semigroups (in the sense of Drazin), yield one of best known examples of a (unique) pseudoinverse, the Moore\xe2\x80\x93Penrose inverse. In this case however the involution a* is not the pseudoinverse. Rather, the pseudoinverse of x is the unique element y such that xyx = x, yxy = y, (xy)* = xy, (yx)* = yx. Since *-regular semigroups generalize inverse semigroups, the unique element defined this way in a *-regular semigroup is called the generalized inverse or Penrose\xe2\x80\x93Moore inverse.'b'A natural generalization of the inverse semigroup is to define an (arbitrary) unary operation \xc2\xb0 such that (a\xc2\xb0)\xc2\xb0 = a for all a in S; this endows S with a type \xe2\x9f\xa82,1\xe2\x9f\xa9 algebra. A semigroup endowed with such an operation is called a U-semigroup. Although it may seem that a\xc2\xb0 will be the inverse of a, this is not necessarily the case. In order to obtain interesting notion(s), the unary operation must somehow interact with the semigroup operation. Two classes of U-semigroups have been studied:[2]'b'Outside semigroup theory, a unique inverse as defined in this section is sometimes called a quasi-inverse. This is generally justified because in most applications (e.g., all examples in this article) associativity holds, which makes this notion a generalization of the left/right inverse relative to an identity.'b'In a monoid, the notion of inverse as defined in the previous section is strictly narrower than the definition given in this section. Only elements in the Green class H1 have an inverse from the unital magma perspective, whereas for any idempotent e, the elements of He have an inverse as defined in this section. Under this more general definition, inverses need not be unique (or exist) in an arbitrary semigroup or monoid. If all elements are regular, then the semigroup (or monoid) is called regular, and every element has at least one inverse. If every element has exactly one inverse as defined in this section, then the semigroup is called an inverse semigroup. Finally, an inverse semigroup with only one idempotent is a group. An inverse semigroup may have an absorbing element 0 because 000 = 0, whereas a group may not.'b"In a semigroup S an element x is called (von Neumann) regular if there exists some element z in S such that xzx = x; z is sometimes called a pseudoinverse. An element y is called (simply) an inverse of x if xyx = x and y = yxy. Every regular element has at least one inverse: if x = xzx then it is easy to verify that y = zxz is an inverse of x as defined in this section. Another easy to prove fact: if y is an inverse of x then e = xy and f = yx are idempotents, that is ee = e and ff = f. Thus, every pair of (mutually) inverse elements gives rise to two idempotents, and ex = xf = x, ye = fy = y, and e acts as a left identity on x, while f acts a right identity, and the left/right roles are reversed for y. This simple observation can be generalized using Green's relations: every idempotent e in an arbitrary semigroup is a left identity for Re and right identity for Le.[1] An intuitive description of this fact is that every pair of mutually inverse elements produces a local left identity, and respectively, a local right identity."b"The definition in the previous section generalizes the notion of inverse in group relative to the notion of identity. It's also possible, albeit less obvious, to generalize the notion of an inverse by dropping the identity element but keeping associativity, i.e., in a semigroup."b'A left-invertible element is left-cancellative, and analogously for right and two-sided.'b''b''b"The word 'inverse' is derived from Latin: inversus that means 'turned upside down', 'overturned'."b"In abstract algebra, the idea of an inverse element generalises concepts of a negation (sign reversal) in relation to addition, and a reciprocal in relation to multiplication. The intuition is of an element that can 'undo' the effect of combination with another given element. While the precise definition of an inverse element varies depending on the algebraic structure involved, these definitions coincide in a group."
Kernel (linear algebra)
b'Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]'b'For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]'b'For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gr\xc3\xb6bner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]'b'If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]'b'The problem of computing the kernel on a computer depends on the nature of the coefficients.'b'are a basis of the kernel of A.'b'The last three columns of B are zero columns. Therefore, the three last vectors of C,'b'Putting the upper part in column echelon form by column operations on the whole matrix gives'b'Then'b'For example, suppose that'b'In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.'b'A basis of the kernel of a matrix may be computed by Gaussian elimination.'b'With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.'b'These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (\xe2\x88\x921,\xe2\x88\x9226,16)T.'b'which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.'b'Note also that the following dot products are zero:'b'The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (\xe2\x88\x921,\xe2\x88\x9226,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.'b'Since c is a free variable, this can be expressed equally well as,'b'for c a scalar.'b'Now we can express an element of the kernel:'b'Rewriting yields:'b'Gauss\xe2\x80\x93Jordan elimination reduces this to:'b'which can be written in matrix form as:'b'which can be expressed as a homogeneous system of linear equations involving x, y, and z:'b'The kernel of this matrix consists of all vectors (x, y, z)\xc2\xa0\xe2\x88\x88\xc2\xa0R3 for which'b'Consider the matrix'b'We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.'b'Geometrically, this says that the solution set to Ax\xc2\xa0=\xc2\xa0b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).'b'It follows that any solution to the equation Ax\xc2\xa0=\xc2\xa0b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax\xc2\xa0=\xc2\xa0b is'b'Thus, the difference of any two solutions to the equation Ax\xc2\xa0=\xc2\xa0b lies in the kernel of A.'b'If u and v are two possible solutions to the above equation, then'b'The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:'b'The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA\xc2\xa0=\xc2\xa00T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.'b'The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank\xe2\x80\x93nullity theorem'b'The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.'b'Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).'b'The product Ax can be written in terms of the dot product of vectors as follows:'b'The kernel of an m \xc3\x97 n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:'b'Thus the kernel of A is the same as the solution set to the above homogeneous equations.'b'The matrix equation is equivalent to a homogeneous system of linear equations:'b'Consider a linear map represented as a m \xc3\x97 n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,'b'If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L:\xc2\xa0V\xc2\xa0\xe2\x86\x92\xc2\xa0W is continuous if and only if the kernel of L is a closed subspace of V.'b'The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.'b'When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.'b'where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.'b'This implies the rank\xe2\x80\x93nullity theorem:'b'It follows that the image of L is isomorphic to the quotient of V by the kernel:'b'The kernel of L is a linear subspace of the domain V.[1] In the linear map L\xc2\xa0: V \xe2\x86\x92 W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:'b''b''b'In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L\xc2\xa0: V \xe2\x86\x92 W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,'
Cramer's rule

Gaussian elimination
b'Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.'b'This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.'b'Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]'b'The Gaussian elimination can be performed over any field, not just the real numbers.'b'One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]'b'This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).'b"The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 \xe2\x88\x92 5n)/6 multiplications, and (2n3 + 3n2 \xe2\x88\x92 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5)."b'All of this applies also to the reduced row echelon form, which is a particular row echelon form.'b'One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A\xe2\x88\x921. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.'b'By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:'b'To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:'b'For example, consider the following matrix'b'A variant of Gaussian elimination called Gauss\xe2\x80\x93Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A\xe2\x88\x921. If the algorithm is unable to reduce the left block to I, then A is not invertible.'b'Computationally, for a n\xc3\x97n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.'b'If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = \xe2\x88\x8fdiag(B) / d.'b'To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:'b'The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.'b'Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss\xe2\x80\x93Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss\xe2\x80\x93Jordan elimination independently.[7]'b'The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]'b'The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.'b'Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.'b'Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.'b'Suppose the goal is to find and describe the set of solutions to the following system of linear equations:'b'A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).'b'It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).'b'For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.'b'For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.'b"If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier."b'There are three types of elementary row operations which may be performed on the rows of a matrix:'b'Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.'b'The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.'b''b''b'Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss\xe2\x80\x93Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.'b'To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.'b'In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777\xe2\x80\x931855), although it was known to Chinese mathematicians as early as 179 CE (see History section).'
Standard basis
b'In physics, the standard basis vectors for a given Euclidean space are sometimes referred to as the versors of the axes of the corresponding Cartesian coordinate system.'b'Gr\xc3\xb6bner bases are also sometimes called standard bases.'b"The existence of other 'standard' bases has become a topic of interest in algebraic geometry, beginning with work of Hodge from 1943 on Grassmannians. It is now a part of representation theory called standard monomial theory. The idea of standard basis in the universal enveloping algebra of a Lie algebra is established by the Poincar\xc3\xa9\xe2\x80\x93Birkhoff\xe2\x80\x93Witt theorem."b'from I into a ring R, which are zero except for a finite number of indices, if we interpret 1 as 1R, the unit in R.'b'of all families'b'All of the preceding are special cases of the family'b'There is a standard basis also for the ring of polynomials in n indeterminates over a field, namely the monomials.'b'are also orthogonal unit vectors, but they are not aligned with the axes of the Cartesian coordinate system, so the basis with these vectors does not meet the definition of standard basis.'b'However, an ordered orthonormal basis is not necessarily a standard basis. For instance the two vectors representing a 30\xc2\xb0 rotation of the 2D standard basis described above, i.e.'b'By definition, the standard basis is a sequence of orthogonal unit vectors. In other words, it is an ordered and orthonormal basis.'b''b''b'the scalars vx,\xc2\xa0vy,\xc2\xa0vz being the scalar components of the vector v.'b'These vectors are a basis in the sense that any other vector can be expressed uniquely as a linear combination of these. For example, every vector v in three-dimensional space can be written uniquely as'b'Here the vector ex points in the x direction, the vector ey points in the y direction, and the vector ez points in the z direction. There are several common notations for these vectors, including {ex,\xc2\xa0ey,\xc2\xa0ez}, {e1,\xc2\xa0e2,\xc2\xa0e3}, {i,\xc2\xa0j,\xc2\xa0k}, and {x,\xc2\xa0y,\xc2\xa0z}. These vectors are sometimes written with a hat to emphasize their status as unit vectors. Each of these vectors is sometimes referred to as the versor of the corresponding Cartesian axis.'b'and the standard basis for three-dimensional space is formed by vectors'b'In mathematics, the standard basis (also called natural basis) for a Euclidean space is the set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. For example, the standard basis for the Euclidean plane is formed by vectors'
Matrix (mathematics)
b'Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]'b'For example, a function \xce\xa6(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, \xe2\x88\x80ai: \xce\xa6(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:'b'Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910\xe2\x80\x931913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:'b'The word has been used in unusual ways by at least two authors of historical importance.'b'The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.'b'Many theorems were first established for small matrices only, for example the Cayley\xe2\x80\x93Hamilton theorem was proved for 2\xc3\x972 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4\xc3\x974 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss\xe2\x80\x93Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.'b'where \xce\xa0 denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"\xe2\x80\x94later called Jacobi determinants by Sylvester\xe2\x80\x94which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker\'s Vorlesungen \xc3\xbcber die Theorie der Determinanten[113] and Weierstrass\' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.'b'The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy \xe2\x88\x92 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomial'b'An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]'b"Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley\xe2\x80\x93Hamilton theorem.[103]"b'The term "matrix" (Latin for "womb", derived from mater\xe2\x80\x94mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:'b'Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th\xe2\x80\x932nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.'b"The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H \xc2\xb7 A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices."b'Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.'b"Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]"b"A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]"b'Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]'b'The first model of quantum mechanics (Heisenberg, 1925) represented the theory\'s operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]'b'Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo\xe2\x80\x93Kobayashi\xe2\x80\x93Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]'b'Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]'b'which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]'b'Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear function'b'Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]'b'The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]'b'Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]'b'If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]'b'Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn \xe2\x86\x92 Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]'b'The Hessian matrix of a differentiable function \xc6\x92: Rn \xe2\x86\x92 R consists of the second derivatives of \xc6\x92 with respect to the several coordinate directions, that is,[81]'b'The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.'b'Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree\xe2\x80\x93Fock method.'b'Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.'b'under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.'b'Complex numbers can be represented by particular real 2-by-2 matrices via'b'There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]'b'An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant\xc2\xa01, a fact that is often used as a part of the characterization of determinants.'b'In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.'b'If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.'b'If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V\xe2\x86\x92W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector\xc2\xa0v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A\xc2\xb7v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.'b'It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.'b'Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]'b'form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or \xe2\x88\x921. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.'b'Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the condition'b'A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.'b'More generally, the set of m\xc3\x97n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n\xc2\xa0=\xc2\xa0m composition of these maps is possible, and this gives rise to the matrix ring of n\xc3\x97n matrices representing the endomorphism ring of Rn.'b'In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]'b'Linear maps Rn \xe2\x86\x92 Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V \xe2\x86\x92 W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such that'b'Matrices do not always have all their entries in the same ring\xc2\xa0\xe2\x80\x93 or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.'b'More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]'b'This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.'b'Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.'b'and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]'b'The eigendecomposition or diagonalization expresses A as a product VDV\xe2\x88\x921, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues \xce\xbb1 to \xce\xbbn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via'b'The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV\xe2\x88\x97, where U and V are unitary matrices and D is a diagonal matrix.'b'There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.'b'Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]'b"may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]"b"An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)"b'In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]'b'Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.'b'To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.'b'Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]'b'The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn\xe2\x88\x92A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(\xce\xbb)\xc2\xa0=\xc2\xa00 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley\xe2\x80\x93Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.'b'are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number \xce\xbb is an eigenvalue of an n\xc3\x97n-matrix A if and only if A\xe2\x88\x92\xce\xbbIn is not invertible, which is equivalent to'b'A number \xce\xbb and a non-zero vector v satisfying'b"Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by \xe2\x88\x921.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]"b'The determinant of a product of square matrices equals the product of their determinants:'b'The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]'b'The determinant of 2-by-2 matrices is given by'b'The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.'b'Also, the trace of a matrix is equal to that of its transpose, that is,'b'This is immediate from the definition of matrix multiplication:'b'The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:'b'The complex analogue of an orthogonal matrix is a unitary matrix.'b'An orthogonal matrix A is necessarily invertible (with inverse A\xe2\x88\x921 = AT), unitary (A\xe2\x88\x921 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or \xe2\x88\x921. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.'b'where I is the identity matrix of size n.'b'which entails'b'An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:'b'Allowing as input two different vectors instead yields the bilinear form associated to A:'b'A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.'b'produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.'b'A symmetric n\xc3\x97n-matrix A is called positive-definite if for all nonzero vectors x\xc2\xa0\xe2\x88\x88\xc2\xa0Rn the associated quadratic form given by'b'where In is the n\xc3\x97n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A\xe2\x88\x921.'b'A square matrix A is called invertible or non-singular if there exists a matrix B such that'b'By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.'b'A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = \xe2\x88\x92AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A\xe2\x88\x97 = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.'b'A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.'b'It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:'b'The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,'b'If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.'b'A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.'b'The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank\xe2\x80\x93nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]'b'The last equality follows from the above-mentioned associativity of matrix multiplication.'b'Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g\xc2\xa0: Rm \xe2\x86\x92 Rk, then the composition g \xe2\x88\x98 f is represented by BA since'b'The following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.'b'For example, the 2\xc3\x972 matrix'b'Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn \xe2\x86\x92 Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn \xe2\x86\x92 Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.'b'where A\xe2\x88\x921 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.'b'Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writing'b'is equivalent to the system of linear equations'b'Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n\xc3\x971-matrix) of n variables x1, x2, ..., xn, and b is an m\xc3\x971-column vector, then the matrix equation'b'A principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]'b'The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]'b'A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:'b'These operations are used in a number of ways, including solving linear equations and finding matrix inverses.'b'There are three types of row operations:'b'Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.'b'whereas'b'that is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:'b'Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m \xe2\x89\xa0 k. Even if both products are defined, they need not be equal, that is, generally'b'where 1 \xe2\x89\xa4 i \xe2\x89\xa4 m and 1 \xe2\x89\xa4 j \xe2\x89\xa4 p.[13] For example, the underlined entry 2340 in the product is calculated as (2 \xc3\x97 1000) + (3 \xc3\x97 100) + (4 \xc3\x97 10) = 2340:'b'Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:'b'Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A\xc2\xa0+\xc2\xa0B\xc2\xa0=\xc2\xa0B\xc2\xa0+\xc2\xa0A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A\xc2\xa0+\xc2\xa0B)T\xc2\xa0=\xc2\xa0AT\xc2\xa0+\xc2\xa0BT. Finally, (AT)T\xc2\xa0=\xc2\xa0A.'b'There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]'b'An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,\xe2\x88\x97 refers to the ith row of A, and a\xe2\x88\x97,j refers to the jth column of A. The set of all m-by-n matrices is denoted \xf0\x9d\x95\x84(m, n).'b'Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-\xc3\x97-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 \xe2\x89\xa4 i \xe2\x89\xa4 m \xe2\x88\x92 1 and 0 \xe2\x89\xa4 j \xe2\x89\xa4 n \xe2\x88\x92 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.'b'In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m \xc3\x97 n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m \xc3\x97 n as a subscript. For instance, the matrix A above is 3 \xc3\x97 4 and can be defined as A = [i \xe2\x88\x92 j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i \xe2\x88\x92 j]3\xc3\x974.'b'Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i \xe2\x88\x92 j.'b'The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):'b'Matrices are commonly written in box brackets or parentheses:'b'Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.'b'The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m\xc2\xa0\xc3\x97\xc2\xa0n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3\xc2\xa0\xc3\x97\xc2\xa02 matrix.'b'The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.'b'A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:'b''b''b'A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.'b'Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.'b"The individual items in an m \xc3\x97 n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n \xc3\x97 Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors."b'In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 \xc3\x97 3 (read "two by three"), because there are two rows and three columns:'
Matrix similarity
b"In the definition of similarity, if the matrix P can be chosen to be a permutation matrix then A and B are permutation-similar; if P can be chosen to be a unitary matrix then A and B are unitarily equivalent. The spectral theorem says that every normal matrix is unitarily equivalent to some diagonal matrix. Specht's theorem states that two matrices are unitarily equivalent if and only if they satisfy certain trace equalities."b'Similarity of matrices does not depend on the base field: if L is a field containing K as a subfield, and A and B are two matrices over K, then A and B are similar as matrices over K if and only if they are similar as matrices over L. This is so because the rational canonical form over K is also the rational canonical form over L. This means that one may use Jordan forms that only exist over a larger field to determine whether the given matrices are similar.'b'Because of this, for a given matrix A, one is interested in finding a simple "normal form" B which is similar to A\xe2\x80\x94the study of A then reduces to the study of the simpler matrix B. For example, A is called diagonalizable if it is similar to a diagonal matrix. Not all matrices are diagonalizable, but at least over the complex numbers (or any algebraically closed field), every matrix is similar to a matrix in Jordan form. Neither of these forms is unique (diagonal entries or Jordan blocks may be permuted) so they are not really normal forms; moreover their determination depends on being able to factor the minimal or characteristic polynomial of A (equivalently to find its eigenvalues). The rational canonical form does not have these drawbacks: it exists over any field, is truly unique, and it can be computed using only arithmetic operations in the field; A and B are similar if and only if they have the same rational canonical form. The rational canonical form is determined by the elementary divisors of A; these can be immediately read off from a matrix in Jordan form, but they can also be determined directly for any matrix by computing the Smith normal form, over the ring of polynomials, of the matrix (with polynomial entries) XIn \xe2\x88\x92 A (the same one whose determinant defines the characteristic polynomial). Note that this Smith normal form is not a normal form of A itself; moreover it is not similar to XIn \xe2\x88\x92 A either, but obtained from the latter by left and right multiplications by different invertible matrices (with polynomial entries).'b'Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator:'b'Similarity is an equivalence relation on the space of square matrices.'b''b''b'A transformation A \xe2\x86\xa6 P\xe2\x88\x921AP is called a similarity transformation or conjugation of the matrix A. In the general linear group, similarity is therefore the same as conjugacy, and similar matrices are also called conjugate; however in a given subgroup H of the general linear group, the notion of conjugacy may be more restrictive than similarity, since it requires that P be chosen to lie in H.'b'for some invertible n-by-n matrix P. Similar matrices represent the same linear operator under two (possibly) different bases, with P being the change of basis matrix.[1][2]'b'In linear algebra, two n-by-n matrices A and B are called similar if'
Coordinate system
b'In geometry and kinematics, coordinate systems are used not only to describe the (linear) position of points, but also to describe the angular position of axes, planes, and rigid bodies.[15] In the latter case, the orientation of a second (typically referred to as "local") coordinate system, fixed to the node, is defined based on the first (typically referred to as "global" or "world" coordinate system). For instance, the orientation of a rigid body can be represented by an orientation matrix, which includes, in its three columns, the Cartesian coordinates of three points. These points are used to define the orientation of the axes of the local system; they are the tips of three unit vectors aligned with those axes.'b'The concept of a coordinate map, or coordinate chart is central to the theory of manifolds. A coordinate map is essentially a coordinate system for a subset of a given space with the property that each point has exactly one set of coordinates. More precisely, a coordinate map is a homeomorphism from an open subset of a space X to an open subset of Rn.[14] It is often not possible to provide one consistent coordinate system for an entire space. In this case, a collection of coordinate maps are put together to form an atlas covering the space. A space equipped with such an atlas is called a manifold and additional structure can be defined on a manifold if the structure is consistent where the coordinate maps overlap. For example, a differentiable manifold is a manifold where the change of coordinates from one coordinate map to another is always a differentiable function.'b'Similarly, coordinate hypersurfaces are the (n \xe2\x88\x92 1)-dimensional spaces resulting from fixing a single coordinate of an n-dimensional coordinate system.[13]'b'In three-dimensional space, if one coordinate is held constant and the other two are allowed to vary, then the resulting surface is called a coordinate surface. For example, the coordinate surfaces obtained by holding \xcf\x81 constant in the spherical coordinate system are the spheres with center at the origin. In three-dimensional space the intersection of two coordinate surfaces is a coordinate curve. In the Cartesian coordinate system we may speak of coordinate planes.'b'In two dimensions, if one of the coordinates in a point coordinate system is held constant and the other coordinate is allowed to vary, then the resulting curve is called a coordinate curve. In the Cartesian coordinate system the coordinate curves are, in fact, straight lines, thus coordinate lines. Specifically, they are the lines parallel to one of the coordinate axes. For other coordinate systems the coordinates curves may be general curves. For example, the coordinate curves in polar coordinates obtained by holding r constant are the circles with center at the origin. Coordinates systems for Euclidean space other than the Cartesian coordinate system are called curvilinear coordinate systems.[12] This procedure does not always make sense, for example there are no coordinate curves in a homogeneous coordinate system.'b'For example, in 1D, if the mapping is a translation of 3 to the right, the first moves the origin from 0 to 3, so that the coordinate of each point becomes 3 less, while the second moves the origin from 0 to \xe2\x88\x923, so that the coordinate of each point becomes 3 more.'b'With every bijection from the space to itself two coordinate transformations can be associated:'b'Because there are often many different possible coordinate systems for describing geometrical figures, it is important to understand how they are related. Such relations are described by coordinate transformations which give formulas for the coordinates in one system in terms of the coordinates in another system. For example, in the plane, if Cartesian coordinates (x,\xc2\xa0y) and polar coordinates (r,\xc2\xa0\xce\xb8) have the same origin, and the polar axis is the positive x axis, then the coordinate transformation from polar to Cartesian coordinates is given by x\xc2\xa0=\xc2\xa0r\xc2\xa0cos\xce\xb8 and y\xc2\xa0=\xc2\xa0r\xc2\xa0sin\xce\xb8.'b'It may occur that systems of coordinates for two different sets of geometric figures are equivalent in terms of their analysis. An example of this is the systems of homogeneous coordinates for points and lines in the projective plane. The two systems in a case like this are said to be dualistic. Dualistic systems have the property that results from one system can be carried over to the other since these results are only different interpretations of the same analytical result; this is known as the principle of duality.[11]'b'Coordinates systems are often used to specify the position of a point, but they may also be used to specify the position of more complex figures such as lines, planes, circles or spheres. For example, Pl\xc3\xbccker coordinates are used to determine the position of a line in space.[10] When there is a need, the type of figure being described is used to distinguish the type of coordinate system, for example the term line coordinates is used for any coordinate system that specifies the position of a line.'b'There are ways of describing curves without coordinates, using intrinsic equations that use invariant quantities such as curvature and arc length. These include:'b'Some other common coordinate systems are the following:'b'A point in the plane may be represented in homogeneous coordinates by a triple (x,\xc2\xa0y,\xc2\xa0z) where x/z and y/z are the Cartesian coordinates of the point.[9] This introduces an "extra" coordinate since only two are needed to specify a point on the plane, but this system is useful in that it represents any point on the projective plane without the use of infinity. In general, a homogeneous coordinate system is one where only the ratios of the coordinates are significant and not the actual values.'b'There are two common methods for extending the polar coordinate system to three dimensions. In the cylindrical coordinate system, a z-coordinate with the same meaning as in Cartesian coordinates is added to the r and \xce\xb8 polar coordinates giving a triple (r,\xc2\xa0\xce\xb8,\xc2\xa0z).[7] Spherical coordinates take this a step further by converting the pair of cylindrical coordinates (r,\xc2\xa0z) to polar coordinates (\xcf\x81,\xc2\xa0\xcf\x86) giving a triple (\xcf\x81,\xc2\xa0\xce\xb8,\xc2\xa0\xcf\x86).[8]'b'Another common coordinate system for the plane is the polar coordinate system.[6] A point is chosen as the pole and a ray from this point is taken as the polar axis. For a given angle \xce\xb8, there is a single line through the pole whose angle with the polar axis is \xce\xb8 (measured counterclockwise from the axis to the line). Then there is a unique point on this line whose signed distance from the origin is r for given number r. For a given pair of coordinates (r,\xc2\xa0\xce\xb8) there is a single point, but any point is represented by many pairs of coordinates. For example, (r,\xc2\xa0\xce\xb8), (r,\xc2\xa0\xce\xb8+2\xcf\x80) and (\xe2\x88\x92r,\xc2\xa0\xce\xb8+\xcf\x80) are all polar coordinates for the same point. The pole is represented by (0, \xce\xb8) for any value of \xce\xb8.'b'Depending on the direction and order of the coordinate axis the system may be a right-hand or a left-hand system. This is one of many coordinate systems.'b'In three dimensions, three perpendicular planes are chosen and the three coordinates of a point are the signed distances to each of the planes.[5] This can be generalized to create n coordinates for any point in n-dimensional Euclidean space.'b'The prototypical example of a coordinate system is the Cartesian coordinate system. In the plane, two perpendicular lines are chosen and the coordinates of a point are taken to be the signed distances to the lines.'b' The simplest example of a coordinate system is the identification of points on a line with real numbers using the number line. In this system, an arbitrary point O (the origin) is chosen on a given line. The coordinate of a point P is defined as the signed distance from O to P, where the signed distance is the distance taken as positive or negative depending on which side of the line P lies. Each point is given a unique coordinate and each real number is the coordinate of a unique point.[4]'b''b''b'In geometry, a coordinate system is a system which uses one or more numbers, or coordinates, to uniquely determine the position of the points or other geometric elements on a manifold such as Euclidean space.[1][2] The order of the coordinates is significant, and they are sometimes identified by their position in an ordered tuple and sometimes by a letter, as in "the x-coordinate". The coordinates are taken to be real numbers in elementary mathematics, but may be complex numbers or elements of a more abstract system such as a commutative ring. The use of a coordinate system allows problems in geometry to be translated into problems about numbers and vice versa; this is the basis of analytic geometry.[3]'
Linear combination

Cardinality
b'From this, one can show that in general the cardinalities of unions and intersections are related by[8]'b'If A and B are disjoint sets, then'b'Both have cardinality'b'The second result was first demonstrated by Cantor in 1878, but it became more apparent in 1890, when Giuseppe Peano introduced the space-filling curves, curved lines that twist and turn enough to fill the whole of any square, or cube, or hypercube, or finite-dimensional space. These curves are not a direct proof that a line has the same number of points as a finite-dimensional space, but they can be used to obtain such a proof.'b"The first of these results is apparent by considering, for instance, the tangent function, which provides a one-to-one correspondence between the interval (\xe2\x88\x92\xc2\xbd\xcf\x80, \xc2\xbd\xcf\x80) and R (see also Hilbert's paradox of the Grand Hotel)."b'Cardinal arithmetic can be used to show not only that the number of points in a real number line is equal to the number of points in any segment of that line, but that this is equal to the number of points on a plane and, indeed, in any finite-dimensional space. These results are highly counterintuitive, because they imply that there exist proper subsets and proper supersets of an infinite set S that have the same size as S, although S contains elements that do not belong to its subsets, and the supersets of S contain elements that are not included in it.'b'However, this hypothesis can neither be proved nor disproved within the widely accepted ZFC axiomatic set theory, if ZFC is consistent.'b'The continuum hypothesis states that there is no cardinal number between the cardinality of the reals and the cardinality of the natural numbers, that is,'b'If the axiom of choice holds, the law of trichotomy holds for cardinality. Thus we can make the following definitions:'b'Assuming AC, the cardinalities of the infinite sets are denoted'b'The relation of having the same cardinality is called equinumerosity, and this is an equivalence relation on the class of all sets. The equivalence class of a set A under this relation then consists of all those sets which have the same cardinality as A. There are two ways to define the "cardinality of a set":'b'Above, "cardinality" was defined functionally. That is, the "cardinality" of a set was not defined as a specific object itself. However, such an object can be defined as follows.'b'If |A| \xe2\x89\xa4 |B| and |B| \xe2\x89\xa4 |A| then |A| = |B| (Cantor\xe2\x80\x93Bernstein\xe2\x80\x93Schroeder theorem). The axiom of choice is equivalent to the statement that |A| \xe2\x89\xa4 |B| or |B| \xe2\x89\xa4 |A| for every A, B.[3][4]'b'While the cardinality of a finite set is just the number of its elements, extending the notion to infinite sets usually starts with defining the notion of comparison of arbitrary (in particular infinite) sets.'b''b''b'The cardinality of a set A is usually denoted |\xc2\xa0A\xc2\xa0|, with a vertical bar on each side; this is the same notation as absolute value and the meaning depends on context. Alternatively, the cardinality of a set A may be denoted by n(A), A, card(A), or #\xe2\x80\x89A.'b'In mathematics, the cardinality of a set is a measure of the "number of elements of the set". For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. There are two approaches to cardinality \xe2\x80\x93 one which compares sets directly using bijections and injections, and another which uses cardinal numbers.[1] The cardinality of a set is also called its size, when no confusion with other notions of size[2] is possible.'
Dimension (vector space)
b'Alternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of "dimension of the operator". These fall under the rubric of "trace class operators" on a Hilbert space, or more generally nuclear operators on a Banach space.'b'The Krull dimension of a commutative ring, named after Wolfgang Krull (1899\xe2\x80\x931971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.'b'One can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.'b'Some simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself. If V is a vector space over a field F then, denoting the dimension of V by dim V, we have:'b'In particular, every complex vector space of dimension n is a real vector space of dimension 2n.'b'If F/K is a field extension, then F is in particular a vector space over K. Furthermore, every F-vector space V is also a K-vector space. The dimensions are related by the formula'b'An important result about dimensions is given by the rank\xe2\x80\x93nullity theorem for linear maps.'b'Any two vector spaces over F having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If B is some set, a vector space with dimension |B| over F can be constructed as follows: take the set F(B) of all functions f\xc2\xa0: B \xe2\x86\x92 F such that f(b) = 0 for all but finitely many b in B. These functions can be added and multiplied with elements of F, and we obtain the desired F-vector space.'b'Rn has the standard basis {e1, ..., en}, where ei is the i-th column of the corresponding identity matrix. Therefore Rn has dimension n.'b'To show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if V is a finite-dimensional vector space and W is a linear subspace of V with dim(W) = dim(V), then W = V.'b'If W is a linear subspace of V, then dim(W) \xe2\x89\xa4 dim(V).'b'The only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.'b'The complex numbers C are both a real and complex vector space; we have dimR(C) = 2 and dimC(C) = 1. So the dimension depends on the base field.'b'as a basis, and therefore we have dimR(R3) = 3. More generally, dimR(Rn) = n, and even more generally, dimF(Fn) = n for any field F.'b'The vector space R3 has'b''b''b'The dimension of the vector space V over the field F can be written as dimF(V) or as [V\xc2\xa0: F], read "dimension of V over F". When F can be inferred from context, dim(V) is typically written.'b'For every vector space there exists a basis,[a] and all bases of a vector space have equal cardinality;[b] as a result, the dimension of a vector space is uniquely defined. We say V is finite-dimensional if the dimension of V is finite, and infinite-dimensional if its dimension is infinite.'b'In mathematics, the dimension of a vector space V is the cardinality (i.e. the number of vectors) of a basis of V over its base field.[1] It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.'
Well-defined
b'A solution to a partial differential equation is said to be well-defined if it is determined by the boundary conditions in a continuous way as the boundary conditions are changed.[1]'b'Unlike with functions, the notational ambiguities can be overcome more or less easily by means of additional definitions, i.\xc2\xa0e. rules of precedence, and/or associativity of the operators. In the programming language C e.\xc2\xa0g. the operator - for subtraction is left-to-right-associative which means that a-b-c is defined as (a-b)-c and the operator = for assignment is right-to-left-associative which means that a=b=c is defined as a=(b=c). In the programming language APL there is only one rule: from right to left \xe2\x88\x92 but parentheses first.'b'In particular, the term well-defined is used with respect to (binary) operations on cosets. In this case one can view the operation as a function of two variables and the property of being well-defined is the same as that for a function. For example, addition on the integers modulo some n can be defined naturally in terms of integer addition.'b'The function f is well-defined, because'b'For example, consider the following function'b'The question of well-definedness of a function classically arises when the defining equation of a function does not (only) refer to the arguments themselves, but (also) to elements of the arguments. This is sometimes unavoidable when the arguments are cosets and the equation refers to coset representatives.'b'Despite these subtle logical problems, it is quite common to anticipatorily use the term definition (without apostrophes) for "definitions" of this kind, firstly because it is sort of a short-hand of the two-step approach, secondly because the relevant mathematical reasoning (step 2.) is the same in both cases, and finally because in mathematical texts the assertion is \xc2\xabup to 100%\xc2\xbb true.'b''b''b'A function that is not well-defined is not the same as a function that is undefined. For example, if f(x) = 1/x, then f(0) is undefined, but this has nothing to do with the question of whether f(x) = 1/x is well-defined. It is; 0 is simply not in the domain of the function.'b'In mathematics, an expression is called well-defined or unambiguous if its definition assigns it a unique interpretation or value. Otherwise, the expression is said to be not well-defined or ambiguous.[1] A function is well-defined if it gives the same result when the representation of the input is changed without changing the value of the input. For instance if f takes real numbers as input, and if f(0.5) does not equal f(1/2) then f is not well-defined (and thus: not a function).[2] The term well-defined is also used to indicate whether a logical statement is unambiguous.'
Dimension theorem for vector spaces

Linear independence
b'If such a linear dependence exists, then the n vectors are linearly dependent. It makes sense to identify two linear dependencies if one arises as a non-zero multiple of the other, because in this case the two describe the same linear relationship among the vectors. Under this identification, the set of all linear dependencies among v1, ...., vn is a projective space.'b'A linear dependence among vectors v1, ..., vn is a tuple (a1, ..., an) with n scalar components, not all zero, such that'b'Take the first derivative of the above equation such that'b'then ai = 0 for all i in {1, ..., n}.'b'Since'b'Suppose that a1, a2, ..., an are elements of R such that'b'Then e1, e2, ..., en are linearly independent.'b'Let V\xc2\xa0=\xc2\xa0Rn and consider the following elements in V, known as the natural basis vectors:'b'If there are more vectors than dimensions, the vectors are linearly dependent. This is illustrated in the example above of three vectors in R2.'b'for all possible lists of m rows. (In case m\xc2\xa0= n, this requires only one determinant, as above. If m\xc2\xa0>\xc2\xa0n, then it is a theorem that the vectors must be linearly dependent.) This fact is valuable for theory; in practical calculations more efficient methods are available.'b'Furthermore, the reverse is true. That is, we can test whether the m vectors are linearly dependent by testing whether'b'Otherwise, suppose we have m vectors of n coordinates, with m\xc2\xa0<\xc2\xa0n. Then A is an n\xc3\x97m matrix and \xce\x9b is a column vector with m entries, and we are again interested in A\xce\x9b\xc2\xa0= 0. As we saw previously, this is equivalent to a list of n equations. Consider the first m rows of A, the first m equations; any solution of the full list of equations must also be true of the reduced list. In fact, if \xe3\x80\x88i1,...,im\xe3\x80\x89 is any list of m rows, then the equation must be true for those rows.'b'Since the determinant is non-zero, the vectors (1, 1) and (\xe2\x88\x923, 2) are linearly independent.'b'We are interested in whether A\xce\x9b\xc2\xa0= 0 for some nonzero vector \xce\x9b. This depends on the determinant of A, which is'b'We may write a linear combination of the columns as'b'In this case, the matrix formed by the vectors is'b'where a3 can be chosen arbitrarily. Thus, the vectors v1, v2 and v3 are linearly dependent.'b'This equation is easily solved to define non-zero ai,'b'Rearrange to solve for v3 and obtain,'b'Row reduce this equation to obtain,'b'are linearly dependent, form the matrix equation,'b'In order to determine if the three vectors in R4,'b'This shows that ai = 0, which means that the vectors v1 = (1, 1) and v2 = (\xe2\x88\x923, 2) are linearly independent.'b'The same row reduction presented above yields,'b'or'b'Two vectors: Now consider the linear dependence of the two vectors v1 = (1, 1), v2 = (\xe2\x88\x923, 2), and check,'b'which shows that non-zero ai exist such that v3 = (2, 4) can be defined in terms of v1 = (1, 1), v2 = (\xe2\x88\x923, 2). Thus, the three vectors are linearly dependent.'b'We can now rearrange this equation to obtain'b'Continue the row reduction by (i) dividing the second row by 5, and then (ii) multiplying by 3 and adding to the first row, that is'b'Row reduce this matrix equation by subtracting the first row from the second to obtain,'b'or'b'Three vectors: Consider the set of vectors v1 = (1, 1), v2 = (\xe2\x88\x923, 2) and v3 = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such that'b'Also note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set. In general, n linearly independent vectors are required to describe any location in n-dimensional space.'b'In this example the "3 miles north" vector and the "4 miles east" vector are linearly independent. That is to say, the north vector cannot be described in terms of the east vector, and vice versa. The third "5 miles northeast" vector is a linear combination of the other two vectors, and it makes the set of vectors linearly dependent, that is, one of the three vectors is unnecessary.'b'A geographic example may help to clarify the concept of linear independence. A person describing the location of a certain place might say, "It is 3 miles north and 4 miles east of here." This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth\'s surface). The person might add, "The place is 5 miles northeast of here." Although this last statement is true, it is not necessary.'b'A set of vectors which is linearly independent and spans some vector space, forms a basis for that vector space. For example, the vector space of all polynomials in x over the reals has the (infinite) subset {1, x, x2, ...} as a basis.'b'A set X of elements of V is linearly independent if the corresponding family {x}x\xe2\x88\x88X is linearly independent. Equivalently, a family is dependent if a member is in the linear span of the rest of the family, i.e., a member is a linear combination of the rest of the family. The trivial case of the empty family must be regarded as linearly independent for theorems to apply.'b'where the index set J is a nonempty, finite subset of I.'b'In order to allow the number of linearly independent vectors in a vector space to be countably infinite, it is useful to define linear dependence as follows. More generally, let V be a vector space over a field K, and let {vi | i\xe2\x88\x88I} be a family of elements of V. The family is linearly dependent over K if there exists a family {aj | j\xe2\x88\x88J} of elements of K, not all zero, such that'b''b''b'A vector space can be of finite-dimension or infinite-dimension depending on the number of linearly independent basis vectors. The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a basis for a vector space.'b'In the theory of vector spaces, a set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent. These concepts are central to the definition of dimension.[1]'
Basis (linear algebra)
b"This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent."b'Hence Lmax is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.'b'If Lmax would not span V, there would exist some vector w of V that cannot be expressed as a linear combination of elements of Lmax (with coefficients in the field F). In particular, w cannot be an element of Lmax. Let Lw = Lmax \xe2\x88\xaa {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of Lmax, and Lmax is independent). As Lmax \xe2\x8a\x86 Lw, and Lmax \xe2\x89\xa0 Lw (because Lw contains the vector w that is not contained in Lmax), this contradicts the maximality of Lmax. Thus this shows that Lmax spans V.'b'It remains to prove that Lmax is a basis of V. Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.'b"As X is nonempty, and every totally ordered subset of (X, \xe2\x8a\x86) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax \xe2\x8a\x86 L for some element L of X, then L = Lmax."b'Since (Y, \xe2\x8a\x86) is totally ordered, every finite subset of LY is a subset of an element of Y, which is a linearly independent subset of V, and hence every finite subset of LY is linearly independent. Thus LY is linearly independent, so LY is an element of X. Therefore, LY is an upper bound for Y in (X, \xe2\x8a\x86): it is an element of X, that contains every element Y.'b'Let Y be a subset of X that is totally ordered by \xe2\x8a\x86, and let LY be the union of all the elements of Y (which are themselves certain subsets of V).'b'The set X is nonempty since the empty set is an independent subset of V, and it is partially ordered by inclusion, which is denoted, as usual, by \xe2\x8a\x86.'b'Let V be any vector space over some field F. Let X be the set of all linearly independent subsets of V.'b'The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the n-dimensional cube [\xe2\x88\x921,1]n as a function of dimension, n. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within \xcf\x80/2 \xc2\xb10.037\xcf\x80/2 then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within \xcf\x80/2 \xc2\xb10.037\xcf\x80/2 then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each n, 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.'b'In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in n-dimensional ball. Choose N independent random vectors from a ball (they are independent and identically distributed). Let \xce\xb8 be a small positive number. Then for'b'For a probability distribution in Rn with a probability density function, such as the equidistribution in a n-dimensional ball with respect to Lebesgue measure, it can be shown that randomly and independently chosen n-vectors will form a basis with probability one, which is due to the fact that n linearly dependent vectors x1,..., xn in Rn should satisfy the equation det[x1,..., xn]=0 (zero determinant of the matrix with columns xi), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.[5][6]'b'for suitable (real or complex) coefficients ak, bk. But many[2] square-integrable functions cannot be represented as finite linear combinations of these basis functions, which therefore do not comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.'b'The functions {1} \xe2\x88\xaa { sin(nx), cos(nx)\xc2\xa0: n = 1, 2, 3, ... } are linearly independent, and every function f that is square-integrable on [0, 2\xcf\x80] is an "infinite linear combination" of them, in the sense that'b'In the study of Fourier series, one learns that the functions {1} \xe2\x88\xaa { sin(nx), cos(nx)\xc2\xa0: n = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2\xcf\x80] that are square-integrable on this interval, i.e., functions f satisfying'b'The common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces \xe2\x80\x93 a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fr\xc3\xa9chet spaces.'b'The maps sending a vector v to the components aj(v) are linear maps from V to F, because of \xcf\x86\xe2\x88\x921 is linear. Hence they are linear functionals. They form a basis for the dual space of V, called the dual basis.'b'The inverse of the linear isomorphism \xcf\x86 determined by an ordered basis {vi} equips V with coordinates: if, for a vector v \xe2\x88\x88 V, \xcf\x86\xe2\x88\x921(v) = (a1, a2,...,an) \xe2\x88\x88 Fn, then the components aj = aj(v) are the coordinates of v in the sense that v = a1(v) v1 + a2(v) v2 + ... + an(v) vn.'b'These two constructions are clearly inverse to each other. Thus ordered bases for V are in 1-1 correspondence with linear isomorphisms Fn \xe2\x86\x92 V.'b'where x = x1e1 + x2e2 + ... + xnen is an element of Fn. It is not hard to check that \xcf\x86 is a linear isomorphism.'b'Conversely, given an ordered basis, consider the map defined by'b'where {ei} is the standard basis for Fn.'b'is a linear isomorphism. Define an ordered basis {vi} for V by'b'Suppose first that'b'Proof. The proof makes use of the fact that the standard basis of Fn is an ordered basis.'b'Suppose V is an n-dimensional vector space over a field F. A choice of an ordered basis for V is equivalent to a choice of a linear isomorphism \xcf\x86 from the coordinate space Fn to V.'b'A basis is just a linearly independent set of vectors with or without a given ordering. For many purposes it is convenient to work with an ordered basis. For example, when working with a coordinate representation of a vector it is customary to speak of the "first" or "second" coordinate, which makes sense only if an ordering is specified for the basis. For finite-dimensional vector spaces one typically indexes a basis {vi} by the first n integers. An ordered basis is also called a frame.'b'Since the above matrix has a nonzero determinant, its columns form a basis of R2. See: invertible matrix.'b'Simply compute the determinant'b'Since (\xe2\x88\x921,2) is clearly not a multiple of (1,1) and since (1,1) is not the zero vector, these two vectors are linearly independent. Since the dimension of R2 is 2, the two vectors already form a basis of R2 without needing any extension.'b'Subtracting the first equation from the second, we get:'b'Then we have to solve the equations:'b'Part II: To prove that these two vectors generate R2, we have to let (a, b) be an arbitrary element of R2, and show that there exist numbers r, s \xe2\x88\x88 R such that:'b'Hence we have linear independence.'b'Adding this equation to the first equation then:'b'Subtracting the first equation from the second, we obtain:'b'(i.e., they are linearly dependent). Then:'b'To prove that they are linearly independent, suppose that there are numbers a, b such that:'b'We have to prove that these two vectors are linearly independent and that they generate R2.'b'Often, a mathematical result can be proven in more than one way. Here, using three different proofs, we show that the vectors (1,1) and (\xe2\x88\x921,2) form a basis for R2.'b'A similar question is when does a subset S contain a basis. This occurs if and only if S spans V. In this case, S will usually contain several different bases.'b'Let S be a subset of a vector space V. To extend S to a basis means to find a basis B that contains S as a subset. This can be done if and only if S is linearly independent. Almost always, there is more than one such B, except in rather special circumstances (i.e. S is already a basis, or S is empty and V has two elements).'b'A set of vectors can be represented by a matrix of which each column consists of the components of the corresponding vector of the set. As a basis is a set of vectors, a basis can be given by a matrix of this kind. The change of basis of any object of the space is related to this matrix. For example, coordinate tuples change with its inverse.'b'Given a vector space V over a field F and suppose that {v1, ..., vn} and {\xce\xb11, ..., \xce\xb1n} are two bases for V. By definition, if \xce\xbe is a vector in V then \xce\xbe = x1\xce\xb11 + ... + xn\xce\xb1n for a unique choice of scalars x1, ..., xn in F called the coordinates of \xce\xbe relative to the ordered basis {\xce\xb11, ..., \xce\xb1n}. The vector x = (x1, ..., xn) in Fn is called the coordinate tuple of \xce\xbe (relative to this basis). The unique linear map \xcf\x86\xc2\xa0: Fn \xe2\x86\x92 V with \xcf\x86(vj) = \xce\xb1j for j = 1, ..., n is called the coordinate isomorphism for V and the basis {\xce\xb11, ..., \xce\xb1n}. Thus \xcf\x86(x) = \xce\xbe if and only if \xce\xbe = x1\xce\xb11 + ... + xn\xce\xb1n.'b'In M22, {M1,1, M1,2, M2,1, M2,2}, where M22 is the set of all 2\xc3\x972 matrices. and Mm,n is the 2\xc3\x972 matrix with a 1 in the m,n position and zeros everywhere else.'b'In P2, where P2 is the set of all polynomials of degree at most 2, {1, x, x2} is the standard basis.'b'In Rn, {e1, ..., en}, where ei is the ith column of the identity matrix.'b'Standard bases for example:'b'Also many vector sets can be attributed a standard basis which comprises both spanning and linearly independent vectors.'b'Every vector space has a basis. The proof of this requires the axiom of choice. All bases of a vector space have the same cardinality (number of elements), called the dimension of the vector space. This result is known as the dimension theorem, and requires the ultrafilter lemma, a strictly weaker form of the axiom of choice.'b'Again, B denotes a subset of a vector space V. Then, B is a basis if and only if any of the following equivalent conditions are met:'b'It is often convenient to list the basis vectors in a specific order, for example, when considering the transformation matrix of a linear map with respect to a basis. We then speak of an ordered basis, which we define to be a sequence (rather than a set) of linearly independent vectors that span V: see Ordered bases and coordinates below.'b'The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors. Settings that permit infinite linear combinations allow alternative definitions of the basis concept: see Related notions below.'b'A vector space that has a finite basis is called finite-dimensional. To deal with infinite-dimensional spaces, we must generalize the above definition to include infinite basis sets. We therefore say that a set (finite or infinite) B \xe2\x8a\x82 V is a basis, if'b'The numbers ai are called the coordinates of the vector x with respect to the basis B, and by the first property they are uniquely determined.'b'In more detail, suppose that B = { v1, \xe2\x80\xa6, vn } is a finite subset of a vector space V over a field F (such as the real or complex numbers R or C). Then B is a basis if it satisfies the following conditions:'b'A basis B of a vector space V over a field F is a linearly independent subset of V that spans V.'b''b''b'Given a basis of a vector space V, every element of V can be expressed uniquely as a linear combination of basis vectors, whose coefficients are referred to as vector coordinates or components. A vector space can have several distinct sets of basis vectors; however each such set has the same number of elements, with this number being the dimension of the vector space.'b'In mathematics, a set of elements (vectors) in a vector space V is called a basis, or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set.[1] In more general terms, a basis is a linearly independent spanning set.'
Axiom of choice

Rational number
b"The metric space (Q,dp) is not complete, and its completion is the p-adic number field Qp. Ostrowski's theorem states that any non-trivial absolute value on the rational numbers Q is equivalent to either the usual real absolute value or a p-adic absolute value."b'Then dp(x,y) = |x \xe2\x88\x92 y|p defines a metric on Q.'b'In addition set |0|p = 0. For any rational number a/b, we set |a/b|p = |a|p / |b|p.'b'Let p be a prime number and for any non-zero integer a, let |a|p = p\xe2\x88\x92n, where pn is the highest power of p dividing a.'b'In addition to the absolute value metric mentioned above, there are other metrics which turn Q into a topological field:'b'By virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric d(x,y) = |x \xe2\x88\x92 y|, and this yields a third topology on Q. All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space; the real numbers are the completion of Q under the metric d(x,y) = |x \xe2\x88\x92 y|, above.'b'The rationals are a dense subset of the real numbers: every real number has rational numbers arbitrarily close to it. A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.'b'Any totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.'b'The rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones. For example, for any two fractions such that'b'The set of all rational numbers is countable. Since the set of all real numbers is uncountable, we say that almost all real numbers are irrational, in the sense of Lebesgue measure, i.e. the set of rational numbers is a null set.'b'The algebraic closure of Q, i.e. the field of roots of rational polynomials, is the algebraic numbers.'b'The rationals are the smallest field with characteristic zero: every other field of characteristic zero contains a copy of Q. The rational numbers are therefore the prime field for characteristic zero.'b'The set Q, together with the addition and multiplication operations shown above, forms a field, the field of fractions of the integers Z.'b'The integers may be considered to be rational numbers by the embedding that maps m to [(m,1)].'b'We can also define a total order on Q. Let \xe2\x88\xa7 be the and-symbol and \xe2\x88\xa8 be the or-symbol. We say that [(m1,n1)] \xe2\x89\xa4 [(m2,n2)] if:'b'The canonical choice for [(m,n)] is chosen so that n is positive and gcd(m,n) = 1, i.e. m and n share no common factors, i.e. m and n are coprime. For example, we would write [(1,2)] instead of [(2,4)] or [(\xe2\x88\x9212,\xe2\x88\x9224)], even though [(1,2)] = [(2,4)] = [(\xe2\x88\x9212,\xe2\x88\x9224)].'b'The equivalence relation (m1,n1) ~ (m2,n2) if, and only if, m1n2 \xe2\x88\x92 m2n1 = 0 is a congruence relation, i.e. it is compatible with the addition and multiplication defined above, and we may define Q to be the quotient set (Z \xc3\x97 (Z \\ {0})) / ~, i.e. we identify two pairs (m1,n1) and (m2,n2) if they are equivalent in the above sense. (This construction can be carried out in any integral domain: see field of fractions.) We denote by [(m1,n1)] the equivalence class containing (m1,n1). If (m1,n1) ~ (m2,n2) then, by definition, (m1,n1) belongs to [(m2,n2)] and (m2,n2) belongs to [(m1,n1)]; in this case we can write [(m1,n1)] = [(m2,n2)]. Given any equivalence class [(m,n)] there are a countably infinite number of representation, since'b'and, if m2 \xe2\x89\xa0 0, division by'b'Mathematically we may construct the rational numbers as equivalence classes of ordered pairs of integers (m,n), with n \xe2\x89\xa0 0. This space of equivalence classes is the quotient space (Z \xc3\x97 (Z \\ {0})) / ~, where (m1,n1) ~ (m2,n2) if, and only if, m1n2 \xe2\x88\x92 m2n1 = 0. We can define addition and multiplication of these pairs with the following rules:'b'are different ways to represent the same rational value.'b'where an are integers. Every rational number a/b can be represented as a finite continued fraction, whose coefficients an can be determined by applying the Euclidean algorithm to (a,b).'b'A finite continued fraction is an expression such as'b'If a \xe2\x89\xa0 0, then'b'The result is in canonical form if the same is true for a/b. In particular,'b'If n is a non-negative integer, then'b'Thus, dividing a/b by c/d is equivalent to multiplying a/b by the reciprocal of c/d:'b'If both b and c are nonzero, the division rule is'b'A nonzero rational number a/b has a multiplicative inverse, also called its reciprocal,'b'If a/b is in canonical form, the same is true for its opposite.'b'Every rational number a/b has an additive inverse, often called its opposite,'b'Even if both fractions are in canonical form, the result may be a reducible fraction.'b'The rule for multiplication is:'b'If both fractions are in canonical form, the result is in canonical form if and only if b and d are coprime integers.'b'If both fractions are in canonical form, the result is in canonical form if and only if b and d are coprime integers.'b'Two fractions are added as follows:'b'If either denominator is negative, each fraction with a negative denominator must first be converted into an equivalent form with a positive denominator by changing the signs of both its numerator and denominator.'b'If both denominators are positive, and, in particular, if both fractions are in canonical form,'b'If both fractions are in canonical form then'b'Any integer n can be expressed as the rational number n/1, which is its canonical form as a rational number.'b'Starting from a rational number a/b, its canonical form may be obtained by dividing a and b by their greatest common divisor, and, if b < 0, changing the sign of the resulting numerator and denominator.'b'Every rational number may be expressed in a unique way as an irreducible fraction a/b, where a and b are coprime integers, and b > 0. This is often called the canonical form.'b'The term rational in reference to the set Q refers to the fact that a rational number represents a ratio of two integers. In mathematics, "rational" is often used as a noun abbreviating "rational number". The adjective rational sometimes means that the coefficients are rational numbers. For example, a rational point is a point with rational coordinates (that is a point whose coordinates are rational numbers; a rational matrix is a matrix of rational numbers; a rational polynomial may be a polynomial with rational coefficients, although the term "polynomial over the rationals" is generally preferred, for avoiding confusion with "rational expression" and "rational function" (a polynomial is a rational expression and defines a rational function, even if its coefficients are not rational numbers). However, a rational curve is not a curve defined over the rationals, but a curve which can be parameterized by rational functions.'b''b''b'In mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals.'b'Rational numbers together with addition and multiplication form a field which contains the integers and is contained in any field containing the integers. In other words, the field of rational numbers is a prime field, and a field has characteristic zero if and only if it contains the rational numbers as a subfield. Finite extensions of Q are called algebraic number fields, and the algebraic closure of Q is the field of algebraic numbers.[3]'b'Rational numbers can be formally defined as equivalence classes of pairs of integers (p, q) such that q \xe2\x89\xa0 0, for the equivalence relation defined by (p1, q1) ~ (p2, q2) if, and only if p1q2 = p2q1. With this formal definition, the fraction p/q becomes the standard notation for the equivalence class of (p2, q2).'b'A real number that is not rational is called irrational. Irrational numbers include \xe2\x88\x9a2, \xcf\x80, e, and \xcf\x86. The decimal expansion of an irrational number continues without repeating. Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.[1]'b'The decimal expansion of a rational number always either terminates after a finite number of digits or begins to repeat the same finite sequence of digits over and over. Moreover, any repeating or terminating decimal represents a rational number. These statements hold true not just for base 10, but also for any other integer base (e.g. binary, hexadecimal).'
Linear span
b'(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)'b'Let X be a normed space and let E be any non-empty subset of X. Then'b"Closed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma)."b'The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.'b'The closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.'b'One mathematical formulation of this is'b'In functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.'b'consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,\xe2\x80\xa6,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.'b'The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,\xe2\x80\xa6,an of A, then the sum of cyclic modules,'b'Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].'b'This also indicates that a basis is a minimal spanning set when V is finite-dimensional.'b'Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.'b'Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.'b'This theorem is so well known that at times it is referred to as the definition of span of a set.'b'Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.'b'The set of functions xn where n is a non-negative integer spans the space of polynomials.'b'The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.'b'Another spanning set for the same space is given by {(1,2,3), (0,1,2), (\xe2\x88\x921,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.'b'The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.'b'In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.'b'Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.'b'Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.'b''b''b'In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all subspaces containing that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.'
Linear subspace
b'See the article on null space for an example.'b'If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.'b'This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.'b'See the article on column space for an example.'b'If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.'b'See the article on row space for an example.'b'Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:'b'In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N \xe2\x88\xa9 N\xe2\x8a\xa5 = N \xe2\x89\xa0 {0}. The same case presents the \xe2\x8a\xa5 operation in symplectic vector spaces.'b'If V is an inner product space, then the orthogonal complement \xe2\x8a\xa5 of any subspace of V is again a subspace. This operation, understood as negation (\xc2\xac), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).'b'The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.'b'Here the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:'b'For example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequality'b'If U and W are subspaces, their sum is the subspace'b'For every vector space V, the set {0} and V itself are subspaces of V.[7]'b'Proof:'b'Given subspaces U and W of a vector space V, then their intersection U\xc2\xa0\xe2\x88\xa9\xc2\xa0W\xc2\xa0:= {v\xc2\xa0\xe2\x88\x88\xc2\xa0V\xc2\xa0: v\xc2\xa0is an element of both U and\xc2\xa0W} is also a subspace of V.[6]'b'A subspace cannot lie in any subspace of lesser dimension. If dim\xc2\xa0U\xc2\xa0=\xc2\xa0k, a finite number, and U\xc2\xa0\xe2\x8a\x82\xc2\xa0W, then dim\xc2\xa0W\xc2\xa0=\xc2\xa0k if and only if U\xc2\xa0=\xc2\xa0W.'b'The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).'b'A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).'b'for (t1,\xe2\x80\xaft2,\xe2\x80\xaf...\xe2\x80\xaf,\xe2\x80\xaftk)\xc2\xa0\xe2\x89\xa0 (u1,\xe2\x80\xafu2,\xe2\x80\xaf...\xe2\x80\xaf,\xe2\x80\xafuk).[5] If v1, ..., vk are linearly independent, then the coordinates t1, ..., tk for a vector in the span are uniquely determined.'b'In general, vectors v1,\xe2\x80\xaf...\xe2\x80\xaf,\xe2\x80\xafvk are called linearly independent if'b'In general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1,\xe2\x80\xaf0,\xe2\x80\xaf0), (0,\xe2\x80\xaf0,\xe2\x80\xaf1), and (2,\xe2\x80\xaf0,\xe2\x80\xaf3) is just the xz-plane, with each point on the plane described by infinitely many different values of t1, t2, t3.'b'The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).'b'In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.'b'A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:'b'If the vectors v1,\xe2\x80\xaf...\xe2\x80\xaf,\xe2\x80\xafvk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1,\xe2\x80\xaf...\xe2\x80\xaf,\xe2\x80\xafvk.'b'The set of all possible linear combinations is called the span:'b'In general, a linear combination of vectors v1,\xe2\x80\xafv2,\xe2\x80\xaf...\xe2\x80\xaf,\xe2\x80\xafvk is any vector of the form'b'The expression on the right is called a linear combination of the vectors (2,\xe2\x80\xaf5,\xe2\x80\xaf\xe2\x88\x921) and (3,\xe2\x80\xaf\xe2\x88\x924,\xe2\x80\xaf2). These two vectors are said to span the resulting subspace.'b'In linear algebra, the system of parametric equations can be written as a single vector equation:'b'is a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]'b'For example, the set of all vectors (x,\xe2\x80\xafy,\xe2\x80\xafz) parameterized by the equations'b'The subset of Kn described by a system of homogeneous linear parametric equations is a subspace:'b'Every subspace of Kn can be described as the null space of some matrix (see algorithms, below).'b'The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrix'b'In a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:'b'is a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.'b'For example (over real or rational numbers), the set of all vectors (x,\xe2\x80\xafy,\xe2\x80\xafz) satisfying the equations'b'The solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:'b'It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.'b'A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F\xc2\xa0=\xc2\xa00 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):'b'This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.'b'A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:'b'Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.'b'In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.'b'A way to characterize subspaces is that they are closed under linear combinations. That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W. Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.'b'Examples that extend these themes are common in functional analysis.'b'Example IV: Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions. The same sort of argument as before shows that this is a subspace too.'b'Proof:'b'Example III: Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R. Let C(R) be the subset consisting of continuous functions. Then C(R) is a subspace of RR.'b'In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace. (The equation in example I was z\xc2\xa0=\xc2\xa00, and the equation in example II was x\xc2\xa0=\xc2\xa0y.) Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.'b'Proof:'b'Example II: Let the field be R again, but now let the vector space be the Cartesian plane R2. Take W to be the set of points (x,\xe2\x80\xafy) of R2 such that x\xc2\xa0=\xc2\xa0y. Then W is a subspace of R2.'b'Proof:'b'Example I: Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3. Take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.'b'Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V. Then W is a subspace if:'b''b''b'In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspaces[disambiguation needed].'
Kernel (linear algebra)
b'Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]'b'For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]'b'For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gr\xc3\xb6bner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]'b'If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]'b'The problem of computing the kernel on a computer depends on the nature of the coefficients.'b'are a basis of the kernel of A.'b'The last three columns of B are zero columns. Therefore, the three last vectors of C,'b'Putting the upper part in column echelon form by column operations on the whole matrix gives'b'Then'b'For example, suppose that'b'In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.'b'A basis of the kernel of a matrix may be computed by Gaussian elimination.'b'With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.'b'These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (\xe2\x88\x921,\xe2\x88\x9226,16)T.'b'which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.'b'Note also that the following dot products are zero:'b'The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (\xe2\x88\x921,\xe2\x88\x9226,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.'b'Since c is a free variable, this can be expressed equally well as,'b'for c a scalar.'b'Now we can express an element of the kernel:'b'Rewriting yields:'b'Gauss\xe2\x80\x93Jordan elimination reduces this to:'b'which can be written in matrix form as:'b'which can be expressed as a homogeneous system of linear equations involving x, y, and z:'b'The kernel of this matrix consists of all vectors (x, y, z)\xc2\xa0\xe2\x88\x88\xc2\xa0R3 for which'b'Consider the matrix'b'We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.'b'Geometrically, this says that the solution set to Ax\xc2\xa0=\xc2\xa0b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).'b'It follows that any solution to the equation Ax\xc2\xa0=\xc2\xa0b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax\xc2\xa0=\xc2\xa0b is'b'Thus, the difference of any two solutions to the equation Ax\xc2\xa0=\xc2\xa0b lies in the kernel of A.'b'If u and v are two possible solutions to the above equation, then'b'The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:'b'The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA\xc2\xa0=\xc2\xa00T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.'b'The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank\xe2\x80\x93nullity theorem'b'The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.'b'Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).'b'The product Ax can be written in terms of the dot product of vectors as follows:'b'The kernel of an m \xc3\x97 n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:'b'Thus the kernel of A is the same as the solution set to the above homogeneous equations.'b'The matrix equation is equivalent to a homogeneous system of linear equations:'b'Consider a linear map represented as a m \xc3\x97 n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,'b'If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L:\xc2\xa0V\xc2\xa0\xe2\x86\x92\xc2\xa0W is continuous if and only if the kernel of L is a closed subspace of V.'b'The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.'b'When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.'b'where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.'b'This implies the rank\xe2\x80\x93nullity theorem:'b'It follows that the image of L is isomorphic to the quotient of V by the kernel:'b'The kernel of L is a linear subspace of the domain V.[1] In the linear map L\xc2\xa0: V \xe2\x86\x92 W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:'b''b''b'In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L\xc2\xa0: V \xe2\x86\x92 W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,'
Linear combination

2  2 real matrices

Origin (mathematics)
b'The origin of the complex plane can be referred as the point where real axis and imaginary axis intersect each other. In other words, it is the complex number zero.[5]'b'In Euclidean geometry, the origin may be chosen freely as any convenient point of reference.[4]'b'In a polar coordinate system, the origin may also be called the pole. It does not itself have well-defined polar coordinates, because the polar coordinates of a point include the angle made by the positive x-axis and the ray from the origin to the point, and this ray is not well-defined for the origin itself.[3]'b'In a Cartesian coordinate system, the origin is the point where the axes of the system intersect.[1] The origin divides each of these axes into two halves, a positive and a negative semiaxis.[2] Points can then be located with reference to the origin by giving their numerical coordinates\xe2\x80\x94that is, the positions of their projections along each axis, either in the positive or negative direction. The coordinates of the origin are always all zero, for example (0,0) in two dimensions and (0,0,0) in three.[1]'b''b''b'In physical problems, the choice of origin is often arbitrary, meaning any choice of origin will ultimately give the same answer. This allows one to pick an origin point that makes the mathematics as simple as possible, often by taking advantage of some kind of geometric symmetry.'b'In mathematics, the origin of a Euclidean space is a special point, usually denoted by the letter O, used as a fixed point of reference for the geometry of the surrounding space.'
Bijection
b'When the partial bijection is on the same set, it is sometimes called a one-to-one partial transformation.[4] An example is the M\xc3\xb6bius transformation simply defined on the complex plane, rather than its completion to the extended complex plane.[5]'b'Another way of defining the same notion is to say that a partial bijection from A to B is any relation R (which turns out to be a partial function) with the property that R is the graph of a bijection f:A\xe2\x80\xb2\xe2\x86\x92B\xe2\x80\xb2, where A\xe2\x80\xb2 is a subset of A and B\xe2\x80\xb2 is a subset of B.[3]'b'The notion of one-to-one correspondence generalizes to partial functions, where they are called partial bijections, although partial bijections are only required to be injective. The reason for this relaxation is that a (proper) partial function is already undefined for a portion of its domain; thus there is no compelling reason to constrain its inverse to be a total function, i.e. defined everywhere on its domain. The set of all partial bijections on a given base set is called the symmetric inverse semigroup.[2]'b'Bijections are precisely the isomorphisms in the category Set of sets and set functions. However, the bijections are not always the isomorphisms for more complex categories. For example, in the category Grp of groups, the morphisms must be homomorphisms since they must preserve the group structure, so the isomorphisms are group isomorphisms which are bijective homomorphisms.'b'If X and Y are finite sets, then there exists a bijection between the two sets X and Y if and only if X and Y have the same number of elements. Indeed, in axiomatic set theory, this is taken as the definition of "same number of elements" (equinumerosity), and generalising this definition to infinite sets leads to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.'b'Continuing with the baseball batting line-up example, the function that is being defined takes as input the name of one of the players and outputs the position of that player in the batting order. Since this function is a bijection, it has an inverse function which takes as input a position in the batting order and outputs the player who will be batting in that position.'b'Stated in concise mathematical notation, a function f: X \xe2\x86\x92 Y is bijective if and only if it satisfies the condition'b'A bijection f with domain X (indicated by f: X \xe2\x86\x92 Y in functional notation) also defines a relation starting in Y and going to X (by turning the arrows around). The process of "turning the arrows around" for an arbitrary function does not, in general, yield a function, but properties (3) and (4) of a bijection say that this inverse relation is a function with domain Y. Moreover, properties (1) and (2) then say that this inverse function is a surjection and an injection, that is, the inverse function exists and is also a bijection. Functions that have inverse functions are said to be invertible. A function is invertible if and only if it is a bijection.'b'The instructor was able to conclude that there were just as many seats as there were students, without having to count either set.'b'In a classroom there are a certain number of seats. A bunch of students enter the room and the instructor asks them all to be seated. After a quick look around the room, the instructor declares that there is a bijection between the set of students and the set of seats, where each student is paired with the seat they are sitting in. What the instructor observed in order to reach this conclusion was that:'b'Consider the batting line-up of a baseball or cricket team (or any list of all the players of any sports team where every player holds a specific spot in a line-up). The set X will be the players on the team (of size nine in the case of baseball) and the set Y will be the positions in the batting order (1st, 2nd, 3rd, etc.) The "pairing" is given by which player is in what position in this order. Property (1) is satisfied since each player is somewhere in the list. Property (2) is satisfied since no player bats in two (or more) positions in the order. Property (3) says that for each position in the order, there is some player batting in that position and property (4) states that two or more players are never batting in the same position in the list.'b'Bijections are sometimes denoted by a two-headed rightwards arrow with tail (U+2916 \xe2\xa4\x96 RIGHTWARDS TWO-HEADED ARROW WITH TAIL), as in f\xc2\xa0: X \xe2\xa4\x96 Y. This symbol is a combination of the two-headed rightwards arrow (U+21A0 \xe2\x86\xa0 RIGHTWARDS TWO HEADED ARROW) sometimes used to denote surjections and the rightwards arrow with a barbed tail (U+21A3 \xe2\x86\xa3 RIGHTWARDS ARROW WITH TAIL) sometimes used to denote injections.'b'Satisfying properties (1) and (2) means that a bijection is a function with domain X. It is more common to see properties (1) and (2) written as a single statement: Every element of X is paired with exactly one element of Y. Functions which satisfy property (3) are said to be "onto Y " and are called surjections (or surjective functions). Functions which satisfy property (4) are said to be "one-to-one functions" and are called injections (or injective functions).[1] With this terminology, a bijection is a function which is both a surjection and an injection, or using other words, a bijection is a function which is both "one-to-one" and "onto".'b'For a pairing between X and Y (where Y need not be different from X) to be a bijection, four properties must hold:'b''b''b'Bijective functions are essential to many areas of mathematics including the definitions of isomorphism, homeomorphism, diffeomorphism, permutation group, and projective map.'b'A bijective function from a set to itself is also called a permutation.'b'A bijection from the set X to the set Y has an inverse function from Y to X. If X and Y are finite sets, then the existence of a bijection means they have the same number of elements. For infinite sets the picture is more complicated, leading to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.'b'In mathematics, a bijection, bijective function, or one-to-one correspondence is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. There are no unpaired elements. In mathematical terms, a bijective function f: X \xe2\x86\x92 Y is a one-to-one (injective) and onto (surjective) mapping of a set X to a set Y.'
Isomorphism
b'In the context of category theory, objects are usually at most isomorphic\xe2\x80\x94indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects X and Y, however, one asks if they are equal or not (they are both elements of the set Hom(X,\xc2\xa0Y), hence equality is the proper relationship), particularly in commutative diagrams.'b'are three different descriptions for a mathematical object, all of which are isomorphic, but not equal because they are not all subsets of a single space: the first is a subset of R3, the second is C \xe2\x89\x85 R2[note 4] plus an additional point, and the third is a subquotient of C2'b'which can be presented as the one-point compactification of the complex plane C \xe2\x88\xaa {\xe2\x88\x9e} or as the complex projective line (a quotient space)'b'Generally, saying that two objects are equal is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional space'b'If one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write \xe2\x89\x88 for an unnatural isomorphism and \xe2\x89\x85 for a natural isomorphism, as in V \xe2\x89\x88 V* and V \xe2\x89\x85 V**. This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.'b'However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of "the set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.'b'This corresponds to transforming a column vector (element of V) to a row vector (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis". More subtly, there is a map from a vector space V to its double dual V** = { x: V* \xe2\x86\x92 K} that does not depend on the choice of basis: For all v \xe2\x88\x88 V and \xcf\x86 \xe2\x88\x88 V*,'b'Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word isomorphism (Greek iso-, "same," and -morph, "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.'b'and no one isomorphism is intrinsically better than any other.[note 2][note 3] On this view and in this sense, these two sets are not equal because one cannot consider them identical: one can choose an isomorphism between them, but that is a weaker claim than identity\xe2\x80\x94and valid only in the context of the chosen isomorphism.'b'are equal; they are merely different presentations\xe2\x80\x94the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)\xe2\x80\x94of the same subset of the integers. By contrast, the sets {A,B,C} and {1,2,3} are not equal\xe2\x80\x94the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism\xe2\x80\x94one isomorphism is'b"In certain areas of mathematics, notably category theory, it is valuable to distinguish between equality on the one hand and isomorphism on the other.[3] Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the sets"b'In cybernetics, the good regulator or Conant\xe2\x80\x93Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.'b"In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's Introduction to Mathematical Philosophy."b'In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.'b'In graph theory, an isomorphism between two graphs G and H is a bijective map f from the vertices of G to the vertices of H that preserves the "edge structure" in the sense that there is an edge from vertex u to vertex v in G if and only if there is an edge from \xc6\x92(u) to \xc6\x92(v) in H. See graph isomorphism.'b'In category theory, let the category C consist of two classes, one of objects and the other of morphisms. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism \xc6\x92: a \xe2\x86\x92 b that has an inverse, i.e. there exists a morphism g: b \xe2\x86\x92 a with \xc6\x92g = 1b and g\xc6\x92 = 1a. For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.'b'In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.'b'Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.'b'In abstract algebra, two basic isomorphisms are defined:'b'In a concrete category (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets. In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets. However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).'b'If X = Y, then this is a relation-preserving automorphism.'b'Such an isomorphism is called an order isomorphism or (less commonly) an isotone isomorphism.'b'S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.'b'If one object consists of a set X with a binary relation R and the other object consists of a set Y with a binary relation S then an isomorphism from X to Y is a bijective function \xc6\x92: X \xe2\x86\x92 Y such that:[2]'b'For example, (1,1) + (1,0) = (0,1), which translates in the other system as 1 + 3 = 4.'b'or in general (a,b) \xe2\x86\xa6 (3a + 4b) mod 6.'b'These structures are isomorphic under addition, under the following scheme:'b''b''b'Isomorphisms are formalized using category theory. A morphism f\xc2\xa0: X \xe2\x86\x92 Y in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism g\xc2\xa0: Y \xe2\x86\x92 X in that category such that gf = 1X and fg = 1Y, where 1X and 1Y are the identity morphisms of X and Y, respectively.[1]'b'A canonical isomorphism is a canonical map that is an isomorphism. Two objects are said to be canonically isomorphic if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space V to its second dual space is a canonical isomorphism; on the other hand, V is isomorphic to its dual space but not canonically in general.'b'In topology, where the morphisms are continuous functions, isomorphisms are also called homeomorphisms or bicontinuous functions. In mathematical analysis, where the morphisms are differentiable functions, isomorphisms are also called diffeomorphisms.'b'For most algebraic structures, including groups and rings, a homomorphism is an isomorphism if and only if it is bijective.'b'In mathematics, an isomorphism (from the Ancient Greek: \xe1\xbc\xb4\xcf\x83\xce\xbf\xcf\x82 isos "equal", and \xce\xbc\xce\xbf\xcf\x81\xcf\x86\xce\xae morphe "form" or "shape") is a homomorphism or morphism (i.e. a mathematical mapping) that admits an inverse.[note 1] Two mathematical objects are isomorphic if an isomorphism exists between them. An automorphism is an isomorphism whose source and target coincide. The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.'
Determinant
b'where \xcf\x89j is an nth root of 1.'b'where \xcf\x89 and \xcf\x892 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]'b'Third order'b'Second order'b'where the right-hand side is the continued product of all the differences that can be formed from the n(n\xe2\x88\x921)/2 pairs of numbers taken from x1, x2, \xe2\x80\xa6, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.'b'In general, the nth-order Vandermonde determinant is[33]'b'The third order Vandermonde determinant is'b'The Jacobian also occurs in the inverse function theorem.'b'Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function \xcf\x86: Rn \xe2\x86\x92 Rm is given by'b'the Jacobian matrix is the n \xc3\x97 n matrix whose entries are given by'b'For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. For'b'By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)\xc2\xb7|det(a \xe2\x88\x92 b, b \xe2\x88\x92 c, c \xe2\x88\x92 d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.'b'As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn \xe2\x86\x92 Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn \xe2\x86\x92 Rm is represented by the m \xc3\x97 n matrix A, then the n-dimensional volume of f(S) is given by:'b'More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 \xc3\x97 2 or 3 \xc3\x97 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.'b'The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is \xe2\x88\x921, the basis has the opposite orientation.'b'It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n\xe2\x88\x921 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.'b'As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 \xc3\x97 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), \xe2\x80\xa6, fn(x) (supposed to be n \xe2\x88\x92 1 times differentiable), the Wronskian is defined to be'b"The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises."b"The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]"b"The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy\xe2\x80\x93Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality."b'Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.'b'It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.'b'In Japan, Seki Takakazu (\xe9\x96\xa2 \xe5\xad\x9d\xe5\x92\x8c) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by B\xc3\xa9zout (1764).'b'Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (\xe4\xb9\x9d\xe7\xab\xa0\xe7\xae\x97\xe8\xa1\x93, Chinese scholars, around the 3rd century BCE). In Europe, 2 \xc3\x97 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]'b"Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]"b"Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form."b'If two matrices of order n can be multiplied in time M(n), where M(n) \xe2\x89\xa5 na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith\xe2\x80\x93Winograd algorithm.'b'Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.'b'If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.'b'(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant\xc2\xa01, in which case the formula further simplifies to'b'The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:'b'Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)'b"Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n \xc3\x97 n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants."b'Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.'b"The permanent of a matrix is defined as the determinant, except that the factors sgn(\xcf\x83) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule."b'Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]'b'For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonn\xc3\xa9 determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.'b'Another infinite-dimensional notion of determinant is the functional determinant.'b'The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula'b'For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.'b'For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo\xc2\xa0m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo\xc2\xa0m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (\xe2\x8b\x85)\xc3\x97 (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,'b'holds. In other words, the following diagram commutes:'b'between the group of invertible n \xc3\x97 n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R \xe2\x86\x92 S, there is a map GLn(f): GLn(R) \xe2\x86\x92 GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identity'b'The determinant defines a mapping'b'This definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or \xe2\x88\x921. Such a matrix is called unimodular.'b'This fact also implies that every other n-linear alternating function F: Mn(K) \xe2\x86\x92 K satisfies'b'for any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.'b'from the set of all n \xc3\x97 n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that is'b'The determinant can also be characterized as the unique function'b'The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T\xe2\x80\xb2 on W, where for each w in W we define (T\xe2\x80\xb2w)(x1, \xe2\x80\xa6, xn) = w(Tx1, \xe2\x80\xa6, Txn). As a linear transformation on a one-dimensional space, T\xe2\x80\xb2 is equivalent to a scalar multiple. We call this scalar the determinant of T.'b'For this reason, the highest non-zero exterior power \xce\x9bn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms \xce\x9bkV with k < n.'b'This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 \xe2\x88\xa7 v2 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn to v2 \xe2\x88\xa7 v1 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn, say, also changes its sign.'b'As \xce\x9bnV is one-dimensional, the map \xce\x9bnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to say'b'The determinant of a linear transformation A\xc2\xa0: V \xe2\x86\x92 V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power \xce\x9bnV of V. A induces a linear map'b'for some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.'b'The determinant is therefore also called a similarity invariant. The determinant of a linear transformation'b'The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X\xe2\x88\x921BX. Indeed, repeatedly applying the above identities yields'b'This identity is used in describing the tangent space of certain matrix Lie groups.'b'Yet another equivalent formulation is'b'Expressed in terms of the entries of A, these are'b'where adj(A) denotes the adjugate of A. In particular, if A is invertible, we have'b"By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn \xc3\x97 n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]"b'When D is a 1\xc3\x971 matrix, B is a column vector, and C is a row vector then'b'When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)'b'Generally, if all pairs of n \xc3\x97 n matrices of the np \xc3\x97 np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p \xc3\x97 p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.'b'When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 \xc3\x97 2 matrix holds:[12]'b'as can be seen by employing the decomposition'b'When A is invertible, one has'b'This can be seen from the Leibniz formula, or from a decomposition like (for the former case)'b'Suppose A, B, C, and D are matrices of dimension n \xc3\x97 n, n \xc3\x97 m, m \xc3\x97 n, and m \xc3\x97 m, respectively. Then'b"It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition."b'where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.'b"the solution is given by Cramer's rule:"b'For a matrix equation'b'These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.'b'Also,'b'with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.'b'For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinant'b'is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).'b'where I is the identity matrix. More generally, if'b'An important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,'b'This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way as'b'The formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l \xe2\x80\x93 1)! tr(Al) as'b'where the sum is taken over the set of all integers kl \xe2\x89\xa5 0 satisfying the equation'b'In the general case, this may also be obtained from[9]'b"cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev\xe2\x80\x93LeVerrier algorithm. That is, for generic n, detA = (\xe2\x88\x92)nc0 the signed constant term of the characteristic polynomial, determined recursively from"b'For example, for n = 2, n = 3, and n = 4, respectively,'b'the determinant of A is given by'b'Here exp(A) denotes the matrix exponential of A, because every eigenvalue \xce\xbb of A corresponds to the eigenvalue exp(\xce\xbb) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfying'b'or, for real matrices A,'b'The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,'b'being positive, for all k between 1 and n.'b"A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices"b'where I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).'b'Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equation'b'The product of all non-zero eigenvalues is referred to as pseudo-determinant.'b'From this general result several consequences follow.'b'where Im and In are the m \xc3\x97 m and n \xc3\x97 n identity matrices, respectively.'b"Sylvester's determinant theorem states that for A, an m \xc3\x97 n matrix, and B, an n \xc3\x97 m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):"b"In terms of the adjugate matrix, Laplace's expansion can be written as[7]"b'The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,'b'However, Laplace expansion is efficient for small matrices only.'b'along the second column (j = 2 and the sum runs over i) is given by,'b'Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 \xc3\x97 3 matrix'b"Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n\xe2\x88\x921) \xc3\x97 (n\xe2\x88\x921)-matrix that results from A by removing the i-th row and the j-th column. The expression (\xe2\x88\x921)i+jMi,j is known as a cofactor. The determinant of A is given by"b'In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.'b'The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by'b'Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value\xc2\xa01 on the identity matrix, since the function Mn(K) \xe2\x86\x92 K that maps M \xe2\x86\xa6 det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy\xe2\x80\x93Binet formula, which also provides an independent proof of the multiplicative property.'b'The determinant of a matrix product of square matrices equals the product of their determinants:'b'Here, B is obtained from A by adding \xe2\x88\x921/2\xc3\x97the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = \xe2\x88\x92det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (\xe2\x88\x922) \xc2\xb7 2 \xc2\xb7 4.5 = \xe2\x88\x9218. Therefore, det(A) = \xe2\x88\x92det(D) = +18.'b'can be computed using the following matrices:'b'For example, the determinant of'b'Property 5 says that the determinant on n \xc3\x97 n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property\xc2\xa06; this is essentially the method of Gaussian elimination.'b'Property 2 above implies that properties for columns have their counterparts in terms of rows:'b'Properties 1, 8 and 10 \xe2\x80\x94 which all follow from the Leibniz formula \xe2\x80\x94 completely characterize the determinant; in other words the determinant is the unique function from n \xc3\x97 n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property\xc2\xa09) or else \xc2\xb11 (by properties 1 and\xc2\xa012 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n \xe2\x89\xa5 2,[6] so there is no good definition of the determinant in this setting.'b'A number of additional properties relate to the effects on the determinant of changing particular rows or columns:'b'This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.'b'The determinant has many properties. Some basic properties of determinants are'b'where now each ir and each jr should be summed over 1, \xe2\x80\xa6, n.'b'or using two epsilon symbols as'b'For example, the determinant of a 3 \xc3\x97 3 matrix A (n = 3) is'b'is notation for the product of the entries at positions (i, \xcf\x83i), where i ranges from 1 to n:'b'Here the sum is computed over all permutations \xcf\x83 of the set {1, 2, \xe2\x80\xa6, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering \xcf\x83 is denoted by \xcf\x83i. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to \xcf\x83 = [2, 3, 1], with \xcf\x831 = 2, \xcf\x832 = 3, and \xcf\x833 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation \xcf\x83, sgn(\xcf\x83) denotes the signature of \xcf\x83, a value that is +1 whenever the reordering given by \xcf\x83 can be achieved by successively interchanging two entries an even number of times, and \xe2\x88\x921 whenever it can be achieved by an odd number of such interchanges.'b'The Leibniz formula for the determinant of an n \xc3\x97 n matrix A is'b'The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.'b'The rule of Sarrus is a mnemonic for the 3 \xc3\x97 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 \xc3\x97 3 matrix does not carry over into higher dimensions.'b'which is the Leibniz formula for the determinant of a 3 \xc3\x97 3 matrix.'b'this can be expanded out to give'b'The Laplace formula for the determinant of a 3 \xc3\x97 3 matrix is'b'The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) \xe2\x88\xa7 (c, d)) is the signed area, which is also the determinant ad \xe2\x88\x92 bc.[3]'b'Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.'b"To show that ad \xe2\x88\x92 bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sin\xce\xb8 for the angle \xce\xb8 between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a\xe2\x8a\xa5 = (-b, a), such that |a\xe2\x8a\xa5||b|cos\xce\xb8' , which can be determined by the pattern of the scalar product to be equal to ad \xe2\x88\x92 bc:"b'The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).'b'The absolute value of ad \xe2\x88\x92 bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)'b'If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.'b'The Leibniz formula for the determinant of a 2 \xc3\x97 2 matrix is'b'The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:'b'The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.'b'Assume A is a square matrix with n rows and n columns, so that it can be written as'b'Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is \xe2\x88\x921 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n \xc3\x97 n matrix contributes n! terms), so it will first be given explicitly for the case of 2 \xc3\x97 2 matrices and 3 \xc3\x97 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.'b'where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]'b'Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n \xc3\x97 n matrix A in terms of its column vectors'b'There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 \xc3\x97 4 matrix:'b''b''b'When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.'b'Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.'b'Each determinant of a 2 \xc3\x97 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 \xc3\x97 4 matrix, the determinant of a 5 \xc3\x97 5 matrix, and so forth.'b'Similarly, suppose we have a 3 \xc3\x97 3 matrix A, and we want the specific formula for its determinant |A|:'b'In the case of a 2 \xc3\x97 2 matrix the specific formula for the determinant is:'b'In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.'
Range (mathematics)
b'In both cases, image f \xe2\x8a\x86 range f \xe2\x8a\x86 codomain f, with at least one of the containments being equality.'b'When "range" is used to mean "image", the range of a function f is by definition {y | there exists an x in the domain of f such that y = f(x)}. In this case, the codomain of f must not be specified, because any codomain which contains this image as a (maybe trivial) subset will work.'b'When "range" is used to mean "codomain", the image of a function f is already implicitly defined. It is (by definition of image) the (maybe trivial) subset of the "range" which equals {y | there exists an x in the domain of f such that y = f(x)}.'b'Older books, when they use the word "range", tend to use it to mean what is now called the codomain.[1][2] More modern books, if they use the word "range" at all, generally use it to mean what is now called the image.[3] To avoid any confusion, a number of modern books don\'t use the word "range" at all.[4]'b'As the term "range" can have different meanings, it is considered a good practice to define it the first time it is used in a textbook or article.'b''b''b'The image of a function is the set of all outputs of the function. The image is always a subset of the codomain.'b'The codomain of a function is some arbitrary super-set of image. In real analysis, it is the real numbers. In complex analysis, it is the complex numbers.'b'In mathematics, and more specifically in naive set theory, the range of a function refers to either the codomain or the image of the function, depending upon usage. Modern usage almost always uses range to mean image.'
Kernel (linear algebra)
b'Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]'b'For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]'b'For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gr\xc3\xb6bner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]'b'If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]'b'The problem of computing the kernel on a computer depends on the nature of the coefficients.'b'are a basis of the kernel of A.'b'The last three columns of B are zero columns. Therefore, the three last vectors of C,'b'Putting the upper part in column echelon form by column operations on the whole matrix gives'b'Then'b'For example, suppose that'b'In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.'b'A basis of the kernel of a matrix may be computed by Gaussian elimination.'b'With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.'b'These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (\xe2\x88\x921,\xe2\x88\x9226,16)T.'b'which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.'b'Note also that the following dot products are zero:'b'The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (\xe2\x88\x921,\xe2\x88\x9226,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.'b'Since c is a free variable, this can be expressed equally well as,'b'for c a scalar.'b'Now we can express an element of the kernel:'b'Rewriting yields:'b'Gauss\xe2\x80\x93Jordan elimination reduces this to:'b'which can be written in matrix form as:'b'which can be expressed as a homogeneous system of linear equations involving x, y, and z:'b'The kernel of this matrix consists of all vectors (x, y, z)\xc2\xa0\xe2\x88\x88\xc2\xa0R3 for which'b'Consider the matrix'b'We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.'b'Geometrically, this says that the solution set to Ax\xc2\xa0=\xc2\xa0b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).'b'It follows that any solution to the equation Ax\xc2\xa0=\xc2\xa0b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax\xc2\xa0=\xc2\xa0b is'b'Thus, the difference of any two solutions to the equation Ax\xc2\xa0=\xc2\xa0b lies in the kernel of A.'b'If u and v are two possible solutions to the above equation, then'b'The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:'b'The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA\xc2\xa0=\xc2\xa00T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.'b'The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank\xe2\x80\x93nullity theorem'b'The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.'b'Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).'b'The product Ax can be written in terms of the dot product of vectors as follows:'b'The kernel of an m \xc3\x97 n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:'b'Thus the kernel of A is the same as the solution set to the above homogeneous equations.'b'The matrix equation is equivalent to a homogeneous system of linear equations:'b'Consider a linear map represented as a m \xc3\x97 n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,'b'If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L:\xc2\xa0V\xc2\xa0\xe2\x86\x92\xc2\xa0W is continuous if and only if the kernel of L is a closed subspace of V.'b'The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.'b'When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.'b'where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.'b'This implies the rank\xe2\x80\x93nullity theorem:'b'It follows that the image of L is isomorphic to the quotient of V by the kernel:'b'The kernel of L is a linear subspace of the domain V.[1] In the linear map L\xc2\xa0: V \xe2\x86\x92 W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:'b''b''b'In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L\xc2\xa0: V \xe2\x86\x92 W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,'
Linear map
b'Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.'b'A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.'b'An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).'b'A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.'b'Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.'b'Therefore, the matrix in the new basis is A\xe2\x80\xb2 = B\xe2\x88\x921AB, being B the matrix of the given basis.'b'hence'b'Substituting this in the first expression'b"Given a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v']."b'Let V and W denote vector spaces over a field, F. Let T: V \xe2\x86\x92 W be a linear map.'b'No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.'b'The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \xe2\x86\x92 V \xe2\x86\x92 W \xe2\x86\x92 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\xe2\x80\x93Singer index theorem.[8]'b'For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \xe2\x88\x92 dim(W), by rank\xe2\x80\x93nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.'b'namely the degrees of freedom minus the number of constraints.'b'For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:'b'The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.'b'These can be interpreted thus: given a linear equation f(v) = w to solve,'b'This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequence'b'The number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \xcf\x81(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \xce\xbd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.'b'ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\xe2\x80\x93nullity theorem:'b'If f\xc2\xa0: V \xe2\x86\x92 W is linear, we define the kernel and the image or range of f by'b'If V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \xc3\x97 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \xc3\x97 n invertible matrices with entries in K.'b'An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).'b'A linear transformation f: V \xe2\x86\x92 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \xe2\x86\x92 V.'b'Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.'b'Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.'b'If f\xc2\xa0: V \xe2\x86\x92 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.'b'If f1\xc2\xa0: V \xe2\x86\x92 W and f2\xc2\xa0: V \xe2\x86\x92 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).'b'The inverse of a linear map, when defined, is again a linear map.'b'The composition of linear maps is linear: if f\xc2\xa0: V \xe2\x86\x92 W and g\xc2\xa0: W \xe2\x86\x92 Z are linear, then so is their composition g \xe2\x88\x98 f\xc2\xa0: V \xe2\x86\x92 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.'b'In two-dimensional space R2 linear maps are described by 2 \xc3\x97 2 real matrices. These are some examples:'b'The matrices of a linear transformation can be represented visually:'b'where M is the matrix of f. The symbol \xe2\x88\x97 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.'b'corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),'b'Thus, the function f is entirely determined by the values of aij. If we put these values into an m \xc3\x97 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vector'b'which implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) as'b'If f\xc2\xa0: V \xe2\x86\x92 W is a linear map,'b'Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:'b'If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \xc3\x97 n matrix, then f(x) = Ax describes a linear map Rn \xe2\x86\x92 Rm (see Euclidean space).'b'Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.'b''b''b'In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.'b'A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.'b'An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\xc2\xa0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.'b'In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \xe2\x86\x92 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.'
Map (mathematics)
b'In the communities surrounding programming languages that treat functions as first-class citizens, a map often refers to the binary higher-order function that takes a function f and a list [v0, v1, ..., vn] as arguments and returns [f(v0), f(v1), ..., f(vn)], where n \xe2\x89\xa5 0.'b'In graph theory, a map is a drawing of a graph on a surface without overlapping edges (an embedding). If the surface is a plane then a map is a planar graph, similar to a political map.[3]'b'In formal logic, the term map is sometimes used for a functional predicate, whereas a function is a model of such a predicate in set theory.'b'In category theory, "map" is often used as a synonym for morphism or arrow, thus for something more general than a function.[2]'b'A partial map is a partial function, and a total map is a total function. Related terms like domain, codomain, injective, continuous, etc. can be applied equally to maps and functions, with the same meaning. All these usages can be applied to "maps" as general functions or as functions with special properties.'b'In the theory of dynamical systems, a map denotes an evolution function used to create discrete dynamical systems. See also Poincar\xc3\xa9 map.'b'Sets of maps of special kinds are the subjects of many important theories: see for instance Lie group, mapping class group, permutation group.'b'Some authors, such as Serge Lang,[1] use "function" only to refer to maps in which the codomain is a set of numbers (i.e. a subset of the fields R or C) and the term mapping for more general functions.'b'In many branches of mathematics, the term map is used to mean a function, sometimes with a specific property of particular importance to that branch. For instance, a "map" is a continuous function in topology, a linear transformation in linear algebra, etc.'b''b''b'In mathematics, the term mapping, sometimes shortened to map, refers to either a function, often with some sort of special structure, or a morphism in category theory, which generalizes the idea of a function. There are also a few, less common uses in logic and graph theory.'
Abelian group
b'Among mathematical adjectives derived from the proper name of a mathematician, the word "abelian" is rare in that it is often spelled with a lowercase a, rather than an uppercase A, indicating how ubiquitous the concept is in modern mathematics.[7]'b'Moreover, abelian groups of infinite order lead, quite surprisingly, to deep questions about the set theory commonly assumed to underlie all of mathematics. Take the Whitehead problem: are all Whitehead groups of infinite order also free abelian groups? In the 1970s, Saharon Shelah proved that the Whitehead problem is:'b"Nearly all well-known algebraic structures other than Boolean algebras are undecidable. Hence it is surprising that Tarski's student Wanda Szmielew\xc2\xa0(1955) proved that the first order theory of abelian groups, unlike its nonabelian counterpart, is decidable. This decidability, plus the fundamental theorem of finite abelian groups described above, highlight some of the successes in abelian group theory, but there are still many areas of current research:"b'The collection of all abelian groups, together with the homomorphisms between them, forms the category Ab, the prototype of an abelian category.'b'Many large abelian groups possess a natural topology, which turns them into topological groups.'b'The additive group of a ring is an abelian group, but not all abelian groups are additive groups of rings (with nontrivial multiplication). Some important topics in this area of study are:'b'The classification theorems for finitely generated, divisible, countable periodic, and rank 1 torsion-free abelian groups explained above were all obtained before 1950 and form a foundation of the classification of more general infinite abelian groups. Important technical tools used in classification of infinite abelian groups are pure and basic subgroups. Introduction of various invariants of torsion-free abelian groups has been one avenue of further progress. See the books by Irving Kaplansky, L\xc3\xa1szl\xc3\xb3 Fuchs, Phillip Griffith, and David Arnold, as well as the proceedings of the conferences on Abelian Group Theory published in Lecture Notes in Mathematics for more recent findings.'b'One of the most basic invariants of an infinite abelian group A is its rank: the cardinality of the maximal linearly independent subset of A. Abelian groups of rank 0 are precisely the periodic groups, while torsion-free abelian groups of rank 1 are necessarily subgroups of Q and can be completely described. More generally, a torsion-free abelian group of finite rank r is a subgroup of Qr. On the other hand, the group of p-adic integers Zp is a torsion-free abelian group of infinite Z-rank and the groups Zn\np with different n are non-isomorphic, so this invariant does not even fully capture properties of some familiar groups.'b'An abelian group that is neither periodic nor torsion-free is called mixed. If A is an abelian group and T(A) is its torsion subgroup then the factor group A/T(A) is torsion-free. However, in general the torsion subgroup is not a direct summand of A, so A is not isomorphic to T(A) \xe2\x8a\x95 A/T(A). Thus the theory of mixed groups involves more than simply combining the results about periodic and torsion-free groups.'b'An abelian group is called torsion-free if every non-zero element has infinite order. Several classes of torsion-free abelian groups have been studied extensively:'b'An abelian group is called periodic or torsion, if every element has finite order. A direct sum of finite cyclic groups is periodic. Although the converse statement is not true in general, some special cases are known. The first and second Pr\xc3\xbcfer theorems state that if A is a periodic group, and it either has a bounded exponent, i.e., nA = 0 for some natural number n, or is countable and the p-heights of the elements of A are finite for each p, then A is isomorphic to a direct sum of finite cyclic groups.[6] The cardinality of the set of direct summands isomorphic to Z/pmZ in such a decomposition is an invariant of A. These theorems were later subsumed in the Kulikov criterion. In a different direction, Helmut Ulm found an extension of the second Pr\xc3\xbcfer theorem to countable abelian p-groups with elements of infinite height: those groups are completely classified by means of their Ulm invariants.'b'Two important special classes of infinite abelian groups with diametrically opposite properties are torsion groups and torsion-free groups, exemplified by the groups Q/Z (periodic) and Q (torsion-free).'b"By contrast, classification of general infinitely generated abelian groups is far from complete. Divisible groups, i.e. abelian groups A in which the equation nx = a admits a solution x \xe2\x88\x88 A for any natural number n and element a of A, constitute one important class of infinite abelian groups that can be completely characterized. Every divisible group is isomorphic to a direct sum, with summands isomorphic to Q and Pr\xc3\xbcfer groups Qp/Zp for various prime numbers p, and the cardinality of the set of summands of each type is uniquely determined.[5] Moreover, if a divisible group A is a subgroup of an abelian group G then A admits a direct complement: a subgroup C of G such that G = A \xe2\x8a\x95 C. Thus divisible groups are injective modules in the category of abelian groups, and conversely, every injective abelian group is divisible (Baer's criterion). An abelian group without non-zero divisible subgroups is called reduced."b'The simplest infinite abelian group is the infinite cyclic group Z. Any finitely generated abelian group A is isomorphic to the direct sum of r copies of Z and a finite abelian group, which in turn is decomposable into a direct sum of finitely many cyclic groups of primary orders. Even though the decomposition is not unique, the number r, called the rank of A, and the prime powers giving the orders of finite cyclic summands are uniquely determined.'b'The existence of algorithms for Smith normal form shows that the fundamental theorem of finitely generated abelian groups is not only a theorem of abstract existence, but provides a way for computing expression of finitely generated abelian groups as direct sums.'b'where r is the number of zero rows at the bottom of r (and also the rank of the group). This is the fundamental theorem of finitely generated abelian groups.'b'The Smith normal form of M is a matrix'b'It follows that the study of finitely generated abelian groups is totally equivalent with the study of integer matrices. In particular, changing the generating set of A is equivalent with multiplying M on the left by a unimodular matrix (that is an invertible integer matrix whose inverse is also an integer matrix). Changing the generating set of the kernel of M is equivalent with multiplying M on the right by an unimodular matrix.'b'This homomorphism is surjective, and its kernel is finitely generated (since integers form a Noetherian ring). Let us consider the matrix M with integer entries, such that the entries of its jth column are the coefficients of the jth generator of the kernel. Then, the abelian group is isomorphic to the cokernel of linear map defined by M. Conversely every integer matrix defines a finitely generated abelian group.'b'One can check that this yields the orders in the previous examples as special cases (see Hillar, C., & Rhea, D.).'b'then one has in particular dk \xe2\x89\xa5 k, ck \xe2\x89\xa4 k, and'b'and'b'In the most general case, where the ei and n are arbitrary, the automorphism group is more difficult to determine. It is known, however, that if one defines'b'where GL is the appropriate general linear group. This is easily shown to have order'b'so elements of this subgroup can be viewed as comprising a vector space of dimension n over the finite field of p elements Fp. The automorphisms of this subgroup are therefore given by the invertible linear transformations, so'b'One special case is when n = 1, so that there is only one cyclic prime-power factor in the Sylow p-subgroup P. In this case the theory of automorphisms of a finite cyclic group can be used. Another special case is when n is arbitrary but ei = 1 for 1 \xe2\x89\xa4 i \xe2\x89\xa4 n. Here, one is considering P to be of the form'b'for some n > 0. One needs to find the automorphisms of'b'Given this, the fundamental theorem shows that to compute the automorphism group of G it suffices to compute the automorphism groups of the Sylow p-subgroups separately (that is, all direct sums of cyclic subgroups, each with order a power of p). Fix a prime p and suppose the exponents ei of the cyclic factors of the Sylow p-subgroup are arranged in increasing order:'b'One can apply the fundamental theorem to count (and sometimes determine) the automorphisms of a given finite abelian group G. To do this, one uses the fact that if G splits as a direct sum H \xe2\x8a\x95 K of subgroups of coprime order, then Aut(H \xe2\x8a\x95 K) \xe2\x89\x85 Aut(H) \xe2\x8a\x95 Aut(K).'b'See also list of small groups for finite abelian groups of order 30 or less.'b'For another example, every abelian group of order 8 is isomorphic to either Z8 (the integers 0 to 7 under addition modulo 8), Z4 \xe2\x8a\x95 Z2 (the odd integers 1 to 15 under multiplication modulo 16), or Z2 \xe2\x8a\x95 Z2 \xe2\x8a\x95 Z2.'b'For example, Z15 can be expressed as the direct sum of two cyclic subgroups of order 3 and 5: Z15 \xe2\x89\x85 {0, 5, 10} \xe2\x8a\x95 {0, 3, 6, 9, 12}. The same can be said for any abelian group of order 15, leading to the remarkable conclusion that all abelian groups of order 15 are isomorphic.'b'in either of the following canonical ways:'b'The cyclic group Zmn of order mn is isomorphic to the direct sum of Zm and Zn if and only if m and n are coprime. It follows that any finite abelian group G is isomorphic to a direct sum of the form'b'The classification was proven by Leopold Kronecker in 1870, though it was not stated in modern group-theoretic terms until later, and was preceded by a similar classification of quadratic forms by Gauss in 1801; see history for details.'b'The fundamental theorem of finite abelian groups states that every finite abelian group G can be expressed as the direct sum of cyclic subgroups of prime-power order; it is also known as the basis theorem for finite abelian groups. This is generalized by the fundamental theorem of finitely generated abelian groups, with finite groups being the special case when G has zero rank; this in turn admits numerous further generalizations.'b'Any group of prime order is isomorphic to a cyclic group and therefore abelian. Any group whose order is a square of a prime number is abelian.[4] In fact, for every prime number p there are (up to isomorphism) exactly two groups of order p2, namely Zp2 and Zp\xc3\x97Zp.'b'Cyclic groups of integers modulo n, Z/nZ, were among the first examples of groups. It turns out that an arbitrary finite abelian group is isomorphic to a direct sum of finite cyclic groups of prime power order, and these orders are uniquely determined, forming a complete system of invariants. The automorphism group of a finite abelian group can be described directly in terms of these invariants. The theory had been first developed in the 1879 paper of Georg Frobenius and Ludwig Stickelberger and later was both simplified and generalized to finitely generated modules over a principal ideal domain, forming an important chapter of linear algebra.'b'The center Z(G) of a group G is the set of elements that commute with every element of G. A group G is abelian if and only if it is equal to its center Z(G). The center of a group G is always a characteristic abelian subgroup of G. If the quotient group G/Z(G) of a group by its center is cyclic then G is abelian.[3]'b'Somewhat akin to the dimension of vector spaces, every abelian group has a rank. It is defined as the maximal cardinality of a set of linearly independent elements of the group. The integers and the rational numbers have rank one, as well as every subgroup of the rationals.'b'If f, g\xc2\xa0: G \xe2\x86\x92 H are two group homomorphisms between abelian groups, then their sum f + g, defined by (f + g) (x) = f(x) + g(x), is again a homomorphism. (This is not true if H is a non-abelian group.) The set Hom(G, H) of all group homomorphisms from G to H thus turns into an abelian group in its own right.'b'Theorems about abelian groups (i.e. modules over the principal ideal domain Z) can often be generalized to theorems about modules over an arbitrary principal ideal domain. A typical example is the classification of finitely generated abelian groups which is a specialization of the structure theorem for finitely generated modules over a principal ideal domain. In the case of finitely generated abelian groups, this theorem guarantees that an abelian group splits as a direct sum of a torsion group and a free abelian group. The former may be written as a direct sum of finitely many groups of the form Z/pkZ for p prime, and the latter is a direct sum of finitely many copies of Z.'b'If n is a natural number and x is an element of an abelian group G written additively, then nx can be defined as x + x + ... + x (n summands) and (\xe2\x88\x92n)x = \xe2\x88\x92(nx). In this way, G becomes a module over the ring Z of integers. In fact, the modules over Z can be identified with the abelian groups.'b'Camille Jordan named abelian groups after Norwegian mathematician Niels Henrik Abel, because Abel found that the commutativity of the group of a polynomial implies that the roots of the polynomial can be calculated by using radicals. See Section 6.5 of Cox (2004) for more information on the historical background.'b'In general, matrices, even invertible matrices, do not form an abelian group under multiplication because matrix multiplication is generally not commutative. However, some groups of matrices are abelian groups under matrix multiplication \xe2\x80\x93 one example is the group of 2\xc3\x972 rotation matrices.'b'This is true since if the group is abelian, then gi \xe2\x8b\x85 gj = gj \xe2\x8b\x85 gi. This implies that the (i, j)th entry of the table equals the (j, i)th entry, thus the table is symmetric about the main diagonal.'b'To verify that a finite group is abelian, a table (matrix) \xe2\x80\x93 known as a Cayley table \xe2\x80\x93 can be constructed in a similar fashion to a multiplication table. If the group is G = {g1 = e, g2, ..., gn} under the operation \xe2\x8b\x85, the (i, j)th entry of this table contains the product gi \xe2\x8b\x85 gj. The group is abelian if and only if this table is symmetric about the main diagonal.'b'Generally, the multiplicative notation is the usual notation for groups, while the additive notation is the usual notation for modules and rings. The additive notation may also be used to emphasize that a particular group is abelian, whenever both abelian and non-abelian groups are considered, some notable exceptions being near-rings and partially ordered groups, where an operation is written additively even when non-abelian.'b'There are two main notational conventions for abelian groups \xe2\x80\x93 additive and multiplicative.'b'A group in which the group operation is not commutative is called a "non-abelian group" or "non-commutative group".'b'An abelian group is a set, A, together with an operation \xe2\x80\xa2 that combines any two elements a and b to form another element denoted a \xe2\x80\xa2 b. The symbol \xe2\x80\xa2 is a general placeholder for a concretely given operation. To qualify as an abelian group, the set and operation, (A, \xe2\x80\xa2), must satisfy five requirements known as the abelian group axioms:'b''b''b'The concept of an abelian group is one of the first concepts encountered in undergraduate abstract algebra, from which many other basic concepts, such as modules and vector spaces, are developed. The theory of abelian groups is generally simpler than that of their non-abelian counterparts, and finite abelian groups are very well understood. On the other hand, the theory of infinite abelian groups is an area of current research.'b'In abstract algebra, an abelian group, also called a commutative group, is a group in which the result of applying the group operation to two group elements does not depend on the order in which they are written. That is, these are the groups that obey the axiom of commutativity. Abelian groups generalize the arithmetic of addition of integers. They are named after early 19th century mathematician Niels Henrik Abel.[1]'
Sequence
b'An infinite binary sequence can represent a formal language (a set of strings) by setting the n\xe2\x80\x89th bit of the sequence to 1 if and only if the n\xe2\x80\x89th string (in shortlex order) is in the language. This representation is useful in the diagonalization method for proofs.[12]'b'Infinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science. They are often referred to simply as sequences or streams, as opposed to finite strings. Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}). The set C = {0, 1}\xe2\x88\x9e of all infinite binary sequences is sometimes called the Cantor space.'b"Automata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, \xce\xa3. Most familiar types of automata transition from state to state by reading input letters from \xce\xa3, following edges with matching labels; the ordered input for such an automaton forms a sequence called a word (or input word). The sequence of states encountered by the automaton when processing a word is called a run. A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter. This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter."b'An ordinal-indexed sequence is a generalization of a sequence. If \xce\xb1 is a limit ordinal and X is a set, an \xce\xb1-indexed sequence of elements of X is a function from \xce\xb1 to X. In this terminology an \xcf\x89-indexed sequence is an ordinary sequence.'b'In homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations. Spectral sequences are a generalization of exact sequences, and since their introduction by Jean Leray\xc2\xa0(1946), they have become an important research tool, particularly in homotopy theory.'b'A similar definition can be made for certain other algebraic structures. For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.'b'Note that the sequence of groups and homomorphisms may be either finite or infinite.'b'of groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:'b'In the context of group theory, a sequence'b'If A is a set, the free monoid over A (denoted A*, also called Kleene star of A) is a monoid containing all the finite sequences (or strings) of zero or more elements of A, with the binary operation of concatenation. The free semigroup A+ is the subsemigroup of A* containing all elements except the empty sequence.'b'Abstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.'b'Sequences over a field may also be viewed as vectors in a vector space. Specifically, the set of F-valued sequences (where F is a field) is a function space (in fact, a product space) of F-valued functions over the set of natural numbers.'b'The most important sequences spaces in analysis are the \xe2\x84\x93p spaces, consisting of the p-power summable sequences, with the p-norm. These are special cases of Lp spaces for the counting measure on the set of natural numbers. Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted c and c0, with the sup norm. Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fr\xc3\xa9chet space called an FK-space.'b'A sequence space is a vector space whose elements are infinite sequences of real or complex numbers. Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers. The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication. All sequence spaces are linear subspaces of this space. Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.'b'The most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers. This type can be generalized to sequences of elements of some vector space. In analysis, the vector spaces considered are often function spaces. Even more generally, one can study sequences with elements in some topological space.'b'It may be convenient to have the sequence start with an index different from 1 or 0. For example, the sequence defined by xn = 1/log(n) would be defined only for n \xe2\x89\xa5 2. When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given N.'b'which is to say, infinite sequences of elements indexed by natural numbers.'b'In analysis, when talking about sequences, one will generally consider sequences of the form'b'The topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.'b'Sequences can be generalized to nets or filters. These generalizations allow one to extend some of the above theorems to spaces without metrics.'b'Sequences play an important role in topology, especially in the study of metric spaces. For instance:'b'and say that the sequence diverges or converges to negative infinity.'b'In this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is an = n.'b'Metric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.'b'In contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined by x1 = 1 and xn+1 = xn + 2/xn/2 is Cauchy, but has no rational limit, cf. here. More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.'b'A Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is Cauchy characterization of convergence for sequences:'b'Moreover:'b'An important property of a sequence is convergence. If a sequence converges, it converges to a particular value known as the limit. If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.'b'Some other types of sequences that are easy to define include:'b'A subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements. For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...). The positions of some elements change when other elements are deleted. However, the relative positions are preserved.'b'If the sequence of real numbers (an) is such that all the terms are less than some real number M, then the sequence is said to be bounded from above. In less words, this means that there exists M such that for all n, an \xe2\x89\xa4 M. Any such M is called an upper bound. Likewise, if, for some real m, an \xe2\x89\xa5 m for all n greater than some N, then the sequence is bounded from below and any such m is called a lower bound. If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.'b'The terms nondecreasing and nonincreasing are often used in place of increasing and decreasing in order to avoid any possible confusion with strictly increasing and strictly decreasing, respectively.'b'A sequence of a finite length n is also called an n-tuple. Finite sequences include the empty sequence\xc2\xa0(\xc2\xa0) that has no elements.'b'The length of a sequence is defined as the number of terms in the sequence.'b'Sequences and their limits (see below) are important concepts for studying topological spaces. An important generalization of sequences is the concept of nets. A net is a function from a (possibly uncountable) directed set to a topological space. The notational conventions for sequences normally apply to nets as well.'b'For the purposes of this article, we define a sequence to be a function whose domain is a convex subset of the set of integers. This definition covers several different uses of the word "sequence", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions). However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers. The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice. Many authors also impose a requirement on the codomain of a function before calling it a sequence, by requiring it to be the set R of real numbers,[2] the set C of complex numbers,[3] or a topological space.[4]'b'There are many different notions of sequences in mathematics, some of which (e.g., exact sequence) are not covered by the definitions and notations introduced below.'b'Not all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated. For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).'b"The first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34. A more complicated example of a sequence that is defined recursively is Recaman's sequence.[1] We can define Recaman's sequence by"b'The Fibonacci sequence can be defined using a recursive rule along with two initial elements. The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.'b'To define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it. In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule. The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions. Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.'b'Sequences whose elements are related to the previous elements in a straightforward way are often defined using recursion. This is in contrast to the definition of sequence elements as a function of their position.'b'In some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred. In these cases the index set may be implied by a listing of the first few abstract elements. For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.'b'Other examples of sequences include ones made up of rational numbers, real numbers, and complex numbers. The sequence (.9, .99, .999, .9999, ...) approaches the number 1. In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion. For instance, \xcf\x80 is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...). A related sequence is the sequence of decimal digits of \xcf\x80, i.e. (3, 1, 4, 1, 5, 9, ...). This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.'b'For a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.'b'The Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements. The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).'b'The prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves. Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...). The prime numbers are widely used in mathematics and specifically in number theory.'b'There are a number of ways to denote a sequence, some of which are more useful for specific types of sequences. One way to specify a sequence is to list the elements. For example, the first four odd numbers form the sequence (1, 3, 5, 7). This notation can be used for infinite sequences as well. For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...). Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements. Other ways to denote a sequence are discussed after the examples.'b'A sequence can be thought of as a list of elements with a particular order. Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences. In particular, sequences are the basis for series, which are important in differential equations and analysis. Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.'b''b''b"For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...). In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence\xc2\xa0(\xc2\xa0) is included in most notions of sequence, but may be excluded depending on the context."b'In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed. Like a set, it contains members (also called elements, or terms). The number of elements (possibly infinite) is called the length of the sequence. Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n). The position of an element in a sequence is its rank or index; it is the integer from which the element is the image. It depends on the context or of a specific convention, if the first element has index 0 or 1. When a symbol has been chosen for denoting a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence is generally denoted Fn.'
Function (mathematics)
b'The concept of categorification is an attempt to replace set-theoretic notions by category-theoretic ones. In particular, according to this idea, sets are replaced by categories, while functions between sets are replaced by functors.[15]'b'The idea of structure-preserving functions, or homomorphisms, led to the abstract notion of morphism, the key concept of category theory. In fact, functions f: X \xe2\x86\x92 Y are the morphisms in the category of sets, including the empty set: if the domain X is the empty set, then the subset of X \xc3\x97 Y describing the function is necessarily empty, too. However, this is still a well-defined function. Such a function is called an empty function. In particular, the identity function of the empty set is defined, a requirement for sets to form a category.'b'Traditionally, addition and multiplication are written in the infix notation: x+y and x\xc3\x97y instead of +(x, y) and \xc3\x97(x, y).'b'The familiar binary operations of arithmetic, addition and multiplication, can be viewed as functions from \xe2\x84\x9d\xc3\x97\xe2\x84\x9d to \xe2\x84\x9d. This view is generalized in abstract algebra, where n-ary functions are used to model the operations of arbitrary algebraic structures. For example, an abstract group is defined as a set X and a function f from X\xc3\x97X to X that satisfies certain properties.'b'The concept can still further be extended by considering a function that also produces output that is expressed as several variables. For example, consider the integer divide function, with domain \xe2\x84\xa4\xc3\x97\xe2\x84\x95 and codomain \xe2\x84\xa4\xc3\x97\xe2\x84\x95. The resultant (quotient, remainder) pair is a single value in the codomain seen as a Cartesian product.'b'The function value of the pair (x, y) is f((x, y)). However, it is customary to drop one set of parentheses and consider f(x, y) a function of two variables, x and y. Functions of two variables may be plotted on the three-dimensional Cartesian as ordered triples of the form (x, y, f(x, y)).'b'For example, consider the function that associates two integers to their product: f(x, y) = x\xc2\xb7y. This function can be defined formally as having domain \xe2\x84\xa4\xc3\x97\xe2\x84\xa4, the set of all integer pairs; codomain \xe2\x84\xa4; and, for graph, the set of all pairs ((x, y), x\xc2\xb7y). The first component of any such pair is itself a pair (of integers), while the second component is a single integer.'b'The concept of function can be extended to an object that takes a combination of two (or more) argument values to a single result. This intuitive concept is formalized by a function whose domain is the Cartesian product of two or more sets.'b'In other parts of mathematics, non-single-valued relations are similarly conflated with functions: these are called multivalued functions, with the corresponding term single-valued function for ordinary functions.'b'In some parts of mathematics, including recursion theory and functional analysis, it is convenient to study partial functions in which some values of the domain have no association in the graph; i.e., single-valued relations. For example, the function f such that f(x)\xc2\xa0=\xc2\xa01/x does not define a value for x\xc2\xa0=\xc2\xa00, since division by zero is not defined. Hence f is only a partial function from the real line to the real line. The term total function can be used to stress the fact that every element of the domain does appear as the first element of an ordered pair in the graph.'b'Many operations in set theory, such as the power set, have the class of all sets as their domain, and therefore, although they are informally described as functions, they do not fit the set-theoretical definition outlined above, because a class is not necessarily a set. However some definitions of relations and functions define them as classes of pairs rather than sets of pairs and therefore do include the power set as a function.[14]'b'Functions are commonly defined as a type of relation. A relation from X to Y is a set of ordered pairs (x,\xe2\x80\x89y) with x \xe2\x88\x88 X and y \xe2\x88\x88 Y. A function from X to Y can be described as a relation from X to Y that is left-total and right-unique. However, when X and Y are not specified there is a disagreement about the definition of a relation that parallels that for functions. Normally a relation is just defined as a set of ordered pairs and a correspondence is defined as a triple (X,\xe2\x80\x89Y,\xe2\x80\x89F), however the distinction between the two is often blurred or a relation is never referred to without specifying the two sets. The definition of a function as a triple defines a function as a type of correspondence, whereas the definition of a function as a set of ordered pairs defines a function as a type of relation.'b'An alternative definition of the composite function g(f(x)) defines it for the set of all x in the domain of f such that f(x) is in the domain of g.[13] Thus the real square root of \xe2\x88\x92x2 is a function only defined at 0 where it has the value 0.'b'If a function is defined as a set of ordered pairs with no specific codomain, then f:\xe2\x80\x89X\xe2\x80\x89\xe2\x86\x92\xe2\x80\x89Y indicates that f is a function whose domain is X and whose image is a subset of Y. This is the case in the ISO standard.[7] Y may be referred to as the codomain but then any set including the image of f is a valid codomain of f. This is also referred to by saying that "f maps X into Y"[7] In some usages X and Y may subset the ordered pairs, e.g. the function f on the real numbers such that y=x2 when used as in f:\xe2\x80\x89[0,4]\xe2\x80\x89\xe2\x86\x92\xe2\x80\x89[0,4] means the function defined only on the interval [0,2].[12] With the definition of a function as an ordered triple this would always be considered a partial function.'b'In the other definition a function is defined as a set of ordered pairs where each first element only occurs once. The domain is the set of all the first elements of a pair and there is no explicit codomain separate from the image.[10][11] Concepts like surjective have to be refined for such functions, more specifically by saying that a (given) function is surjective on a (given) set if its image equals that set. For example, we might say a function f is surjective on the set of real numbers.'b'The above definition of "a function from X to Y" is generally agreed on,[citation needed] however there are two different ways a "function" is normally defined where the domain X and codomain Y are not explicitly or implicitly specified. Usually this is not a problem as the domain and codomain normally will be known. With one definition saying the function defined by f(x) = x2 on the reals does not completely specify a function as the codomain is not specified, and in the other it is a valid definition.'b'The second definition can be used to define a set or class of functions without specifying a codomain. For example, the class of functions on the ordinal \xce\xb1. Also, an infinite sequence can be defined as a function on \xcf\x89, the set of finite ordinals.'b"The first definition allows the use of functions without specifying their domain or codomain. For example, this definition is used to state the replacement axiom of von Neumann\xe2\x80\x93Bernays\xe2\x80\x93G\xc3\xb6del set theory: For all classes F and for all sets X, if F is a function, then F[X] is a set. Since this definition uses set theory's definition of image, X does not have to be a subset or subclass of the domain of F."b'The definitions of function and image in set theory are more general than the ones given above\xe2\x80\x94namely, the definition of function does not mention a domain or a codomain, and the definition of the image F[A] does not require that A be a subset of the domain.[8] Set theory also specializes its function definition to the cases where the domain or both the domain and codomain are specified. This produces the following three definitions.[9]'b'When working with curried functions it is customary to use prefix notation with function application considered left-associative, since juxtaposition of multiple arguments\xe2\x80\x94as in (f x y)\xe2\x80\x94naturally maps to evaluation of a curried function. Conversely, the \xe2\x86\x92 and \xe2\x9f\xbc symbols are considered to be right-associative, so that curried functions may be defined by a notation such as f: \xe2\x84\xa4 \xe2\x86\x92 \xe2\x84\xa4 \xe2\x86\x92 \xe2\x84\xa4 = x \xe2\x9f\xbc y \xe2\x9f\xbc x\xc2\xb7y.'b'An alternative approach to handling functions with multiple arguments is to transform them into a chain of functions that each takes a single argument. For instance, one can interpret Add(3,5) to mean "first produce a function that adds 3 to its argument, and then apply the \'Add 3\' function to 5". This transformation is called currying: Add 3 is curry(Add) applied to 3. There is a bijection between the function spaces CA\xc3\x97B and (CB)A.'b'The set of all functions from a set X to a set Y is denoted by X \xe2\x86\x92 Y, by [X \xe2\x86\x92 Y], or by YX. The latter notation is motivated by the fact that, when X and Y are finite and of size |X| and |Y|, then the number of functions X \xe2\x86\x92 Y is |YX| = |Y||X|. This is an example of the convention from enumerative combinatorics that provides notations for sets based on their cardinalities. If X is infinite and there is more than one element in Y then there are uncountably many functions from X to Y, though only countably many of them can be expressed with a formula or algorithm.'b'There are many other special classes of functions that are important to particular branches of mathematics, or particular applications. Here is a partial list:'b'A multivariate function is one which takes several inputs.'b'The following table contains a few particularly important types of real-valued functions:'b'where X is an arbitrary set, their (pointwise) sum f + g and product f \xe2\x8b\x85 g are functions with the same domain and codomain. They are defined by the formulas:'b'Real-valued functions enjoy so-called pointwise operations. That is, given two functions'b'A real-valued function f is one whose codomain is the set of real numbers or a subset thereof. If, in addition, the domain is also a subset of the reals, f is a real valued function of a real variable. The study of such functions is called real analysis.'b'As a simple example, if f converts a temperature in degrees Celsius C to degrees Fahrenheit F, the function converting degrees Fahrenheit to degrees Celsius would be a suitable f\xe2\x88\x921.'b'That is, the two possible compositions of f and f\xe2\x88\x921 need to be the respective identity maps of X and Y.'b'An inverse function for f, denoted by f\xe2\x88\x921, is a function in the opposite direction, from Y to X, satisfying'b'The overriding of f: X \xe2\x86\x92 Y by g: W \xe2\x86\x92 Y (also called overriding union) is an extension of g denoted as (f \xe2\x8a\x95 g): (X \xe2\x88\xaa W) \xe2\x86\x92 Y. Its graph is the set-theoretical union of the graphs of g and f|X \\ W. Thus, it relates any element of the domain of g to its image under g, and any other element of the domain of f to its image under f. Overriding is an associative operation; it has the empty function as an identity element. If f|X \xe2\x88\xa9 W and g|X \xe2\x88\xa9 W are pointwise equal (e.g., the domains of f and g are disjoint), then the union of f and g is defined and is equal to their overriding union. This definition agrees with the definition of union for binary relations.'b'Informally, a restriction of a function f is the result of trimming its domain. More precisely, if S is any subset of X, the restriction of f to S is the function f|S from S to Y such that f|S(s) = f(s) for all s in S. If g is a restriction of f, then it is said that f is an extension of g.'b'The existence of an empty function from \xe2\x88\x85 to \xe2\x88\x85 is required to make the category of sets a category, because in a category, each object needs to have an "identity morphism", and only the empty function is the identity on the object \xe2\x88\x85. The existence of a unique empty function from \xe2\x88\x85 into each set A means that the empty set is an initial object in the category of sets. In terms of cardinal arithmetic, it means that k0 = 1 for every cardinal number k\xe2\x80\x94particularly profound when k = 0 to illustrate the strong statement of indices pertaining to 0.'b'The graph of an empty function is a subset of the Cartesian product \xe2\x88\x85 \xc3\x97 A. Since the product is empty the only such subset is the empty set \xe2\x88\x85. The empty subset is a valid graph since for every x in the domain \xe2\x88\x85 there is a unique y in the codomain A such that (x, y) \xe2\x88\x88 \xe2\x88\x85 \xc3\x97 A. This statement is an example of a vacuous truth since "there is no x in the domain."'b'For any set A, there is exactly one function from the empty set to A, namely the empty function:'b'The unique function over a set X that maps each element to itself is called the identity function for X, and typically denoted by idX. Each set has its own identity function, so the subscript cannot be omitted unless the set can be inferred from context. Under composition, an identity function is "neutral": if f is any function from X to Y, then'b'That is, the order of the composition is important. For example, suppose f(x)\xc2\xa0= x2 and g(x)\xc2\xa0= x+1. Then g(f(x))\xc2\xa0= x2+1, while f(g(x))\xc2\xa0= (x+1)2, which is x2+2x+1, a different function.'b'The above "color-of-the-shape" function is not injective, since two distinct shapes (the red triangle and the red rectangle) are assigned the same value. Moreover, it is not surjective, since the image of the function contains only three, but not all five colors in the codomain.'b'"One-to-one" and "onto" are terms that were more common in the older English language literature; "injective", "surjective", and "bijective" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English. As a word of caution, "a one-to-one function" is one that is injective, while a "one-to-one correspondence" refers to a bijective function. Also, the statement "f maps A onto B" differs from "f maps A into B" in that the former implies that f is an onto function (i.e., surjective), while the latter makes no assertion about the nature of the mapping. In more complicated statements the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms.'b'A function is called injective (or one-to-one; 1-1) if f(a) \xe2\x89\xa0 f(b) for any two elements a, b, a \xe2\x89\xa0 b of the domain. It is called surjective (or onto) if the range is identical to the codomain; that is, f(X) = Y. In other words, every element y in the codomain is mapped to by f from some x in the domain. Finally f is called bijective (or the function is a one-to-one correspondence) if it is both injective and surjective. A function that is injective, surjective, or bijective is referred to as an injection, a surjection, or a bijection, respectively. The existence of injections, surjections, or bijections between sets is the key concept defining the relative cardinalities (sizes) of the sets.'b'Use of f(A) to denote the image of a subset A \xe2\x8a\x86 X is consistent so long as no subset of the domain is also an element of the domain. In some fields (e.g., in set theory, where ordinals are also sets of ordinals) it is convenient or even necessary to distinguish the two concepts; the customary notation is f[A] for the set { f(x): x \xe2\x88\x88 A }. Likewise, some authors use square brackets to avoid confusion between the inverse image and the inverse function. Thus they would write f\xe2\x88\x921[B] and f\xe2\x88\x921[b] for the preimage of a set and a singleton.'b'By definition of a function, the image of an element x of the domain is always a single element y of the codomain. However, the preimage of a singleton set (a set with exactly one element) may in general contain any number of elements. For example, if f(x) = 7 (the constant function taking value 7), then the preimage of {5} is the empty set but the preimage of {7} is the entire domain. It is customary to write f\xe2\x88\x921(b) instead of f\xe2\x88\x921({b}), i.e.'b'So, for example, the preimage of {4, 9} under the squaring function is the set {\xe2\x88\x923,\xe2\x88\x922,2,3}. The term range usually refers to the image,[7] but sometimes it refers to the codomain.'b'If A is any subset of the domain X, then f(A) is the subset of the codomain Y consisting of all images of elements of A. We say the f(A) is the image of A under f. The image of f is given by f(X). On the other hand, the inverse image (or preimage, complete inverse image) of a subset B of the codomain Y under a function f is the subset of the domain X defined by'b'There are a number of general basic properties and notions. In this section, f is a function with domain X and codomain Y.'b'Fundamental results of computability theory show that there are functions that can be precisely defined but are not computable. Moreover, in the sense of cardinality, almost all functions from the integers to integers are not computable. The number of computable functions from integers to integers is countable, because the number of possible algorithms is. The number of all functions from integers to integers is higher: the same as the cardinality of the real numbers. Thus most functions from integers to integers are not computable. Specific examples of uncomputable functions are known, including the busy beaver function and functions related to the halting problem and other undecidable problems.'b'Functions that send integers to integers, or finite strings to finite strings, can sometimes be defined by an algorithm, which gives a precise description of a set of steps for computing the output of the function from its input. Functions definable by an algorithm are called computable functions. For example, the Euclidean algorithm gives a precise process to compute the greatest common divisor of two positive integers. Many of the functions studied in the context of number theory are computable.'b'As an example, the factorial function is defined on the nonnegative integers and produces a nonnegative integer. It is defined by the following inductive algorithm: 0! is defined to be 1, and n! is defined to be n(n\xc2\xa0\xe2\x88\x92\xc2\xa01)! for all positive integers n. The factorial function is denoted with the exclamation mark (serving as the symbol of the function) after the variable (postfix notation).'b'Different formulas or algorithms may describe the same function. For instance f(x) = (x\xe2\x80\x89+\xe2\x80\x891)\xe2\x80\x89(x\xe2\x80\x89\xe2\x88\x92\xe2\x80\x891) is exactly the same function as f(x) = x2\xe2\x80\x89\xe2\x88\x92\xe2\x80\x891.[6] Furthermore, a function need not be described by a formula, expression, or algorithm, nor need it deal with numbers at all: the domain and codomain of a function may be arbitrary sets. One example of a function that acts on non-numeric inputs takes English words as inputs and returns the first letter of the input word as output.'b'The graph of a function is its set of ordered pairs F. This is an abstraction of the idea of a graph as a picture showing the function plotted on a pair of coordinate axes; for example, (3,\xe2\x80\x899), the point above 3 on the horizontal axis and to the right of 9 on the vertical axis, lies on the graph of y = x2.'b'There are many other ways of defining functions. Examples include piecewise definitions, induction or recursion, algebraic or analytic closure, limits, analytic continuation, infinite series, and as solutions to integral and differential equations. The lambda calculus provides a powerful and flexible syntax for defining and combining functions of several variables. In advanced mathematics, some functions exist because of an axiom, such as the Axiom of Choice.'b'A function can be defined by any mathematical condition relating each argument (input value) to the corresponding output value. If the domain is finite, a function f may be defined by simply tabulating all the arguments x and their corresponding function values f(x). More commonly, a function is defined by a formula, or (more generally) an algorithm \xe2\x80\x94 a recipe that tells how to compute the value of f(x) given any x in the domain.'b'In other words, this function has the natural numbers as domain, the integers as codomain. Strictly speaking, a function is properly defined only when the domain and codomain are specified. Moreover, the function'b'The second part is read:'b'The first part can be read as:'b'In most cases, a function is given by a formula, such as f(x) = 2x + 3, but in some cases, especially in advanced mathematics, it is important to also give the set of all allowable inputs, the domain, and a set containing all outputs, the codomain.'b'A general function, to be defined for a particular context, is usually denoted by a single letter, most often the lower-case letters f, g, h. Special functions that have widely recognized names and definitions often have abbreviations, such as sin for the sine function. The value of the function f for an input x is denoted f(x). Sometimes the parentheses are omitted if no ambiguity arises, as with x itself, but are always required if the input is, for example, a binomial such as x + 1. By convention, general functions are displayed using an italicized letter while special functions are set in roman type.'b'In this context, the elements of X are called arguments of f. For each argument x, the corresponding unique y in the codomain is called the function value at x or the image of x under f. It is written as f(x). One says that f associates y with x or maps x to y. This is abbreviated by'b'or'b'A function f is commonly declared by stating its domain X and codomain Y using the expression'b'where the color is the actual color of the given shape. Thus, the pair ("red triangle", "red") is in the function, but the pair ("yellow rectangle", "red") is not.'b'The "color-of-the-shape" function described above consists of the set of those ordered pairs,'b'Considering the "color-of-the-shape" function above, the set X is the domain consisting of the four shapes, while Y is the codomain consisting of five colors. There are twenty possible ordered pairs (four shapes times five colors), one of which is'b'A function f from X to Y is a subset of the Cartesian product X \xc3\x97 Y subject to the following condition: every element of X is the first component of one and only one ordered pair in the subset.[4] In other words, for every x in X there is exactly one element y such that the ordered pair (x, y) is contained in the subset defining the function f. This formal definition is a precise rendition of the idea that to each x is associated an element y of Y, namely the uniquely specified element y with the property just mentioned.'b'In order to avoid the use of the informally defined concepts of "rules" and "associates", the above intuitive explanation of functions is completed with a formal definition. This definition relies on the notion of the Cartesian product. The Cartesian product of two sets X and Y is the set of all ordered pairs, written (x, y), where x is an element of X and y is an element of Y. The x and the y are called the components of the ordered pair. The Cartesian product of X and Y is denoted by X \xc3\x97 Y.'b'The term range is sometimes used either for the codomain or for the set of all the actual values a function has.'b'A third example of a function has the set of polygons as domain and the set of natural numbers as codomain. The function associates a polygon with its number of vertices. For example, a triangle is associated with the number 3, a square with the number 4, and so on.'b'A second example of a function is the following: the domain is chosen to be the set of natural numbers (1, 2, 3, 4, ...), and the codomain is the set of integers (..., \xe2\x88\x923, \xe2\x88\x922, \xe2\x88\x921, 0, 1, 2, 3, ...). The function associates to any natural number n the number 4\xe2\x88\x92n. For example, to 1 it associates 3 and to 10 it associates \xe2\x88\x926.'b'The input to a function is called the argument and the output is called the value. The set of all permitted inputs to a given function is called the domain of the function, while the set of permissible outputs is called the codomain. Thus, the domain of the "color-of-the-shape function" is the set of the four shapes, and the codomain consists of the five colors. The concept of a function does not require that every possible output is the value of some argument, e.g. the color blue is not the color of any of the four shapes in X.'b'For an example of a function, let X be the set consisting of four shapes: a red triangle, a yellow rectangle, a green hexagon, and a red square; and let Y be the set consisting of five colors: red, blue, green, pink, and yellow. Linking each shape to its color is a function from X to Y: each shape is linked to a color (i.e., an element in Y), and each shape is "linked", or "mapped", to exactly one color. There is no shape that lacks a color and no shape that has more than one color. This function will be referred to as the "color-of-the-shape function".'b''b''b'In analogy with arithmetic, it is possible to define addition, subtraction, multiplication, and division of functions, in those cases where the output is a number. Another important operation defined on functions is function composition, where the output from one function becomes the input to another function.'b'In modern mathematics,[3] a function is defined by its set of inputs, called the domain; a set containing the set of outputs, and possibly additional elements, as members, called its codomain (or target); and the set of all input-output pairs, called its graph. Sometimes the codomain is called the function\'s "range", but more commonly the word "range" is used to mean, instead, specifically the set of outputs (this is also called the image of the function). For example, we could define a function using the rule f(x) = x2 by saying that the domain and codomain are the real numbers, and that the graph consists of all pairs of real numbers (x, x2). The image of this function is the set of non-negative real numbers. Collections of functions with the same domain and the same codomain are called function spaces, the properties of which are studied in such mathematical disciplines as real analysis, complex analysis, and functional analysis.'b'Functions of various kinds are "the central objects of investigation"[2] in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. A picture, with some input, output pairs shown on Cartesian coordinates, is called the graph of the function, and may give an intuitive idea of some of the main features of the function, for example its maximums and minimums. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. In this case, if there is reason to believe the function is smooth, it is sometimes assumed that the values in between the points on the table also exist, but are not known with the accuracy of the measured points. For example, a table might give the temperature at a particular place at specified times, with the assumption that the temperature is a smooth function of time, even though the temperature at times other than those measured is not known. Another form in which functions are given is the implicit function, for example as an inverse to another function or as a solution of a differential equation.'b'In mathematics, a function[1] is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number x to its square x2. The output of a function f corresponding to an input x is denoted by f(x) (read "f\xc2\xa0of\xc2\xa0x"). In this example, if the input is \xe2\x88\x923, then the output is 9, and we may write f(\xe2\x88\x923) = 9. Likewise, if the input is 3, then the output is also 9, and we may write f(3) = 9. (The same output may be produced by more than one input, but each input gives only one output.) The input variable(s) are sometimes referred to as the argument(s) of the function.'
Polynomial ring
b'The skew-polynomial ring is defined similarly for a ring R and a ring endomorphism f of R, by extending the multiplication from the relation X\xc2\xb7r = f(r)\xc2\xb7X to produce an associative multiplication that distributes over the standard addition. More generally, given a homomorphism F from the monoid N of the positive integers into the endomorphism ring of R, the formula Xn\xc2\xb7r = F(n)(r)\xc2\xb7Xn allows constructing a skew-polynomial ring.(Lam 2001, \xc2\xa71,ex 1.11) Skew polynomial rings are closely related to crossed product algebras.'b'This relation may be extended to define a skew multiplication between two polynomials in X with coefficients in R, which make them a non-commutative ring.'b'A differential polynomial ring is a ring of differential operators formed from a ring R and a derivation \xce\xb4 of R into R. This derivation operates on R, and will be denoted X, when viewed as an operator. The elements of R also operate on R by multiplication. The composition of operators is denoted as the usual multiplication. It follows that the relation \xce\xb4(ab) = a\xce\xb4(b) + \xce\xb4(a)b may be rewritten as'b'Other generalizations of polynomials are differential and skew-polynomial rings.'b'Just as the polynomial ring in n variables with coefficients in the commutative ring R is the free commutative R-algebra of rank n, the noncommutative polynomial ring in n variables with coefficients in the commutative ring R is the free associative, unital R-algebra on n generators, which is noncommutative when n\xc2\xa0>\xc2\xa01.'b'For polynomial rings of more than one variable, the products X\xc2\xb7Y and Y\xc2\xb7X are simply defined to be equal. A more general notion of polynomial ring is obtained when the distinction between these two formal products is maintained. Formally, the polynomial ring in n noncommuting variables with coefficients in the ring R is the monoid ring R[N], where the monoid N is the free monoid on n letters, also known as the set of all strings over an alphabet of n symbols, with multiplication given by concatenation. Neither the coefficients nor the variables need commute amongst themselves, but the coefficients and variables commute with each other.'b'Power series generalize the choice of exponent in a different direction by allowing infinitely many nonzero terms. This requires various hypotheses on the monoid N used for the exponents, to ensure that the sums in the Cauchy product are finite sums. Alternatively, a topology can be placed on the ring, and then one restricts to convergent infinite sums. For the standard choice of N, the non-negative integers, there is no trouble, and the ring of formal power series is defined as the set of functions from N to a ring R with addition component-wise, and multiplication given by the Cauchy product. The ring of power series can be seen as the completion of the polynomial ring.'b'Several interesting examples of rings and groups are formed by taking N to be the additive monoid of non-negative rational numbers, (Osbourne 2000, \xc2\xa74.4). See also Puiseux series.'b'Some authors such as (Lang 2002, II,\xc2\xa73) go so far as to take this monoid definition as the starting point, and regular single variable polynomials are the special case where N is the monoid of non-negative integers. Polynomials in several variables simply take N to be the direct product of several copies of the monoid of non-negative integers.'b'where the latter sum is taken over all i, j in N that sum to n.'b'and'b'and then the formulas for addition and multiplication are the familiar:'b'When N is commutative, it is convenient to denote the function a in R[N] as the formal sum:'b'A simple generalization only changes the set from which the exponents on the variable are drawn. The formulas for addition and multiplication make sense as long as one can add exponents: Xi \xc2\xb7 Xj = Xi+j. A set for which addition makes sense (is closed and associative) is called a monoid. The set of functions from a monoid N to a ring R which are nonzero at only finitely many places can be given the structure of a ring known as R[N], the monoid ring of N with coefficients in R. The addition is defined component-wise, so that if c = a + b, then cn = an + bn for every n in N. The multiplication is defined as the Cauchy product, so that if c = a \xc2\xb7 b, then for each n in N, cn is the sum of all aibj where i, j range over all pairs of elements of N which sum to n.'b'In the case of infinitely many indeterminates, one can consider a ring strictly larger than the polynomial ring but smaller than the power series ring, by taking the subring of the latter formed by power series whose monomials have a bounded degree. Its elements still have a finite degree and are therefore somewhat like polynomials, but it is possible for instance to take the sum of all indeterminates, which is not a polynomial. A ring of this kind plays a role in constructing the ring of symmetric functions.'b'One slight generalization of polynomial rings is to allow for infinitely many indeterminates. Each monomial still involves only a finite number of indeterminates (so that its degree remains finite), and each polynomial is a still a (finite) linear combination of monomials. Thus, any individual polynomial involves only finitely many indeterminates, and any finite computation involving polynomials remains inside some subring of polynomials in finitely many indeterminates.'b'Polynomial rings have been generalized in a great many ways, including polynomial rings with generalized exponents, power series rings, noncommutative polynomial rings, and skew-polynomial rings.'b'In the following properties, R is a commutative ring and S = R[X1,\xe2\x80\xa6, Xn] is the ring of polynomials in n variables over R. The ring extension R \xe2\x8a\x82 S can be built from R in n steps, by successively adjoining X1,\xe2\x80\xa6, Xn. Thus to establish each of the properties below, it is sufficient to consider the case n = 1.'b'One of the basic techniques in commutative algebra is to relate properties of a ring with properties of its subrings. The notation R \xe2\x8a\x82 S indicates that a ring R is a subring of a ring S. In this case S is called an overring of R and one speaks of a ring extension. This works particularly well for polynomial rings and allows one to establish many important properties of the ring of polynomials in several variables over a field, K[X1,\xe2\x80\xa6, Xn], by induction in n.'b'A group of fundamental results concerning the relation between ideals of the polynomial ring K[X1,\xe2\x80\xa6, Xn] and algebraic subsets of Kn originating with David Hilbert is known under the name Nullstellensatz (literally: "zero-locus theorem").'b'Polynomials in n variables with coefficients in K form a commutative ring denoted K[X1,\xe2\x80\xa6, Xn], or sometimes K[X], where X is a symbol representing the full set of variables, X = (X1,\xe2\x80\xa6, Xn), and called the polynomial ring in n variables. The polynomial ring in n variables can be obtained by repeated application of K[X] (the order by which is irrelevant). For example, K[X1, X2] is isomorphic to K[X1][X2]. This ring plays fundamental role in algebraic geometry. Many results in commutative and homological algebra originated in the study of its ideals and modules over this ring.'b'and the degree of a polynomial p is the largest degree of a monomial occurring with non-zero coefficient in the expansion of p.'b'The product X\xce\xb1 is called the monomial of multidegree \xce\xb1. A polynomial is a finite linear combination of monomials with coefficients in K'b'A polynomial in n variables X1, \xe2\x80\xa6, Xn with coefficients in a field K is defined analogously to a polynomial in one variable, but the notation is more cumbersome. For any multi-index \xce\xb1 = (\xce\xb11, \xe2\x80\xa6, \xce\xb1n), where each \xce\xb1i is a non-negative integer, let'b''b'explaining why the sentences "Let P be a polynomial" and "Let P(X) be a polynomial" are equivalent.'b'(in the first example R = K, and in the second one R = K[X]). Substituting X by itself results in'b'we have'b'Let K be a field or, more generally, a commutative ring, and R a ring containing K. For any polynomial P in K[X] and any element a in R, the substitution of X by a in P defines an element of R, which is denoted P(a). This element is obtained by, after the substitution, carrying on, in R, the operations indicated by the expression of the polynomial. This computation is called the evaluation of P at a. For example, if we have'b'This homomorphism is given by the same formula as before, but it is not surjective in general. The existence and uniqueness of such a homomorphism \xcf\x86 expresses a certain universal property of the ring of polynomials in one variable and explains the ubiquity of polynomial rings in various questions and constructions of ring theory and commutative algebra.'b'More generally, given a (not necessarily commutative) ring A containing K and an element a of A that commutes with all elements of K, there is a unique ring homomorphism from the polynomial ring K[X] to A that maps X to a:'b'A particularly important application is to the case when the larger ring L is a field. Then the polynomial p must be irreducible. Conversely, the primitive element theorem states that any finite separable field extension L/K can be generated by a single element \xce\xb8 \xe2\x88\x88 L and the preceding theory then gives a concrete description of the field L as the quotient of the polynomial ring K[X] by a principal ideal generated by an irreducible polynomial p. As an illustration, the field C of complex numbers is an extension of the field R of real numbers generated by a single element i such that i2 + 1 = 0. Accordingly, the polynomial X2 + 1 is irreducible over R and'b'By the assumption, any element of L appears as the right hand side of the last expression for suitable m and elements a0, ..., am of K. Therefore, \xcf\x86 is surjective and L is a homomorphic image of K[X]. More formally, let Ker \xcf\x86 be the kernel of \xcf\x86. It is an ideal of K[X] and by the first isomorphism theorem for rings, L is isomorphic to the quotient of the polynomial ring K[X] by the ideal Ker\xc2\xa0\xcf\x86. Since the polynomial ring is a principal ideal domain, this ideal is principal: there exists a polynomial p \xe2\x88\x88 K[X] such that'b'Suppose that a commutative ring L contains K and there exists an element \xce\xb8 of L such that the ring L is generated by \xce\xb8 over K. Thus any element of L is a linear combination of powers of \xce\xb8 with coefficients in K. Then there is a unique ring homomorphism \xcf\x86 from K[X] into L which does not affect the elements of K itself (it is the identity map on K) and maps each power of X to the same power of \xce\xb8. Its effect on the general polynomial amounts to "replacing X with \xce\xb8":'b'The ring K[X] of polynomials over K is obtained from K by adjoining one element, X. It turns out that any commutative ring L containing K and generated as a ring by a single element in addition to K can be described using K[X]. In particular, this applies to finite field extensions of K.'b'Another corollary of the polynomial division with the remainder is the fact that every proper ideal I of K[X] is principal, i.e. I consists of the multiples of a single polynomial f. Thus the polynomial ring K[X] is a principal ideal domain, and for the same reason every Euclidean domain is a principal ideal domain. Also every principal ideal domain is a unique-factorization domain. These deductions make essential use of the fact that the polynomial coefficients lie in a field, namely in the polynomial division step, which requires the leading coefficient of q, which is only known to be non-zero, to have an inverse. If R is an integral domain that is not a field then R[X] is neither a Euclidean domain nor a principal ideal domain; however it could still be a unique factorization domain (and will be so if and only if R itself is a unique factorization domain, for instance if it is Z or another polynomial ring).'b'where the quotient u and the remainder r are polynomials, the degree of r is less than the degree of q, and a decomposition with these properties is unique. The quotient and the remainder are found using the polynomial long division. The degree of the polynomial now plays a role similar to the absolute value of an integer: it is strictly less in the remainder r than it is in q, and when repeating this step such decrease cannot go on indefinitely. Therefore, eventually some division will be exact, at which point the last non-zero remainder is the greatest common divisor of the initial two polynomials. Using the existence of greatest common divisors, Gauss was able to simultaneously rigorously prove the fundamental theorem of arithmetic for integers and its generalization to polynomials. In fact there exist other commutative rings than Z and K[X] that similarly admit an analogue of the Euclidean algorithm; all such rings are called Euclidean rings. Rings for which there exists unique (in an appropriate sense) factorization of nonzero elements into irreducible factors are called unique factorization domains or factorial rings; the given construction shows that all Euclidean rings, and in particular Z and K[X], are unique factorization domains.'b"The next property of the polynomial ring is much deeper. Already Euclid noted that every positive integer can be uniquely factored into a product of primes \xe2\x80\x94 this statement is now called the fundamental theorem of arithmetic. The proof is based on Euclid's algorithm for finding the greatest common divisor of natural numbers. At each step of this algorithm, a pair (a, b), a > b, of natural numbers is replaced by a new pair (b, r), where r is the remainder from the division of a by b, and the new numbers are smaller. Gauss remarked that the procedure of division with the remainder can also be defined for polynomials: given two polynomials p and q, where q \xe2\x89\xa0 0, one can write"b'It follows immediately that if K is an integral domain then so is K[X].[10]'b'If K is a field, or more generally an integral domain, then from the definition of multiplication,[9]'b'The degree of a polynomial p, written deg(p) is the largest k such that the coefficient of Xk is not zero.[4] In this case the coefficient pk is called the leading coefficient.[5] In the special case of zero polynomial, all of whose coefficients are zero, the degree has been variously left undefined,[6] defined to be \xe2\x88\x921,[7] or defined to be a special symbol \xe2\x88\x92\xe2\x88\x9e.[8]'b'More generally, the field K can be replaced by any commutative ring R when taking the same construction as above, giving rise to the polynomial ring over R, which is denoted R[X].'b'is considered an alternate notation for the sequence (p0, p1, p2, ..., pm, 0, 0, ...).'b'Another equivalent definition is often preferred, although less intuitive, because it is easier to make it completely rigorous, which consists in defining a polynomial as an infinite sequence of elements of K, (p0, p1, p2, ... ) having the property that only a finite number of the elements are nonzero, or equivalently, a sequence for which there is some m so that pn = 0 for n > m. In this case, the expression'b'It is easy to verify that these three operations satisfy the axioms of a commutative algebra. Therefore, polynomial rings are also called polynomial algebras.'b'The scalar multiplication is the special case of the multiplication where p = p0 is reduced to its term which is independent of X, that is'b'If necessary, the polynomials p and q are extended by adding "dummy terms" with zero coefficients, so that the expressions for ri and si are always defined. Specifically, if m < n, then pi = 0 for m < i \xe2\x89\xa4 n.'b'and'b'where k = max(m, n), l = m + n,'b'and'b'then'b'and'b'The polynomial ring in X over K is equipped with an addition, a multiplication and a scalar multiplication that make it a commutative algebra. These operations are defined according to the ordinary rules for manipulating algebraic expressions. Specifically, if'b'This terminology is suggested by real or complex polynomial functions. However, in general, X and its powers, Xk, are treated as formal symbols, not as elements of the field K or functions over it. One can think of the ring K[X] as arising from K by adding one new element X that is external to K and requiring that X commute with all elements of K.'b'Two polynomials are defined to be equal when the corresponding coefficient of each Xk is equal.'b'for any nonnegative integers k and l. The symbol X is called an indeterminate[2] or variable.[3]'b'where p0, p1, ..., pm, the coefficients of p, are elements of K, and X, X2, are symbols, which are considered as "powers of X", and, by convention, follow the usual rules of exponentiation: X0 = 1, X1 = X, and'b'The polynomial ring, K[X], in X over a field K is defined[1] as the set of expressions, called polynomials in X, of the form'b''b''b'A closely related notion is that of the ring of polynomial functions on a vector space.'b"In mathematics, especially in the field of abstract algebra, a polynomial ring or polynomial algebra is a ring (which is also a commutative algebra) formed from the set of polynomials in one or more indeterminates (traditionally also called variables) with coefficients in another ring, often a field. Polynomial rings have influenced much of mathematics, from the Hilbert basis theorem, to the construction of splitting fields, and to the understanding of a linear operator. Many important conjectures involving polynomial rings, such as Serre's problem, have influenced the study of other rings, and have influenced even the definition of other rings, such as group rings and rings of formal power series."
Matrix (mathematics)
b'Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]'b'For example, a function \xce\xa6(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, \xe2\x88\x80ai: \xce\xa6(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:'b'Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910\xe2\x80\x931913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:'b'The word has been used in unusual ways by at least two authors of historical importance.'b'The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.'b'Many theorems were first established for small matrices only, for example the Cayley\xe2\x80\x93Hamilton theorem was proved for 2\xc3\x972 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4\xc3\x974 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss\xe2\x80\x93Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.'b'where \xce\xa0 denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"\xe2\x80\x94later called Jacobi determinants by Sylvester\xe2\x80\x94which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker\'s Vorlesungen \xc3\xbcber die Theorie der Determinanten[113] and Weierstrass\' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.'b'The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy \xe2\x88\x92 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomial'b'An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]'b"Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley\xe2\x80\x93Hamilton theorem.[103]"b'The term "matrix" (Latin for "womb", derived from mater\xe2\x80\x94mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:'b'Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th\xe2\x80\x932nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.'b"The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H \xc2\xb7 A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices."b'Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.'b"Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]"b"A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]"b'Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]'b'The first model of quantum mechanics (Heisenberg, 1925) represented the theory\'s operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]'b'Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo\xe2\x80\x93Kobayashi\xe2\x80\x93Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]'b'Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]'b'which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]'b'Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear function'b'Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]'b'The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]'b'Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]'b'If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]'b'Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn \xe2\x86\x92 Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]'b'The Hessian matrix of a differentiable function \xc6\x92: Rn \xe2\x86\x92 R consists of the second derivatives of \xc6\x92 with respect to the several coordinate directions, that is,[81]'b'The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.'b'Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree\xe2\x80\x93Fock method.'b'Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.'b'under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.'b'Complex numbers can be represented by particular real 2-by-2 matrices via'b'There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]'b'An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant\xc2\xa01, a fact that is often used as a part of the characterization of determinants.'b'In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.'b'If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.'b'If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V\xe2\x86\x92W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector\xc2\xa0v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A\xc2\xb7v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.'b'It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.'b'Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]'b'form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or \xe2\x88\x921. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.'b'Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the condition'b'A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.'b'More generally, the set of m\xc3\x97n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n\xc2\xa0=\xc2\xa0m composition of these maps is possible, and this gives rise to the matrix ring of n\xc3\x97n matrices representing the endomorphism ring of Rn.'b'In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]'b'Linear maps Rn \xe2\x86\x92 Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V \xe2\x86\x92 W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such that'b'Matrices do not always have all their entries in the same ring\xc2\xa0\xe2\x80\x93 or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.'b'More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]'b'This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.'b'Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.'b'and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]'b'The eigendecomposition or diagonalization expresses A as a product VDV\xe2\x88\x921, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues \xce\xbb1 to \xce\xbbn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via'b'The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV\xe2\x88\x97, where U and V are unitary matrices and D is a diagonal matrix.'b'There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.'b'Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]'b"may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]"b"An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)"b'In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]'b'Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.'b'To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.'b'Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]'b'The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn\xe2\x88\x92A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(\xce\xbb)\xc2\xa0=\xc2\xa00 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley\xe2\x80\x93Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.'b'are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number \xce\xbb is an eigenvalue of an n\xc3\x97n-matrix A if and only if A\xe2\x88\x92\xce\xbbIn is not invertible, which is equivalent to'b'A number \xce\xbb and a non-zero vector v satisfying'b"Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by \xe2\x88\x921.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]"b'The determinant of a product of square matrices equals the product of their determinants:'b'The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]'b'The determinant of 2-by-2 matrices is given by'b'The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.'b'Also, the trace of a matrix is equal to that of its transpose, that is,'b'This is immediate from the definition of matrix multiplication:'b'The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:'b'The complex analogue of an orthogonal matrix is a unitary matrix.'b'An orthogonal matrix A is necessarily invertible (with inverse A\xe2\x88\x921 = AT), unitary (A\xe2\x88\x921 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or \xe2\x88\x921. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.'b'where I is the identity matrix of size n.'b'which entails'b'An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:'b'Allowing as input two different vectors instead yields the bilinear form associated to A:'b'A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.'b'produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.'b'A symmetric n\xc3\x97n-matrix A is called positive-definite if for all nonzero vectors x\xc2\xa0\xe2\x88\x88\xc2\xa0Rn the associated quadratic form given by'b'where In is the n\xc3\x97n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A\xe2\x88\x921.'b'A square matrix A is called invertible or non-singular if there exists a matrix B such that'b'By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.'b'A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = \xe2\x88\x92AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A\xe2\x88\x97 = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.'b'A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.'b'It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:'b'The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,'b'If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.'b'A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.'b'The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank\xe2\x80\x93nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]'b'The last equality follows from the above-mentioned associativity of matrix multiplication.'b'Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g\xc2\xa0: Rm \xe2\x86\x92 Rk, then the composition g \xe2\x88\x98 f is represented by BA since'b'The following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.'b'For example, the 2\xc3\x972 matrix'b'Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn \xe2\x86\x92 Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn \xe2\x86\x92 Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.'b'where A\xe2\x88\x921 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.'b'Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writing'b'is equivalent to the system of linear equations'b'Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n\xc3\x971-matrix) of n variables x1, x2, ..., xn, and b is an m\xc3\x971-column vector, then the matrix equation'b'A principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]'b'The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]'b'A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:'b'These operations are used in a number of ways, including solving linear equations and finding matrix inverses.'b'There are three types of row operations:'b'Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.'b'whereas'b'that is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:'b'Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m \xe2\x89\xa0 k. Even if both products are defined, they need not be equal, that is, generally'b'where 1 \xe2\x89\xa4 i \xe2\x89\xa4 m and 1 \xe2\x89\xa4 j \xe2\x89\xa4 p.[13] For example, the underlined entry 2340 in the product is calculated as (2 \xc3\x97 1000) + (3 \xc3\x97 100) + (4 \xc3\x97 10) = 2340:'b'Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:'b'Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A\xc2\xa0+\xc2\xa0B\xc2\xa0=\xc2\xa0B\xc2\xa0+\xc2\xa0A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A\xc2\xa0+\xc2\xa0B)T\xc2\xa0=\xc2\xa0AT\xc2\xa0+\xc2\xa0BT. Finally, (AT)T\xc2\xa0=\xc2\xa0A.'b'There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]'b'An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,\xe2\x88\x97 refers to the ith row of A, and a\xe2\x88\x97,j refers to the jth column of A. The set of all m-by-n matrices is denoted \xf0\x9d\x95\x84(m, n).'b'Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-\xc3\x97-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 \xe2\x89\xa4 i \xe2\x89\xa4 m \xe2\x88\x92 1 and 0 \xe2\x89\xa4 j \xe2\x89\xa4 n \xe2\x88\x92 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.'b'In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m \xc3\x97 n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m \xc3\x97 n as a subscript. For instance, the matrix A above is 3 \xc3\x97 4 and can be defined as A = [i \xe2\x88\x92 j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i \xe2\x88\x92 j]3\xc3\x974.'b'Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i \xe2\x88\x92 j.'b'The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):'b'Matrices are commonly written in box brackets or parentheses:'b'Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.'b'The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m\xc2\xa0\xc3\x97\xc2\xa0n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3\xc2\xa0\xc3\x97\xc2\xa02 matrix.'b'The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.'b'A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:'b''b''b'A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.'b'Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.'b"The individual items in an m \xc3\x97 n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n \xc3\x97 Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors."b'In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 \xc3\x97 3 (read "two by three"), because there are two rows and three columns:'
Field (mathematics)

Real number
b'The real numbers can be generalized and extended in several different directions:'b'In mathematics, real is used as an adjective, meaning that the underlying field is the field of the real numbers (or the real field). For example, real matrix, real polynomial and real Lie algebra. The word is also used as a noun, meaning a real number (as in "the set of all reals").'b'The notation Rn refers to the cartesian product of n copies of R, which is an n-dimensional vector space over the field of the real numbers; this vector space may be identified to the n-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3\xe2\x80\x91dimensional space.'b'The sets of positive real numbers and negative real numbers are often noted R+ and R\xe2\x88\x92,[14] respectively; R+ and R\xe2\x88\x92 are also used.[15] The non-negative real numbers can be noted R\xe2\x89\xa50 but one often sees this set noted R+ \xe2\x88\xaa {0}.[14] In French mathematics, the positive real numbers and negative real numbers commonly include zero, and these sets are noted respectively \xe2\x84\x9d+ and \xe2\x84\x9d\xe2\x88\x92.[15] In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are noted \xe2\x84\x9d+* and \xe2\x84\x9d\xe2\x88\x92*.[15]'b'Mathematicians use the symbol R, or, alternatively, \xe2\x84\x9d, the letter "R" in blackboard bold (encoded in Unicode as U+211D \xe2\x84\x9d DOUBLE-STRUCK CAPITAL R (HTML\xc2\xa0&#8477;)), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression field of real numbers is frequently used when its algebraic properties are under consideration.'b'In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".'b'A real number is called computable if there exists an algorithm that yields its digits. Because there are only countably many algorithms,[13] but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.'b'With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.'b'Physicists have occasionally suggested that a more fundamental theory would replace the real numbers with quantities that do not form a continuum, but such proposals remain speculative.[11]'b'In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.'b'Edward Nelson\'s internal set theory enriches the Zermelo\xe2\x80\x93Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson\'s theory).'b'The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.'b'The real numbers are most often formalized using the Zermelo\xe2\x80\x93Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.[10]'b'The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering \xe2\x89\xa4 of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.[9]'b'The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo\xe2\x80\x93Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set B of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of B is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.'b'The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the L\xc3\xb6wenheim\xe2\x80\x93Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.'b'The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.'b'Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.'b'The real numbers form a metric space: the distance between x and y is defined as the absolute value |x \xe2\x88\x92 y|. By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology\xc2\xa0\xe2\x80\x93 in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension\xc2\xa01. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.'b'As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.'b"The reals are uncountable; that is: there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory."b'But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the largest Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.'b'These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the only uniformly complete ordered field, but it is the only uniformly complete Archimedean field, and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.'b'Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.'b'First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element z, z + 1 is larger), so this is not the sense that is meant.'b'The real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.'b'converges to a real number for every x, because the sums'b'For example, the standard series of the exponential function'b'The completeness property of the reals is the basis on which calculus, and, more generally mathematical analysis are built. In particular, the test that a sequence is a Cauchy sequence allows proving that a sequence has a limit, without computing it, and even without knowing it.'b'The set of rational numbers is not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421; ...), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number (in the real numbers, in contrast, it converges to the positive square root of 2).'b'Every convergent sequence is a Cauchy sequence, and the converse is true for real numbers, and this means that the topological space of the real numbers is complete.'b'A sequence (xn) converges to the limit x if its elements eventually come and remain arbitrarily close to x, that is, if for any \xce\xb5 > 0 there exists an integer N (possibly depending on \xce\xb5) such that the distance |xn \xe2\x88\x92 x| is less than \xce\xb5 for n greater than N.'b'A sequence (xn) of real numbers is called a Cauchy sequence if for any \xce\xb5 > 0 there exists an integer N (possibly depending on \xce\xb5) such that the distance |xn \xe2\x88\x92 xm| is less than \xce\xb5 for all n and m that are both greater than N. This definition, originally provided by Cauchy, formalizes the fact that the xn eventually come and remain arbitrarily close to each other.'b'A main reason for using real numbers is that the reals contain all limits. More precisely, a sequence of real numbers has a limit, which is a real number, if (and only if) its elements eventually come and remain arbitrarily close to each other. This is formally defined in the following, and means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section).\xc2\xa0:'b'More formally, the real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.'b'A real number may be either rational or irrational; either algebraic or transcendental; and either positive, negative, or zero. Real numbers are used to measure continuous quantities. They may be expressed by decimal representations that have an infinite sequence of digits to the right of the decimal point; these are often represented in the same form as 324.823122147... The ellipsis (three dots) indicates that there would still be more digits to come.'b'The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; ...) converges to a unique real number, in this case \xcf\x80. For details and other constructions of real numbers, see construction of the real numbers.'b"For another axiomatization of \xe2\x84\x9d, see Tarski's axiomatization of the reals."b'The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields \xe2\x84\x9d1 and \xe2\x84\x9d2, there exists a unique field isomorphism from \xe2\x84\x9d1 to \xe2\x84\x9d2, allowing us to think of them as essentially the same mathematical object.'b'These properties imply Archimedean property (which is not implied by other definitions of completeness). That is, the set of integers is not upper-bounded in the reals. In fact, if this were false, then the integers would have a least upper bound N; then, N \xe2\x80\x93 1 would not be an upper bound, and there would be an integer n such that n > N \xe2\x80\x93 1, and thus n + 1 > N, which is a contradiction with the upper-bound property of N.'b'The last property is what differentiates the reals from the rationals. For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.'b'Let \xe2\x84\x9d denote the set of all real numbers. Then:'b"The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was given by Georg Cantor in 1871. In 1874, he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof."b"\xc3\x89variste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither e nor e2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers; Georg Cantor (1873) extended and greatly simplified this proof.[7] Charles Hermite (1873) first proved that e is transcendental, and Ferdinand von Lindemann (1882), showed that \xcf\x80 is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz[citation needed] and Paul Gordan.[8]"b'In the 18th and 19th centuries, there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that \xcf\x80 cannot be rational; Adrien-Marie Legendre (1794) completed the proof,[5] and showed that \xcf\x80 is not the square root of a rational number.[6] Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel\xe2\x80\x93Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.'b'In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.'b'In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.'b'The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects,[2] which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers.[3] The Egyptian mathematician Ab\xc5\xab K\xc4\x81mil Shuj\xc4\x81 ibn Aslam (c. 850\xe2\x80\x93930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.[4]'b'Simple fractions were used by the Egyptians around 1000\xc2\xa0BC; the Vedic "Sulba Sutras" ("The rules of chords") in, c. 600 BC, include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava (c. 750\xe2\x80\x93690 BC), who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined.[1] Around 500\xc2\xa0BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.'b''b''b'These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers\xc2\xa0\xe2\x80\x93 indeed, the realization that a better definition was needed\xc2\xa0\xe2\x80\x93 was one of the most important developments of 19th-century mathematics. The current standard axiomatic definition is that real numbers form the unique Dedekind-complete ordered field (\xe2\x84\x9d\xc2\xa0; +\xc2\xa0; \xc2\xb7\xc2\xa0; <), up to an isomorphism,[a] whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or infinite decimal representations, together with precise interpretations for the arithmetic operations and the order relation. All these definitions satisfy the axiomatic definition and are thus equivalent.'b'The real numbers include all the rational numbers, such as the integer \xe2\x88\x925 and the fraction 4/3, and all the irrational numbers, such as \xe2\x88\x9a2 (1.41421356..., the square root of 2, an irrational algebraic number). Included within the irrationals are the transcendental numbers, such as \xcf\x80 (3.14159265...). Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation, such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.'b'In mathematics, a real number is a value that represents a quantity along a line. The adjective real in this context was introduced in the 17th century by Ren\xc3\xa9 Descartes, who distinguished between real and imaginary roots of polynomials.'
Set (mathematics)
b'The complement of A intersected with B is equal to the complement of A union to the complement of B.'b'The complement of A union B equals the complement of A intersected with the complement of B.'b'If A and B are any two sets then,'b'Augustus De Morgan stated two laws about sets.'b'A more general form of the principle can be used to find the cardinality of any finite union of sets:'b'The inclusion\xe2\x80\x93exclusion principle is a counting technique that can be used to count the number of elements in a union of two sets, if the size of each set and the size of their intersection are known. It can be expressed symbolically as'b'For most purposes, however, naive set theory is still useful.'b'The reason is that the phrase well-defined is not very well-defined. It was important to free set theory of these paradoxes because nearly all of mathematics was being redefined in terms of set theory. In an attempt to avoid these paradoxes, set theory was axiomatized based on first-order logic, and thus axiomatic set theory was born.'b'Although initially naive set theory, which defines a set merely as any well-defined collection, was well accepted, it soon ran into several obstacles. It was found that this definition spawned several paradoxes, most notably:'b'One of the main applications of naive set theory is constructing relations. A relation from a domain A to a codomain B is a subset of the Cartesian product A \xc3\x97 B. Given this concept, we are quick to see that the set F of all ordered pairs (x, x2), where x is real, is quite familiar. It has a domain set R and a codomain set that is also R, because the set of all squares is subset of the set of all reals. If placed in functional notation, this relation becomes f(x) = x2. The reason these two are equivalent is for any given value, y that the function is defined for, its corresponding ordered pair, (y, y2) is a member of the set F.'b'Set theory is seen as the foundation from which virtually all of mathematics can be derived. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.'b'Let A and B be finite sets; then the cardinality of the Cartesian product is the product of the cardinalities:'b'Some basic properties of Cartesian products:'b'Examples:'b'A new set can be constructed by associating every element of one set with every element of another set. The Cartesian product of two sets A and B, denoted by A \xc3\x97 B is the set of all ordered pairs (a, b) such that a is a member of A and b is a member of B.'b'For example, the symmetric difference of {7,8,9,10} and {9,10,11,12} is the set {7,8,11,12}. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring (with the empty set as neutral element) and intersection as the multiplication of the ring.'b'An extension of the complement is the symmetric difference, defined for sets A, B as'b'Some basic properties of complements:'b'Examples:'b'In certain settings all sets under discussion are considered to be subsets of a given universal set U. In such cases, U \\ A is called the absolute complement or simply complement of A, and is denoted by A\xe2\x80\xb2.'b'Two sets can also be "subtracted". The relative complement of B in A (also called the set-theoretic difference of A and B), denoted by A \\ B (or A \xe2\x88\x92 B), is the set of all elements that are members of A but not members of B. Note that it is valid to "subtract" members of a set that are not in the set, such as removing the element green from the set {1, 2, 3}; doing so has no effect.'b'Some basic properties of intersections:'b'Examples:'b'A new set can also be constructed by determining which members two sets have "in common". The intersection of A and B, denoted by A \xe2\x88\xa9 B, is the set of all things that are members of both A and B. If A \xe2\x88\xa9 B = \xe2\x88\x85, then A and B are said to be disjoint.'b'Some basic properties of unions:'b'Examples:'b'Two sets can be "added" together. The union of A and B, denoted by A\xc2\xa0\xe2\x88\xaa\xc2\xa0B, is the set of all things that are members of either A or B.'b'There are several fundamental operations for constructing new sets from given sets.'b'Each of the above sets of numbers has an infinite number of elements, and each can be considered to be a proper subset of the sets listed below it. The primes are used less frequently than the others outside of number theory and related fields.'b'Positive and negative sets are denoted by a superscript - or +. For example, \xe2\x84\x9a+ represents the set of positive rational numbers.'b'Many of these sets are represented using blackboard bold or bold typeface. Special sets of numbers include'b'There are some sets or kinds of sets that hold great mathematical importance and are referred to with such regularity that they have acquired special names and notational conventions to identify them. One of these is the empty set, denoted {} or \xe2\x88\x85. A set with exactly one element, x, is a unit set, or singleton, {x}.[2]'b'Some sets have infinite cardinality. The set N of natural numbers, for instance, is infinite. Some infinite cardinalities are greater than others. For instance, the set of real numbers has greater cardinality than the set of natural numbers. However, it can be shown that the cardinality of (which is to say, the number of points on) a straight line is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.'b'There is a unique set with no members, called the empty set (or the null set), which is denoted by the symbol \xe2\x88\x85 (other notations are used; see empty set). The cardinality of the empty set is zero. For example, the set of all three-sided squares has zero members and thus is the empty set. Though it may seem trivial, the empty set, like the number zero, is important in mathematics. Indeed, the existence of this set is one of the fundamental concepts of axiomatic set theory.'b'The cardinality |\xe2\x80\x89S\xe2\x80\x89| of a set S is "the number of members of S." For example, if B = {blue, white, red}, then |\xe2\x80\x89B\xe2\x80\x89| = 3.'b'Every partition of a set S is a subset of the powerset of S.'b'The power set of an infinite (either countable or uncountable) set is always uncountable. Moreover, the power set of a set is always strictly "bigger" than the original set in the sense that there is no way to pair every element of S with exactly one element of P(S). (There is never an onto map or surjection from S onto P(S).)'b'The power set of a finite set with n elements has 2n elements. For example, the set {1, 2, 3} contains three elements, and the power set shown above contains 23 = 8 elements.'b'The power set of a set S is the set of all subsets of S. The power set contains S itself and the empty set because these are both subsets of S. For example, the power set of the set {1, 2, 3} is {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, \xe2\x88\x85}. The power set of a set S is usually written as P(S).'b'A partition of a set S is a set of nonempty subsets of S such that every element x in S is in exactly one of these subsets.'b'An obvious but useful identity, which can often be used to show that two seemingly different sets are equal:'b'Every set is a subset of the universal set:'b'The empty set is a subset of every set and every set is a subset of itself:'b'Examples:'b'The expressions A \xe2\x8a\x82 B and B \xe2\x8a\x83 A are used differently by different authors; some authors use them to mean the same as A \xe2\x8a\x86 B (respectively B \xe2\x8a\x87 A), whereas others use them to mean the same as A \xe2\x8a\x8a B (respectively B \xe2\x8a\x8b A).'b'If A is a subset of, but not equal to, B, then A is called a proper subset of B, written A \xe2\x8a\x8a B (A is a proper subset of B) or B \xe2\x8a\x8b A (B is a proper superset of A).'b'If every member of set A is also a member of set B, then A is said to be a subset of B, written A \xe2\x8a\x86 B (also pronounced A is contained in B). Equivalently, we can write B \xe2\x8a\x87 A, read as B is a superset of A, B includes A, or B contains A. The relationship between sets established by \xe2\x8a\x86 is called inclusion or containment.'b'For example, with respect to the sets A = {1,2,3,4}, B = {blue, white, red}, and F = {n2 \xe2\x88\x92 4\xc2\xa0: n is an integer; and 0 \xe2\x89\xa4 n \xe2\x89\xa4 19} defined above,'b'If B is a set and x is one of the objects of B, this is denoted x \xe2\x88\x88 B, and is read as "x belongs to B", or "x is an element of B". If y is not a member of B then this is written as y \xe2\x88\x89 B, and is read as "y does not belong to B".'b'In this notation, the colon (":") means "such that", and the description can be interpreted as "F is the set of all numbers of the form n2 \xe2\x88\x92 4, such that n is a whole number in the range from 0 to 19 inclusive." Sometimes the vertical bar ("|") is used instead of the colon.'b'The notation with braces may also be used in an intensional specification of a set. In this usage, the braces have the meaning "the set of all ...". So, E = {playing card suits} is the set whose four members are \xe2\x99\xa0, \xe2\x99\xa6, \xe2\x99\xa5, and \xe2\x99\xa3. A more general form of this is set-builder notation, through which, for instance, the set F of the twenty smallest integers that are four less than perfect square can be denoted'b'where the ellipsis ("...") indicates that the list continues in the obvious way. Ellipses may also be used where sets have infinitely many members. Thus the set of positive even numbers can be written as {2, 4, 6, 8, ... }.'b'For sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally as'b'In an extensional definition, a set member can be listed two or more times, for example, {11, 6, 6}. However, per extensionality, two definitions of sets which differ only in that one of the definitions lists set members multiple times, define, in fact, the same set. Hence, the set {11, 6, 6} is exactly identical to the set {11, 6}. Moreover, the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple). We can illustrate these two important points with an example:'b'One often has the choice of specifying a set either intensionally or extensionally. In the examples above, for instance, A = C and B = D.'b'The second way is by extension \xe2\x80\x93 that is, listing each member of the set. An extensional definition is denoted by enclosing the list of members in curly brackets:'b'There are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:'b'For technical reasons, Cantor\'s definition turned out to be inadequate; today, in contexts where more rigor is required, one can use axiomatic set theory, in which the notion of a "set" is taken as a primitive notion and the properties of sets are defined by a collection of axioms. The most basic properties are that a set can have elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other; this property is called the extensionality of sets.'b'Sets are conventionally denoted with capital letters. Sets A and B are equal if and only if they have precisely the same elements.[2]'b"A set is a well-defined collection of distinct objects. The objects that make up a set (also known as the set's elements or members) can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, one of the founders of set theory, gave the following definition of a set at the beginning of his Beitr\xc3\xa4ge zur Begr\xc3\xbcndung der transfiniten Mengenlehre:[1]"b''b''b'The German word Menge, rendered as "set" in English, was coined by Bernard Bolzano in his work The Paradoxes of the Infinite.'b'In mathematics, a set is a collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree.'
Binary operation
b'Note that the dot product of two vectors is not a binary operation, external or otherwise, as it maps from S\xc3\x97 S to K, where K is a field and S is a vector space over K.'b'An external binary operation may alternatively be viewed as an action; K is acting on S.'b'An example of an external binary operation is scalar multiplication in linear algebra. Here K is a field and S is a vector space over that field.'b'An external binary operation is a binary function from K \xc3\x97 S to S. This differs from a binary operation in the strict sense in that K need not be S; its elements come from outside.'b'A binary operation f on a set S may be viewed as a ternary relation on S, that is, the set of triples (a, b, f(a,b)) in S \xc3\x97 S \xc3\x97 S for all a and b in S.'b'However:'b'A binary operation, ab, depends on the ordered pair (a, b) and so (ab)c (where the parentheses here mean first operate on the ordered pair (a, b) and then operate on the result of that using the ordered pair ((ab), c)) depends in general on the ordered pair ((a, b), c). Thus, for the general, non-associative case, binary operations can be represented with binary trees.'b'Binary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses. They are also called, respectively, Polish notation and reverse Polish notation.'b'Binary operations are often written using infix notation such as a \xe2\x88\x97 b, a + b, a \xc2\xb7 b or (by juxtaposition with no symbol) ab rather than by functional notation of the form f(a, b). Powers are usually also written without operator, but with the second argument as superscript.'b'Division (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative. Tetration (\xe2\x86\x91\xe2\x86\x91), as a binary operation on the natural numbers, is not commutative or associative and has no identity element.'b'On the set of natural numbers N, the binary operation exponentiation, f(a,b) = ab, is not commutative since, in general, ab \xe2\x89\xa0 ba and is also not associative since f(f(a, b), c) \xe2\x89\xa0 f(a, f(b, c)). For instance, with a = 2, b = 3 and c = 2, f(23,2) = f(8,2) = 82 = 64, but f(2,32) = f(2,9) = 29 = 512. By changing the set N to the set of integers Z, this binary operation becomes a partial binary operation since it is now undefined when a = 0 and b is any negative integer. For either set, this operation has a right identity (which is 1) since f(a, 1) = a for all a in the set, which is not an identity (two sided identity) since f(1, b) \xe2\x89\xa0 b in general.'b'On the set of real numbers R, subtraction, that is, f(a, b) = a \xe2\x88\x92 b, is a binary operation which is not commutative since, in general, a \xe2\x88\x92 b \xe2\x89\xa0 b \xe2\x88\x92 a. It is also not associative, since, in general, a \xe2\x88\x92 (b \xe2\x88\x92 c) \xe2\x89\xa0 (a \xe2\x88\x92 b) \xe2\x88\x92 c; for instance, 1 \xe2\x88\x92 (2 \xe2\x88\x92 3) = 2 but (1 \xe2\x88\x92 2) \xe2\x88\x92 3 = \xe2\x88\x924.'b'The first three examples above are commutative and all of the above examples are associative.'b'Many binary operations of interest in both algebra and formal logic are commutative, satisfying f(a, b) = f(b, a) for all elements a and b in S, or associative, satisfying f(f(a, b), c) = f(a, f(b, c)) for all a, b and c in S. Many also have identity elements and inverse elements.'b'Typical examples of binary operations are the addition (+) and multiplication (\xc3\x97) of numbers and matrices as well as composition of functions on a single set. For instance,'b'Binary operations are the keystone of algebraic structures studied in abstract algebra: they are essential in the definitions of groups, monoids, semigroups, rings, and more. Most generally, a magma is a set together with some binary operation defined on it.'b'Sometimes, especially in computer science, the term is used for any binary function.'b"Because the result of performing the operation on a pair of elements of S is again an element of S, the operation is called a closed binary operation on S (or sometimes expressed as having the property of closure).[4] If f is not a function, but is instead a partial function, it is called a partial binary operation. For instance, division of real numbers is a partial binary operation, because one can't divide by zero: a/0 is not defined for any real a. Note however that both in algebra and model theory the binary operations considered are defined on all of S \xc3\x97 S."b'More precisely, a binary operation on a set S is a map which sends elements of the Cartesian product S \xc3\x97 S to S:[1][2][3]'b''b''b'In mathematics, a binary operation on a set is a calculation that combines two elements of the set (called operands) to produce another element of the set. More formally, a binary operation is an operation of arity of two whose two domains and one codomain are the same set. Examples include the familiar elementary arithmetic operations of addition, subtraction, multiplication and division. Other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication and conjugation in groups.'
Element (mathematics)

Euclidean vector
b'This distinction between vectors and pseudovectors is often ignored, but it becomes important in studying symmetry properties. See parity (physics).'b'One example of a pseudovector is angular velocity. Driving in a car, and looking forward, each of the wheels has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the reflection of this angular velocity vector points to the right, but the actual angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include magnetic field, torque, or more generally any cross product of two (true) vectors.'b'Some vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip and gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the orientation of space. A vector which gains a minus sign when the orientation of space changes is called a pseudovector or an axial vector. Ordinary vectors are sometimes called true vectors or polar vectors to distinguish them from pseudovectors. Pseudovectors occur most frequently as the cross product of two ordinary vectors.'b'In the language of differential geometry, the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a contravariant vector to be a tensor of contravariant rank one. Alternatively, a contravariant vector is defined to be a tangent vector, and the rules for transforming a contravariant vector follow from the chain rule.'b'Therefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely as'b'Work is the dot product of force and displacement'b"Force is a vector with dimensions of mass\xc3\x97length/time2 and Newton's second law is the scalar multiplication"b'Acceleration a of a point is vector which is the time derivative of velocity. Its dimensions are length/time2.'b'where x0 is the position at time t=0. Velocity is the time derivative of position. Its dimensions are length/time.'b'The velocity v of a point or particle is a vector, its length gives the speed. For constant velocity the position at time t will be'b'which specifies the position of y relative to x. The length of this vector gives the straight-line distance from x to y. Displacement has the dimensions of length.'b'Given two points x = (x1, x2, x3), y = (y1, y2, y3) their displacement is a vector'b'The position vector has dimensions of length.'b'The position of a point x = (x1, x2, x3) in three-dimensional space can be represented as a position vector whose base point is the origin'b'Often in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter t. For instance, if r represents the position vector of a particle, then r(t) gives a parametric representation of the trajectory of the particle. Vector-valued functions can be differentiated and integrated by differentiating or integrating the components of the vector, and many of the familiar rules from calculus continue to hold for the derivative and integral of vector-valued functions.'b'In abstract vector spaces, the length of the arrow depends on a dimensionless scale. If it represents, for example, a force, the "scale" is of physical dimension length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if "1 newton" and "5 m" are both represented with an arrow of 2\xc2\xa0cm, the scales are 1:250 and 1 m:50 N respectively. Equal length of vectors of different dimension has no particular significance unless there is some proportionality constant inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.'b'Vectors have many uses in physics and other sciences.'b'By applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.[13]'b'The advantage of this method is that a direction cosine matrix can usually be obtained independently by using Euler angles or a quaternion to relate the two vector bases, so the basis conversions can be performed directly, without having to work out all the dot products described above.'b'The properties of a direction cosine matrix, C are [[14]]:'b'By referring collectively to e1, e2, e3 as the e basis and to n1, n2, n3 as the n basis, the matrix containing all the cjk is known as the "transformation matrix from e to n", or the "rotation matrix from e to n" (because it can be imagined as the "rotation" of a vector from one basis to another), or the "direction cosine matrix from e to n"[13] (because it contains direction cosines). The properties of a rotation matrix are such that its inverse is equal to its transpose. This means that the "rotation matrix from e to n" is the transpose of "rotation matrix from n to e".'b'This matrix equation relates the scalar components of a in the n basis (u,v, and w) with those in the e basis (p, q, and r). Each matrix element cjk is the direction cosine relating nj to ek.[13] The term direction cosine refers to the cosine of the angle between two unit vectors, which is also equal to their dot product.[13] Therefore,'b'and these equations can be expressed as the single matrix equation'b'Replacing each dot product with a unique scalar gives'b'Distributing the dot-multiplication gives'b'The values of p, q, r, and u, v, w relate to the unit vectors in such a way that the resulting vector sum is exactly the same physical vector a in both cases. It is common to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle). In such a case it is necessary to develop a method to convert between bases so the basic vector operations such as addition and subtraction can be performed. One way to express u, v, w in terms of p, q, r is to use column matrices along with a direction cosine matrix containing the information that relates the two bases. Such an expression can be formed by substitution of the above equations to form'b'and the scalar components in the n basis are, by definition,'b'In another orthnormal basis n = {n1, n2, n3} that is not necessarily aligned with e, the vector a is expressed as'b'The scalar components in the e basis are, by definition,'b'All examples thus far have dealt with vectors expressed in terms of the same basis, namely, the e basis {e1, e2, e3}. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector. In the e basis, a vector a is expressed, by definition, as'b'The scalar triple product is linear in all three entries and anti-symmetric in the following sense:'b'In components (with respect to a right-handed orthonormal basis), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the determinant of the 3-by-3 matrix having the three vectors as rows'b'It has three primary uses. First, the absolute value of the box product is the volume of the parallelepiped which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are linearly dependent, which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors a, b and c are right-handed.'b'The scalar triple product (also called the box product or mixed triple product) is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by (a b c) and defined as:'b'For arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector (see below).'b'The cross product can be written as'b'The length of a\xc2\xa0\xc3\x97\xc2\xa0b can be interpreted as the area of the parallelogram having a and b as sides.'b'The cross product a\xc2\xa0\xc3\x97\xc2\xa0b is defined so that a, b, and a\xc2\xa0\xc3\x97\xc2\xa0b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.'b'where \xce\xb8 is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist two unit vectors that are perpendicular to both a and b, namely, n and (\xe2\x80\x93n).'b'The cross product (also called the vector product or outer product) is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a\xc2\xa0\xc3\x97\xc2\xa0b, is a vector perpendicular to both a and b and is defined as'b'The dot product can also be defined as the sum of the products of the components of each vector as'b'where \xce\xb8 is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of the component of b that points in the same direction as a.'b'The dot product of two vectors a and b (sometimes called the inner product, or, since its result is a scalar, the scalar product) is denoted by a\xc2\xa0\xe2\x88\x99\xc2\xa0b and is defined as:'b'If r is negative, then the vector changes direction: it flips around by an angle of 180\xc2\xb0. Two examples (r = \xe2\x88\x921 and r = 2) are given below:'b'Intuitively, multiplying by a scalar r stretches a vector out by a factor of r. Geometrically, this can be visualized (at least in the case when r is an integer) as placing r copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.'b'A vector may also be multiplied, or re-scaled, by a real number r. In the context of conventional vector algebra, these real numbers are often called scalars (from scale) to distinguish them from vectors. The operation of multiplying a vector by a scalar is called scalar multiplication. The resulting vector is'b'Subtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a \xe2\x88\x92 b, as illustrated below:'b'The difference of a and b is'b'The addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:'b'Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b is'b'To normalize a vector a = [a1, a2, a3], scale the vector by the reciprocal of its length \xe2\x80\x96a\xe2\x80\x96. That is:'b'A unit vector is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as normalizing a vector. A unit vector is often indicated with a hat as in \xc3\xa2.'b'This happens to be equal to the square root of the dot product, discussed below, of the vector with itself:'b'which is a consequence of the Pythagorean theorem since the basis vectors e1, e2, e3 are orthogonal unit vectors.'b'The length of the vector a can be computed with the Euclidean norm'b'The length or magnitude or norm of the vector a is denoted by \xe2\x80\x96a\xe2\x80\x96 or, less commonly, |a|, which is not to be confused with the absolute value (a scalar "norm").'b'Two vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.'b'are opposite if'b'and'b'Two vectors are opposite if they have the same magnitude but opposite direction. So two vectors'b'are equal if'b'and'b'Two vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectors'b'and assumes that all vectors have the origin as a common base point. A vector a will be written as'b'The following section uses the Cartesian coordinate system with basis vectors'b'In these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a global coordinate system, or inertial reference frame).'b'A vector can also be broken up with respect to "non-fixed" basis vectors that change their orientation as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively normal, and tangent to a surface (see figure). Moreover, the radial and tangential components of a vector relate to the radius of rotation of an object. The former is parallel to the radius and the latter is orthogonal to it.[12]'b"The choice of a basis doesn't affect the properties of a vector or its behaviour under transformations."b'The decomposition or resolution[11] of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.'b'As explained above a vector is often described by a set of vector components that add up to form the given vector. Typically, these components are the projections of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be decomposed or resolved with respect to that set.'b'The notation ei is compatible with the index notation and the summation convention commonly used in higher level mathematics, physics, and engineering.'b'where a1, a2, a3 are called the vector components (or vector projections) of a on the basis vectors or, equivalently, on the corresponding Cartesian axes x, y, and z (see figure), while a1, a2, a3 are the respective scalar components (or scalar projections).'b'or'b'These have the intuitive interpretation as vectors of unit length pointing up the x, y, and z axis of a Cartesian coordinate system, respectively. In terms of these, any vector a in R3 can be expressed in the form:'b'Another way to represent a vector in n-dimensions is to introduce the standard basis vectors. For instance, in three dimensions, there are three of them:'b'These numbers are often arranged into a column vector or row vector, particularly when dealing with matrices, as follows:'b'This can be generalised to n-dimensional Euclidean space (or Rn).'b'In three dimensional Euclidean space (or R3), vectors are identified with triples of scalar components:'b'As an example in two dimensions (see figure), the vector from the origin O = (0,0) to the point A = (2,3) is simply written as'b'In order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an n-dimensional Euclidean space can be represented as coordinate vectors in a Cartesian coordinate system. The endpoint of a vector can be identified with an ordered list of n real numbers (n-tuple). These numbers are the coordinates of the endpoint of the vector, with respect to a given Cartesian coordinate system, and are typically called the scalar components (or scalar projections) of the vector on the axes of the coordinate system.'b'On a two-dimensional diagram, sometimes a vector perpendicular to the plane of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 \xe2\x8a\x99) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 \xe2\x8a\x97) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an arrow head on and viewing the flights of an arrow from the back.'b"Vectors are usually shown in graphs or other diagrams as arrows (directed line segments), as illustrated in the figure. Here the point A is called the origin, tail, base, or initial point; point B is called the head, tip, endpoint, terminal point or final point. The length of the arrow is proportional to the vector's magnitude, while the direction in which the arrow points indicates the vector's direction."b'In pure mathematics, a vector is any element of a vector space over some field and is often represented as a coordinate vector. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has "magnitude and direction".'b'In physics, as well as mathematics, a vector is often identified with a tuple of components, or list of numbers, that act as scalar coefficients for a set of basis vectors. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called covariant or contravariant depending on how the transformation of the vector\'s components is related to the transformation of the basis. In general, contravariant vectors are "regular vectors" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as gradient. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1\xc2\xa0m becomes 1000\xc2\xa0mm\xe2\x80\x93a contravariant change in numerical value. In contrast, a gradient of 1\xc2\xa0K/m becomes 0.001\xc2\xa0K/mm\xe2\x80\x93a covariant change in value. See covariance and contravariance of vectors. Tensors are another type of quantity that behave in this way; a vector is one type of tensor.'b'However, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of vector spaces (for free vectors) and affine spaces (for bound vectors, as each represented by an ordered pair of "points"). An important example is Minkowski space that is important to our understanding of special relativity, where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from thermodynamics, where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.[10]'b"In the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a length or magnitude and a direction to vectors. In addition, the notion of direction is strictly associated with the notion of an angle between two vectors. If the dot product of two vectors is defined \xe2\x80\x94 a scalar-valued product of two vectors \xe2\x80\x94, then it's also possible to define a length; the dot product gives a convenient algebraic characterization of both angle (a function of the dot product between any two non-zero vectors) and length (the square root of the dot product of a vector by itself). In three dimensions, it is further possible to define the cross product, which supplies an algebraic characterization of the area and orientation in space of the parallelogram defined by two vectors (used as sides of the parallelogram). In any dimension (and, in particular, higher dimensions), it's possible to define the exterior product, which (among other things) supplies an algebraic characterization of the area and orientation in space of the n-dimensional parallelotope defined by n vectors."b'This coordinate representation of free vectors allows their algebraic features to be expressed in a convenient numerical fashion. For example, the sum of the two (free) vectors (1,2,3) and (\xe2\x88\x922,0,4) is the (free) vector'b"In Cartesian coordinates a free vector may be thought of in terms of a corresponding bound vector, in this sense, whose initial point has the coordinates of the origin O = (0,0,0). It is then determined by the coordinates of that bound vector's terminal point. Thus the free vector represented by (1,0,0) is a vector of unit length pointing along the direction of the positive x-axis."b"Vectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is velocity, the magnitude of which is speed. For example, the velocity 5 meters per second upward could be represented by the vector (0,5) (in 2 dimensions with the positive y axis as 'up'). Another quantity represented by a vector is force, since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, displacement, linear acceleration, angular acceleration, linear momentum, and angular momentum. Other physical vectors, such as the electric and magnetic field, are represented as a system of vectors at each point of a physical space; that is, a vector field. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition: Angular displacement and electric current. Consequently, these are not vectors."b"Since the physicist's concept of force has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force F of 15 newtons. If the positive axis is also directed rightward, then F is represented by the vector 15 N, and if positive points leftward, then the vector for F is \xe2\x88\x9215 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement \xce\x94s of 4 meters would be 4 m or \xe2\x88\x924 m, depending on its direction, and its magnitude would be 4 m regardless."b'The term vector also has generalizations to higher dimensions and to more formal approaches with much wider applications.'b'This article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as geometric, spatial, or Euclidean vectors.'b'In physics and engineering, a vector is typically regarded as a geometric entity characterized by a magnitude and a direction. It is formally defined as a directed line segment, or arrow, in a Euclidean space.[8] In pure mathematics, a vector is defined more generally as any element of a vector space. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called Euclidean space.'b"Josiah Willard Gibbs, who was exposed to quaternions through James Clerk Maxwell's Treatise on Electricity and Magnetism, separated off their vector part for independent treatment. The first half of Gibbs's Elements of Vector Analysis, published in 1881, presents what is essentially the modern system of vector analysis.[6] In 1901 Edwin Bidwell Wilson published Vector Analysis, adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus."b'In 1878 Elements of Dynamic was published by William Kingdon Clifford. Clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.'b'Peter Guthrie Tait carried the quaternion standard after Hamilton. His 1867 Elementary Treatise of Quaternions included extensive treatment of the nabla or del operator \xe2\x88\x87.'b"Several other mathematicians developed vector-like systems in the middle of the nineteenth century, including Augustin Cauchy, Hermann Grassmann, August M\xc3\xb6bius, Comte de Saint-Venant, and Matthew O'Brien. Grassmann's 1840 work Theorie der Ebbe und Flut (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.[6]"b'The term vector was introduced by William Rowan Hamilton as part of a quaternion, which is a sum q = s + v of a Real number s (also called scalar) and a 3-dimensional vector. Like Bellavitis, Hamilton viewed vectors as representative of classes of equipollent directed segments. As complex numbers use an imaginary unit to complement the real line, Hamilton considered the vector v to be the imaginary part of a quaternion:'b'Giusto Bellavitis abstracted the basic idea in 1835 when he established the concept of equipollence. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an equivalence relation on the pairs of points (bipoints) in the plane and thus erected the first space of vectors in the plane.[6]:52\xe2\x80\x934'b'The concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.[6]'b''b''b'Vectors play an important role in physics: the velocity and acceleration of a moving object and the forces acting on it can all be described with vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can still be represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.'b'A vector is what is needed to "carry" the point A to the point B; the Latin word vector means "carrier".[4] It was first used by 18th century astronomers investigating planet rotation around the Sun.[5] The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from A to B. Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.'
Scalar multiplication
b'where i, j, k are the quaternion units. The non-commutativity of quaternion multiplication prevents the transition of changing ij = +k to ji = \xe2\x88\x92k.'b'For quaternion scalars and matrices:'b'For a real scalar and matrix:'b'When the underlying ring is commutative, for example, the real or complex number field, these two multiplications are the same, and are simply called scalar multiplication. However, for matrices over a more general ring that are not commutative, such as the quaternions, they may not be equal.'b'explicitly:'b'Similarly, the right scalar multiplication of a matrix A with a scalar \xce\xbb is defined to be'b'explicitly:'b'The left scalar multiplication of a matrix A with a scalar \xce\xbb gives another matrix \xce\xbbA of the same size as A. The entries of \xce\xbbA are defined by'b'The same idea applies if K is a commutative ring and V is a module over K. K can even be a rig, but then there is no additive inverse. If K is not commutative, the distinct operations left scalar multiplication cv and right scalar multiplication vc may be defined.'b'When V is Kn, scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such.'b'As a special case, V may be taken to be K itself and scalar multiplication may then be taken to be simply the multiplication in the field.'b'Scalar multiplication may be viewed as an external binary operation or as an action of the field on the vector space. A geometric interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor.'b'Here + is addition either in the field or in the vector space, as appropriate; and 0 is the additive identity in either. Juxtaposition indicates either scalar multiplication or the multiplication operation in the field.'b'Scalar multiplication obeys the following rules (vector in boldface):'b'In general, if K is a field and V is a vector space over K, then scalar multiplication is a function from K \xc3\x97 V to V. The result of applying this function to c in K and v in V is denoted cv.'b''b''b'In mathematics, scalar multiplication is one of the basic operations defining a vector space in linear algebra[1][2][3] (or more generally, a module in abstract algebra[4][5]). In common geometrical contexts, scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction. The term "scalar" itself derives from this usage: a scalar is that which scales vectors. Scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and must be distinguished from inner product of two vectors (where the product is a scalar).'
Axiom
b'Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.'b'There is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms. The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.'b'The objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The L\xc3\xb6wenheim\xe2\x80\x93Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.'b'Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid\'s postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.'b'The Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed G\xc3\xb6del to establish his famous second incompleteness theorem.[12]'b'This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.'b'The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.'b'Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo\xe2\x80\x93Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann\xe2\x80\x93Bernays\xe2\x80\x93G\xc3\xb6del set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse\xe2\x80\x93Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.[citation needed]'b'This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.'b'Thus, an axiom is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.'b'Non-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.'b'Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought[citation needed] that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.'b'Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not tautologies. Another name for a non-logical axiom is postulate.[11]'b'Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:'b'These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.[10]'b'Other axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed.[9]'b'These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms at least some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.'b'In the field of mathematical logic, a clear distinction is made between two notions of axioms: logical and non-logical (somewhat similar to the ancient distinction between "axioms" and "postulates" respectively).'b'Regardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither "proves" nor "disproves" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.'b'As a consequence, it is not necessary to explicitly cite Einstein\'s axioms, the more so since they concern subtle points on the "reality" and "locality" of experiments.'b'Another paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of "theorems" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics "hidden variables" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein\'s axioms compared to using Bohr\'s axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr\'s axioms, not Einstein\'s. (Bohr\'s axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)'b"In 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity."b"Axioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment."b'It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo\xe2\x80\x93Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo\xe2\x80\x93Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.'b"The formalist project suffered a decisive setback, when in 1931 G\xc3\xb6del showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, G\xc3\xb6del proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory."b"In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of na\xc3\xafve set theory raised the possibility that any such system could turn out to be inconsistent."b"It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms."b'In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.'b'Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincar\xc3\xa9, Hilbert, and G\xc3\xb6del are some of the key figures in this development.'b'It is not correct to say that the axioms of field theory are "propositions that are regarded as true without proof." Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.'b'When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.'b'Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without any particular application in mind. The distinction between an "axiom" and a "postulate" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid\'s fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like "line" and "parallel" with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience.'b'A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.'b'The classical approach is well-illustrated by Euclid\'s Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).'b'At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a postulate. While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.[8]'b'An "axiom", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that'b"The ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view."b'The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.'b'Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid\'s books, Proclus remarks that, "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property."[7] Boethius translated \'postulate\' as petitio and called the axioms notiones communes but in later manuscripts this usage was not always strictly kept.'b'The root meaning of the word postulate is to "demand"; for instance, Euclid demands that one agree that some things can be done, e.g. any two points can be joined by a straight line, etc.[6]'b'The word axiom comes from the Greek word \xe1\xbc\x80\xce\xbe\xce\xaf\xcf\x89\xce\xbc\xce\xb1 (ax\xc3\xad\xc5\x8dma), a verbal noun from the verb \xe1\xbc\x80\xce\xbe\xce\xb9\xcf\x8c\xce\xb5\xce\xb9\xce\xbd (axioein), meaning "to deem worthy", but also "to require", which in turn comes from \xe1\xbc\x84\xce\xbe\xce\xb9\xce\xbf\xcf\x82 (\xc3\xa1xios), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof.'b''b''b'In both senses, an axiom is any mathematical statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom, or any mathematical statement, to be "true" is an open question[citation needed] in the philosophy of mathematics.[5]'b'As used in mathematics, the term axiom is used in two related but distinguishable senses: "logical axioms" and "non-logical axioms". Logical axioms are usually statements that are taken to be true within the system of logic they define (e.g., (A and B) implies A), often shown in symbolic form, while non-logical axioms (e.g., a + b = b + a) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, "axiom", "postulate", and "assumption" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain.'b'The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] As used in modern logic, an axiom is simply a premise or starting point for reasoning.[4]'b"An axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek ax\xc3\xad\xc5\x8dma (\xe1\xbc\x80\xce\xbe\xce\xaf\xcf\x89\xce\xbc\xce\xb1) 'that which is thought worthy or fit' or 'that which commends itself as evident.'[1][2]"
School Mathematics Study Group
b"Perhaps the most authoritative collection of materials from the School Mathematics Study Group is now housed in the Archives of American Mathematics in the University of Texas at Austin's Center for American History."b'The School Mathematics Study Group (SMSG) was an American academic think tank focused on the subject of reform in mathematics education. Directed by Edward G. Begle and financed by the National Science Foundation, the group was created in the wake of the Sputnik crisis in 1958 and tasked with creating and implementing mathematics curricula for primary and secondary education, which it did until its termination in 1977. The efforts of the SMSG yielded a reform in mathematics education known as New Math which was promulgated in a series of reports, which culminated in a series published by Random House called the New Mathematical Library. In the early years SMSG also rushed out a set of draft textbooks in typewritten paperback format for elementary, middle and high school students.'
Secondary school
b''b'A secondary school, locally may be called high school or senior high school. In some countries there are two phases to secondary education (ISCED 2) and (ISCED 3), here the junior high school, intermediate school, lower secondary school or middle school occurs between the primary school (ISCED 1) and high school.'b'The UK government published this downwardly revised space formula in 2014. It said the floor area should be 1050m\xc2\xb2 (+ 350m\xc2\xb2 if there is a sixth form) + 6.3m\xc2\xb2/pupil place for 11- to 16-year-olds + 7m\xc2\xb2/pupil place for post-16s. The external finishes were to be downgraded to meet a build cost of \xc2\xa31113/m\xc2\xb2. [9]'b"Government accountants having read the advice then publish minimum guidelines on schools. These enable environmental modelling and establishing building costs. Future design plans are audited to ensure that these standards are met but not exceeded. Government ministries continue to press for the 'minimum' space and cost standards to be reduced."b'The building providing the education has to fulfil the needs of: The students, the teachers, the non-teaching support staff, the administrators and the community. It has to meet general government building guidelines, health requirements, minimal functional requirements for classrooms, toilets and showers, electricity and services, preparation and storage of textbooks and basic teaching aids. [8] An optimum secondary school will meet the minimum conditions and will have\xc2\xa0:'b'According to standards used in the UK, a general classroom for 30 students needs to be 55\xc2\xa0m\xc2\xb2, or more generously 62\xc2\xa0m\xc2\xb2. A general art room for 30 students needs to be 83\xc2\xa0m\xc2\xb2, but 104\xc2\xa0m\xc2\xb2 for 3D textile work. A drama studio or a specialist science laboratory for 30 needs to be 90\xc2\xa0m\xc2\xb2. Examples are given on how this can be configured for a 1,200 place secondary (practical specialism).[6] and 1,850 place secondary school.[7]'b'Each country will have a different education system and priorities. [5] Schools need to accommodate students, staff, storage, mechanical and electrical systems, storage, support staff, ancillary staff and administration. The number of rooms required can be determined from the predicted roll of the school and the area needed.'b'School building design does not happen in isolation. The building (or school campus) needs to accommodate:'b"Within the English speaking world, there are three widely used systems to describe the age of the child. The first is the 'equivalent ages', then countries that base their education systems on the 'English model' use one of two methods to identify the year group, while countries that base their systems on the 'American K-12 model' refer to their year groups as 'grades'. This terminology extends into research literature. Below is a convenient comparison [4]"b'[3]'b''b''b'Secondary schools typically follow on from primary schools and lead into vocational and tertiary education. Attendance is compulsory in most countries for students between the ages of 11 and 16. The organisations, buildings, and terminology are more or less unique in each country.[1][2]'b'A secondary school is both an organization that provides secondary education and the building where this takes place. Some secondary schools can provide both lower secondary education and upper secondary education (levels 2 and 3 of the ISCED scale), but these can also be provided in separate schools, as in the American middle school- high school system.'
Singular-value decomposition
b'Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[23] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965,[24] which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch[25] published a variant of the Golub/Kahan algorithm that is still the one most-used today.'b'The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[22] they saw it as a generalization of the principal axis transformation for Hermitian matrices.'b'Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:'b'where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. \xcf\x83i are called the singular values of M. {Uei} (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.'b'The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero \xce\xbb in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M\xe2\x88\x97M. Applying the diagonalization result, the unitary image of its positive square root Tf\xc2\xa0 has a set of orthonormal eigenvectors {ei} corresponding to strictly positive eigenvalues {\xcf\x83i}. For any \xcf\x88 \xe2\x88\x88 H,'b'and notice that U V* is still a partial isometry while VTf V* is positive.'b'As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply write'b'is a unitary operator.'b'This can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such that'b'The factorization M = U\xce\xa3V\xe2\x88\x97 can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X,\xc2\xa0\xce\xbc), and a non-negative measurable f such that'b'TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:'b'Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.'b'In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.'b'where \xcf\x83i are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert\xe2\x80\x93Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:'b'Since the trace is invariant under unitary equivalence, this shows'b'So the induced norm is'b'The singular values are related to another norm on the space of operators. Consider the Hilbert\xe2\x80\x93Schmidt inner product on the n \xc3\x97 n matrices, defined by'b"The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)\xc2\xbd] (the eigenvalues of M* M are the squares of the singular values)."b'But, in the matrix case, (M* M)\xc2\xbd is a normal matrix, so ||M* M||\xc2\xbd is the largest eigenvalue of (M* M)\xc2\xbd, i.e. the largest singular value of M.'b'The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard l2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spaces'b'The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [20]'b'Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values \xce\xa3t are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t\xe2\x89\xaar. The matrix Ut is thus m\xc3\x97t, \xce\xa3t is t\xc3\x97t diagonal, and Vt* is t\xc3\x97n.'b'Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values \xce\xa3r are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r\xc2\xa0\xe2\x89\xaa\xc2\xa0n. The matrix Ur is thus m\xc3\x97r, \xce\xa3r is r\xc3\x97r diagonal, and Vr* is r\xc3\x97n.'b'The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if\xc2\xa0n\xc2\xa0\xe2\x89\xaa\xc2\xa0m.'b"Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n\xc2\xa0\xe2\x89\xaa\xc2\xa0m. The matrix U'n is thus m\xc3\x97n, \xce\xa3n is n\xc3\x97n diagonal, and V is n\xc3\x97n."b'In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. The following can be distinguished for an m\xc3\x97n matrix M of rank r:'b'The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M \xe2\x87\x92 Q R and LQ decomposition of R gives R \xe2\x87\x92 L P*. Thus, at every iteration, we have M \xe2\x87\x92 Q L P*, update M \xe2\x87\x90 L and repeat the orthogonalizations. Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation. But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.'b'There is an alternative way which is not explicitly using the eigenvalue decomposition.[19] Usually the singular-value problem of a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or'b'The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 \xc3\x97 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 \xc3\x97 2 eigenvalue methods (Golub & Van Loan 1996, \xc2\xa78.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).'b'The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[17] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[18] routine for the computation of the singular-value decomposition.'b'The first step can be done using Householder reflections for a cost of 4mn2 \xe2\x88\x92 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).'b'The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flops), assuming that m \xe2\x89\xa5 n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).'b'To get a more visual flavour of singular values and SVD factorization \xe2\x80\x94 at least when working on real vector spaces \xe2\x80\x94 consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V\xe2\x88\x97 sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching coefficients. The composition D \xe2\x88\x98 V\xe2\x88\x97 then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U \xe2\x88\x98 D \xe2\x88\x98 V\xe2\x88\x97 coincides with T.'b'The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T\xc2\xa0: Kn \xe2\x86\x92 Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.'b'where \xcf\x83i is the i-th diagonal entry of \xce\xa3, and T(Vi) = 0 for i > min(m,n).'b'has a particularly simple description with respect to these orthonormal bases: we have'b'The linear transformation'b'Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).'b'The passage from real to complex is similar to the eigenvalue case.'b'More singular vectors and singular values can be found by maximizing \xcf\x83(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.'b'This proves the statement.'b'Plugging this into the pair of equations above, we have'b'After some algebra, this becomes'b'Proof: Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:'b'Consider the function \xcf\x83 restricted to Sm\xe2\x88\x921 \xc3\x97 Sn\xe2\x88\x921. Since both Sm\xe2\x88\x921 and Sn\xe2\x88\x921 are compact sets, their product is also compact. Furthermore, since \xcf\x83 is continuous, it attains a largest value for at least one pair of vectors u \xe2\x88\x88 Sm\xe2\x88\x921 and v \xe2\x88\x88 Sn\xe2\x88\x921. This largest value is denoted \xcf\x831 and the corresponding vectors are denoted u1 and v1. Since \xcf\x831 is the largest value of \xcf\x83(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.'b'Let M denote an m \xc3\x97 n matrix with real entries. Let Sm\xe2\x88\x921 and Sn\xe2\x88\x921 denote the sets of unit 2-norm vectors in Rm and Rn respectively. Define the function'b'The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.'b'Notice the argument could begin with diagonalizing MM\xe2\x88\x97 rather than M\xe2\x88\x97M (This shows directly that MM\xe2\x88\x97 and M\xe2\x88\x97M have the same non-zero eigenvalues).'b'which is the desired result:'b'For V1 we already have V2 to make it unitary. Now, define'b'the columns in U1 are orthonormal and can be extended to an orthonormal basis. This means, we can choose U2 such that the following matrix is unitary:'b'We see that this is almost the desired result, except that U1 and V1 are not unitary in general since they might not be square. However, we do know that for U1, the number of rows is no smaller than the number of columns since the dimensions of D is no greater than m and n. Also, since'b'Then'b'where the subscripts on the identity matrices are there to keep in mind that they are of different dimensions. Define'b'The second equation implies MV2 = 0. Also, since V is unitary:'b'Therefore:'b'where D is diagonal and positive definite. Partition V appropriately so we can write'b'Let M be an m \xc3\x97 n complex matrix. Since M\xe2\x88\x97M is positive semi-definite and Hermitian, by the spectral theorem, there exists a unitary n \xc3\x97 n matrix V such that'b'This section gives these two arguments for existence of singular-value decomposition.'b'Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.'b'A short calculation shows the above leads to Mu = \xce\xbbu (symmetry of M is needed here). Therefore, \xce\xbb is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.'b'where the nabla symbol, \xe2\x88\x87, is the del operator.'b'By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| \xe2\x89\xa4 1}. By the Lagrange multipliers theorem, u necessarily satisfies'b'An eigenvalue \xce\xbb of a matrix M is characterized by the algebraic relation Mu = \xce\xbbu. When M is Hermitian, a variational characterization is also available. Let M be a real n \xc3\x97 n symmetric matrix. Define'b'In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU\xe2\x88\x97 for a unitary matrix U and a diagonal matrix D. When M is also positive semi-definite, the decomposition M = UDU\xe2\x88\x97 is also a singular-value decomposition. Otherwise, it can be recast as an SVD by moving the phase of each \xcf\x83i to either its corresponding Vi or Ui. The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: M=SR, where S=U\xce\xa3U* is positive semidefinite and normal, and R=UV* is unitary.'b'The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:'b'Given an SVD of M, as described above, the following two relations hold:'b'The singular-value decomposition is very general in the sense that it can be applied to any m \xc3\x97 n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices. Nevertheless, the two decompositions are related.'b'Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]'b'Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm [14], which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.'b"Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]"b'SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]'b'One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.'b'The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the \xce\xa3 matrix is larger than one.'b'The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.'b'The SVD and pseudoinverse have been successfully applied to signal processing[4], Image Processing [5] and big data, e.g., in genomic signal processing.[6][7][8][9]'b"The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules."b'This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.'b'A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,'b'It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O \xe2\x88\x92 A. The solution is the product UV\xe2\x88\x97.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV\xe2\x88\x97 where I is the identity matrix, so that if A = U\xce\xa3V\xe2\x88\x97 then the product A = UV\xe2\x88\x97 amounts to replacing the singular values with ones.'b'which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]'b"Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,"b'Here Ui and Vi are the i-th columns of the corresponding SVD matrices, \xcf\x83i are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero \xcf\x83i is exactly the rank of the matrix.'b'As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \xce\xa3. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix.'b'Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.'b'A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.'b"A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution. It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x\xe2\x88\x97A = 0, with x\xe2\x88\x97 denoting the conjugate transpose of x, is called a left null vector of A."b'where \xce\xa3+ is the pseudoinverse of \xce\xa3, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.'b'The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = U\xce\xa3V\xe2\x88\x97 is'b'Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ei\xcf\x86 (for the real case up to a sign). Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor. In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.'b'As an exception, the left and right singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank\xe2\x80\x93nullity theorem cannot be the same dimension if m \xe2\x89\xa0 n. Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m \xe2\x88\x92 n orthogonal vectors from the cokernel. Conversely, if m < n, then V is padded by n \xe2\x88\x92 m orthogonal vectors from the kernel. However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right singular vectors.'b'the diagonal entries of \xce\xa3 are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values. Consequently, the above theorem implies that:'b'In any singular-value decomposition'b'is also a valid singular-value decomposition.'b'Notice \xce\xa3 is zero outside of the diagonal and one diagonal element is zero. Furthermore, because the matrices U and V\xe2\x88\x97 are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below. In this case, because U and V\xe2\x88\x97 are real valued, each is an orthogonal matrix.'b'A singular-value decomposition of this matrix is given by U\xce\xa3V\xe2\x88\x97'b'Consider the 4 \xc3\x97 5 matrix'b'Since U and V\xe2\x88\x97 are unitary, the columns of each of them form a set of orthonormal vectors, which can be regarded as basis vectors. The matrix M maps the basis vector Vi to the stretched unit vector \xcf\x83i Ui (see below for further details). By the definition of a unitary matrix, the same is true for their conjugate transposes U\xe2\x88\x97 and V, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of U, U\xe2\x88\x97, V, and V\xe2\x88\x97 are orthonormal bases.'b'As shown in the figure, the singular values can be interpreted as the semiaxes of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n \xc3\x97 n square matrix being viewed as the semiaxes of an n-dimensional ellipsoid. Similarly, the singular values of any m \xc3\x97 n matrix can be viewed as the semiaxes of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.'b"Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = V\xce\xa3V\xe2\x88\x97) with eigenvalue scale factors \xcf\x83i along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV\xe2\x88\x97). If the rotation is done first, M = P'R, then R is the same and P' = U\xce\xa3U\xe2\x88\x97 has the same eigenvalues, but is stretched along different (post-rotated) directions. This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate."b'In the special, yet common case when M is an m \xc3\x97 m real square matrix with positive determinant, U, V\xe2\x88\x97, and \xce\xa3 are real m \xc3\x97 m matrices as well, \xce\xa3 can be regarded as a scaling matrix, and U, V\xe2\x88\x97 can be viewed as rotation matrices. Thus the expression U\xce\xa3V\xe2\x88\x97 can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure above explains how a shear matrix can be described as such a sequence.'b'The diagonal entries \xcf\x83i of \xce\xa3 are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, \xce\xa3, is uniquely determined by M (though not the matrices U and V, see below).'b'where'b'Suppose M is a m \xc3\x97 n matrix whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. Then there exists a factorization, called a singular value decomposition of M, of the form'b'Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.'b'The singular-value decomposition can be computed using the following observations:'
Determinant
b'where \xcf\x89j is an nth root of 1.'b'where \xcf\x89 and \xcf\x892 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]'b'Third order'b'Second order'b'where the right-hand side is the continued product of all the differences that can be formed from the n(n\xe2\x88\x921)/2 pairs of numbers taken from x1, x2, \xe2\x80\xa6, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.'b'In general, the nth-order Vandermonde determinant is[33]'b'The third order Vandermonde determinant is'b'The Jacobian also occurs in the inverse function theorem.'b'Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function \xcf\x86: Rn \xe2\x86\x92 Rm is given by'b'the Jacobian matrix is the n \xc3\x97 n matrix whose entries are given by'b'For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. For'b'By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)\xc2\xb7|det(a \xe2\x88\x92 b, b \xe2\x88\x92 c, c \xe2\x88\x92 d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.'b'As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn \xe2\x86\x92 Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn \xe2\x86\x92 Rm is represented by the m \xc3\x97 n matrix A, then the n-dimensional volume of f(S) is given by:'b'More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 \xc3\x97 2 or 3 \xc3\x97 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.'b'The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is \xe2\x88\x921, the basis has the opposite orientation.'b'It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n\xe2\x88\x921 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.'b'As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 \xc3\x97 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), \xe2\x80\xa6, fn(x) (supposed to be n \xe2\x88\x92 1 times differentiable), the Wronskian is defined to be'b"The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises."b"The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]"b"The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy\xe2\x80\x93Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality."b'Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.'b'It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.'b'In Japan, Seki Takakazu (\xe9\x96\xa2 \xe5\xad\x9d\xe5\x92\x8c) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by B\xc3\xa9zout (1764).'b'Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (\xe4\xb9\x9d\xe7\xab\xa0\xe7\xae\x97\xe8\xa1\x93, Chinese scholars, around the 3rd century BCE). In Europe, 2 \xc3\x97 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]'b"Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]"b"Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form."b'If two matrices of order n can be multiplied in time M(n), where M(n) \xe2\x89\xa5 na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith\xe2\x80\x93Winograd algorithm.'b'Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.'b'If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.'b'(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant\xc2\xa01, in which case the formula further simplifies to'b'The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:'b'Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)'b"Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n \xc3\x97 n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants."b'Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.'b"The permanent of a matrix is defined as the determinant, except that the factors sgn(\xcf\x83) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule."b'Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]'b'For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonn\xc3\xa9 determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.'b'Another infinite-dimensional notion of determinant is the functional determinant.'b'The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula'b'For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.'b'For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo\xc2\xa0m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo\xc2\xa0m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (\xe2\x8b\x85)\xc3\x97 (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,'b'holds. In other words, the following diagram commutes:'b'between the group of invertible n \xc3\x97 n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R \xe2\x86\x92 S, there is a map GLn(f): GLn(R) \xe2\x86\x92 GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identity'b'The determinant defines a mapping'b'This definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or \xe2\x88\x921. Such a matrix is called unimodular.'b'This fact also implies that every other n-linear alternating function F: Mn(K) \xe2\x86\x92 K satisfies'b'for any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.'b'from the set of all n \xc3\x97 n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that is'b'The determinant can also be characterized as the unique function'b'The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T\xe2\x80\xb2 on W, where for each w in W we define (T\xe2\x80\xb2w)(x1, \xe2\x80\xa6, xn) = w(Tx1, \xe2\x80\xa6, Txn). As a linear transformation on a one-dimensional space, T\xe2\x80\xb2 is equivalent to a scalar multiple. We call this scalar the determinant of T.'b'For this reason, the highest non-zero exterior power \xce\x9bn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms \xce\x9bkV with k < n.'b'This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 \xe2\x88\xa7 v2 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn to v2 \xe2\x88\xa7 v1 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn, say, also changes its sign.'b'As \xce\x9bnV is one-dimensional, the map \xce\x9bnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to say'b'The determinant of a linear transformation A\xc2\xa0: V \xe2\x86\x92 V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power \xce\x9bnV of V. A induces a linear map'b'for some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.'b'The determinant is therefore also called a similarity invariant. The determinant of a linear transformation'b'The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X\xe2\x88\x921BX. Indeed, repeatedly applying the above identities yields'b'This identity is used in describing the tangent space of certain matrix Lie groups.'b'Yet another equivalent formulation is'b'Expressed in terms of the entries of A, these are'b'where adj(A) denotes the adjugate of A. In particular, if A is invertible, we have'b"By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn \xc3\x97 n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]"b'When D is a 1\xc3\x971 matrix, B is a column vector, and C is a row vector then'b'When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)'b'Generally, if all pairs of n \xc3\x97 n matrices of the np \xc3\x97 np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p \xc3\x97 p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.'b'When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 \xc3\x97 2 matrix holds:[12]'b'as can be seen by employing the decomposition'b'When A is invertible, one has'b'This can be seen from the Leibniz formula, or from a decomposition like (for the former case)'b'Suppose A, B, C, and D are matrices of dimension n \xc3\x97 n, n \xc3\x97 m, m \xc3\x97 n, and m \xc3\x97 m, respectively. Then'b"It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition."b'where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.'b"the solution is given by Cramer's rule:"b'For a matrix equation'b'These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.'b'Also,'b'with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.'b'For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinant'b'is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).'b'where I is the identity matrix. More generally, if'b'An important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,'b'This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way as'b'The formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l \xe2\x80\x93 1)! tr(Al) as'b'where the sum is taken over the set of all integers kl \xe2\x89\xa5 0 satisfying the equation'b'In the general case, this may also be obtained from[9]'b"cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev\xe2\x80\x93LeVerrier algorithm. That is, for generic n, detA = (\xe2\x88\x92)nc0 the signed constant term of the characteristic polynomial, determined recursively from"b'For example, for n = 2, n = 3, and n = 4, respectively,'b'the determinant of A is given by'b'Here exp(A) denotes the matrix exponential of A, because every eigenvalue \xce\xbb of A corresponds to the eigenvalue exp(\xce\xbb) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfying'b'or, for real matrices A,'b'The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,'b'being positive, for all k between 1 and n.'b"A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices"b'where I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).'b'Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equation'b'The product of all non-zero eigenvalues is referred to as pseudo-determinant.'b'From this general result several consequences follow.'b'where Im and In are the m \xc3\x97 m and n \xc3\x97 n identity matrices, respectively.'b"Sylvester's determinant theorem states that for A, an m \xc3\x97 n matrix, and B, an n \xc3\x97 m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):"b"In terms of the adjugate matrix, Laplace's expansion can be written as[7]"b'The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,'b'However, Laplace expansion is efficient for small matrices only.'b'along the second column (j = 2 and the sum runs over i) is given by,'b'Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 \xc3\x97 3 matrix'b"Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n\xe2\x88\x921) \xc3\x97 (n\xe2\x88\x921)-matrix that results from A by removing the i-th row and the j-th column. The expression (\xe2\x88\x921)i+jMi,j is known as a cofactor. The determinant of A is given by"b'In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.'b'The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by'b'Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value\xc2\xa01 on the identity matrix, since the function Mn(K) \xe2\x86\x92 K that maps M \xe2\x86\xa6 det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy\xe2\x80\x93Binet formula, which also provides an independent proof of the multiplicative property.'b'The determinant of a matrix product of square matrices equals the product of their determinants:'b'Here, B is obtained from A by adding \xe2\x88\x921/2\xc3\x97the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = \xe2\x88\x92det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (\xe2\x88\x922) \xc2\xb7 2 \xc2\xb7 4.5 = \xe2\x88\x9218. Therefore, det(A) = \xe2\x88\x92det(D) = +18.'b'can be computed using the following matrices:'b'For example, the determinant of'b'Property 5 says that the determinant on n \xc3\x97 n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property\xc2\xa06; this is essentially the method of Gaussian elimination.'b'Property 2 above implies that properties for columns have their counterparts in terms of rows:'b'Properties 1, 8 and 10 \xe2\x80\x94 which all follow from the Leibniz formula \xe2\x80\x94 completely characterize the determinant; in other words the determinant is the unique function from n \xc3\x97 n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property\xc2\xa09) or else \xc2\xb11 (by properties 1 and\xc2\xa012 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n \xe2\x89\xa5 2,[6] so there is no good definition of the determinant in this setting.'b'A number of additional properties relate to the effects on the determinant of changing particular rows or columns:'b'This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.'b'The determinant has many properties. Some basic properties of determinants are'b'where now each ir and each jr should be summed over 1, \xe2\x80\xa6, n.'b'or using two epsilon symbols as'b'For example, the determinant of a 3 \xc3\x97 3 matrix A (n = 3) is'b'is notation for the product of the entries at positions (i, \xcf\x83i), where i ranges from 1 to n:'b'Here the sum is computed over all permutations \xcf\x83 of the set {1, 2, \xe2\x80\xa6, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering \xcf\x83 is denoted by \xcf\x83i. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to \xcf\x83 = [2, 3, 1], with \xcf\x831 = 2, \xcf\x832 = 3, and \xcf\x833 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation \xcf\x83, sgn(\xcf\x83) denotes the signature of \xcf\x83, a value that is +1 whenever the reordering given by \xcf\x83 can be achieved by successively interchanging two entries an even number of times, and \xe2\x88\x921 whenever it can be achieved by an odd number of such interchanges.'b'The Leibniz formula for the determinant of an n \xc3\x97 n matrix A is'b'The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.'b'The rule of Sarrus is a mnemonic for the 3 \xc3\x97 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 \xc3\x97 3 matrix does not carry over into higher dimensions.'b'which is the Leibniz formula for the determinant of a 3 \xc3\x97 3 matrix.'b'this can be expanded out to give'b'The Laplace formula for the determinant of a 3 \xc3\x97 3 matrix is'b'The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) \xe2\x88\xa7 (c, d)) is the signed area, which is also the determinant ad \xe2\x88\x92 bc.[3]'b'Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.'b"To show that ad \xe2\x88\x92 bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sin\xce\xb8 for the angle \xce\xb8 between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a\xe2\x8a\xa5 = (-b, a), such that |a\xe2\x8a\xa5||b|cos\xce\xb8' , which can be determined by the pattern of the scalar product to be equal to ad \xe2\x88\x92 bc:"b'The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).'b'The absolute value of ad \xe2\x88\x92 bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)'b'If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.'b'The Leibniz formula for the determinant of a 2 \xc3\x97 2 matrix is'b'The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:'b'The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.'b'Assume A is a square matrix with n rows and n columns, so that it can be written as'b'Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is \xe2\x88\x921 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n \xc3\x97 n matrix contributes n! terms), so it will first be given explicitly for the case of 2 \xc3\x97 2 matrices and 3 \xc3\x97 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.'b'where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]'b'Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n \xc3\x97 n matrix A in terms of its column vectors'b'There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 \xc3\x97 4 matrix:'b''b''b'When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.'b'Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.'b'Each determinant of a 2 \xc3\x97 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 \xc3\x97 4 matrix, the determinant of a 5 \xc3\x97 5 matrix, and so forth.'b'Similarly, suppose we have a 3 \xc3\x97 3 matrix A, and we want the specific formula for its determinant |A|:'b'In the case of a 2 \xc3\x97 2 matrix the specific formula for the determinant is:'b'In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.'
Gaussian elimination
b'Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.'b'This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.'b'Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]'b'The Gaussian elimination can be performed over any field, not just the real numbers.'b'One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]'b'This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).'b"The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 \xe2\x88\x92 5n)/6 multiplications, and (2n3 + 3n2 \xe2\x88\x92 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5)."b'All of this applies also to the reduced row echelon form, which is a particular row echelon form.'b'One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A\xe2\x88\x921. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.'b'By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:'b'To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:'b'For example, consider the following matrix'b'A variant of Gaussian elimination called Gauss\xe2\x80\x93Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A\xe2\x88\x921. If the algorithm is unable to reduce the left block to I, then A is not invertible.'b'Computationally, for a n\xc3\x97n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.'b'If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = \xe2\x88\x8fdiag(B) / d.'b'To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:'b'The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.'b'Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss\xe2\x80\x93Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss\xe2\x80\x93Jordan elimination independently.[7]'b'The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]'b'The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.'b'Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.'b'Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.'b'Suppose the goal is to find and describe the set of solutions to the following system of linear equations:'b'A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).'b'It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).'b'For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.'b'For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.'b"If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier."b'There are three types of elementary row operations which may be performed on the rows of a matrix:'b'Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.'b'The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.'b''b''b'Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss\xe2\x80\x93Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.'b'To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.'b'In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777\xe2\x80\x931855), although it was known to Chinese mathematicians as early as 179 CE (see History section).'
Hseyin Tevfik Pasha

Giuseppe Peano
b'In 1925 Peano switched Chairs unofficially from Infinitesimal Calculus to Complementary Mathematics, a field which better suited his current style of mathematics. This move became official in 1931. Giuseppe Peano continued teaching at Turin University until the day before he died, when he suffered a fatal heart attack.'b'During the years 1913\xe2\x80\x931918, Peano published several papers that dealt with the remainder term for various numerical quadrature formulas, and introduced the Peano kernel.[4]'b'After his mother died in 1910, Peano divided his time between teaching, working on texts aimed for secondary schooling including a dictionary of mathematics, and developing and promoting his and other auxiliary languages, becoming a revered member of the international auxiliary language movement. He used his membership of the Accademia dei Lincei to present papers written by friends and colleagues who were not members (the Accademia recorded and published all presented papers given in sessions).'b"Also in 1908, Peano took over the chair of higher analysis at Turin (this appointment was to last for only two years). He was elected the director of Academia pro Interlingua. Having previously created Idiom Neutral, the Academy effectively chose to abandon it in favor of Peano's Latino sine flexione."b'The year 1908 was important for Peano. The fifth and final edition of the Formulario project, titled Formulario Mathematico, was published. It contained 4200 formulae and theorems, all completely stated and most of them proved. The book received little attention since much of the content was dated by this time. However, it remains a significant contribution to mathematical literature. The comments and examples were written in Latino sine flexione.'b'In 1903 Peano announced his work on an international auxiliary language called Latino sine flexione ("Latin without inflexion," later called Interlingua, and the precursor of the Interlingua of the IALA). This was an important project for him (along with finding contributors for \'Formulario\'). The idea was to use Latin vocabulary, since this was widely known, but simplify the grammar as much as possible and remove all irregular and anomalous forms to make it easier to learn. In one speech, he started speaking in Latin and, as he described each simplification, introduced it into his speech so that by the end he was talking in his new language.'b'By 1901, Peano was at the peak of his mathematical career. He had made advances in the areas of analysis, foundations and logic, made many contributions to the teaching of calculus and also contributed to the fields of differential equations and vector analysis. Peano played a key role in the axiomatization of mathematics and was a leading pioneer in the development of mathematical logic. Peano had by this stage become heavily involved with the Formulario project and his teaching began to suffer. In fact, he became so determined to teach his new mathematical symbols that the calculus in his course was neglected. As a result, he was dismissed from the Royal Military Academy but retained his post at Turin University.'b'Peano\'s students Mario Pieri and Alessandro Padoa had papers presented at the philosophy congress also. For the mathematical congress, Peano did not speak, but Padoa\'s memorable presentation has been frequently recalled. A resolution calling for the formation of an "international auxiliary language" to facilitate the spread of mathematical (and commercial) ideas, was proposed; Peano fully supported it.'b'Paris was the venue for the Second International Congress of Mathematicians in 1900. The conference was preceded by the First International Conference of Philosophy where Peano was a member of the patronage committee. He presented a paper which posed the question of correctly formed definitions in mathematics, i.e. "how do you define a definition?". This became one of Peano\'s main philosophical interests for the rest of his life. At the conference Peano met Bertrand Russell and gave him a copy of Formulario. Russell was so struck by Peano\'s innovative logical symbols that he left the conference and returned home to study Peano\'s text.'b'In 1898 he presented a note to the Academy about binary numeration and its ability to be used to represent the sounds of languages. He also became so frustrated with publishing delays (due to his demand that formulae be printed on one line) that he purchased a printing press.'b'In 1890 Peano founded the journal Rivista di Matematica, which published its first issue in January 1891.[3] In 1891 Peano started the Formulario Project. It was to be an "Encyclopedia of Mathematics", containing all known formulae and theorems of mathematical science using a standard notation invented by Peano. In 1897, the first International Congress of Mathematicians was held in Z\xc3\xbcrich. Peano was a key participant, presenting a paper on mathematical logic. He also started to become increasingly occupied with Formulario to the detriment of his other work.'b"In 1887, Peano married Carola Crosio, the daughter of the Turin-based painter Luigi Crosio, known for painting the Refugium Peccatorum Madonna.[2] In 1886, he began teaching concurrently at the Royal Military Academy, and was promoted to Professor First Class in 1889. In that year he published the Peano axioms, a formal foundation for the collection of natural numbers. The next year, the University of Turin also granted him his full professorship. Peano's famous space-filling curve appeared in 1890 as a counterexample. He used it to show that a continuous curve cannot always be enclosed in an arbitrarily small region. This was an early example of what came to be known as a fractal."b"Peano was born and raised on a farm at Spinetta, a hamlet now belonging to Cuneo, Piedmont, Italy. He attended the Liceo classico Cavour in Turin, and enrolled at the University of Turin in 1876, graduating in 1880 with high honors, after which the University employed him to assist first Enrico D'Ovidio, and then Angelo Genocchi, the Chair of calculus. Due to Genocchi's poor health, Peano took over the teaching of calculus course within two years. His first major work, a textbook on calculus, was published in 1884 and was credited to Genocchi. A few years later, Peano published his first book dealing with mathematical logic. Here the modern symbols for the union and intersection of sets appeared for the first time.[1]"b''b''b'Giuseppe Peano (Italian:\xc2\xa0[d\xca\x92u\xcb\x88z\xc9\x9bppe pe\xcb\x88a\xcb\x90no]; 27 August 1858 \xe2\x80\x93 20 April 1932) was an Italian mathematician and glottologist. The author of over 200 books and papers, he was a founder of mathematical logic and set theory, to which he contributed much notation. The standard axiomatization of the natural numbers is named the Peano axioms in his honor. As part of this effort, he made key contributions to the modern rigorous and systematic treatment of the method of mathematical induction. He spent most of his career teaching mathematics at the University of Turin. He also wrote an international auxiliary language, the \'Latin sine flexione\' ("Latin without inflections"), which is a simplified version of the Classical Latin. Most of his books and papers are in Latin sine flexione, other ones are in Italian.'
Abstract algebra
b'In physics, groups are used to represent symmetry operations, and the usage of group theory could simplify differential equations. In gauge theory, the requirement of local symmetry can be used to deduce the equations describing a system. The groups that describe those symmetries are Lie groups, and the study of Lie groups and Lie algebras reveals much about the physical system; for instance, the number of force carriers in a theory is equal to the dimension of the Lie algebra, and these bosons interact with the force they mediate if the Lie algebra is nonabelian.[2]'b"Because of its generality, abstract algebra is used in many fields of mathematics and science. For instance, algebraic topology uses algebraic objects to study topologies. The Poincar\xc3\xa9 conjecture, proved in 2003, asserts that the fundamental group of a manifold, which encodes information about connectedness, can be used to determine whether a manifold is a sphere or not. Algebraic number theory studies various number rings that generalize the set of integers. Using tools of algebraic number theory, Andrew Wiles proved Fermat's Last Theorem."b'Examples involving several operations include:'b'Examples of algebraic structures with a single binary operation are:'b'By abstracting away various amounts of detail, mathematicians have defined various algebraic structures that are used in many areas of mathematics. For instance, almost all systems studied are sets, to which the theorems of set theory apply. Those sets that have a certain binary operation defined on them form magmas, to which the concepts concerning magmas, as well those concerning sets, apply. We can add additional constraints on the algebraic structure, such as associativity (to form semigroups); identity, and inverses (to form groups); and other more complex structures. With additional structure, more theorems could be proved, but the generality is reduced. The "hierarchy" of algebraic objects (in terms of generality) creates a hierarchy of the corresponding theories: for instance, the theorems of group theory may be used when studying rings (algebraic objects that have two binary operations with certain axioms) since a ring is a group over one of its operations. In general there is a balance between the amount of generality and the richness of the theory: more general structures have usually fewer nontrivial theorems and fewer applications.'b"These processes were occurring throughout all of mathematics, but became especially pronounced in algebra. Formal definition through primitive operations and axioms were proposed for many basic algebraic structures, such as groups, rings, and fields. Hence such things as group theory and ring theory took their places in pure mathematics. The algebraic investigations of general fields by Ernst Steinitz and of commutative and then general rings by David Hilbert, Emil Artin and Emmy Noether, building up on the work of Ernst Kummer, Leopold Kronecker and Richard Dedekind, who had considered ideals in commutative rings, and of Georg Frobenius and Issai Schur, concerning representation theory of groups, came to define abstract algebra. These developments of the last quarter of the 19th century and the first quarter of 20th century were systematically exposed in Bartel van der Waerden's Moderne algebra, the two-volume monograph published in 1930\xe2\x80\x931931 that forever changed for the mathematical world the meaning of the word algebra from the theory of equations to the theory of algebraic structures."b'The end of the 19th and the beginning of the 20th century saw a tremendous shift in the methodology of mathematics. Abstract algebra emerged around the start of the 20th century, under the name modern algebra. Its study was part of the drive for more intellectual rigor in mathematics. Initially, the assumptions in classical algebra, on which the whole of mathematics (and major parts of the natural sciences) depend, took the form of axiomatic systems. No longer satisfied with establishing properties of concrete objects, mathematicians started to turn their attention to general theory. Formal definitions of certain algebraic structures began to emerge in the 19th century. For example, results about various groups of permutations came to be seen as instances of general theorems that concern a general notion of an abstract group. Questions of structure and classification of various mathematical objects came to forefront.'b"The abstract notion of a group appeared for the first time in Arthur Cayley's papers in 1854. Cayley realized that a group need not be a permutation group (or even finite), and may instead consist of matrices, whose algebraic properties, such as multiplication and inverses, he systematically investigated in succeeding years. Much later Cayley would revisit the question whether abstract groups were more general than permutation groups, and establish that, in fact, any group is isomorphic to a group of permutations."b'The theory of permutation groups received further far-reaching development in the hands of Augustin Cauchy and Camille Jordan, both through introduction of new concepts and, primarily, a great wealth of results about special classes of permutation groups and even some general theorems. Among other things, Jordan defined a notion of isomorphism, still in the context of permutation groups and, incidentally, it was he who put the term group in wide use.'b'Note, however, that he got by without formalizing the concept of a group, or even of a permutation group. The next step was taken by \xc3\x89variste Galois in 1832, although his work remained unpublished until 1846, when he considered for the first time what is now called the closure property of a group of permutations, which he expressed as'b'Paolo Ruffini was the first person to develop the theory of permutation groups, and like his predecessors, also in the context of solving algebraic equations. His goal was to establish the impossibility of an algebraic solution to a general algebraic equation of degree greater than four. En route to this goal he introduced the notion of the order of an element of a group, conjugacy, the cycle decomposition of elements of permutation groups and the notions of primitive and imprimitive and proved some important theorems relating these concepts, such as'b"Permutations were studied by Joseph-Louis Lagrange in his 1770 paper R\xc3\xa9flexions sur la r\xc3\xa9solution alg\xc3\xa9brique des \xc3\xa9quations (Thoughts on the algebraic solution of equations) devoted to solutions of algebraic equations, in which he introduced Lagrange resolvents. Lagrange's goal was to understand why equations of third and fourth degree admit formulae for solutions, and he identified as key objects permutations of the roots. An important novel step taken by Lagrange in this paper was the abstract view of the roots, i.e. as symbols and not as numbers. However, he did not consider composition of permutations. Serendipitously, the first edition of Edward Waring's Meditationes Algebraicae (Meditations on Algebra) appeared in the same year, with an expanded version published in 1782. Waring proved the main theorem on symmetric functions, and specially considered the relation between the roots of a quartic equation and its resolvent cubic. M\xc3\xa9moire sur la r\xc3\xa9solution des \xc3\xa9quations (Memoire on the Solving of Equations) of Alexandre Vandermonde (1771) developed the theory of symmetric functions from a slightly different angle, but like Lagrange, with the goal of understanding solvability of algebraic equations."b"Leonhard Euler considered algebraic operations on numbers modulo an integer, modular arithmetic, in his generalization of Fermat's little theorem. These investigations were taken much further by Carl Friedrich Gauss, who considered the structure of multiplicative groups of residues mod n and established many properties of cyclic and more general abelian groups that arise in this way. In his investigations of composition of binary quadratic forms, Gauss explicitly stated the associative law for the composition of forms, but like Euler before him, he seems to have been more interested in concrete results than in general theory. In 1870, Leopold Kronecker gave a definition of an abelian group in the context of ideal class groups of a number field, generalizing Gauss's work; but it appears he did not tie his definition with previous work on groups, particularly permutation groups. In 1882, considering the same question, Heinrich M. Weber realized the connection and gave a similar definition that involved the cancellation property but omitted the existence of the inverse element, which was sufficient in his context (finite groups)."b'There were several threads in the early development of group theory, in modern language loosely corresponding to number theory, theory of equations, and geometry.'b'Numerous textbooks in abstract algebra start with axiomatic definitions of various algebraic structures and then proceed to establish their properties. This creates a false impression that in algebra axioms had come first and then served as a motivation and as a basis of further study. The true order of historical development was almost exactly the opposite. For example, the hypercomplex numbers of the nineteenth century had kinematic and physical motivations but challenged comprehension. Most theories that are now recognized as parts of algebra started as collections of disparate facts from various branches of mathematics, acquired a common theme that served as a core around which various results were grouped, and finally became unified on a basis of a common set of concepts. An archetypical example of this progressive synthesis can be seen in the history of group theory.'b'As in other parts of mathematics, concrete problems and examples have played important roles in the development of abstract algebra. Through the end of the nineteenth century, many\xc2\xa0\xe2\x80\x93 perhaps most\xc2\xa0\xe2\x80\x93 of these problems were in some way related to the theory of algebraic equations. Major themes include:'b''b''b'Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called variety of groups.'b'Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory is a formalism that allows a unified way for expressing properties and constructions that are similar for various structures.'b'In algebra, which is a broad division of mathematics, abstract algebra (occasionally called modern algebra) is the study of algebraic structures. Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras. The term abstract algebra was coined in the early 20th century to distinguish this area of study from the other parts of algebra.'
Quantum mechanics
b'Each term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.'b'and'b'with coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy via'b'and'b'The solutions are superpositions of left- and right-moving waves:'b'The potential in this case is given by:'b'This is another example illustrating the quantification of energy for bound states.'b'and the corresponding energy levels are'b'where Hn are the Hermite polynomials'b'This problem can either be treated by directly solving the Schr\xc3\xb6dinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given by'b'As in the classical case, the potential for the quantum harmonic oscillator is given by'b'This is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.'b'The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.'b'A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.'b'The quantization of energy levels follows from this constraint on k, since'b'in which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of \xcf\x80,'b'and D = 0. At x = L,'b'The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where \xcf\x88 must be zero. Thus, at x = 0,'b"or, from Euler's formula,"b'The general solutions of the Schr\xc3\xb6dinger equation for the particle in a box are'b'the previous equation is evocative of the classic kinetic energy analogue,'b'With the differential operator defined by'b"For example, consider a free particle. In quantum mechanics, a free matter is described by a wave function. The particle properties of the matter become apparent when we measure its position and velocity. The wave properties of the matter become apparent when we measure its wave properties like interference. The wave\xe2\x80\x93particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics. Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position\xe2\x80\x94or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.[85] In an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.[86]"b'Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures.[83] Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms.[84] Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.'b'While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement.[82] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.'b'Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.'b'A more distant goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.[81]'b"An inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.[80]"b'Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.'b'Many electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum mechanics is necessary to understanding and designing such electronic devices.'b'Many modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.'b'In many aspects modern technology operates at a scale where quantum effects are significant.'b'Quantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry. Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved.[79] Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.'b'Quantum mechanics has had enormous[78] success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).'b'The Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[76] This is not accomplished by introducing some "new axiom" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical - not just formally mathematical, as in other interpretations - quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett\'s interpretation is perfectly consistent with John Bell\'s experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these "parallel universes" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist\'s memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation.[77] Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.'b'Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.'b'John Bell showed that this "EPR" paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables.[75] Alain Aspect\'s initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement.'b'Albert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, "God does not play with dice".[74] He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein\xe2\x80\x93Podolsky\xe2\x80\x93Rosen paradox.'b'The Copenhagen interpretation \xe2\x80\x94 due largely to Niels Bohr and Werner Heisenberg \xe2\x80\x94 remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of "causality." It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.'b'Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born\'s basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics."[72] According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[73]'b'Another popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete. More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616\xc3\x9710\xe2\x88\x9235 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.'b'The quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or "quantum electromagnetism"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity,[69][70] has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field.[71] Beyond this "grand unification", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However\xc2\xa0\xe2\x80\x94 and while special relativity is parsimoniously incorporated into quantum electrodynamics\xc2\xa0\xe2\x80\x94 the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely "compactified" (or infinitely curved) and not readily amenable to measurement or probing.'b'Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th and 21st century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering G\xc3\xb6del\'s Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture "G\xc3\xb6del and the End of Physics" (2002).[68]'b"Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.[67]"b"Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this.[66] For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed."b'For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.[64][65]'b'In Niels Bohr\'s mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics.[54][55][56][57] The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or "state" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition.[58][59] In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous "state" in the classical sense of that word.[60][61] Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined.[58] Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schr\xc3\xb6dinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.[62][63]'b'A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.[53]'b'Quantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein\xe2\x80\x93Podolsky\xe2\x80\x93Rosen (EPR) paradox \xe2\x80\x94 an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism.[48] Quantum interference involves adding together probability amplitudes, whereas classical "waves" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems.[49] Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically.[50] This is in accordance with the following observations:'b'Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[45] According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles).[46] The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers.[47] However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.'b'Classical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.[44]'b'It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.'b'Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.[43]'b'When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.'b'The rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space\xe2\x80\x94although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This "high energy" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.'b'Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.[41] In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or "observables". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom).[42] An alternative formulation of quantum mechanics is Feynman\'s path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.'b'There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics - matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schr\xc3\xb6dinger).[40]'b'There exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the "semi-classical equation of motion" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.'b'The Schr\xc3\xb6dinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the "wave-like" behavior of quantum states. As it turns out, analytic solutions of the Schr\xc3\xb6dinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom\xe2\x80\x94which contains just one more electron than does the hydrogen atom\xe2\x80\x94has defied all attempts at a fully analytic treatment.'b'Some wave functions produce probability distributions that are constant, or independent of time\xe2\x80\x94such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).[39]'b"Wave functions change as time progresses. The Schr\xc3\xb6dinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schr\xc3\xb6dinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.[38]"b'During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.[36][37]'b'The time evolution of a quantum state is described by the Schr\xc3\xb6dinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that - given a wave function at an initial time - it makes a definite prediction of what the wave function will be at any later time.[35]'b'In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle\'s position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs); rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates). Usually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or "generalized" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process[33] that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates. For example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result.[29] It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.[34]'b'Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction.[30][31] Naturally, these probabilities will depend on the quantum state at the "instant" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable ("eigen" can be translated from German as meaning "inherent" or "characteristic").[32]'b'The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr\xe2\x80\x93Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of "wave function collapse" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[29]'b"According to one interpretation, as the result of a measurement the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable\xe2\x80\x94which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute."b'In the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space.[27] This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability, often referred to as "clouds", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg\'s uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.[28]'b"In the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac,[22] David Hilbert,[23] John von Neumann,[24] and Hermann Weyl,[25] the possible states of a quantum mechanical system are symbolized[26] as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space\xe2\x80\x94variously called the state space or the associated Hilbert space of the system\xe2\x80\x94that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system\xe2\x80\x94for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues."b'Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:'b'Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.'b'Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave\xe2\x80\x93particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.[21]'b'The word quantum derives from the Latin, meaning "how great" or "how much".[18] In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics.[19][better\xc2\xa0source\xc2\xa0needed] Some fundamental aspects of the theory are still actively studied.[20]'b'While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,[16] and superfluids.[17]'b"By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann[15] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.[citation needed]"b'It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave\xe2\x80\x93particle duality.[citation needed]'b"In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[citation needed]"b'The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schr\xc3\xb6dinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.'b'Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.[14]'b"Planck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself.[12] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[13] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work."b"where h is Planck's constant."b'According to Planck, each energy element (E) is proportional to its frequency (\xce\xbd):'b"Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld.[11] This phase is known as old quantum theory."b"Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887). Around 1900-1910, the atomic theory and the corpuscular theory of light[10] first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively."b"In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation,[9] known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics."b'In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck.[8] Planck\'s hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets) precisely matched the observed patterns of black-body radiation.'b'Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[7] In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.'b''b''b'Important applications of quantum theory[5] include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.[6]'b"Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schr\xc3\xb6dinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle."b'Classical physics (the physics existing before quantum mechanics) is a set of fundamental theories which describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that: energy, momentum and other quantities of a system may be restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to the precision with which quantities can be known (uncertainty principle).[note 1]'b'Quantum mechanics (QM; also known as quantum physics or quantum theory), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.[2]'
Special relativity
b'On the other hand, the existence of antiparticles leads to the conclusion that relativistic quantum mechanics is not enough for a more accurate and complete theory of particle interactions. Instead, a theory of particles interpreted as quantized fields, called quantum field theory, becomes necessary; in which particles can be created and destroyed throughout space and time.'b'In 1928, Paul Dirac constructed an influential relativistic wave equation, now known as the Dirac equation in his honour,[59] that is fully compatible both with special relativity and with the final version of quantum theory existing after 1926. This equation explained not only the intrinsic angular momentum of the electrons called spin, it also led to the prediction of the antiparticle of the electron (the positron),[59][60] and fine structure could only be fully explained with special relativity. It was the first foundation of relativistic quantum mechanics. In non-relativistic quantum mechanics, spin is phenomenological and cannot be explained.'b'The early Bohr\xe2\x80\x93Sommerfeld atomic model explained the fine structure of alkali metal atoms using both special relativity and the preliminary knowledge on quantum mechanics of the time.[58]'b'Special relativity can be combined with quantum mechanics to form relativistic quantum mechanics and Quantum electrodynamics. It is an unsolved problem in physics how general relativity and quantum mechanics can be unified; quantum gravity and a "theory of everything", which require a unification including general relativity too, are active and ongoing areas in theoretical research.'b'Despite the success of the Theory of Special Relativity, there are still detractors who insist on the existence of the aether. The basis for this is the experiment performed by Georges Sagnac that produced the Sagnac effect. However, this effect has been proven to reconcile with Special Relativity.'b'Particle accelerators routinely accelerate and measure the properties of particles moving at near the speed of light, where their behavior is completely consistent with relativity theory and inconsistent with the earlier Newtonian mechanics. These machines would simply not work if they were not engineered according to relativistic principles. In addition, a considerable number of modern experiments have been conducted to test special relativity. Some examples:'b"Several experiments predating Einstein's 1905 paper are now interpreted as evidence for relativity. Of these it is known Einstein was aware of the Fizeau experiment before 1905,[57] and historians have concluded that Einstein was at least aware of the Michelson\xe2\x80\x93Morley experiment as early as 1899 despite claims he made in his later years that it played no role in his development of the theory.[22]"b'Newtonian mechanics mathematically follows from special relativity at small velocities (compared to the speed of light) \xe2\x80\x93 thus Newtonian mechanics can be considered as a special relativity of slow moving bodies. See classical mechanics for a more detailed discussion.'b'Special relativity is mathematically self-consistent, and it is an organic part of all modern physical theories, most notably quantum field theory, string theory, and general relativity (in the limiting case of negligible gravitational fields).'b'Special relativity in its Minkowski spacetime is accurate only when the absolute value of the gravitational potential is much less than c2 in the region of interest.[55] In a strong gravitational field, one must use general relativity. General relativity becomes special relativity at the limit of a weak field. At very small scales, such as at the Planck length and below, quantum effects must be taken into consideration resulting in quantum gravity. However, at macroscopic scales and in the absence of strong gravitational fields, special relativity is experimentally tested to extremely high degree of accuracy (10\xe2\x88\x9220)[56] and thus accepted by the physics community. Experimental results which appear to contradict it are not reproducible and are thus widely believed to be due to experimental errors.'b"Maxwell's equations in the 3D form are already consistent with the physical content of special relativity, although they are easier to manipulate in a manifestly covariant form, i.e. in the language of tensor calculus.[54]"b"The Lorentz transformation of the electric field of a moving charge into a non-moving observer's reference frame results in the appearance of a mathematical term commonly called the magnetic field. Conversely, the magnetic field generated by a moving charge disappears and becomes a purely electrostatic field in a comoving frame of reference. Maxwell's equations are thus simply an empirical fit to special relativistic effects in a classical model of the Universe. As electric and magnetic fields are reference frame dependent and thus intertwined, one speaks of electromagnetic fields. Special relativity provides the transformation rules for how an electromagnetic field in one inertial frame appears in another inertial frame."b'Theoretical investigation in classical electromagnetism led to the discovery of wave propagation. Equations generalizing the electromagnetic effects found that finite propagation speed of the E and B fields required certain behaviors on charged particles. The general study of moving charges forms the Li\xc3\xa9nard\xe2\x80\x93Wiechert potential, which is a step towards special relativity.'b'In a continuous medium, the 3D density of force combines with the density of power to form a covariant 4-vector. The spatial part is the result of dividing the force on a small cell (in 3-space) by the volume of that cell. The time component is \xe2\x88\x921/c times the power transferred to that cell divided by the volume of the cell. This will be used below in the section on electromagnetism.'b'In the rest frame of the object, the time component of the four force is zero unless the "invariant mass" of the object is changing (this requires a non-closed system in which energy/mass is being directly added or removed from the object) in which case it is the negative of that rate of change of mass, times c. In general, though, the components of the four force are not equal to the components of the three-force, because the three force is defined by the rate of change of momentum with respect to coordinate time, i.e. dp/dt while the four force is defined by the rate of change of momentum with respect to proper time, i.e. dp/d\xcf\x84.'b"If a particle is not traveling at c, one can transform the 3D force from the particle's co-moving reference frame into the observer's reference frame. This yields a 4-vector called the four-force. It is the rate of change of the above energy momentum four-vector with respect to proper time. The covariant version of the four-force is:"b"To use Newton's third law of motion, both forces must be defined as the rate of change of momentum with respect to the same time coordinate. That is, it requires the 3D force defined above. Unfortunately, there is no tensor in 4D which contains the components of the 3D force vector among its components."b'Note that the mass of systems measured in their center of momentum frame (where total momentum is zero) is given by the total energy of the system in this frame. It may not be equal to the sum of individual system masses measured in other frames.'b'The rest energy is related to the mass according to the celebrated equation discussed above:'b'We see that the rest energy is an independent invariant. A rest energy can be calculated even for particles and systems in motion, by translating to a frame in which momentum is zero.'b"We can work out what this invariant is by first arguing that, since it is a scalar, it doesn't matter in which reference frame we calculate it, and then by transforming to a frame where the total momentum is zero."b'The invariant magnitude of the momentum 4-vector generates the energy\xe2\x80\x93momentum relation:'b'So in special relativity, the acceleration four-vector and the velocity four-vector are orthogonal.'b'which means all velocity four-vectors have a magnitude of c. This is an expression of the fact that there is no such thing as being at coordinate rest in relativity: at the least, you are always moving forward through time. Differentiating the above equation by \xcf\x84 produces:'b'The 4-velocity U\xce\xbc has an invariant form:'b'is an invariant. Notice that when the line element dX2 is negative that \xe2\x88\x9a\xe2\x88\x92dX2 is the differential of proper time, while when dX2 is positive, \xe2\x88\x9adX2 is differential of the proper distance.'b'so the squared length of the differential of the position four-vector dX\xce\xbc constructed using'b'The coordinate differentials transform also contravariantly:'b"similarly for higher order tensors. Invariant expressions, particularly inner products of 4-vectors with themselves, provide equations that are useful for calculations, because one doesn't need to perform Lorentz transformations to determine the invariants."b'One can extend this idea to tensors of higher order, for a second order tensor we can form the invariants:'b'Invariant means that it takes the same value in all inertial frames, because it is a scalar (0 rank tensor), and so no \xce\x9b appears in its trivial transformation. The magnitude of the 4-vector T is the positive square root of the inner product with itself:'b'The metric can be used for raising and lowering indices on vectors and tensors. Invariants can be constructed using the metric, the inner product of a 4-vector T with another 4-vector S is:'b'and this is the physical symmetry underlying special relativity.'b'The Poincar\xc3\xa9 group is the most general group of transformations which preserves the Minkowski metric:'b'The metric tensor allows one to define the inner product of two vectors, which in turn allows one to assign a magnitude to the vector. Given the four-dimensional nature of spacetime the Minkowski metric \xce\xb7 has components (valid in any inertial reference frame) which can be arranged in a 4 \xc3\x97 4 matrix:'b'The electromagnetic field tensor is another second order antisymmetric tensor field, with six components: three for the electric field and another three for the magnetic field. There is also the stress\xe2\x80\x93energy tensor for the electromagnetic field, namely the electromagnetic stress\xe2\x80\x93energy tensor.'b'An example of a four dimensional second order antisymmetric tensor is the relativistic angular momentum, which has six components: three are the classical angular momentum, and the other three are related to the boost of the center of mass of the system. The derivative of the relativistic angular momentum with respect to proper time is the relativistic torque, also second order antisymmetric tensor.'b'More generally, most physical quantities are best described as (components of) tensors. So to transform from one frame to another, we use the well-known tensor transformation law[53]'b'The postulates of special relativity constrain the exact form the Lorentz transformation matrices take.'b'More generally, the covariant components of a 4-vector transform according to the inverse Lorentz transformation:'b"only in Cartesian coordinates. It's the covariant derivative which transforms in manifest covariance, in Cartesian coordinates this happens to reduce to the partial derivatives, but not in other coordinates."b'that is:'b'The four-gradient of a scalar field \xcf\x86 transforms covariantly rather than contravariantly:'b'The transformation rules for three-dimensional velocities and accelerations are very awkward; even above in standard configuration the velocity equations are quite complicated owing to their non-linearity. On the other hand, the transformation of four-velocity and four-acceleration are simpler by means of the Lorentz transformation matrix.'b'The four-acceleration is the proper time derivative of 4-velocity:'b'where m is the invariant mass.'b'where the Lorentz factor is:'b'Examples of other 4-vectors include the four-velocity U\xce\xbc, defined as the derivative of the position 4-vector with respect to proper time:'b'where we define X0 = ct so that the time coordinate has the same dimension of distance as the other spatial dimensions; so that space and time are treated equally.[50][51][52] Now the transformation of the contravariant components of the position 4-vector can be compactly written as:'b'The simplest example of a four-vector is the position of an event in spacetime, which constitutes a timelike component ct and spacelike component x = (x, y, z), in a contravariant position four vector with components:'b'In Newtonian mechanics, quantities which have magnitude and direction are mathematically described as 3d vectors in Euclidean space, and in general they are parametrized by time. In special relativity, this notion is extended by adding the appropriate timelike quantity to a spacelike vector quantity, and we have 4d vectors, or "four vectors", in Minkowski spacetime. The components of vectors are written using tensor index notation, as this has numerous advantages. The notation makes it clear the equations are manifestly covariant under the Poincar\xc3\xa9 group, thus bypassing the tedious calculations to check this fact. In constructing such equations, we often find that equations previously thought to be unrelated are, in fact, closely connected being part of the same tensor equation. Recognizing other physical quantities as tensors simplifies their transformation laws. Throughout, upper indices (superscripts) are contravariant indices rather than exponents except when they indicate a square (this should be clear from the context), and lower indices (subscripts) are covariant indices. For simplicity and consistency with the earlier equations, Cartesian coordinates will be used.'b'The Lorentz transformation in standard configuration above, i.e. for a boost in the x direction, can be recast into matrix form as follows:'b'Above, the Lorentz transformation for the time coordinate and three space coordinates illustrates that they are intertwined. This is true more generally: certain pairs of "timelike" and "spacelike" quantities naturally combine on equal footing under the same Lorentz transformation.'b'Note that, in 4d spacetime, the concept of the center of mass becomes more complicated, see center of mass (relativistic).'b'The geometry of Minkowski space can be depicted using Minkowski diagrams, which are useful also in understanding many of the thought-experiments in special relativity.'b"The cone in the \xe2\x88\x92t region is the information that the point is 'receiving', while the cone in the +t section is the information that the point is 'sending'."b'so'b'If we extend this to three spatial dimensions, the null geodesics are the 4-dimensional cone:'b'\xe2\x80\x8awhich is the equation of a circle of radius c\xe2\x80\x89dt.'b'or simply'b'we see that the null geodesics lie along a dual-cone (see image right) defined by the equation;'b'If we reduce the spatial dimensions to 2, so that we can represent the physics in a 3D space'b'Some authors use X0 = t, with factors of c elsewhere to compensate; for instance, spatial coordinates are divided by c or factors of c\xc2\xb12 are included in the metric tensor.[49] These numerous conventions can be superseded by using natural units where c = 1. Then space and time have equivalent units, and no factors of c appear anywhere.'b'The actual form of ds above depends on the metric and on the choices for the X0 coordinate. To make the time coordinate look like the space coordinates, it can be treated as imaginary: X0 = ict (this is called a Wick rotation). According to Misner, Thorne and Wheeler (1971, \xc2\xa72.3), ultimately the deeper understanding of both special and general relativity will come from the study of the Minkowski metric (described below) and to take X0 = ct, rather than a "disguised" Euclidean metric using ict as the time coordinate.'b'where dX = (dX0, dX1, dX2, dX3) are the differentials of the four spacetime dimensions. This suggests a deep theoretical insight: special relativity is simply a rotational symmetry of our spacetime, analogous to the rotational symmetry of Euclidean space (see image right).[48] Just as Euclidean space uses a Euclidean metric, so spacetime uses a Minkowski metric. Basically, special relativity can be stated as the invariance of any spacetime interval (that is the 4D distance between any two events) when viewed from any inertial reference frame. All equations and effects of special relativity can be derived from this rotational symmetry (the Poincar\xc3\xa9 group) of Minkowski spacetime.'b'where dx = (dx1, dx2, dx3) are the differentials of the three spatial dimensions. In Minkowski geometry, there is an extra dimension with coordinate X0 derived from time, such that the distance differential fulfills'b'In 3D space, the differential of distance (line element) ds is defined by'b"Special relativity uses a 'flat' 4-dimensional Minkowski space\xc2\xa0\xe2\x80\x93 an example of a spacetime. Minkowski spacetime appears to be very similar to the standard 3-dimensional Euclidean space, but there is a crucial difference with respect to time."b'Therefore, if causality is to be preserved, one of the consequences of special relativity is that no information signal or material object can travel faster than light in vacuum. However, some "things" can still move faster than light. For example, the location where the beam of a search light hits the bottom of a cloud can move faster than light when the search light is turned rapidly.[45][46]'b"The interval AC in the diagram is 'space-like'; i.e., there is a frame of reference in which events A and C occur simultaneously, separated only in space. There are also frames in which A precedes C (as shown) and frames in which C precedes A. If it were possible for a cause-and-effect relationship to exist between events A and C, then paradoxes of causality would result. For example, if A was the cause, and C the effect, then there would be frames of reference in which the effect preceded the cause. Although this in itself won't give rise to a paradox, one can show[43][44] that faster than light signals can be sent back into one's own past. A causal paradox can then be constructed by sending the signal if and only if no signal was received previously."b"In diagram 2 the interval AB is 'time-like'; i.e., there is a frame of reference in which events A and B occur at the same location in space, separated only by occurring at different times. If A precedes B in that frame, then A precedes B in all frames. It is hypothetically possible for matter (or information) to travel from A to B, so there can be a causal relationship (with A the cause and B the effect)."b"where v(t) is the velocity at a time, t, a is the acceleration of 1g and t is the time as measured by people on Earth.[40] Therefore, after 1 year of accelerating at 9.81\xc2\xa0m/s2, the spaceship will be travelling at v = 0.77c relative to Earth. Time dilation will increase the travellers life span as seen from the reference frame of the Earth to 2.7 years, but his lifespan measured by a clock travelling with him will not change. During his journey, people on Earth will experience more time than he does. A 5-year round trip for him will take 6\xc2\xbd Earth years and cover a distance of over 6 light-years. A 20-year round trip for him (5 years accelerating, 5 decelerating, twice each) will land him back on Earth having travelled for 335 Earth years and a distance of 331 light years.[41] A full 40-year trip at 1 g will appear on Earth to last 58,000 years and cover a distance of 55,000 light years. A 40-year trip at 1.1 g will take 148,000 Earth years and cover about 140,000 light years. A one-way 28 year (14 years accelerating, 14 decelerating as measured with the astronaut's clock) trip at 1 g acceleration could reach 2,000,000 light-years to the Andromeda Galaxy.[41] This same time dilation is why a muon travelling close to c is observed to travel much further than c times its half-life (when at rest).[42]"b"Since one can not travel faster than light, one might conclude that a human can never travel farther from Earth than 40 light years if the traveler is active between the ages of 20 and 60. One would easily think that a traveler would never be able to reach more than the very few solar systems which exist within the limit of 20\xe2\x80\x9340 light years from the earth. But that would be a mistaken conclusion. Because of time dilation, a hypothetical spaceship can travel thousands of light years during the pilot's 40 active years. If a spaceship could be built that accelerates at a constant 1 g, it will, after a little less than a year, be travelling at almost the speed of light as seen from Earth. This is described by:"b"Einstein acknowledged the controversy over his derivation in his 1907 survey paper on special relativity. There he notes that it is problematic to rely on Maxwell's equations for the heuristic mass\xe2\x80\x93energy argument. The argument in his 1905 paper can be carried out with the emission of any massless particles, but the Maxwell equations are implicitly used to make it obvious that the emission of light in particular can be achieved only by doing work. To emit electromagnetic waves, all you have to do is shake a charged particle, and this is clearly doing work, so that the emission is of energy.[38][39]"b'The energy and momentum are properties of matter and radiation, and it is impossible to deduce that they form a four-vector just from the two basic postulates of special relativity by themselves, because these don\'t talk about matter or radiation, they only talk about space and time. The derivation therefore requires some additional physical reasoning. In his 1905 paper, Einstein used the additional principles that Newtonian mechanics should hold for slow velocities, so that there is one energy scalar and one three-vector momentum at slow velocities, and that the conservation law for energy and momentum is exactly true in relativity. Furthermore, he assumed that the energy of light is transformed by the same Doppler-shift factor as its frequency, which he had previously shown to be true based on Maxwell\'s equations.[1] The first of Einstein\'s papers on this subject was "Does the Inertia of a Body Depend upon its Energy Content?" in 1905.[35] Although Einstein\'s argument in this paper is nearly universally accepted by physicists as correct, even self-evident, many authors over the years have suggested that it is wrong.[36] Other authors suggest that the argument was merely inconclusive because it relied on some implicit assumptions.[37]'b'Mass\xe2\x80\x93energy equivalence is a consequence of special relativity. The energy and momentum, which are separate in Newtonian mechanics, form a four-vector in relativity, and this relates the time component (the energy) to the space components (the momentum) in a non-trivial way. For an object at rest, the energy\xe2\x80\x93momentum four-vector is (E/c, 0, 0, 0): it has a time component which is the energy, and three space components which are zero. By changing frames with a Lorentz transformation in the x direction with a small value of the velocity v, the energy momentum four-vector becomes (E/c, Ev/c2, 0, 0). The momentum is equal to the energy multiplied by the velocity divided by c2. As such, the Newtonian mass of an object, which is the ratio of the momentum to the velocity for slow velocities, is equal to E/c2.'b'In addition to the papers referenced above\xe2\x80\x94which give derivations of the Lorentz transformation and describe the foundations of special relativity\xe2\x80\x94Einstein also wrote at least four papers giving heuristic arguments for the equivalence (and transmutability) of mass and energy, for E = mc2.'b'The energy content of an object at rest with mass m equals mc2. Conservation of energy implies that, in any reaction, a decrease of the sum of the masses of particles must be accompanied by an increase in kinetic energies of the particles after the reaction. Similarly, the mass of an object can be increased by taking in kinetic energies.'b"As an object's speed approaches the speed of light from an observer's point of view, its relativistic mass increases thereby making it more and more difficult to accelerate it from within the observer's frame of reference."b"The orientation of an object (i.e. the alignment of its axes with the observer's axes) may be different for different observers. Unlike other relativistic effects, this effect becomes quite significant at fairly low velocities as can be seen in the spin of moving particles."b'The usual example given is that of a train (frame S\xe2\x80\xb2 above) traveling due east with a velocity v with respect to the tracks (frame S). A child inside the train throws a baseball due east with a velocity u\xe2\x80\xb2 with respect to the train. In nonrelativistic physics, an observer at rest on the tracks will measure the velocity of the baseball (due east) as u = u\xe2\x80\xb2 + v, while in special relativity this is no longer true; instead the velocity of the baseball (due east) is given by the second equation: u = (u\xe2\x80\xb2 + v)/(1 + u\xe2\x80\xb2v/c2). Again, there is nothing special about the x or east directions. This formalism applies to any direction by considering parallel and perpendicular components of motion to the direction of relative velocity v, see main article for details.'b'Notice that if the object were moving at the speed of light in the S system (i.e. u = c), then it would also be moving at the speed of light in the S\xe2\x80\xb2 system. Also, if both u and v are small with respect to the speed of light, we will recover the intuitive Galilean transformation of velocities'b'The other frame S will measure:'b'Velocities (speeds) do not simply add. If the observer in S measures an object moving along the x axis at velocity u, then the observer in the S\xe2\x80\xb2 system, a frame of reference moving at velocity v in the x direction with respect to S, will measure the object moving with velocity u\xe2\x80\xb2 where (from the Lorentz transformations above):'b'This shows that the length (\xce\x94x\xe2\x80\xb2) of the rod as measured in the frame in which it is moving (S\xe2\x80\xb2), is shorter than its length (\xce\x94x) in its own rest frame (S).'b'Similarly, suppose a measuring rod is at rest and aligned along the x-axis in the unprimed system S. In this system, the length of this rod is written as \xce\x94x. To measure the length of this rod in the system S\xe2\x80\xb2, in which the rod is moving, the distances x\xe2\x80\xb2 to the end points of the rod must be measured simultaneously in that system S\xe2\x80\xb2. In other words, the measurement is characterized by \xce\x94t\xe2\x80\xb2 = 0, which can be combined with the fourth equation to find the relation between the lengths \xce\x94x and \xce\x94x\xe2\x80\xb2:'b'The dimensions (e.g., length) of an object as measured by one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage).'b"This shows that the time (\xce\x94t\xe2\x80\xb2) between the two ticks as seen in the frame in which the clock is moving (S\xe2\x80\xb2), is longer than the time (\xce\x94t) between these ticks as measured in the rest frame of the clock (S). Time dilation explains a number of physical phenomena; for example, the lifetime of muons produced by cosmic rays impinging on the Earth's atmosphere is measured to be greater than the lifetimes of muons measured in the laboratory.[34]"b'Suppose a clock is at rest in the unprimed system S. The location of the clock on two different ticks is then characterized by \xce\x94x = 0. To find the relation between the times between these ticks as measured in both systems, the first equation can be used to find:'b"The time lapse between two events is not invariant from one observer to another, but is dependent on the relative speeds of the observers' reference frames (e.g., the twin paradox which concerns a twin who flies off in a spaceship traveling near the speed of light and returns to discover that his or her twin sibling has aged much more)."b'it is clear that two events that are simultaneous in frame S (satisfying \xce\x94t = 0), are not necessarily simultaneous in another inertial frame S\xe2\x80\xb2 (satisfying \xce\x94t\xe2\x80\xb2 = 0). Only if these events are additionally co-local in frame S (satisfying \xce\x94x = 0), will they be simultaneous in another frame S\xe2\x80\xb2.'b'From the first equation of the Lorentz transformation in terms of coordinate differences'b'Two events happening in two different locations that occur simultaneously in the reference frame of one inertial observer, may occur non-simultaneously in the reference frame of another inertial observer (lack of absolute simultaneity).'b'The consequences of special relativity can be derived from the Lorentz transformation equations.[33] These transformations, and hence special relativity, lead to different physical predictions than those of Newtonian mechanics when relative velocities become comparable to the speed of light. The speed of light is so much larger than anything humans encounter that some of the effects predicted by relativity are initially counterintuitive.'b'Another example where visual appearance is at odds with measurement comes from the observation of apparent superluminal motion in various radio galaxies, BL Lac objects, quasars, and other astronomical objects that eject relativistic-speed jets of matter at narrow angles with respect to the viewer. An optical illusion results giving the appearance of faster than light travel.[29][30][31] In Fig.\xc2\xa01\xe2\x80\x9114, galaxy M87 streams out a high-speed jet of subatomic particles almost directly towards us, but Penrose\xe2\x80\x93Terrell rotation causes the jet to appear to be moving laterally in the same manner that the appearance of the cube in Fig.\xc2\xa01\xe2\x80\x9113 has been stretched out.[32]'b"Fig.\xc2\xa01\xe2\x80\x9113 illustrates a cube viewed from a distance of four times the length of its sides. At high speeds, the sides of the cube that are perpendicular to the direction of motion appear hyperbolic in shape. The cube is actually not rotated. Rather, light from the rear of the cube takes longer to reach one's eyes compared with light from the front, during which time the cube has moved to the right. This illusion has come to be known as Terrell rotation or the Terrell\xe2\x80\x93Penrose effect.[note 1]"b''b"For many years, the distinction between the two had not been generally appreciated. For example, it had generally been thought that a length contracted object passing by an observer would in fact actually be seen as length contracted. In 1959, James Terrell and Roger Penrose independently pointed out that differential time lag effects in signals reaching the observer from the different parts of a moving object result in a fast moving object's visual appearance being quite different from its measured shape. For example, a receding object would appear contracted, an approaching object would appear elongated, and a passing object would have a skew appearance that has been likened to a rotation.[24][25][26][27] A sphere in motion retains the appearance of a sphere, although images on the surface of the sphere will appear distorted.[28]"b'That being said, scientists make a fundamental distinction between measurement or observation on the one hand, versus visual appearance, or what one sees.'b'Time dilation and length contraction are not optical illusions, but genuine effects. Measurements of these effects are not an artifact of Doppler shift, nor are they the result of neglecting to take into account the time it takes light to travel from an event to an observer.'b'These effects are explicitly related to our way of measuring time intervals between events which occur at the same place in a given coordinate system (called "co-local" events). These time intervals will be different in another coordinate system moving with respect to the first, unless the events are also simultaneous. Similarly, these effects also relate to our measured distances between separated but simultaneous events in a given coordinate system of choice. If these events are not co-local, but are separated by distance (space), they will not occur at the same spatial distance from each other when seen from another moving coordinate system. However, the spacetime interval will be the same for all observers.'b'we get'b'Writing the Lorentz transformation and its inverse in terms of coordinate differences, where for instance one event has coordinates (x1, t1) and (x\xe2\x80\xb21, t\xe2\x80\xb21), another event has coordinates (x2, t2) and (x\xe2\x80\xb22, t\xe2\x80\xb22), and the differences are defined as'b'A quantity invariant under Lorentz transformations is known as a Lorentz scalar.'b'There is nothing special about the x-axis, the transformation can apply to the y or z axes, or indeed in any direction, which can be done by directions parallel to the motion (which are warped by the \xce\xb3 factor) and perpendicular; see main article for details.'b'is the Lorentz factor and c is the speed of light in vacuum, and the velocity v of S\xe2\x80\xb2 is parallel to the x-axis. The y and z coordinates are unaffected; only the x and t coordinates are transformed. These Lorentz transformations form a one-parameter group of linear mappings, that parameter being called rapidity.'b'where'b'Define the event to have spacetime coordinates (t,x,y,z) in system S and (t\xe2\x80\xb2,x\xe2\x80\xb2,y\xe2\x80\xb2,z\xe2\x80\xb2) in a reference frame moving at a velocity v with respect to that frame, S\xe2\x80\xb2. Then the Lorentz transformation specifies that these coordinates are related in the following way:'b"Since there is no absolute reference frame in relativity theory, a concept of 'moving' doesn't strictly exist, as everything is always moving with respect to some other reference frame. Instead, any two frames that move at the same speed in the same direction are said to be comoving. Therefore, S and S\xe2\x80\xb2 are not comoving."b'Suppose we have a second reference frame S\xe2\x80\xb2, whose spatial axes and clock exactly coincide with that of S at time zero, but it is moving at a constant velocity v with respect to S along the x-axis.'b'In relativity theory we often want to calculate the position of a point from a different reference point.'b'For example, the explosion of a firecracker may be considered to be an "event". We can completely specify an event by its four spacetime coordinates: The time of occurrence and its 3-dimensional spatial location define a reference point. Let\'s call this reference frame S.'b'An event is an occurrence that can be assigned a single unique time and location in space relative to a reference frame: it is a "point" in spacetime. Since the speed of light is constant in relativity in each and every reference frame, pulses of light can be used to unambiguously measure distances and refer back the times that events occurred to the clock, even though light takes time to reach the clock after the event has transpired.'b"Reference frames play a crucial role in relativity theory. The term reference frame as used here is an observational perspective in space which is not undergoing any change in motion (acceleration), from which a position can be measured along 3 spatial axes. In addition, a reference frame has the ability to determine measurements of the time of events using a 'clock' (any reference device with uniform periodicity)."b'The principle of relativity, which states that physical laws have the same form in each inertial reference frame, dates back to Galileo, and was incorporated into Newtonian physics. However, in the late 19th century, the existence of electromagnetic waves led physicists to suggest that the universe was filled with a substance that they called "aether", which would act as the medium through which these waves, or vibrations travelled. The aether was thought to constitute an absolute reference frame against which speeds could be measured, and could be considered fixed and motionless. Aether supposedly possessed some wonderful properties: it was sufficiently elastic to support electromagnetic waves, and those waves could interact with matter, yet it offered no resistance to bodies passing through it. The results of various experiments, including the Michelson\xe2\x80\x93Morley experiment, led to the theory of special relativity, by showing that there was no aether.[23] Einstein\'s solution was to discard the notion of an aether and the absolute state of rest. In relativity, any reference frame moving with uniform motion will observe the same laws of physics. In particular, the speed of light in vacuum is always measured to be c, even when measured by multiple systems that are moving at different (but constant) velocities.'b"The constancy of the speed of light was motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous ether. There is conflicting evidence on the extent to which Einstein was influenced by the null result of the Michelson\xe2\x80\x93Morley experiment.[21][22] In any case, the null result of the Michelson\xe2\x80\x93Morley experiment helped the notion of the constancy of the speed of light gain widespread and rapid acceptance."b'From the principle of relativity alone without assuming the constancy of the speed of light (i.e. using the isotropy of space and the symmetry implied by the principle of special relativity) one can show that the spacetime transformations between inertial frames are either Euclidean, Galilean, or Lorentzian. In the Lorentzian case, one can then obtain relativistic interval conservation and a certain finite limiting speed. Experiments suggest that this speed is the speed of light in vacuum.[19][20]'b'Thus many modern treatments of special relativity base it on the single postulate of universal Lorentz covariance, or, equivalently, on the single postulate of Minkowski spacetime.[17][18]'b'Einstein consistently based the derivation of Lorentz invariance (the essential core of special relativity) on just the two basic principles of relativity and light-speed invariance. He wrote:'b"Many of Einstein's papers present derivations of the Lorentz transformation based upon these two principles.[16]"b'Henri Poincar\xc3\xa9 provided the mathematical framework for relativity theory by proving that Lorentz transformations are a subset of his Poincar\xc3\xa9 group of symmetry transformations. Einstein later derived these transformations from his axioms.'b"Following Einstein's original presentation of special relativity in 1905, many different sets of postulates have been proposed in various alternative derivations.[14] However, the most common set of postulates remains those employed by Einstein in his original paper. A more mathematical statement of the Principle of Relativity made later by Einstein, which introduces the concept of simplicity not mentioned above is:"b'The derivation of special relativity depends not only on these two explicit postulates, but also on several tacit assumptions (made in almost all theories of physics), including the isotropy and homogeneity of space and the independence of measuring rods and clocks from their past history.[13]'b'Einstein discerned two fundamental propositions that seemed to be the most assured, regardless of the exact validity of the (then) known laws of either mechanics or electrodynamics. These propositions were the constancy of the speed of light and the independence of physical laws (especially the constancy of the speed of light) from the choice of inertial system. In his initial presentation of special relativity in 1905 he expressed these postulates as:[1]'b"Galileo Galilei had already postulated that there is no absolute and well-defined state of rest (no privileged reference frames), a principle now called Galileo's principle of relativity. Einstein extended this principle so that it accounted for the constant speed of light,[10] a phenomenon that had been recently observed in the Michelson\xe2\x80\x93Morley experiment. He also postulated that it holds for all the laws of physics, including both the laws of mechanics and of electrodynamics.[11]"b'As Galilean relativity is now considered an approximation of special relativity that is valid for low speeds, special relativity is considered an approximation of general relativity that is valid for weak gravitational fields, i.e. at a sufficiently small scale and in conditions of free fall. Whereas general relativity incorporates noneuclidean geometry in order to represent gravitational effects as the geometric curvature of spacetime, special relativity is restricted to the flat spacetime known as Minkowski space. A locally Lorentz-invariant frame that abides by special relativity can be defined at sufficiently small scales, even in curved spacetime.'b'The theory is "special" in that it only applies in the special case where the curvature of spacetime due to gravity is negligible.[6][7] In order to include gravity, Einstein formulated general relativity in 1915. Special relativity, contrary to some outdated descriptions, is capable of handling accelerations as well as accelerated frames of reference.[8][9]'b'A defining feature of special relativity is the replacement of the Galilean transformations of Newtonian mechanics with the Lorentz transformations. Time and space cannot be defined separately from each other. Rather, space and time are interwoven into a single continuum known as spacetime. Events that occur at the same time for one observer can occur at different times for another.'b'Special relativity implies a wide range of consequences, which have been experimentally verified,[3] including length contraction, time dilation, relativistic mass, mass\xe2\x80\x93energy equivalence, a universal speed limit and relativity of simultaneity. It has replaced the conventional notion of an absolute universal time with the notion of a time that is dependent on reference frame and spatial position. Rather than an invariant time interval between two events, there is an invariant spacetime interval. Combined with other laws of physics, the two postulates of special relativity predict the equivalence of mass and energy, as expressed in the mass\xe2\x80\x93energy equivalence formula E\xc2\xa0=\xc2\xa0mc2, where c is the speed of light in a vacuum.[4][5]'b'Not until Einstein developed general relativity, to incorporate general (i.e., including accelerated) frames of reference and gravity, was the phrase "special relativity" employed. A translation that has often been used is "restricted relativity"; "special" really means "special case".[2]'b'It was originally proposed by Albert Einstein in a paper published 26 September 1905 titled "On the Electrodynamics of Moving Bodies".[1] The inconsistency of Newtonian mechanics with Maxwell\'s equations of electromagnetism and the lack of experimental confirmation for a hypothesized luminiferous aether led to the development of special relativity, which corrects mechanics to handle situations involving motions at a significant fraction of the speed of light (known as relativistic velocities). As of today, special relativity is the most accurate model of motion at any speed when gravitational effects are negligible. Even so, the Newtonian mechanics model is still useful (due to its simplicity and high accuracy) as an approximation at small velocities relative to the speed of light.'b"In physics, special relativity (SR, also known as the special theory of relativity or STR) is the generally accepted and experimentally well-confirmed physical theory regarding the relationship between space and time. In Albert Einstein's original pedagogical treatment, it is based on two postulates:"
Statistics
b'Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.'b'In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:'b'Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:'b'Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was "required learning" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.'b'Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on "experimental" and "empirical" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.'b'The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.'b"Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions."b'There are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.'b'"Applied statistics" comprises descriptive statistics and the application of inferential statistics.[57][58] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.'b'Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[56]'b'The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of "Type II" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[55]'b'The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher\'s most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[43][44][45][46] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher\'s linear discriminator and Fisher information.[47] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher\'s principle[48]). Nevertheless, A. W. F. Edwards has remarked that it is "probably the most celebrated argument in evolutionary biology".[48] (about the sex ratio), the Fisherian runaway,[49][50][51][52][53][54] a concept in sexual selection about a positive feedback runaway affect found in evolution.'b'Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which "is never proved or established, but is possibly disproved, in the course of experimentation".[41][42]'b"The modern field of statistics emerged in the late 19th and early 20th century in three stages.[36] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics\xe2\x80\x94height, weight, eyelash length among others.[37] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[38] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[39] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[40]"b'Its mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[35] The method of least squares was first described by Adrien-Marie Legendre in 1805.'b'Some scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[34] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.'b'Statistical methods date back at least to the 5th century BC.[citation needed]'b'The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)'b'To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[33]'b'Ways to avoid misuse of statistics include using proper diagrams and avoiding bias.[29] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[30] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[29] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[30] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[31] According to Huff, "The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism."[32]'b'There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[27] A mistrust and misunderstanding of statistics is associated with the quotation, "There are three kinds of lies: lies, damned lies, and statistics". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[27] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[28]'b'Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data\xe2\x80\x94which measures the extent to which a trend could be caused by random variation in the sample\xe2\x80\x94may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.'b'Misuse of statistics can produce subtle, but serious errors in description and interpretation\xe2\x80\x94subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.'b'Some well-known statistical tests and procedures are:'b'Some problems are usually associated with this framework (See criticism of hypothesis testing):'b'While in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.'b'Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.'b"The standard approach[22] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false."b'Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).'b'In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.'b'Most studies only sample part of a population, so results don\'t fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by "probability", that is as a Bayesian probability.'b'Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.[25]'b'Many statistical methods seek to minimize the residual sum of squares, and these are called "methods of least squares" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.'b'Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.'b'A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).'b'Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.'b'Working from a null hypothesis, two basic forms of error are recognized:'b'What statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.'b'The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence "beyond a reasonable doubt". However, "failure to reject H0" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not "prove" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.'b'Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[23][24]'b'This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.'b'Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.'b'Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.'b"A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value."b'Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.'b'A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.'b'Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[22] The population being examined is described by a probability distribution that may have unknown parameters.'b'The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. "The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer" (Hand, 2004, p.\xc2\xa082).[21]'b'Other categorizations have been proposed. For example, Mosteller and Tukey (1977)[17] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[18] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[19] van den Berg (1991).[20]'b'Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.'b'Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.'b'An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group.[16] A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.'b'Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[15]'b'The basic steps of a statistical experiment are:'b'A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data\xe2\x80\x94like natural experiments and observational studies[14]\xe2\x80\x94for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.'b'Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction\xe2\x80\x94inductively inferring from samples to the parameters of a larger or total population.'b'To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.'b'When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.'b'When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.'b'Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).'b'In applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal".'b'Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[12][13]'b'Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[8] or as a branch of mathematics.[9] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[10][11]'b'Some definitions are:'b'Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. In more recent years statistics has relied more on statistical software to produce tests such as descriptive analysis[5]'b'Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.'b'A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and synthetic data drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative").[4] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]'b"Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[3] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena."b'When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.'b'Statistics is a branch of mathematics dealing with the collection, analysis, interpretation, presentation, and organization of data.[1][2] In applying statistics to, for example, a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as "all people living in a country" or "every atom composing a crystal". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1] See glossary of probability and statistics.'
Algorithm
b'A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church\xe2\x80\x93Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.'b'Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church\xe2\x80\x93Turing thesis. But he did this in the following context (boldface in original):'b"Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of \xce\xbb-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and G\xc3\xb6del and their use of recursion in particular G\xc3\xb6del's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936\xe2\x80\x9337) in their mechanism-models of computation."b"J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):"b'A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:'b'"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:'b"Turing's reduction yields the following:"b'Turing\xe2\x80\x94his model of computation is now called a Turing machine\xe2\x80\x94begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[97]'b'Alan Turing\'s work[95] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing\'s biographer believed that Turing\'s use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by asking himself what was meant by calling a typewriter \'mechanical\'".[96] Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we[who?] might conjecture that all were influences.'b'His symbol space would be'b'Emil Post (1936) described the actions of a "computer" (human being) as follows:'b'Here is a remarkable coincidence[according to whom?] of two men not knowing each other but describing a process of men-as-computers working on computations\xe2\x80\x94and they yield virtually identical definitions.'b'Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser\'s \xce\xbb-calculus[85] a finely honed definition of "general recursion" from the work of G\xc3\xb6del acting on suggestions of Jacques Herbrand (cf. G\xc3\xb6del\'s Princeton lectures of 1934) and subsequent simplifications by Kleene.[86] Church\'s proof[87] that the Entscheidungsproblem was unsolvable, Emil Post\'s definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction.[88] Alan Turing\'s proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"[89]\xe2\x80\x94in effect almost identical to Post\'s "formulation", J. Barkley Rosser\'s definition of "effective method" in terms of "a machine".[90] S. C. Kleene\'s proposal of a precursor to "Church thesis" that he called "Thesis I",[91] and a few years later Kleene\'s renaming his Thesis "Church\'s Thesis"[92] and proposing "Turing\'s Thesis".[93]'b"The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali-Forti paradox (1897), the Russell paradox (1902\xe2\x80\x9303), and the Richard Paradox.[84] The resultant considerations led to Kurt G\xc3\xb6del's paper (1931)\xe2\x80\x94he specifically cites the paradox of the liar\xe2\x80\x94that completely reduces rules of recursion to numbers."b'But Heijenoort gives Frege (1879) this kudos: Frege\'s is "perhaps the most important single work ever written in logic. ... in which we see a " \'formula language\', that is a lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules".[83] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910\xe2\x80\x931913).'b'Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888\xe2\x80\x931889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano\'s The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".[82]'b'Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):'b'Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome\' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".[80]'b'Jacquard loom, Hollerith punch cards, telegraphy and telephony\xe2\x80\x94the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[79] By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.'b'This machine he displayed in 1870 before the Fellows of the Royal Society.[77] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon\'s abacus ... [And] [a]gain, corresponding to Prof. Jevons\'s logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".[78]'b'Logical machines 1870\xe2\x80\x94Stanley Jevons\' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] . . .". With this machine he could analyze a "syllogism or any other simple logical argument".[76]'b'The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular the verge escapement[73] that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[74] led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"\xe2\x80\x94the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century.[75] Lovelace is credited with the first creation of an algorithm intended for processing on a computer \xe2\x80\x93 Babbage\'s analytical engine, the first device considered a real Turing-complete computer instead of just a calculator \xe2\x80\x93 and is sometimes called "history\'s first programmer" as a result, though a full implementation of Babbage\'s second device would not be realized until decades after her lifetime.'b'The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz\'s notion of the calculus ratiocinator (ca 1680):'b'Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p.\xc2\xa016\xe2\x80\x9341). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post\xe2\x80\x93Turing machine computations.'b"Algorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus,[70][8]:Ch 9.2 and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[8]:Ch 9.1 Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[71]"b'Researcher, Andrew Tutt, argues that algorithms should be overseen by a specialist regulatory agency, similar to FDA. His academic work emphasizes that the rise of increasingly complex algorithms calls for the need to think about the effects of algorithms today. Due to the nature and complexity of algorithms, it will prove to be difficult to hold algorithms accountable under criminal law. Tutt recognizes that while some algorithms will be beneficial to help meet technological demand, others should not be used or sold if they fail to meet safety requirements. Thus, for Tutt, algorithms will require "closer forms of federal uniformity, expert judgment, political independence, and pre-market review to prevent the introduction of unacceptably dangerous algorithms into the market".[68] The issue of algorithmic accountability[69] (the responsibility of algorithm designers to provide evidence of potential or realised harms) is of particular relevance in the field of dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms (see Explainable AI).'b'Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).'b'Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys\' LZW patent.'b'The adjective "continuous" when applied to the word "algorithm" can mean:'b'Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.'b'Algorithms can be classified by the amount of time they need to complete compared to their input size:'b'Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.'b'Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.'b'For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:'b'Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories include many different types of algorithms. Some common paradigms are:'b'One way to classify algorithms is by implementation means.'b'There are various ways to classify algorithms, each with its own merits.'b'To illustrate the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[60] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[61] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.'b'Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization. Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[59]'b'The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.'b"Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays."b'It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.'b'The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B and R, S would require a detailed analysis.'b'The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm;[58] rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.'b'Can the algorithms be improved?: Once the programmer judges a program "fit" and "effective"\xe2\x80\x94that is, it computes the function intended by its author\xe2\x80\x94then the question becomes, can it be improved?'b'Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[57] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.'b'Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid\'s algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[55] Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.[56]'b'But exceptional cases must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).'b'Does an algorithm do what its author wants it to do? A few test cases usually suffice to confirm core functionality. One source[54] uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.'b'How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A \xe2\x86\x90 A \xe2\x88\x92 B, and a B \xe2\x89\xa4 A loop that computes B \xe2\x86\x90 B \xe2\x88\x92 A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend \xe2\x88\x92 Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.'b'The following version can be used with Object Oriented languages:'b'The following version of Euclid\'s algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more types of instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by \xe2\x86\x90.'b'DONE:'b'OUTPUT:'b"E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location."b'E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.'b'E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.'b'E0: [Ensure r \xe2\x89\xa5 s.]'b'INPUT:'b"The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2\xe2\x80\x934:"b"Only a few instruction types are required to execute Euclid's algorithm\xe2\x80\x94some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction."b'Euclid\'s original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers\' common measure is in fact the greatest.[53] While Nicomachus\' algorithm is the same as Euclid\'s, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus\' algorithm.'b"For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \xe2\x80\x9cproper\xe2\x80\x9d; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero)."b'Euclid\'s algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements.[50] Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s.[51] In modern words, remainder r = l \xe2\x88\x92 q\xc3\x97s, q being the quotient, or remainder r is the "modulus", the integer-fractional part left over after the division.[52]'b'(Quasi-)formal description: Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:'b'High-level description:'b'One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description English prose, as:'b'Canonical flowchart symbols[49]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The B\xc3\xb6hm\xe2\x80\x93Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures, are shown in the diagram.'b'Structured programming, canonical structures: Per the Church\xe2\x80\x93Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky\'s demonstrations, Turing completeness requires only four instruction types\xe2\x80\x94conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[45] Tausworthe augments the three B\xc3\xb6hm-Jacopini canonical structures:[46] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[47] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[48]'b'But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters".[44] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid\'s algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky\'s "decrement").'b'This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).'b'Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[42] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don\'t, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[43]'b'Minsky describes a more congenial variation of Lambek\'s "abacus" model in his "Very Simple Bases for Computability".[38] Minsky\'s machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF\xe2\x80\x93THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky\'s machine includes three assignment (replacement, substitution)[39] operations: ZERO (e.g. the contents of location replaced by 0: L \xe2\x86\x90 0), SUCCESSOR (e.g. L \xe2\x86\x90 L+1), and DECREMENT (e.g. L \xe2\x86\x90 L \xe2\x88\x92 1).[40] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.[41]'b'Computers (and computors), models of computation: A computer (or human "computor"[32]) is a restricted type of machine, a "discrete deterministic mechanical device"[33] that blindly follows its instructions.[34] Melzak\'s and Lambek\'s primitive models[35] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[36] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[37]'b"Unfortunately there may be a tradeoff between goodness (speed) and elegance (compactness)\xe2\x80\x94an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below."b'Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[31]'b'Chaitin prefaces his definition with: "I\'ll show you can\'t prove that a program is \'elegant\'"\xe2\x80\x94such a proof would solve the Halting problem (ibid).'b'"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:'b'In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.'b'Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.'b'For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.'b'Representations of algorithms can be classed into three accepted levels of Turing machine description:[28]'b'There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).'b'Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.'b'For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.'b'So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.'b'Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.'b'For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).'b'Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.'b"Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):"b'The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.'b'An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as y = m + n \xe2\x80\x93 two arbitrary "input variables" m and n that produce an output y. But various authors\' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):'b'Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word in the following quotation:'b'A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flow chart above and as an example in a later section.'b'An informal definition could be "a set of rules that precisely defines a sequence of operations."[18] which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[19]'b'The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.'b'which translates as:'b'Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:'b'In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn\'t until the late 19th century that "algorithm" took on the meaning that it has in modern English.'b'Al-Khw\xc4\x81rizm\xc4\xab (Persian: \xd8\xae\xd9\x88\xd8\xa7\xd8\xb1\xd8\xb2\xd9\x85\xdb\x8c\xe2\x80\x8e, c. 780\xe2\x80\x93850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means \'the native of Khwarezm\', a region that was part of Greater Iran and is now in Uzbekistan.[13][14] About 825, he wrote a treatise in the Arabic language on the Hindu\xe2\x80\x93Arabic numeral system, which was translated into Latin in the 12th century under the title Algoritmi de numero Indorum. This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator\'s Latinization of Al-Khwarizmi\'s name.[15] Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra.[16] In late medieval Latin, algorismus, English \'algorism\', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word \xe1\xbc\x80\xcf\x81\xce\xb9\xce\xb8\xce\xbc\xcf\x8c\xcf\x82 \'number\' (cf. \'arithmetic\'), the Latin word was altered to algorithmus, and the corresponding English term \'algorithm\' is first attested in the 17th century; the modern sense was introduced in the 19th century.[17]'b'The word \'algorithm\' probably has its roots in latinizing the name of Al-Khwarizmi in a first step to algorismus.[11][12]. There is no prior root for algorism- or algorit-, neither in Latin, nor in Greek. The word seems to have evolved as the name for the "new" mathematics coming to Europe, characterized by the use of the then emerging Hindu\xe2\x80\x93Arabic numerals, and was probably influenced by the Greek word arithmos, i.e. \xce\xb1\xcf\x81\xce\xb9\xce\xb8\xce\xbc\xcf\x8c\xcf\x82, meaning "number" (cf. arithmetics), leading to the "t" in algorithm.'b''b''b'The concept of algorithm has existed for centuries and the use of the concept can be ascribed to Greek mathematicians, e.g. the sieve of Eratosthenes and Euclid\'s algorithm;[8] the term algorithm itself derives from the 9th Century mathematician Mu\xe1\xb8\xa5ammad ibn M\xc5\xabs\xc4\x81 al\'Khw\xc4\x81rizm\xc4\xab, latinized \'Algoritmi\'. A partial formalization of what would become the modern notion of algorithm began with attempts to solve the Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability"[9] or "effective method";[10] those formalizations included the G\xc3\xb6del\xe2\x80\x93Herbrand\xe2\x80\x93Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church\'s lambda calculus of 1936, Emil Post\'s "Formulation 1" of 1936, and Alan Turing\'s Turing machines of 1936\xe2\x80\x937 and 1939.'b'An algorithm is an effective method that can be expressed within a finite amount of space and time[1] and in a well-defined formal language[2] for calculating a function.[3] Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well-defined successive states, eventually producing "output"[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[7]'
Hermann Grassmann
b'These philological accomplishments were honored during his lifetime; he was elected to the American Oriental Society and in 1876, he received an honorary doctorate from the University of T\xc3\xbcbingen.'b"Grassmann also discovered a sound law of Indo-European languages, which was named Grassmann's Law in his honor."b"Grassmann's mathematical ideas began to spread only towards the end of his life. 30 years after the publication of A1 the publisher wrote to Grassmann: \xe2\x80\x9cYour book Die Ausdehnungslehre has been out of print for some time. Since your work hardly sold at all, roughly 600 copies were used in 1864 as waste paper and the remaining few odd copies have now been sold out, with the exception of the one copy in our library\xe2\x80\x9d.[12] Disappointed by the reception of his work in mathematical circles, Grassmann lost his contacts with mathematicians as well as his interest in geometry. The last years of his life he turned to historical linguistics and the study of Sanskrit. He wrote books on German grammar, collected folk songs, and learned Sanskrit. He wrote a 2,000-page dictionary and a translation of the Rigveda (more than 1,000 pages) which earned him a membership of the American Orientalists' Society. In modern Rigvedic studies Grassmann's work is often cited. In 1955 the third edition of his dictionary to Rigveda was issued.[7]"b'Adh\xc3\xa9mar Jean Claude Barr\xc3\xa9 de Saint-Venant developed a vector calculus similar to that of Grassmann which he published in 1845. He then entered into a dispute with Grassmann about which of the two had thought of the ideas first. Grassmann had published his results in 1844, but Saint-Venant claimed that he had first developed these ideas in 1832.'b"For an introduction to the role of Grassmann's work in contemporary mathematical physics see The Road to Reality[11] by Roger Penrose."b"Comprehension of Grassmann awaited the concept of vector spaces which then could express the multilinear algebra of his extension theory. To establish the priority of Grassmann over Hamilton, Josiah Willard Gibbs urged Grassmann's heirs to have the 1840 essay on tides published.[10] A. N. Whitehead's first monograph, the Universal Algebra (1898), included the first systematic exposition in English of the theory of extension and the exterior algebra. With the rise of differential geometry the exterior algebra was applied to differential forms."b"In 1872 Victor Schlegel published the first part of his System der Raumlehre which used Grassmann's approach to derive ancient and modern results in plane geometry. Felix Klein wrote a negative review of Schlegel's book citing its incompleteness and lack of perspective on Grassmann. Schlegel followed in 1875 with a second part of his System according to Grassmann, this time developing higher geometry. Meanwhile, Klein was advancing his Erlangen Program which also expanded the scope of geometry.[9]"b"One of the first mathematicians to appreciate Grassmann's ideas during his lifetime was Hermann Hankel, whose 1867 Theorie der complexen Zahlensysteme"b"In 1840s, mathematicians were generally unprepared to understand Grassmann's ideas.[7] In the 1860s and 1870s various mathematicians came to ideas similar to that of Grassmann's, but Grassmann himself was not interested in mathematics anymore.[7]"b"In 1862, Grassmann published a thoroughly rewritten second edition of A1, hoping to earn belated recognition for his theory of extension, and containing the definitive exposition of his linear algebra. The result, Die Ausdehnungslehre: Vollst\xc3\xa4ndig und in strenger Form bearbeitet [The Theory of Extension, Thoroughly and Rigorously Treated], hereinafter denoted A2, fared no better than A1, even though A2's manner of exposition anticipates the textbooks of the 20th century."b'Grassmann (1861) set out the first axiomatic presentation of arithmetic, making free use of the principle of induction. Peano and his followers cited this work freely starting around 1890. Lloyd C. Kannenberg published an English translation of The Ausdehnungslehre and Other works in 1995 (ISBN\xc2\xa00-8126-9275-6. -- ISBN\xc2\xa00-8126-9276-4).'b"In 1853, Grassmann published a theory of how colors mix; it and its three color laws are still taught, as Grassmann's law. Grassmann's work on this subject was inconsistent with that of Helmholtz. Grassmann also wrote on crystallography, electromagnetism, and mechanics."b"In 1846, M\xc3\xb6bius invited Grassmann to enter a competition to solve a problem first proposed by Leibniz: to devise a geometric calculus devoid of coordinates and metric properties (what Leibniz termed analysis situs). Grassmann's Geometrische Analyse gekn\xc3\xbcpft an die von Leibniz erfundene geometrische Charakteristik,[6] was the winning entry (also the only entry). M\xc3\xb6bius, as one of the judges, criticized the way Grassmann introduced abstract notions without giving the reader any intuition as to why those notions were of value."b'A1 was a revolutionary text, too far ahead of its time to be appreciated. When Grassmann submitted it to apply for a professorship in 1847, the ministry asked Ernst Kummer for a report. Kummer assured that there were good ideas in it, but found the exposition deficient and advised against giving Grassmann a university position. Over the next 10-odd years, Grassmann wrote a variety of work applying his theory of extension, including his 1845 Neue Theorie der Elektrodynamik[5] and several papers on algebraic curves and surfaces, in the hope that these applications would lead others to take his theory seriously.'b'Following an idea of Grassmann\'s father, A1 also defined the exterior product, also called "combinatorial product" (in German: \xc3\xa4u\xc3\x9feres Produkt[3] or kombinatorisches Produkt[4]), the key operation of an algebra now called exterior algebra. (One should keep in mind that in Grassmann\'s day, the only axiomatic theory was Euclidean geometry, and the general notion of an abstract algebra had yet to be defined.) In 1878, William Kingdon Clifford joined this exterior algebra to William Rowan Hamilton\'s quaternions by replacing Grassmann\'s rule epep = 0 by the rule epep = 1. (For quaternions, we have the rule i2 = j2 = k2 = \xe2\x88\x921.) For more details, see exterior algebra.'b"Fearnley-Sander (1979) describes Grassmann's foundation of linear algebra as follows:"b'In 1844, Grassmann published his masterpiece, his Die Lineale Ausdehnungslehre, ein neuer Zweig der Mathematik[1] [The Theory of Linear Extension, a New Branch of Mathematics], hereinafter denoted A1 and commonly referred to as the Ausdehnungslehre,[2] which translates as "theory of extension" or "theory of extensive magnitudes." Since A1 proposed a new foundation for all of mathematics, the work began with quite general definitions of a philosophical nature. Grassmann then showed that once geometry is put into the algebraic form he advocated, the number three has no privileged role as the number of spatial dimensions; the number of possible dimensions is in fact unbounded.'b"One of the many examinations for which Grassmann sat required that he submit an essay on the theory of the tides. In 1840, he did so, taking the basic theory from Laplace's M\xc3\xa9canique c\xc3\xa9leste and from Lagrange's M\xc3\xa9canique analytique, but expositing this theory making use of the vector methods he had been mulling over since 1832. This essay, first published in the Collected Works of 1894\xe2\x80\x931911, contains the first known appearance of what is now called linear algebra and the notion of a vector space. He went on to develop those methods in his A1 and A2 (see references)."b'Grassmann had eleven children, seven of whom reached adulthood. A son, Hermann Ernst Grassmann, became a professor of mathematics at the University of Giessen.'b'Starting during the political turmoil in Germany, 1848\xe2\x80\x9349, Hermann and his brother Robert published a Stettin newspaper, Deutsche Wochenschrift f\xc3\xbcr Staat, Kirche und Volksleben, calling for German unification under a constitutional monarchy. (This eventuated in 1871.) After writing a series of articles on constitutional law, Hermann parted company with the newspaper, finding himself increasingly at odds with its political direction.'b'In 1847, he was made an "Oberlehrer" or head teacher. In 1852, he was appointed to his late father\'s position at the Stettin Gymnasium, thereby acquiring the title of Professor. In 1847, he asked the Prussian Ministry of Education to be considered for a university position, whereupon that Ministry asked Kummer for his opinion of Grassmann. Kummer wrote back saying that Grassmann\'s 1846 prize essay (see below) contained "... commendably good material expressed in a deficient form." Kummer\'s report ended any chance that Grassmann might obtain a university post. This episode proved the norm; time and again, leading figures of Grassmann\'s day failed to recognize the value of his mathematics.'b'In 1834 Grassmann began teaching mathematics at the Gewerbeschule in Berlin. A year later, he returned to Stettin to teach mathematics, physics, German, Latin, and religious studies at a new school, the Otto Schule. Over the next four years, Grassmann passed examinations enabling him to teach mathematics, physics, chemistry, and mineralogy at all secondary school levels.'b'Although lacking university training in mathematics, it was the field that most interested him when he returned to Stettin in 1830 after completing his studies in Berlin. After a year of preparation, he sat the examinations needed to teach mathematics in a gymnasium, but achieved a result good enough to allow him to teach only at the lower levels. Around this time, he made his first significant mathematical discoveries, ones that led him to the important ideas he set out in his 1844 paper referred to as A1 (see references).'b'Grassmann was an undistinguished student until he obtained a high mark on the examinations for admission to Prussian universities. Beginning in 1827, he studied theology at the University of Berlin, also taking classes in classical languages, philosophy, and literature. He does not appear to have taken courses in mathematics or physics.'b'Grassmann was the third of 12 children of Justus G\xc3\xbcnter Grassmann, an ordained minister who taught mathematics and physics at the Stettin Gymnasium, where Hermann was educated.'b''b''b'Hermann G\xc3\xbcnther Grassmann (German: Gra\xc3\x9fmann; April 15, 1809 \xe2\x80\x93 September 26, 1877) was a German polymath, known in his day as a linguist and now also as a mathematician. He was also a physicist, neohumanist, general scholar, and publisher. His mathematical work was little noted until he was in his sixties.'
James Joseph Sylvester
b'Sylvester House, a portion of an undergraduate dormitory at Johns Hopkins University, is named in his honor. Several professorships there are named in his honor also.'b'Sylvester invented a great number of mathematical terms such as "matrix" (in 1850),[6] "graph" (combinatorics)[7] and "discriminant".[8] He coined the term "totient" for Euler\'s totient function \xcf\x86(n).[9] His collected scientific work fills four volumes. In 1880, the Royal Society of London awarded Sylvester the Copley Medal, its highest award for scientific achievement; in 1901, it instituted the Sylvester Medal in his memory, to encourage mathematical research after his death in Oxford. In Discrete geometry he is remembered for Sylvester\'s Problem and a result on the orchard problem.'b'In 1883, he returned to England to take up the Savilian Professor of Geometry at Oxford University. He held this chair until his death, although in 1892 the University appointed a deputy professor to the same chair.'b'In 1876[4] Sylvester again crossed the Atlantic Ocean to become the inaugural professor of mathematics at the new Johns Hopkins University in Baltimore, Maryland. His salary was $5,000 (quite generous for the time), which he demanded be paid in gold. After negotiation, agreement was reached on a salary that was not paid in gold.[5] In 1878 he founded the American Journal of Mathematics. The only other mathematical journal in the US at that time was the Analyst, which eventually became the Annals of Mathematics.'b'In 1872, he finally received his B.A. and M.A. from Cambridge, having been denied the degrees due to his being a Jew.[1]'b"One of Sylvester's lifelong passions was for poetry; he read and translated works from the original French, German, Italian, Latin and Greek, and many of his mathematical papers contain illustrative quotes from classical poetry. Following his early retirement, Sylvester (1870) published a book entitled The Laws of Verse in which he attempted to codify a set of laws for prosody in poetry."b'On his return to England, he was hired in 1844 by the Equity and Law Life Assurance Society for which he developed successful actuarial models and served as de facto CEO, a position that required a law degree. As a result, he studied for the Bar, meeting a fellow British mathematician studying law, Arthur Cayley, with whom he made significant contributions to invariant theory and also matrix theory during a long collaboration.[3][incomplete short citation] He did not obtain a position teaching university mathematics until 1855, when he was appointed professor of mathematics at the Royal Military Academy, Woolwich, from which he retired in 1869, because the compulsory retirement age was 55. The Woolwich academy initially refused to pay Sylvester his full pension, and only relented after a prolonged public controversy, during which Sylvester took his case to the letters page of The Times.'b"Sylvester began his study of mathematics at St John's College, Cambridge in 1831,[1] where his tutor was John Hymers. Although his studies were interrupted for almost two years due to a prolonged illness, he nevertheless ranked second in Cambridge's famous mathematical examination, the tripos, for which he sat in 1837. However, Sylvester was not issued a degree, because graduates at that time were required to state their acceptance of the Thirty-Nine Articles of the Church of England, and Sylvester could not do so because he was Jewish. For the same reason, he was unable to compete for a Fellowship or obtain a Smith's prize.[2] In 1838, Sylvester became professor of natural philosophy at University College London and in 1839 a Fellow of the Royal Society of London. In 1841, he was awarded a BA and an MA by Trinity College, Dublin. In the same year he moved to the United States to become a professor of mathematics at the University of Virginia, but left after less than four months following a violent encounter with two students he had disciplined. He moved to New York City and began friendships with the Harvard mathematician Benjamin Peirce (father of Charles Sanders Peirce) and the Princeton physicist Joseph Henry. However, he left in November 1843 after being denied appointment as Professor of Mathematics at Columbia College (now University), again for his Judaism, and returned to England."b'Sylvester was born James Joseph in London, England. His father, Abraham Joseph, was a merchant. James adopted the surname Sylvester when his older brother did so upon emigration to the United States\xe2\x80\x94a country which at that time required all immigrants to have a given name, a middle name, and a surname. At the age of 14, Sylvester was a student of Augustus De Morgan at the University of London. His family withdrew him from the University after he was accused of stabbing a fellow student with a knife. Subsequently, he attended the Liverpool Royal Institution.'b''b''b'James Joseph Sylvester FRS (3 September 1814\xc2\xa0\xe2\x80\x93 15 March 1897) was an English mathematician. He made fundamental contributions to matrix theory, invariant theory, number theory, partition theory, and combinatorics. He played a leadership role in American mathematics in the later half of the 19th century as a professor at the Johns Hopkins University and as founder of the American Journal of Mathematics. At his death, he was professor at Oxford.'
Arthur Cayley
b'A number of mathematical terms are named after him:'b'An 1874 portrait of Cayley by Lowes Cato Dickinson and an 1884 portrait by William Longmaid are in the collection of Trinity College, Cambridge.[8]'b'Cayley is buried in the Mill Road cemetery, Cambridge.'b'The remainder of his papers were edited by Andrew Forsyth, his successor in the Sadleirian Chair. The Collected Mathematical papers number thirteen quarto volumes, and contain 967 papers. Cayley retained to the last his fondness for novel-reading and for travelling. He also took special pleasure in paintings and architecture, and he practiced water-colour painting, which he found useful sometimes in making mathematical diagrams.'b'In 1889 the Cambridge University Press requested him to prepare his mathematical papers for publication in a collected form\xe2\x80\x94a request which he appreciated very much. They are printed in magnificent quarto volumes, of which seven appeared under his own editorship. While editing these volumes, he was suffering from a painful internal malady, to which he succumbed on 26 January 1895, in the 74th year of his age. When the funeral took place, a great assemblage met in Trinity Chapel, comprising members of the University, official representatives of Russia and America, and many of the most illustrious philosophers of Britain.'b"In 1883 Cayley was President of the British Association for the Advancement of Science. The meeting was held at Southport, in the north of England. As the President's address is one of the great popular events of the meeting, and brings out an audience of general culture, it is usually made as little technical as possible. Cayley (1996) took for his subject the Progress of Pure Mathematics."b'In 1893 Cayley became a foreign member of the Royal Netherlands Academy of Arts and Sciences.[7]'b'In 1881 he received from the Johns Hopkins University, Baltimore, where Sylvester was then professor of mathematics, an invitation to deliver a course of lectures. He accepted the invitation, and lectured at Baltimore during the first five months of 1882 on the subject of the Abelian and Theta Functions.'b"In 1876 he published a Treatise on Elliptic Functions. He took great interest in the movement for the University education of women. At Cambridge the women's colleges are Girton and Newnham. In the early days of Girton College he gave direct help in teaching, and for some years he was chairman of the council of Newnham College, in the progress of which he took the keenest interest to the last."b'In addition to his work on algebra, Cayley made fundamental contributions to algebraic geometry. Cayley and Salmon discovered the 27 lines on a cubic surface. Cayley constructed the Chow variety of all curves in projective 3-space.[6] He founded the algebro-geometric theory of ruled surfaces.'b'The other duty of the chair \xe2\x80\x94 the advancement of mathematical science \xe2\x80\x94 was discharged in a handsome manner by the long series of memoirs that he published, ranging over every department of pure mathematics. But it was also discharged in a much less obtrusive way; he became the standing referee on the merits of mathematical papers to many societies both at home and abroad.'b'At first the teaching duty of the Sadleirian professorship was limited to a course of lectures extending over one of the terms of the academic year; but when the University was reformed about 1886, and part of the college funds applied to the better endowment of the University professors, the lectures were extended over two terms. For many years the attendance was small, and came almost entirely from those who had finished their career of preparation for competitive examinations; after the reform the attendance numbered about fifteen. The subject lectured on was generally that of the memoir on which the professor was for the time engaged.'b'At Cambridge University the ancient professorship of pure mathematics is denominated by the Lucasian, and is the chair that had been occupied by Isaac Newton. Around 1860, certain funds bequeathed by Lady Sadleir to the University, having become useless for their original purpose, were employed to establish another professorship of pure mathematics, called the Sadleirian. The duties of the new professor were defined to be "to explain and teach the principles of pure mathematics and to apply himself to the advancement of that science." To this chair Cayley was elected when 42 years old. He gave up a lucrative practice for a modest salary; but he never regretted the exchange, for the chair at Cambridge enabled him to end the divided allegiance between law and mathematics, and to devote his energies to the pursuit that he liked best. He at once married and settled down in Cambridge. More fortunate than Hamilton in his choice, his home life was one of great happiness. His friend and fellow investigator, Sylvester, once remarked that Cayley had been much more fortunate than himself; that they both lived as bachelors in London, but that Cayley had married and settled down to a quiet and peaceful life at Cambridge; whereas he had never married, and had been fighting the world all his days.'b"His friend J. J. Sylvester, his senior by five years at Cambridge, was then an actuary, resident in London; they used to walk together round the courts of Lincoln's Inn, discussing the theory of invariants and covariants. During this period of his life, extending over fourteen years, Cayley produced between two and three hundred papers.[4]"b"Because of the limited tenure of his fellowship it was necessary to choose a profession; like De Morgan, Cayley chose law, and at age 25 entered at Lincoln's Inn, London. He made a specialty of conveyancing. It was while he was a pupil at the bar examination that he went to Dublin to hear Hamilton's lectures on quaternions.[4]"b"Cayley's tutor at Cambridge was George Peacock and his private coach was William Hopkins. He finished his undergraduate course by winning the place of Senior Wrangler, and the first Smith's prize.[3] His next step was to take the M.A. degree, and win a Fellowship by competitive examination. He continued to reside at Cambridge University for four years; during which time he took some pupils, but his main work was the preparation of 28 memoirs to the Mathematical Journal."b'At the unusually early age of 17 Cayley began residence at Trinity College, Cambridge. The cause of the Analytical Society had now triumphed, and the Cambridge Mathematical Journal had been instituted by Gregory and Robert Leslie Ellis. To this journal, at the age of twenty, Cayley contributed three papers, on subjects that had been suggested by reading the M\xc3\xa9canique analytique of Lagrange and some of the works of Laplace.'b"Arthur Cayley was born in Richmond, London, England, on 16 August 1821. His father, Henry Cayley, was a distant cousin of Sir George Cayley, the aeronautics engineer innovator, and descended from an ancient Yorkshire family. He settled in Saint Petersburg, Russia, as a merchant. His mother was Maria Antonia Doughty, daughter of William Doughty. According to some writers she was Russian, but her father's name indicates an English origin. His brother was the linguist Charles Bagot Cayley. Arthur spent his first eight years in Saint Petersburg. In 1829 his parents were settled permanently at Blackheath, near London. Arthur was sent to a private school. At age 14 he was sent to King's College School. The school's master observed indications of mathematical genius and advised the father to educate his son not for his own business, as he had intended, but to enter the University of Cambridge."b''b''b'He postulated the Cayley\xe2\x80\x93Hamilton theorem\xe2\x80\x94that every square matrix is a root of its own characteristic polynomial, and verified it for matrices of order 2 and 3.[1] He was the first to define the concept of a group in the modern way\xe2\x80\x94as a set with a binary operation satisfying certain laws.[2] Formerly, when mathematicians spoke of "groups", they had meant permutation groups. Cayley\'s theorem is named in honour of Cayley.'b'As a child, Cayley enjoyed solving complex maths problems for amusement. He entered Trinity College, Cambridge, where he excelled in Greek, French, German, and Italian, as well as mathematics. He worked as a lawyer for 14 years.'b'Arthur Cayley F.R.S. (/\xcb\x88ke\xc9\xaali/; 16 August 1821 \xe2\x80\x93 26 January 1895) was a British mathematician. He helped found the modern British school of pure mathematics.'
Determinant
b'where \xcf\x89j is an nth root of 1.'b'where \xcf\x89 and \xcf\x892 are the complex cube roots of 1. In general, the nth-order circulant determinant is[33]'b'Third order'b'Second order'b'where the right-hand side is the continued product of all the differences that can be formed from the n(n\xe2\x88\x921)/2 pairs of numbers taken from x1, x2, \xe2\x80\xa6, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.'b'In general, the nth-order Vandermonde determinant is[33]'b'The third order Vandermonde determinant is'b'The Jacobian also occurs in the inverse function theorem.'b'Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function \xcf\x86: Rn \xe2\x86\x92 Rm is given by'b'the Jacobian matrix is the n \xc3\x97 n matrix whose entries are given by'b'For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. For'b'By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)\xc2\xb7|det(a \xe2\x88\x92 b, b \xe2\x88\x92 c, c \xe2\x88\x92 d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.'b'As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn \xe2\x86\x92 Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn \xe2\x86\x92 Rm is represented by the m \xc3\x97 n matrix A, then the n-dimensional volume of f(S) is given by:'b'More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 \xc3\x97 2 or 3 \xc3\x97 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.'b'The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is \xe2\x88\x921, the basis has the opposite orientation.'b'It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n\xe2\x88\x921 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.'b'As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 \xc3\x97 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), \xe2\x80\xa6, fn(x) (supposed to be n \xe2\x88\x92 1 times differentiable), the Wronskian is defined to be'b"The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises."b"The next important figure was Jacobi[23] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[31][32]"b"The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy\xe2\x80\x93Binet formula.) In this he used the word determinant in its present sense,[28][29] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[22][30] With him begins the theory in its generality."b'Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.'b'It was Vandermonde (1771) who first recognized determinants as independent functions.[22] Laplace (1772)[26][27] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.'b'In Japan, Seki Takakazu (\xe9\x96\xa2 \xe5\xad\x9d\xe5\x92\x8c) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by B\xc3\xa9zout (1764).'b'Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (\xe4\xb9\x9d\xe7\xab\xa0\xe7\xae\x97\xe8\xa1\x93, Chinese scholars, around the 3rd century BCE). In Europe, 2 \xc3\x97 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[22][23][24][25]'b"Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[20] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[21]"b"Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form."b'If two matrices of order n can be multiplied in time M(n), where M(n) \xe2\x89\xa5 na for some a > 2, then the determinant can be computed in time O(M(n)).[19] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith\xe2\x80\x93Winograd algorithm.'b'Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[18] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.'b'If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.'b'(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant\xc2\xa01, in which case the formula further simplifies to'b'The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:'b'Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)'b"Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n \xc3\x97 n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants."b'Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.'b"The permanent of a matrix is defined as the determinant, except that the factors sgn(\xcf\x83) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule."b'Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]'b'For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonn\xc3\xa9 determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.'b'Another infinite-dimensional notion of determinant is the functional determinant.'b'The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula'b'For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.'b'For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo\xc2\xa0m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo\xc2\xa0m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (\xe2\x8b\x85)\xc3\x97 (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,'b'holds. In other words, the following diagram commutes:'b'between the group of invertible n \xc3\x97 n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R \xe2\x86\x92 S, there is a map GLn(f): GLn(R) \xe2\x86\x92 GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identity'b'The determinant defines a mapping'b'This definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or \xe2\x88\x921. Such a matrix is called unimodular.'b'This fact also implies that every other n-linear alternating function F: Mn(K) \xe2\x86\x92 K satisfies'b'for any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.'b'from the set of all n \xc3\x97 n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that is'b'The determinant can also be characterized as the unique function'b'The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T\xe2\x80\xb2 on W, where for each w in W we define (T\xe2\x80\xb2w)(x1, \xe2\x80\xa6, xn) = w(Tx1, \xe2\x80\xa6, Txn). As a linear transformation on a one-dimensional space, T\xe2\x80\xb2 is equivalent to a scalar multiple. We call this scalar the determinant of T.'b'For this reason, the highest non-zero exterior power \xce\x9bn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms \xce\x9bkV with k < n.'b'This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 \xe2\x88\xa7 v2 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn to v2 \xe2\x88\xa7 v1 \xe2\x88\xa7 v3 \xe2\x88\xa7 \xe2\x80\xa6 \xe2\x88\xa7 vn, say, also changes its sign.'b'As \xce\x9bnV is one-dimensional, the map \xce\x9bnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to say'b'The determinant of a linear transformation A\xc2\xa0: V \xe2\x86\x92 V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power \xce\x9bnV of V. A induces a linear map'b'for some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.'b'The determinant is therefore also called a similarity invariant. The determinant of a linear transformation'b'The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X\xe2\x88\x921BX. Indeed, repeatedly applying the above identities yields'b'This identity is used in describing the tangent space of certain matrix Lie groups.'b'Yet another equivalent formulation is'b'Expressed in terms of the entries of A, these are'b'where adj(A) denotes the adjugate of A. In particular, if A is invertible, we have'b"By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn \xc3\x97 n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]"b'When D is a 1\xc3\x971 matrix, B is a column vector, and C is a row vector then'b'When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)'b'Generally, if all pairs of n \xc3\x97 n matrices of the np \xc3\x97 np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p \xc3\x97 p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.'b'When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 \xc3\x97 2 matrix holds:[12]'b'as can be seen by employing the decomposition'b'When A is invertible, one has'b'This can be seen from the Leibniz formula, or from a decomposition like (for the former case)'b'Suppose A, B, C, and D are matrices of dimension n \xc3\x97 n, n \xc3\x97 m, m \xc3\x97 n, and m \xc3\x97 m, respectively. Then'b"It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition."b'where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.'b"the solution is given by Cramer's rule:"b'For a matrix equation'b'These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.'b'Also,'b'with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.'b'For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinant'b'is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).'b'where I is the identity matrix. More generally, if'b'An important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,'b'This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way as'b'The formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l \xe2\x80\x93 1)! tr(Al) as'b'where the sum is taken over the set of all integers kl \xe2\x89\xa5 0 satisfying the equation'b'In the general case, this may also be obtained from[9]'b"cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev\xe2\x80\x93LeVerrier algorithm. That is, for generic n, detA = (\xe2\x88\x92)nc0 the signed constant term of the characteristic polynomial, determined recursively from"b'For example, for n = 2, n = 3, and n = 4, respectively,'b'the determinant of A is given by'b'Here exp(A) denotes the matrix exponential of A, because every eigenvalue \xce\xbb of A corresponds to the eigenvalue exp(\xce\xbb) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfying'b'or, for real matrices A,'b'The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,'b'being positive, for all k between 1 and n.'b"A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices"b'where I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).'b'Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equation'b'The product of all non-zero eigenvalues is referred to as pseudo-determinant.'b'From this general result several consequences follow.'b'where Im and In are the m \xc3\x97 m and n \xc3\x97 n identity matrices, respectively.'b"Sylvester's determinant theorem states that for A, an m \xc3\x97 n matrix, and B, an n \xc3\x97 m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):"b"In terms of the adjugate matrix, Laplace's expansion can be written as[7]"b'The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,'b'However, Laplace expansion is efficient for small matrices only.'b'along the second column (j = 2 and the sum runs over i) is given by,'b'Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 \xc3\x97 3 matrix'b"Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n\xe2\x88\x921) \xc3\x97 (n\xe2\x88\x921)-matrix that results from A by removing the i-th row and the j-th column. The expression (\xe2\x88\x921)i+jMi,j is known as a cofactor. The determinant of A is given by"b'In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.'b'The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by'b'Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value\xc2\xa01 on the identity matrix, since the function Mn(K) \xe2\x86\x92 K that maps M \xe2\x86\xa6 det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy\xe2\x80\x93Binet formula, which also provides an independent proof of the multiplicative property.'b'The determinant of a matrix product of square matrices equals the product of their determinants:'b'Here, B is obtained from A by adding \xe2\x88\x921/2\xc3\x97the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = \xe2\x88\x92det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (\xe2\x88\x922) \xc2\xb7 2 \xc2\xb7 4.5 = \xe2\x88\x9218. Therefore, det(A) = \xe2\x88\x92det(D) = +18.'b'can be computed using the following matrices:'b'For example, the determinant of'b'Property 5 says that the determinant on n \xc3\x97 n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property\xc2\xa06; this is essentially the method of Gaussian elimination.'b'Property 2 above implies that properties for columns have their counterparts in terms of rows:'b'Properties 1, 8 and 10 \xe2\x80\x94 which all follow from the Leibniz formula \xe2\x80\x94 completely characterize the determinant; in other words the determinant is the unique function from n \xc3\x97 n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property\xc2\xa09) or else \xc2\xb11 (by properties 1 and\xc2\xa012 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n \xe2\x89\xa5 2,[6] so there is no good definition of the determinant in this setting.'b'A number of additional properties relate to the effects on the determinant of changing particular rows or columns:'b'This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.'b'The determinant has many properties. Some basic properties of determinants are'b'where now each ir and each jr should be summed over 1, \xe2\x80\xa6, n.'b'or using two epsilon symbols as'b'For example, the determinant of a 3 \xc3\x97 3 matrix A (n = 3) is'b'is notation for the product of the entries at positions (i, \xcf\x83i), where i ranges from 1 to n:'b'Here the sum is computed over all permutations \xcf\x83 of the set {1, 2, \xe2\x80\xa6, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering \xcf\x83 is denoted by \xcf\x83i. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to \xcf\x83 = [2, 3, 1], with \xcf\x831 = 2, \xcf\x832 = 3, and \xcf\x833 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation \xcf\x83, sgn(\xcf\x83) denotes the signature of \xcf\x83, a value that is +1 whenever the reordering given by \xcf\x83 can be achieved by successively interchanging two entries an even number of times, and \xe2\x88\x921 whenever it can be achieved by an odd number of such interchanges.'b'The Leibniz formula for the determinant of an n \xc3\x97 n matrix A is'b'The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.'b'The rule of Sarrus is a mnemonic for the 3 \xc3\x97 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 \xc3\x97 3 matrix does not carry over into higher dimensions.'b'which is the Leibniz formula for the determinant of a 3 \xc3\x97 3 matrix.'b'this can be expanded out to give'b'The Laplace formula for the determinant of a 3 \xc3\x97 3 matrix is'b'The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) \xe2\x88\xa7 (c, d)) is the signed area, which is also the determinant ad \xe2\x88\x92 bc.[3]'b'Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.'b"To show that ad \xe2\x88\x92 bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sin\xce\xb8 for the angle \xce\xb8 between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a\xe2\x8a\xa5 = (-b, a), such that |a\xe2\x8a\xa5||b|cos\xce\xb8' , which can be determined by the pattern of the scalar product to be equal to ad \xe2\x88\x92 bc:"b'The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).'b'The absolute value of ad \xe2\x88\x92 bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)'b'If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.'b'The Leibniz formula for the determinant of a 2 \xc3\x97 2 matrix is'b'The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:'b'The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.'b'Assume A is a square matrix with n rows and n columns, so that it can be written as'b'Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is \xe2\x88\x921 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n \xc3\x97 n matrix contributes n! terms), so it will first be given explicitly for the case of 2 \xc3\x97 2 matrices and 3 \xc3\x97 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.'b'where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]'b'Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n \xc3\x97 n matrix A in terms of its column vectors'b'There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 \xc3\x97 4 matrix:'b''b''b'When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.'b'Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.'b'Each determinant of a 2 \xc3\x97 2 matrix in this equation is called a "minor" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 \xc3\x97 4 matrix, the determinant of a 5 \xc3\x97 5 matrix, and so forth.'b'Similarly, suppose we have a 3 \xc3\x97 3 matrix A, and we want the specific formula for its determinant |A|:'b'In the case of a 2 \xc3\x97 2 matrix the specific formula for the determinant is:'b'In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.'
System of linear equations
b'This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.'b'Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.'b'Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described as'b'There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:'b'These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. Numerical solutions to a homogeneous system can be found with a singular value decomposition.'b'Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) \xe2\x89\xa0 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:'b'where A is an m \xc3\x97 n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.'b'A homogeneous system is equivalent to a matrix equation of the form'b'A system of linear equations is homogeneous if all of the constant terms are zero:'b'There is also a quantum algorithm for linear systems of equations.[3]'b'A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.'b'If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.'b'While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.'b"Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]"b'For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.'b'is given by'b"Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the system"b'The last matrix is in reduced row echelon form, and represents the system x = \xe2\x88\x9215, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.'b'There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:'b'Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.'b'This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:'b'In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:'b'Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = \xe2\x88\x9215. Therefore, the solution set is the single point (x, y, z) = (\xe2\x88\x9215, 8, 2).'b'Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:'b'Solving the first equation for x gives x = 5 + 2z \xe2\x88\x92 3y, and plugging this into the second and third equation yields'b'For example, consider the following system:'b'The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:'b'Here x is the free variable, and y and z are dependent.'b'Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:'b'Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.'b'Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.'b'The solution set to this system can be described by the following equations:'b'For example, consider the following system:'b'To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.'b'There are several algorithms for solving a system of linear equations.'b'Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.'b'Putting it another way, according to the Rouch\xc3\xa9\xe2\x80\x93Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.'b'In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.'b'are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.'b'It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equations'b'are inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.'b'For example, the equations'b'A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.'b'are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.'b'For a more complicated example, the equations'b'are not independent \xe2\x80\x94 they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.'b'For example, the equations'b'The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.'b'A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.'b'It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point).'b'The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.'b'The following pictures illustrate this trichotomy in the case of two variables:'b'In the first case, the dimension of the solution set is, in general, equal to n \xe2\x88\x92 m, where n is the number of variables and m is the number of equations.'b'In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.'b'For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.'b'For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]'b'For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.'b'A linear system may behave in any one of three possible ways:'b'A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.'b'The number of vectors in a basis for the span is now expressed as the rank of the matrix.'b'where A is an m\xc3\x97n matrix, x is a column vector with n entries, and b is a column vector with m entries.'b'The vector equation is equivalent to a matrix equation of the form'b'This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.'b'One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.'b'Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.'b'A general system of m linear equations with n unknowns can be written as'b'Now substitute this expression for x into the bottom equation:'b'The simplest kind of linear system involves two equations and two variables:'b''b''b'Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gr\xc3\xb6bner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.'b'In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.'b'since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.'b'is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by'b'In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1] For example,'
Gottfried Wilhelm Leibniz
b"Six important collections of English translations are Wiener (1951), Parkinson (1966), Loemker (1969), Ariew and Garber (1989), Woolhouse and Francks (1998), and Strickland (2006). The ongoing critical edition of all of Leibniz's writings is S\xc3\xa4mtliche Schriften und Briefe.[132]"b'The year given is usually that in which the work was completed, not of its eventual publication.'b'The systematic cataloguing of all of Leibniz\'s Nachlass began in 1901. It was hampered by two world wars and decades of German division in two states with the cold war\'s "iron curtain" in between, separating scholars, and also scattering portions of his literary estates. The ambitious project has had to deal with seven languages contained in some 200,000 pages of written and printed paper. In 1985 it was reorganized and included in a joint program of German federal and state (L\xc3\xa4nder) academies. Since then the branches in Potsdam, M\xc3\xbcnster, Hanover and Berlin have jointly published 57 volumes of the critical edition, with an average of 870 pages, and prepared index and concordance works.'b"The extant parts of the critical edition[132] of Leibniz's writings are organized as follows:"b'Leibniz mainly wrote in three languages: scholastic Latin, French and German. During his lifetime, he published many pamphlets and scholarly articles, but only two "philosophical" books, the Combinatorial Art and the Th\xc3\xa9odic\xc3\xa9e. (He published numerous pamphlets, often anonymous, on behalf of the House of Brunswick-L\xc3\xbcneburg, most notably the "De jure suprematum" a major consideration of the nature of sovereignty.) One substantial book appeared posthumously, his Nouveaux essais sur l\'entendement humain, which Leibniz had withheld from publication after the death of John Locke. Only in 1895, when Bodemann completed his catalogue of Leibniz\'s manuscripts and correspondence, did the enormous extent of Leibniz\'s Nachlass become clear: about 15,000 letters to more than 1000 recipients plus more than 40,000 other items. Moreover, quite a few of these letters are of essay length. Much of his vast correspondence, especially the letters dated after 1700, remains unpublished, and much of what is published has been so only in recent decades. The amount, variety, and disorder of Leibniz\'s writings are a predictable result of a situation he described in a letter as follows:'b"The collection of manuscript papers of Leibniz at the Gottfried Wilhelm Leibniz Bibliothek \xe2\x80\x93 Nieders\xc3\xa4chische Landesbibliothek were inscribed on UNESCO's Memory of the World Register in 2007.[130]"b'In 1985, the German government created the Leibniz Prize, offering an annual award of 1.55\xc2\xa0million euros for experimental results and 770,000 euros for theoretical ones. It was the worlds largest prize for scientific achievement prior to the Fundamental Physics Prize.'b'Nicholas Jolley has surmised that Leibniz\'s reputation as a philosopher is now perhaps higher than at any time since he was alive.[129] Analytic and contemporary philosophy continue to invoke his notions of identity, individuation, and possible worlds. Work in the history of 17th- and 18th-century ideas has revealed more clearly the 17th-century "Intellectual Revolution" that preceded the better-known Industrial and commercial revolutions of the 18th and 19th centuries.'b"In 1900, Bertrand Russell published a critical study of Leibniz's metaphysics.[128] Shortly thereafter, Louis Couturat published an important study of Leibniz, and edited a volume of Leibniz's heretofore unpublished writings, mainly on logic. They made Leibniz somewhat respectable among 20th-century analytical and linguistic philosophers in the English-speaking world (Leibniz had already been of great influence to many Germans such as Bernhard Riemann). For example, Leibniz's phrase salva veritate, meaning interchangeability without loss of or compromising the truth, recurs in Willard Quine's writings. Nevertheless, the secondary literature on Leibniz did not really blossom until after World War II. This is especially true of English speaking countries; in Gregory Brown's bibliography fewer than 30 of the English language entries were published before 1946. American Leibniz studies owe much to Leroy Loemker (1904\xe2\x80\x931985) through his translations and his interpretive essays in LeClerc (1973)."b"Leibniz's long march to his present glory began with the 1765 publication of the Nouveaux Essais, which Kant read closely. In 1768, Louis Dutens edited the first multi-volume edition of Leibniz's writings, followed in the 19th century by a number of editions, including those edited by Erdmann, Foucher de Careil, Gerhardt, Gerland, Klopp, and Mollat. Publication of Leibniz's correspondence with notables such as Antoine Arnauld, Samuel Clarke, Sophia of Hanover, and her daughter Sophia Charlotte of Hanover, began."b"Much of Europe came to doubt that Leibniz had discovered calculus independently of Newton, and hence his whole work in mathematics and physics was neglected. Voltaire, an admirer of Newton, also wrote Candide at least in part to discredit Leibniz's claim to having discovered calculus and Leibniz's charge that Newton's theory of universal gravitation was incorrect.[citation needed]"b'When Leibniz died, his reputation was in decline. He was remembered for only one book, the Th\xc3\xa9odic\xc3\xa9e,[126] whose supposed central argument Voltaire lampooned in his popular book Candide, which concludes with the character Candide saying, "Non liquet" (it is not clear), a term that was applied during the Roman Republic to a legal verdict of "not proven". Voltaire\'s depiction of Leibniz\'s ideas was so influential that many believed it to be an accurate description. Thus Voltaire and his Candide bear some of the blame for the lingering failure to appreciate and understand Leibniz\'s ideas. Leibniz had an ardent disciple, Christian Wolff, whose dogmatic and facile outlook did Leibniz\'s reputation much harm. He also influenced David Hume who read his Th\xc3\xa9odic\xc3\xa9e and used some of his ideas.[127] In any event, philosophical fashion was moving away from the rationalism and system building of the 17th century, of which Leibniz had been such an ardent proponent. His work on law, diplomacy, and history was seen as of ephemeral interest. The vastness and richness of his correspondence went unrecognized.'b"Leibniz also wrote a short paper, Primae veritates, first published by Louis Couturat in 1903 (pp.\xc2\xa0518\xe2\x80\x93523)[124] summarizing his views on metaphysics. The paper is undated; that he wrote it while in Vienna in 1689 was determined only in 1999, when the ongoing critical edition finally published Leibniz's philosophical writings for the period 1677\xe2\x80\x9390.[125] Couturat's reading of this paper was the launching point for much 20th-century thinking about Leibniz, especially among analytic philosophers. But after a meticulous study of all of Leibniz's philosophical writings up to 1688\xe2\x80\x94a study the 1999 additions to the critical edition made possible\xe2\x80\x94Mercer (2001) begged to differ with Couturat's reading; the jury is still out."b'While making his grand tour of European archives to research the Brunswick family history that he never completed, Leibniz stopped in Vienna between May 1688 and February 1689, where he did much legal and diplomatic work for the Brunswicks. He visited mines, talked with mine engineers, and tried to negotiate export contracts for lead from the ducal mines in the Harz mountains. His proposal that the streets of Vienna be lit with lamps burning rapeseed oil was implemented. During a formal audience with the Austrian Emperor and in subsequent memoranda, he advocated reorganizing the Austrian economy, reforming the coinage of much of central Europe, negotiating a Concordat between the Habsburgs and the Vatican, and creating an imperial research library, official archive, and public insurance fund. He wrote and published an important paper on mechanics.'b'Leibniz\'s attraction to Chinese philosophy originates from his perception that Chinese philosophy was similar to his own.[122] The historian E.R. Hughes suggests that Leibniz\'s ideas of "simple substance" and "pre-established harmony" were directly influenced by Confucianism, pointing to the fact that they were conceived during the period that he was reading Confucius Sinarum Philosophus.[122]'b'Leibniz was perhaps the first major European intellectual to take a close interest in Chinese civilization, which he knew by corresponding with, and reading other works by, European Christian missionaries posted in China. Having read Confucius Sinarum Philosophus on the first year of its publication,[122] he concluded that Europeans could learn much from the Confucian ethical tradition. He mulled over the possibility that the Chinese characters were an unwitting form of his universal characteristic. He noted with fascination how the I Ching hexagrams correspond to the binary numbers from 000000 to 111111, and concluded that this mapping was evidence of major Chinese accomplishments in the sort of philosophical mathematics he admired.[123]'b'He published the princeps editio (first modern edition) of the late medieval Chronicon Holtzatiae, a Latin chronicle of the County of Holstein.'b'Leibniz the philologist was an avid student of languages, eagerly latching on to any information about vocabulary and grammar that came his way. He refuted the belief, widely held by Christian scholars in his day, that Hebrew was the primeval language of the human race. He also refuted the argument, advanced by Swedish scholars in his day, that a form of proto-Swedish was the ancestor of the Germanic languages. He puzzled over the origins of the Slavic languages, was aware of the existence of Sanskrit, and was fascinated by classical Chinese.'b"Leibniz devoted considerable intellectual and diplomatic effort to what would now be called ecumenical endeavor, seeking to reconcile first the Roman Catholic and Lutheran churches, and later the Lutheran and Reformed churches. In this respect, he followed the example of his early patrons, Baron von Boyneburg and the Duke John Frederick\xe2\x80\x94both cradle Lutherans who converted to Catholicism as adults\xe2\x80\x94who did what they could to encourage the reunion of the two faiths, and who warmly welcomed such endeavors by others. (The House of Brunswick remained Lutheran because the Duke's children did not follow their father.) These efforts included corresponding with the French bishop Jacques-B\xc3\xa9nigne Bossuet, and involved Leibniz in some theological controversy. He evidently thought that the thoroughgoing application of reason would suffice to heal the breach caused by the Reformation."b'But at the same time, he arrived to propose an interreligious and multicultural project to create a universal system of justice, which required from him a broad interdisciplinary perspective. In order to propose it, he combined linguistics, especially sinology, moral and law philosophy, management, economics, and politics.[120]'b'In 1677, Leibniz called for a European confederation, governed by a council or senate, whose members would represent entire nations and would be free to vote their consciences;[119] this is sometimes considered an anticipation of the European Union. He believed that Europe would adopt a uniform religion. He reiterated these proposals in 1715.'b"While Leibniz was no apologist for absolute monarchy like Hobbes, or for tyranny in any form, neither did he echo the political and constitutional views of his contemporary John Locke, views invoked in support of democracy, in 18th-century America and later elsewhere. The following excerpt from a 1695 letter to Baron J. C. Boyneburg's son Philipp is very revealing of Leibniz's political sentiments:"b"With the possible exception of Marcus Aurelius, no philosopher has ever had as much experience with practical affairs of state as Leibniz. Leibniz's writings on law, ethics, and politics[116] were long overlooked by English-speaking scholars, but this has changed of late.[117]"b'Leibniz emphasized that research was a collaborative endeavor. Hence he warmly advocated the formation of national scientific societies along the lines of the British Royal Society and the French Academie Royale des Sciences. More specifically, in his correspondence and travels he urged the creation of such societies in Dresden, Saint Petersburg, Vienna, and Berlin. Only one such project came to fruition; in 1700, the Berlin Academy of Sciences was created. Leibniz drew up its first statutes, and served as its first President for the remainder of his life. That Academy evolved into the German Academy of Sciences, the publisher of the ongoing critical edition of his works.[115]'b'He called for the creation of an empirical database as a way to further all sciences. His characteristica universalis, calculus ratiocinator, and a "community of minds"\xe2\x80\x94intended, among other things, to bring political and religious unity to Europe\xe2\x80\x94can be seen as distant unwitting anticipations of artificial languages (e.g., Esperanto and its rivals), symbolic logic, even the World Wide Web.'b'Later in Leibniz\xe2\x80\x99s career (after the death of von Boinburg), Leibniz moved to Paris and accepted a position as a librarian in the Hanoverian court of Johann Friedrich, Duke of Brunswick-Luneburg. Leibniz\xe2\x80\x99s predecessor, Tobias Fleischer, had already created a cataloging system for the Duke\xe2\x80\x99s library but it was a clumsy attempt. At this library, Leibniz focused more on advancing the library than on the cataloging. For instance, within a month of taking the new position, he developed a comprehensive plan to expand the library. He was one of the first to consider developing a core collection for a library and felt \xe2\x80\x9cthat a library for display and ostentation is a luxury and indeed superfluous, but a well-stocked and organized library is important and useful for all areas of human endeavor and is to be regarded on the same level as schools and churches\xe2\x80\x9d.[114] Unfortunately, Leibniz lacked the funds to develop the library in this manner. After working at this library, by the end of 1690 Leibnez was appointed as privy-councilor and librarian of the Bibliotheca Augusta at Wolfenbuettel. It was an extensive library with at least 25,946 printed volumes[114]. At this library, Leibniz sought to improve the catalog. He was not allowed to make complete changes to the existing closed catalog, but was allowed to improve upon it so he started on that task immediately. He created an alphabetical author catalog and had also created other cataloging methods that were not implemented. While serving as librarian of the ducal libraries in Hanover and Wolfenbuettel, Leibniz effectively became one of the founders of library science. He also designed a book indexing system in ignorance of the only other such system then extant, that of the Bodleian Library at Oxford University. He also called on publishers to distribute abstracts of all new titles they produced each year, in a standard form that would facilitate indexing. He hoped that this abstracting project would eventually include everything printed from his day back to Gutenberg. Neither proposal met with success at the time, but something like them became standard practice among English language publishers during the 20th century, under the aegis of the Library of Congress and the British Library.'b"Leibniz was groping towards hardware and software concepts worked out much later by Charles Babbage and Ada Lovelace. In 1679, while mulling over his binary arithmetic, Leibniz imagined a machine in which binary numbers were represented by marbles, governed by a rudimentary sort of punched cards.[113] Modern electronic digital computers replace Leibniz's marbles moving by gravity with shift registers, voltage gradients, and pulses of electrons, but otherwise they run roughly as Leibniz envisioned in 1679."b'In 1671, Leibniz began to invent a machine that could execute all four arithmetic operations, gradually improving it over a number of years. This "stepped reckoner" attracted fair attention and was the basis of his election to the Royal Society in 1673. A number of such machines were made during his years in Hanover by a craftsman working under his supervision. They were not an unambiguous success because they did not fully mechanize the carry operation. Couturat reported finding an unpublished note by Leibniz, dated 1674, describing a machine capable of performing some algebraic operations.[110] Leibniz also devised a (now reproduced) cipher machine, recovered by Nicholas Rescher in 2010.[111] In 1693, Leibniz described a design of a machine which could, in theory, integrate differential equations, which he called "integraph".[112]'b'Leibniz may have been the first computer scientist and information theorist.[106] Early in life, he documented the binary numeral system (base 2), then revisited that system throughout his career.[107] Leibniz may have plagiarized Juan Caramuel y Lobkowitz as he was familiar with his works on the binary system.[108] He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.[109]'b"In 1906, Garland published a volume of Leibniz's writings bearing on his many practical inventions and engineering work. To date, few of these writings have been translated into English. Nevertheless, it is well understood that Leibniz was a serious inventor, engineer, and applied scientist, with great respect for practical life. Following the motto theoria cum praxi, he urged that theory be combined with practical application, and thus has been claimed as the father of applied science. He designed wind-driven propellers and water pumps, mining machines to extract ore, hydraulic presses, lamps, submarines, clocks, etc. With Denis Papin, he invented a steam engine. He even proposed a method for desalinating water. From 1680 to 1685, he struggled to overcome the chronic flooding that afflicted the ducal silver mines in the Harz Mountains, but did not succeed.[105]"b'In public health, he advocated establishing a medical administrative authority, with powers over epidemiology and veterinary medicine. He worked to set up a coherent medical training program, oriented towards public health and preventive measures. In economic policy, he proposed tax reforms and a national insurance program, and discussed the balance of trade. He even proposed something akin to what much later emerged as game theory. In sociology he laid the ground for communication theory.'b'Leibniz found his most important interpreter in Wilhelm Wundt, founder of psychology as a discipline. Wundt used the "\xe2\x80\xa6 nisi intellectu ipse" quotation 1862 on the title page of his Beitr\xc3\xa4ge zur Theorie der Sinneswahrnehmung (Contributions on the Theory of Sensory Perception) and published a detailed and aspiring monograph on Leibniz[97] Wundt shaped the term apperception, introduced by Leibniz, into an experimental psychologically based apperception psychology that included neuropsychological modelling \xe2\x80\x93 an excellent example of how a concept created by a great philosopher could stimulate a psychological research program. One principle in the thinking of Leibniz played a fundamental role: \xe2\x80\x9cthe principle of equality of separate but corresponding viewpoints.\xe2\x80\x9d Wundt characterized this style of thought (perspectivism) in a way that also applied for him \xe2\x80\x93 viewpoints that "supplement one another, while also being able to appear as opposites that only resolve themselves when considered more deeply."[98][99] Much of Leibniz\'s work went on to have a great impact on the field of psychology.[100] Leibniz thought that there are many petites perceptions, or small perceptions of which we perceive but of which we are unaware. He believed that by the principle that phenomena found in nature were continuous by default, it was likely that the transition between conscious and unconscious states had intermediary steps.[101] For this to be true, there must also be a portion of the mind of which we are unaware at any given time. His theory regarding consciousness in relation to the principle of continuity can be seen as an early theory regarding the stages of sleep. In this way, Leibniz\'s theory of perception can be viewed as one of many theories leading up to the idea of the unconscious. Leibniz was a direct influence on Ernst Platner, who is credited with originally coining the term Unbewu\xc3\x9ftseyn (unconscious).[102] Additionally, the idea of subliminal stimuli can be traced back to his theory of small perceptions.[103] Leibniz\'s ideas regarding music and tonal perception went on to influence the laboratory studies of Wilhelm Wundt.[104]'b'Psychology had been a central interest of Leibniz.[90][91] He appears to be an "underappreciated pioneer of psychology" [92] He wrote on topics which are now regarded as fields of psychology: attention and consciousness, memory, learning (association), motivation (the act of "striving"), emergent individuality, the general dynamics of development (evolution). His discussions in the New Essays and Monadology often rely on everyday observations such as the behaviour of a dog or the noise of the sea, and he develops intuitive analogies (the synchronous running of clocks or the balance spring of a clock). He also devised postulates and principles that apply to psychology: the continuum of the unnoticed petite perceptions to the distinct, self-aware apperception, and psychophysical parallelism from the point of view of causality and of purpose: \xe2\x80\x9cSouls act according to the laws of final causes, through aspirations, ends and means. Bodies act according to the laws of efficient causes, i.e. the laws of motion. And these two realms, that of efficient causes and that of final causes, harmonize with one another.\xe2\x80\x9d [93] This idea refers to the mind-body problem, stating that the mind and brain do not act upon each other, but act alongside each other separately but in harmony.[94] Leibniz, however, did not use the term psychologia.[95] Leibniz\xe2\x80\x99 epistemological position \xe2\x80\x93 against John Locke and English empiricism (sensualism) \xe2\x80\x93 was made clear: \xe2\x80\x9cNihil est in intellectu quod non fuerit in sensu, nisi intellectu ipse.\xe2\x80\x9d \xe2\x80\x93 \xe2\x80\x9cNothing is in the intellect that was not first in the senses, except the intellect itself.\xe2\x80\x9d [96] Principles that are not present in sensory impressions can be recognised in human perception and consciousness: logical inferences, categories of thought, the principle of causality and the principle of purpose (teleology).'b'By proposing that the earth has a molten core, he anticipated modern geology. In embryology, he was a preformationist, but also proposed that organisms are the outcome of a combination of an infinite number of possible microstructures and of their powers. In the life sciences and paleontology, he revealed an amazing transformist intuition, fueled by his study of comparative anatomy and fossils. One of his principal works on this subject, Protogaea, unpublished in his lifetime, has recently been published in English for the first time. He worked out a primal organismic theory.[89] In medicine, he exhorted the physicians of his time\xe2\x80\x94with some results\xe2\x80\x94to ground their theories in detailed comparative observations and verified experiments, and to distinguish firmly scientific and metaphysical points of view.'b'Leibniz\'s vis viva (Latin for "living force") is mv2, twice the modern kinetic energy. He realized that the total energy would be conserved in certain mechanical systems, so he considered it an innate motive characteristic of matter.[88] Here too his thinking gave rise to another regrettable nationalistic dispute. His vis viva was seen as rivaling the conservation of momentum championed by Newton in England and by Descartes in France; hence academics in those countries tended to neglect Leibniz\'s idea. In reality, both energy and momentum are conserved, so the two approaches are equally valid.'b"The principle of sufficient reason has been invoked in recent cosmology, and his identity of indiscernibles in quantum mechanics, a field some even credit him with having anticipated in some sense. Those who advocate digital philosophy, a recent direction in cosmology, claim Leibniz as a precursor. In addition to his theories about the nature of reality, Leibniz's contributions to the development of calculus have also had a major impact on physics."b"One of Leibniz's projects was to recast Newton's theory as a vortex theory.[87] However, his project went beyond vortex theory, since at its heart there was an attempt to explain one of the most difficult problems in physics, that of the origin of the cohesion of matter.[87]"b"Leibniz held a relationist notion of space and time, against Newton's substantivalist views.[84][85][86] According to Newton's substantivalism, space and time are entities in their own right, existing independently of things. Leibniz's relationism, on the other hand, describes space and time as systems of relations that exist between objects. The rise of general relativity and subsequent work in the history of physics has put Leibniz's stance in a more favorable light."b'Until the discovery of subatomic particles and the quantum mechanics governing them, many of Leibniz\'s speculative ideas about aspects of nature not reducible to statics and dynamics made little sense. For instance, he anticipated Albert Einstein by arguing, against Newton, that space, time and motion are relative, not absolute: "As for my own opinion, I have said more than once, that I hold space to be something merely relative, as time is, that I hold it to be an order of coexistences, as time is an order of successions."[83]'b"Leibniz contributed a fair amount to the statics and dynamics emerging around him, often disagreeing with Descartes and Newton. He devised a new theory of motion (dynamics) based on kinetic energy and potential energy, which posited space as relative, whereas Newton was thoroughly convinced that space was absolute. An important example of Leibniz's mature physical thinking is his Specimen Dynamicum of 1695.[82]"b"Leibniz's writings are currently discussed, not only for their anticipations and possible discoveries not yet recognized, but as ways of advancing present knowledge. Much of his writing on physics is included in Gerhardt's Mathematical Writings."b'Thus the fractal geometry promoted by Mandelbrot drew on Leibniz\'s notions of self-similarity and the principle of continuity: Natura non facit saltus.[53] We also see that when Leibniz wrote, in a metaphysical vein, that "the straight line is a curve, any part of which is similar to the whole", he was anticipating topology by more than two centuries. As for "packing", Leibniz told his friend and correspondent Des Bosses to imagine a circle, then to inscribe within it three congruent circles with maximum radius; the latter smaller circles could be filled with three even smaller circles by the same procedure. This process can be continued infinitely, from which arises a good idea of self-similarity. Leibniz\'s improvement of Euclid\'s axiom contains the same concept.'b'But Hideaki Hirano argues differently, quoting Mandelbrot:[80]'b'Leibniz was the first to use the term analysis situs,[78] later used in the 19th century to refer to what is now known as topology. There are two takes on this situation. On the one hand, Mates, citing a 1954 paper in German by Jacob Freudenthal, argues:'b"The use of infinitesimals in mathematics was frowned upon by followers of Karl Weierstrass,[citation needed] but survived in science and engineering, and even in rigorous mathematics, via the fundamental computational device known as the differential. Beginning in 1960, Abraham Robinson worked out a rigorous foundation for Leibniz's infinitesimals, using model theory, in the context of a field of hyperreal numbers. The resulting non-standard analysis can be seen as a belated vindication of Leibniz's mathematical reasoning. Robinson's transfer principle is a mathematical implementation of Leibniz's heuristic law of continuity, while the standard part function implements the Leibnizian transcendental law of homogeneity."b'From 1711 until his death, Leibniz was engaged in a dispute with John Keill, Newton and others, over whether Leibniz had invented calculus independently of Newton. This subject is treated at length in the article Leibniz\xe2\x80\x93Newton calculus controversy.'b"Leibniz exploited infinitesimals in developing calculus, manipulating them in ways suggesting that they had paradoxical algebraic properties. George Berkeley, in a tract called The Analyst and also in De Motu, criticized these. A recent study argues that Leibnizian calculus was free of contradictions, and was better grounded than Berkeley's empiricist criticisms.[77]"b'Leibniz is credited, along with Sir Isaac Newton, with the discovery of calculus (differential and integral calculus). According to Leibniz\'s notebooks, a critical breakthrough occurred on 11 November 1675, when he employed integral calculus for the first time to find the area under the graph of a function y = f(x).[72] He introduced several notations used to this day, for instance the integral sign \xe2\x88\xab, representing an elongated S, from the Latin word summa, and the d used for differentials, from the Latin word differentia. This cleverly suggestive notation for calculus is probably his most enduring mathematical legacy. Leibniz did not publish anything about his calculus until 1684.[73] Leibniz expressed the inverse relation of integration and differentiation, later called the fundamental theorem of calculus, by means of a figure[74] in his 1693 paper Supplementum geometriae dimensoriae....[75] However, James Gregory is credited for the theorem\'s discovery in geometric form, Isaac Barrow proved a more generalized geometric version, and Newton developed supporting theory. The concept became more transparent as developed through Leibniz\'s formalism and new notation.[76] The product rule of differential calculus is still called "Leibniz\'s law". In addition, the theorem that tells how and when to differentiate under the integral sign is called the Leibniz integral rule.'b"Leibniz was the first to see that the coefficients of a system of linear equations could be arranged into an array, now called a matrix, which can be manipulated to find the solution of the system, if any. This method was later called Gaussian elimination. Leibniz's discoveries of Boolean algebra and of symbolic logic, also relevant to mathematics, are discussed in the preceding section. The best overview of Leibniz's writings on calculus may be found in Bos (1974).[71]"b'Although the mathematical notion of function was implicit in trigonometric and logarithmic tables, which existed in his day, Leibniz was the first, in 1692 and 1694, to employ it explicitly, to denote any of several geometric concepts derived from a curve, such as abscissa, ordinate, tangent, chord, and the perpendicular.[70] In the 18th century, "function" lost these geometrical associations.'b'Russell\'s principal work on Leibniz found that many of Leibniz\'s most startling philosophical ideas and claims (e.g., that each of the fundamental monads mirrors the whole universe) follow logically from Leibniz\'s conscious choice to reject relations between things as unreal. He regarded such relations as (real) qualities of things (Leibniz admitted unary predicates only): For him "Mary is the mother of John" describes separate qualities of Mary and of John. This view contrasts with the relational logic of De Morgan, Peirce, Schr\xc3\xb6der and Russell himself, now standard in predicate logic. Notably, Leibniz also declared space and time to be inherently relational.[69]'b'Leibniz published nothing on formal logic in his lifetime; most of what he wrote on the subject consists of working drafts. In his book History of Western Philosophy, Bertrand Russell went so far as to claim that Leibniz had developed logic in his unpublished writings to a level which was reached only 200 years later.'b'The formal logic that emerged early in the 20th century also requires, at minimum, unary negation and quantified variables ranging over some universe of discourse.'b"Leibniz is one of the most important logicians between Aristotle and 1847, when George Boole and Augustus De Morgan each published books that began modern formal logic. Leibniz enunciated the principal properties of what we now call conjunction, disjunction, negation, identity, set inclusion, and the empty set. The principles of Leibniz's logic and, arguably, of his whole philosophy, reduce to two:"b'What Leibniz actually intended by his characteristica universalis and calculus ratiocinator, and the extent to which modern formal logic does justice to calculus, may never be established.[68]'b'Because Leibniz was a mathematical novice when he first wrote about the characteristic, at first he did not conceive it as an algebra but rather as a universal language or script. Only in 1676 did he conceive of a kind of "algebra of thought", modeled on and including conventional algebra and its notation. The resulting characteristic included a logical calculus, some combinatorics, algebra, his analysis situs (geometry of situation), a universal concept language, and more.'b"Complex thoughts would be represented by combining characters for simpler thoughts. Leibniz saw that the uniqueness of prime factorization suggests a central role for prime numbers in the universal characteristic, a striking anticipation of G\xc3\xb6del numbering. Granted, there is no intuitive or mnemonic way to number any set of elementary concepts using the prime numbers. Leibniz's idea of reasoning through a universal language of symbols and calculations, however, remarkably foreshadows great 20th century developments in formal systems, such as Turing completeness, where computation was used to define equivalent universal languages (see Turing degree)."b'But Leibniz took his speculations much further. Defining a character as any written sign, he then defined a "real" character as one that represents an idea directly and not simply as the word embodying the idea. Some real characters, such as the notation of logic, serve only to facilitate reasoning. Many characters well known in his day, including Egyptian hieroglyphics, Chinese characters, and the symbols of astronomy and chemistry, he deemed not real.[66] Instead, he proposed the creation of a characteristica universalis or "universal characteristic", built on an alphabet of human thought in which each fundamental concept would be represented by a unique "real" character:'b"Leibniz thought symbols were important for human understanding. He attached so much importance to the development of good notations that he attributed all his discoveries in mathematics to this. His notation for calculus is an example of his skill in this regard. Peirce, a 19th-century pioneer of semiotics, shared Leibniz's passion for symbols and notation, and his belief that these are essential to a well-running logic and mathematics."b"Leibniz's calculus ratiocinator, which resembles symbolic logic, can be viewed as a way of making such calculations feasible. Leibniz wrote memoranda[65] that can now be read as groping attempts to get symbolic logic\xe2\x80\x94and thus his calculus\xe2\x80\x94off the ground. These writings remained unpublished until the appearance of a selection edited by C.I. Gerhardt (1859). L. Couturat published a selection in 1901; by this time the main developments of modern logic had been created by Charles Sanders Peirce and by Gottlob Frege."b'Leibniz believed that much of human reasoning could be reduced to calculations of a sort, and that such calculations could resolve many differences of opinion:'b'Leibniz wrote: "Why is there something rather than nothing? The sufficient reason ... is found in a substance which ... is a necessary being bearing the reason for its existence within itself."[61] Martin Heidegger called this question "the fundamental question of metaphysics".[62][63]'b'Because God is "an absolutely perfect being" (I), Leibniz argues that God would be acting imperfectly if he acted with any less perfection than what he is able of (III). His syllogism then ends with the statement that God has made the world perfectly in all ways. This also effects how we should view God and his will. Leibniz states that, in lieu of God\xe2\x80\x99s will, we have to understand that God "is the best of all masters" and he will know when his good succeeds, so we, therefore, must act in conformity to his good will \xe2\x80\x93 or as much of it as we understand (IV). In our view of God, Leibniz declares that we cannot admire the work solely because of the maker, lest we mar the glory and love God in doing so. Instead, we must admire the maker for the work he has done (II). Effectively, Leibniz states that if we say the earth is good because of the will of God, and not good according to some standards of goodness, then how can we praise God for what he has done if contrary actions are also praiseworthy by this definition (II). Leibniz then asserts that different principles and geometry cannot simply be from the will of God, but must follow from his understanding.[60]'b'For Leibniz, "God is an absolutely perfect being." He describes this perfection later in section VI as the simplest form of something with the most substantial outcome (VI). Along these lines, he declares that every type of perfection "pertains to him (God) in the highest degree" (I). Even though his types of perfections are not specifically drawn out, Leibniz highlights the one thing that, to him, does certify imperfections and proves that God is perfect: "that one acts imperfectly if he acts with less perfection than he is capable of", and since God is a perfect being, he cannot act imperfectly (III). Because God cannot act imperfectly, the decisions he makes pertaining to the world must be perfect. Leibniz also comforts readers, stating that because he has done everything to the most perfect degree; those who love him cannot be injured. However, to love God is a subject of difficulty as Leibniz believes that we are "not disposed to wish for that which God desires" because we have the ability to alter our disposition (IV). In accordance with this, many act as rebels, but Leibniz says that the only way we can truly love God is by being content "with all that comes to us according to his will" (IV).'b'Further, although human actions flow from prior causes that ultimately arise in God, and therefore are known as a metaphysical certainty to God, an individual\'s free will is exercised within natural laws, where choices are merely contingently necessary, to be decided in the event by a "wonderful spontaneity" that provides individuals an escape from rigorous predestination.'b'Because reason and faith must be entirely reconciled, any tenet of faith which could not be defended by reason must be rejected. Leibniz then approached one of the central criticisms of Christian theism:[59] if God is all good, all wise and all powerful, how did evil come into the world? The answer (according to Leibniz) is that, while God is indeed unlimited in wisdom and power, his human creations, as creations, are limited both in their wisdom and in their will (power to act). This predisposes humans to false beliefs, wrong decisions and ineffective actions in the exercise of their free will. God does not arbitrarily inflict pain and suffering on humans; rather he permits both moral evil (sin) and physical evil (pain and suffering) as the necessary consequences of metaphysical evil (imperfection), as a means by which humans can identify and correct their erroneous decisions, and as a contrast to true good.'b'Leibniz asserted that the truths of theology (religion) and philosophy cannot contradict each other, since reason and faith are both "gifts of God" so that their conflict would imply God contending against himself. The Theodicy is Leibniz\'s attempt to reconcile his personal philosophical system with his interpretation of the tenets of Christianity.[58] This project was motivated in part by Leibniz\'s belief, shared by many conservative philosophers and theologians during the Enlightenment, in the rational and enlightened nature of the Christian religion as compared to its purportedly less advanced non-Western counterparts. It was also shaped by Leibniz\'s belief in the perfectibility of human nature (if humanity relied on correct philosophy and religion as a guide), and by his belief that metaphysical necessity must have a rational or logical foundation, even if this metaphysical causality seemed inexplicable in terms of physical necessity (the natural laws identified by science).'b'The Theodicy[57] tries to justify the apparent imperfections of the world by claiming that it is optimal among all possible worlds. It must be the best possible and most balanced world, because it was created by an all powerful and all knowing God, who would not choose to create an imperfect world if a better world could be known to him or possible to exist. In effect, apparent flaws that can be identified in this world must exist in every possible world, because otherwise God would have chosen to create the world that excluded those flaws.'b'Monads are purported to have gotten rid of the problematic:'b'The ontological essence of a monad is its irreducible simplicity. Unlike atoms, monads possess no material or spatial character. They also differ from atoms by their complete mutual independence, so that interactions among monads are only apparent. Instead, by virtue of the principle of pre-established harmony, each monad follows a preprogrammed set of "instructions" peculiar to itself, so that a monad "knows" what to do at each moment. By virtue of these intrinsic instructions, each monad is like a little mirror of the universe. Monads need not be "small"; e.g., each human being constitutes a monad, in which case free will is problematic.'b'Leibniz\'s best known contribution to metaphysics is his theory of monads, as exposited in Monadologie. According to Leibniz, monads are elementary particles with blurred perceptions of one another. Monads can also be compared to the corpuscles of the Mechanical Philosophy of Ren\xc3\xa9 Descartes and others. Monads are the ultimate elements of the universe. The monads are "substantial forms of being" with the following properties: they are eternal, indecomposable, individual, subject to their own laws, un-interacting, and each reflecting the entire universe in a pre-established harmony (a historically important example of panpsychism). Monads are centers of force; substance is force, while space, matter, and motion are merely phenomenal.'b'Leibniz would on occasion give a rational defense of a specific principle, but more often took them for granted.[56]'b'Leibniz variously invoked one or another of seven fundamental philosophical Principles:[50]'b"Unlike Descartes and Spinoza, Leibniz had a thorough university education in philosophy. He was influenced by his Leipzig professor Jakob Thomasius, who also supervised his BA thesis in philosophy.[4] Leibniz also eagerly read Francisco Su\xc3\xa1rez, a Spanish Jesuit respected even in Lutheran universities. Leibniz was deeply interested in the new methods and conclusions of Descartes, Huygens, Newton, and Boyle, but viewed their work through a lens heavily tinted by scholastic notions. Yet it remains the case that Leibniz's methods and concerns often anticipate the logic, and analytic and linguistic philosophy of the 20th century."b"Leibniz met Spinoza in 1676, read some of his unpublished writings, and has since been suspected of appropriating some of Spinoza's ideas. While Leibniz admired Spinoza's powerful intellect, he was also forthrightly dismayed by Spinoza's conclusions,[49] especially when these were inconsistent with Christian orthodoxy."b'Leibniz dated his beginning as a philosopher to his Discourse on Metaphysics, which he composed in 1686 as a commentary on a running dispute between Nicolas Malebranche and Antoine Arnauld. This led to an extensive and valuable correspondence with Arnauld;[47] it and the Discourse were not published until the 19th century. In 1695, Leibniz made his public entr\xc3\xa9e into European philosophy with a journal article titled "New System of the Nature and Communication of Substances".[48] Between 1695 and 1705, he composed his New Essays on Human Understanding, a lengthy commentary on John Locke\'s 1690 An Essay Concerning Human Understanding, but upon learning of Locke\'s 1704 death, lost the desire to publish it, so that the New Essays were not published until 1765. The Monadologie, composed in 1714 and published posthumously, consists of 90 aphorisms.'b"Leibniz's philosophical thinking appears fragmented, because his philosophical writings consist mainly of a multitude of short pieces: journal articles, manuscripts published long after his death, and many letters to many correspondents. He wrote only two book-length philosophical treatises, of which only the Th\xc3\xa9odic\xc3\xa9e of 1710 was published in his lifetime."b"Leibniz never married. He complained on occasion about money, but the fair sum he left to his sole heir, his sister's stepson, proved that the Brunswicks had, by and large, paid him well. In his diplomatic endeavors, he at times verged on the unscrupulous, as was all too often the case with professional diplomats of his day. On several occasions, Leibniz backdated and altered personal manuscripts, actions which put him in a bad light during the calculus controversy. On the other hand, he was charming, well-mannered, and not without humor and imagination.[41] He had many friends and admirers all over Europe. On Leibniz's religious views, though he was a protestant, Leibniz learned to appreciate the good sides of Catholicism through his patrons and colleagues. He never admitted the Protestant view of Pope as an Antichrist.[42] Leibniz was claimed as a philosophical theist.[43][44][45][46]"b'Leibniz died in Hanover in 1716: at the time, he was so out of favor that neither George I (who happened to be near Hanover at that time) nor any fellow courtier other than his personal secretary attended the funeral. Even though Leibniz was a life member of the Royal Society and the Berlin Academy of Sciences, neither organization saw fit to honor his passing. His grave went unmarked for more than 50 years. Leibniz was eulogized by Fontenelle, before the French Academy of Sciences in Paris, which had admitted him as a foreign member in 1700. The eulogy was composed at the behest of the Duchess of Orleans, a niece of the Electress Sophia.'b'In 1711, while traveling in northern Europe, the Russian Tsar Peter the Great stopped in Hanover and met Leibniz, who then took some interest in Russian matters for the rest of his life. In 1712, Leibniz began a two-year residence in Vienna, where he was appointed Imperial Court Councillor to the Habsburgs. On the death of Queen Anne in 1714, Elector George Louis became King George I of Great Britain, under the terms of the 1701 Act of Settlement. Even though Leibniz had done much to bring about this happy event, it was not to be his hour of glory. Despite the intercession of the Princess of Wales, Caroline of Ansbach, George I forbade Leibniz to join him in London until he completed at least one volume of the history of the Brunswick family his father had commissioned nearly 30 years earlier. Moreover, for George I to include Leibniz in his London court would have been deemed insulting to Newton, who was seen as having won the calculus priority dispute and whose standing in British official circles could not have been higher. Finally, his dear friend and defender, the Dowager Electress Sophia, died in 1714.'b"In 1708, John Keill, writing in the journal of the Royal Society and with Newton's presumed blessing, accused Leibniz of having plagiarised Newton's calculus.[40] Thus began the calculus priority dispute which darkened the remainder of Leibniz's life. A formal investigation by the Royal Society (in which Newton was an unacknowledged participant), undertaken in response to Leibniz's demand for a retraction, upheld Keill's charge. Historians of mathematics writing since 1900 or so have tended to acquit Leibniz, pointing to important differences between Leibniz's and Newton's versions of calculus."b'Leibniz was appointed Librarian of the Herzog August Library in Wolfenb\xc3\xbcttel, Lower Saxony, in 1691.'b"The Elector Ernest Augustus commissioned Leibniz to write a history of the House of Brunswick, going back to the time of Charlemagne or earlier, hoping that the resulting book would advance his dynastic ambitions. From 1687 to 1690, Leibniz traveled extensively in Germany, Austria, and Italy, seeking and finding archival materials bearing on this project. Decades went by but no history appeared; the next Elector became quite annoyed at Leibniz's apparent dilatoriness. Leibniz never finished the project, in part because of his huge output on many other fronts, but also because he insisted on writing a meticulously researched and erudite book based on archival sources, when his patrons would have been quite happy with a short popular book, one perhaps little more than a genealogy with commentary, to be completed in three years or less. They never knew that he had in fact carried out a fair part of his assigned task: when the material Leibniz had written and collected for his history of the House of Brunswick was finally published in the 19th century, it filled three volumes."b"The Brunswicks tolerated the enormous effort Leibniz devoted to intellectual pursuits unrelated to his duties as a courtier, pursuits such as perfecting calculus, writing about other mathematics, logic, physics, and philosophy, and keeping up a vast correspondence. He began working on calculus in 1674; the earliest evidence of its use in his surviving notebooks is 1675. By 1677 he had a coherent system in hand, but did not publish it until 1684. Leibniz's most important mathematical papers were published between 1682 and 1692, usually in a journal which he and Otto Mencke founded in 1682, the Acta Eruditorum. That journal played a key role in advancing his mathematical and scientific reputation, which in turn enhanced his eminence in diplomacy, history, theology, and philosophy."b"The population of Hanover was only about 10,000, and its provinciality eventually grated on Leibniz. Nevertheless, to be a major courtier to the House of Brunswick was quite an honor, especially in light of the meteoric rise in the prestige of that House during Leibniz's association with it. In 1692, the Duke of Brunswick became a hereditary Elector of the Holy Roman Empire. The British Act of Settlement 1701 designated the Electress Sophia and her descent as the royal family of England, once both King William III and his sister-in-law and successor, Queen Anne, were dead. Leibniz played a role in the initiatives and negotiations leading up to that Act, but not always an effective one. For example, something he published anonymously in England, thinking to promote the Brunswick cause, was formally censured by the British Parliament."b'Among the few people in north Germany to accept Leibniz were the Electress Sophia of Hanover (1630\xe2\x80\x931714), her daughter Sophia Charlotte of Hanover (1668\xe2\x80\x931705), the Queen of Prussia and his avowed disciple, and Caroline of Ansbach, the consort of her grandson, the future George II. To each of these women he was correspondent, adviser, and friend. In turn, they all approved of Leibniz more than did their spouses and the future king George I of Great Britain.[39]'b'In 1677, he was promoted, at his request, to Privy Counselor of Justice, a post he held for the rest of his life. Leibniz served three consecutive rulers of the House of Brunswick as historian, political adviser, and most consequentially, as librarian of the ducal library. He thenceforth employed his pen on all the various political, historical, and theological matters involving the House of Brunswick; the resulting documents form a valuable part of the historical record for the period.'b"Leibniz managed to delay his arrival in Hanover until the end of 1676 after making one more short journey to London, where Newton accused him of having seen Newton's unpublished work on calculus in advance.[37] This was alleged to be evidence supporting the accusation, made decades later, that he had stolen calculus from Newton. On the journey from London to Hanover, Leibniz stopped in The Hague where he met van Leeuwenhoek, the discoverer of microorganisms. He also spent several days in intense discussion with Spinoza, who had just completed his masterwork, the Ethics.[38]"b'In 1675 he tried to get admitted to the French Academy of Sciences as a foreign honorary member, but it was considered that there were already enough foreigners there and so no invitation came. He left Paris in October 1676.'b'In this regard, a 1669 invitation from the John Frederick of Brunswick to visit Hanover proved to have been fateful. Leibniz had declined the invitation, but had begun corresponding with the duke in 1671. In 1673, the duke offered Leibniz the post of counsellor. Leibniz very reluctantly accepted the position two years later, only after it became clear that no employment in Paris, whose intellectual stimulation he relished, or with the Habsburg imperial court, was forthcoming.[citation needed]'b"The mission ended abruptly when news of the Elector's death (12 February 1673) reached them. Leibniz promptly returned to Paris and not, as had been planned, to Mainz.[36] The sudden deaths of his two patrons in the same winter meant that Leibniz had to find a new basis for his career."b"When it became clear that France would not implement its part of Leibniz's Egyptian plan, the Elector sent his nephew, escorted by Leibniz, on a related mission to the English government in London, early in 1673.[35] There Leibniz came into acquaintance of Henry Oldenburg and John Collins. He met with the Royal Society where he demonstrated a calculating machine that he had designed and had been building since 1670. The machine was able to execute all four basic operations (adding, subtracting, multiplying, and dividing), and the society quickly made him an external member."b'Thus Leibniz went to Paris in 1672. Soon after arriving, he met Dutch physicist and mathematician Christiaan Huygens and realised that his own knowledge of mathematics and physics was patchy. With Huygens as his mentor, he began a program of self-study that soon pushed him to making major contributions to both subjects, including discovering his version of the differential and integral calculus. He met Nicolas Malebranche and Antoine Arnauld, the leading French philosophers of the day, and studied the writings of Descartes and Pascal, unpublished as well as published.[34] He befriended a German mathematician, Ehrenfried Walther von Tschirnhaus; they corresponded for the rest of their lives.'b"Von Boyneburg did much to promote Leibniz's reputation, and the latter's memoranda and letters began to attract favorable notice. After Leibniz's service to the Elector there soon followed a diplomatic role. He published an essay, under the pseudonym of a fictitious Polish nobleman, arguing (unsuccessfully) for the German candidate for the Polish crown. The main force in European geopolitics during Leibniz's adult life was the ambition of Louis XIV of France, backed by French military and economic might. Meanwhile, the Thirty Years' War had left German-speaking Europe exhausted, fragmented, and economically backward. Leibniz proposed to protect German-speaking Europe by distracting Louis as follows. France would be invited to take Egypt as a stepping stone towards an eventual conquest of the Dutch East Indies. In return, France would agree to leave Germany and the Netherlands undisturbed. This plan obtained the Elector's cautious support. In 1672, the French government invited Leibniz to Paris for discussion,[33] but the plan was soon overtaken by the outbreak of the Franco-Dutch War and became irrelevant. Napoleon's failed invasion of Egypt in 1798 can be seen as an unwitting, late implementation of Leibniz's plan, after the Eastern hemisphere colonial supremacy in Europe had already passed from the Dutch to the British.[citation needed]"b"Leibniz's first position was as a salaried secretary to an alchemical society in Nuremberg.[30] He knew fairly little about the subject at that time but presented himself as deeply learned. He soon met Johann Christian von Boyneburg (1622\xe2\x80\x931672), the dismissed chief minister of the Elector of Mainz, Johann Philipp von Sch\xc3\xb6nborn.[31] Von Boyneburg hired Leibniz as an assistant, and shortly thereafter reconciled with the Elector and introduced Leibniz to him. Leibniz then dedicated an essay on law to the Elector in the hope of obtaining employment. The stratagem worked; the Elector asked Leibniz to assist with the redrafting of the legal code for the Electorate.[32] In 1669, Leibniz was appointed assessor in the Court of Appeal. Although von Boyneburg died late in 1672, Leibniz remained under the employment of his widow until she dismissed him in 1674.[citation needed]"b'As an adult, Leibniz often introduced himself as "Gottfried von Leibniz". Many posthumously published editions of his writings presented his name on the title page as "Freiherr G. W. von Leibniz." However, no document has ever been found from any contemporary government that stated his appointment to any form of nobility.[29]'b'Leibniz then enrolled in the University of Altdorf and quickly submitted a thesis, which he had probably been working on earlier in Leipzig.[27] The title of his thesis was Disputatio Inauguralis de Casibus Perplexis in Jure (Inaugural Disputation on Ambiguous Legal Cases).[21] Leibniz earned his license to practice law and his Doctorate in Law in November 1666. He next declined the offer of an academic appointment at Altdorf, saying that "my thoughts were turned in an entirely different direction".[28]'b"In early 1666, at age 19, Leibniz wrote his first book, De Arte Combinatoria (On the Combinatorial Art), the first part of which was also his habilitation thesis in Philosophy, which he defended in March 1666.[21][23] His next goal was to earn his license and Doctorate in Law, which normally required three years of study. In 1666, the University of Leipzig turned down Leibniz's doctoral application and refused to grant him a Doctorate in Law, most likely due to his relative youth.[24][25] Leibniz subsequently left Leipzig.[26]"b"In April 1661 he enrolled in his father's former university at age 15,[1][20] and completed his bachelor's degree in Philosophy in December 1662. He defended his Disputatio Metaphysica de Principio Individui (Metaphysical Disputation on the Principle of Individuation),[21] which addressed the principle of individuation, on 9 June 1663. Leibniz earned his master's degree in Philosophy on 7 February 1664. He published and defended a dissertation Specimen Quaestionum Philosophicarum ex Jure collectarum (An Essay of Collected Philosophical Problems of Right),[21] arguing for both a theoretical and a pedagogical relationship between philosophy and law, in December 1664. After one year of legal studies, he was awarded his bachelor's degree in Law on 28 September 1665.[22] His dissertation was titled De conditionibus (On Conditions).[21]"b"Leibniz's father had been a Professor of Moral Philosophy at the University of Leipzig, and the boy later inherited his father's personal library. He was given free access to it from the age of seven. While Leibniz's schoolwork was largely confined to the study of a small canon of authorities, his father's library enabled him to study a wide variety of advanced philosophical and theological works\xe2\x80\x94ones that he would not have otherwise been able to read until his college years.[18] Access to his father's library, largely written in Latin, also led to his proficiency in the Latin language, which he achieved by the age of 12. He also composed 300 hexameters of Latin verse, in a single morning, for a special event at school at the age of 13.[19]"b'Leibniz was baptized on 3 July of that year at St. Nicholas Church, Leipzig; his godfather was the Lutheran theologian Martin Geier\xc2\xa0(de).[16] His father died when he was six and years old, and from that point on he was raised by his mother.[17]'b'In English:'b"Gottfried Leibniz was born on 1 July 1646, toward the end of the Thirty Years' War, in Leipzig, Saxony, to Friedrich Leibniz and Catharina Schmuck. Friedrich noted in his family journal:"b''b''b"Leibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in philosophy, probability theory, biology, medicine, geology, psychology, linguistics, and computer science. He wrote works on philosophy, politics, law, ethics, theology, history, and philology. Leibniz also contributed to the field of library science. While serving as overseer of the Wolfenb\xc3\xbcttel library in Germany, he devised a cataloging system that would serve as a guide for many of Europe's largest libraries.[11] Leibniz's contributions to this vast array of subjects were scattered in various learned journals, in tens of thousands of letters, and in unpublished manuscripts. He wrote in several languages, but primarily in Latin, French, and German.[12] There is no complete gathering of the writings of Leibniz translated into English.[13]"b'In philosophy, Leibniz is most noted for his optimism, i.e. his conclusion that our Universe is, in a restricted sense, the best possible one that God could have created, an idea that was often lampooned by others such as Voltaire. Leibniz, along with Ren\xc3\xa9 Descartes and Baruch Spinoza, was one of the three great 17th-century advocates of rationalism. The work of Leibniz anticipated modern logic and analytic philosophy, but his philosophy also looks back to the scholastic tradition, in which conclusions are produced by applying reason to first principles or prior definitions rather than to empirical evidence.'b"Gottfried Wilhelm (von) Leibniz (/\xcb\x88la\xc9\xaabn\xc9\xaats/;[5] German: [\xcb\x88\xc9\xa1\xc9\x94tf\xca\x81i\xcb\x90t \xcb\x88v\xc9\xaalh\xc9\x9blm f\xc9\x94n \xcb\x88la\xc9\xaabn\xc9\xaats][6] or [\xcb\x88la\xc9\xaapn\xc9\xaats];[7] French: Godefroi Guillaume Leibnitz;[8] 1 July 1646 [O.S. 21 June] \xe2\x80\x93 14 November 1716) was a German polymath and philosopher who occupies a prominent place in the history of mathematics and the history of philosophy, having developed differential and integral calculus independently of Isaac Newton.[9] Leibniz's notation has been widely used ever since it was published. It was only in the 20th century that his Law of Continuity and Transcendental Law of Homogeneity found mathematical implementation (by means of non-standard analysis). He became one of the most prolific inventors in the field of mechanical calculators. While working on adding automatic multiplication and division to Pascal's calculator, he was the first to describe a pinwheel calculator in 1685[10] and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, which is the foundation of virtually all digital computers."
Gabriel Cramer
b'He did extensive travel throughout Europe in the late 1730s, which greatly influenced his works in mathematics. He died in 1752 at Bagnols-sur-C\xc3\xa8ze while traveling in southern France to restore his health.'b"In 1750 he published Cramer's rule, giving a general formula for the solution for any unknown in a linear equation system having a unique solution, in terms of determinants implied by the system. This rule is still standard."b"He edited the works of the two elder Bernoullis, and wrote on the physical cause of the spheroidal shape of the planets and the motion of their apsides (1730), and on Newton's treatment of cubic curves (1746)."b"He published his best-known work in his forties. This included his treatise on algebraic curves (1750). It contains the earliest demonstration that a curve of the n-th degree is determined by n(n + 3)/2 points on it, in general position. (See Cramer's theorem (algebraic curves).) This led to the misconception that is Cramer's paradox, concerning the number of intersections of two curves compared to the number of points that determine a curve."b'In 1728 he proposed a solution to the St. Petersburg Paradox that came very close to the concept of expected utility theory given ten years later by Daniel Bernoulli.'b'Cramer showed promise in mathematics from an early age. At 18 he received his doctorate and at 20 he was co-chair[1] of mathematics at the University of Geneva.'b''b''b'Gabriel Cramer (French:\xc2\xa0[k\xca\x81am\xc9\x9b\xca\x81]; 31 July 1704 \xe2\x80\x93 4 January 1752) was a Genevan mathematician. He was the son of physician Jean Cramer and Anne Mallet Cramer.'
Cramer's rule

Carl Friedrich Gauss
b"Gauss's collective works are online at dz-srv1.sub.uni-goettingen.de Uni-goettingen.de includes German translations of Latin texts and commentaries by various authorities."b"In 1929 the Polish mathematician Marian Rejewski, who helped to solve the German Enigma cipher machine in December 1932, began studying actuarial statistics at G\xc3\xb6ttingen. At the request of his Pozna\xc5\x84 University professor, Zdzis\xc5\x82aw Krygowski, on arriving at G\xc3\xb6ttingen Rejewski laid flowers on Gauss's grave.[69]"b'Things named in honor of Gauss include:'b'In 2007 a bust of Gauss was placed in the Walhalla temple.[67]'b"Daniel Kehlmann's 2005 novel Die Vermessung der Welt, translated into English as Measuring the World (2006), explores Gauss's life and work through a lens of historical fiction, contrasting them with those of the German explorer Alexander von Humboldt. A film version directed by Detlev Buck was released in 2012.[66]"b"From 1989 through 2001, Gauss's portrait, a normal distribution curve and some prominent G\xc3\xb6ttingen buildings were featured on the German ten-mark banknote. The reverse featured the approach for Hanover. Germany has also issued three postage stamps honoring Gauss. One (no. 725) appeared in 1955 on the hundredth anniversary of his death; two others, nos. 1246 and 1811, in 1977, the 200th anniversary of his birth."b'He referred to mathematics as "the queen of sciences"[64] and supposedly once espoused a belief in the necessity of immediately understanding Euler\'s identity as a benchmark pursuant to becoming a first-class mathematician.[65]'b'According to Isaac Asimov, Gauss was once interrupted in the middle of a problem and told that his wife was dying. He is purported to have said, "Tell her to wait a moment till I\'m done."[63] This anecdote is briefly discussed in G. Waldo Dunnington\'s Gauss, Titan of Science where it is suggested that it is an apocryphal story.'b"Gauss's presumed method was to realize that pairwise addition of terms from opposite ends of the list yielded identical intermediate sums: 1\xc2\xa0+\xc2\xa0100\xc2\xa0=\xc2\xa0101, 2\xc2\xa0+\xc2\xa099\xc2\xa0=\xc2\xa0101, 3\xc2\xa0+\xc2\xa098\xc2\xa0=\xc2\xa0101, and so on, for a total sum of 50\xc2\xa0\xc3\x97\xc2\xa0101\xc2\xa0=\xc2\xa05050. However, the details of the story are at best uncertain (see[8] for discussion of the original Wolfgang Sartorius von Waltershausen source and the changes in other versions); some authors, such as Joseph Rotman in his book A first course in Abstract Algebra, question whether it ever happened."b'Another story has it that in primary school after the young Gauss misbehaved, his teacher, J.G. B\xc3\xbcttner, gave him a task: add a list of integers in arithmetic progression; as the story is most often told, these were the numbers from 1 to 100. The young Gauss reputedly produced the correct answer within seconds, to the astonishment of his teacher and his assistant Martin Bartels.'b'There are several stories of his early genius. According to one, his gifts became very apparent at the age of three when he corrected, mentally and without fault in his calculations, an error his father had made on paper while calculating finances.'b'The British mathematician Henry John Stephen Smith (1826\xe2\x80\x931883) gave the following appraisal of Gauss:'b'In 1821, he was made a foreign member of the Royal Swedish Academy of Sciences. Gauss was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1822.[61]'b'That is, curvature does not depend on how the surface might be embedded in 3-dimensional space or 2-dimensional space.'b"The geodetic survey of Hanover, which required Gauss to spend summers traveling on horseback for a decade,[60] fueled Gauss's interest in differential geometry and topology, fields of mathematics dealing with curves and surfaces. Among other things, he came up with the notion of Gaussian curvature. This led in 1828 to an important theorem, the Theorema Egregium (remarkable theorem), establishing an important property of the notion of curvature. Informally, the theorem says that the curvature of a surface can be determined entirely by measuring angles and distances on the surface."b'Letters from Gauss years before 1829 reveal him obscurely discussing the problem of parallel lines. Waldo Dunnington, a biographer of Gauss, argues in Gauss, Titan of Science that Gauss was in fact in full possession of non-Euclidean geometry long before it was published by Bolyai, but that he refused to publish any of it because of his fear of controversy.[58][59]'b'This unproved statement put a strain on his relationship with Bolyai who thought that Gauss was "stealing" his idea.[57]'b'Bolyai\'s son, J\xc3\xa1nos Bolyai, discovered non-Euclidean geometry in 1829; his work was published in 1832. After seeing it, Gauss wrote to Farkas Bolyai: "To praise it would amount to praising myself. For the entire content of the work\xc2\xa0... coincides almost exactly with my own meditations which have occupied my mind for the past thirty or thirty-five years."'b'Research on these geometries led to, among other things, Einstein\'s theory of general relativity, which describes the universe as non-Euclidean. His friend Farkas Wolfgang Bolyai with whom Gauss had sworn "brotherhood and the banner of truth" as a student, had tried in vain for many years to prove the parallel postulate from Euclid\'s other axioms of geometry.'b"Gauss also claimed to have discovered the possibility of non-Euclidean geometries but never published it. This discovery was a major paradigm shift in mathematics, as it freed mathematicians from the mistaken belief that Euclid's axioms were the only way to make geometry consistent and non-contradictory."b'In 1818 Gauss, putting his calculation skills to practical use, carried out a geodetic survey of the Kingdom of Hanover, linking up with previous Danish surveys. To aid the survey, Gauss invented the heliotrope, an instrument that uses a mirror to reflect sunlight over great distances, to measure positions.'b'Gauss proved the method under the assumption of normally distributed errors (see Gauss\xe2\x80\x93Markov theorem; see also Gaussian). The method had been described earlier by Adrien-Marie Legendre in 1805, but Gauss claimed that he had been using it since 1794 or 1795.[55] In the history of statistics, this disagreement is called the "priority dispute over the discovery of the method of least squares."[56]'b'The discovery of Ceres led Gauss to his work on a theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as Theoria motus corporum coelestium in sectionibus conicis solem ambientum (Theory of motion of the celestial bodies moving in conic sections around the Sun). In the process, he so streamlined the cumbersome mathematics of 18th-century orbital prediction that his work remains a cornerstone of astronomical computation.[citation needed] It introduced the Gaussian gravitational constant, and contained an influential treatment of the method of least squares, a procedure used in all sciences to this day to minimize the impact of measurement error.'b'Zach noted that "without the intelligent work and calculations of Doctor Gauss we might not have found Ceres again". Though Gauss had up to that point been financially supported by his stipend from the Duke, he doubted the security of this arrangement, and also did not believe pure mathematics to be important enough to deserve support. Thus he sought a position in astronomy, and in 1807 was appointed Professor of Astronomy and Director of the astronomical observatory in G\xc3\xb6ttingen, a post he held for the remainder of his life.'b'One such method was the fast Fourier transform. While this method is traditionally attributed to a 1965 paper by J. W. Cooley and J. W. Tukey, Gauss developed it as a trigonometric interpolation method. His paper, Theoria Interpolationis Methodo Nova Tractata,[53] was only published posthumously in Volume 3 of his collected works. This paper predates the first presentation by Joseph Fourier on the subject in 1807.[54]'b"Gauss's method involved determining a conic section in space, given one focus (the Sun) and the conic's intersection with three given lines (lines of sight from the Earth, which is itself moving on an ellipse, to the planet) and given the time it takes the planet to traverse the arcs determined by these lines (from which the lengths of the arcs can be calculated by Kepler's Second Law). This problem leads to an equation of the eighth degree, of which one solution, the Earth's orbit, is known. The solution sought is then separated from the remaining six based on physical conditions. In this work, Gauss used comprehensive approximation methods which he created for that purpose.[52]"b'Gauss, who was 24 at the time, heard about the problem and tackled it. After three months of intense work, he predicted a position for Ceres in December 1801\xe2\x80\x94just about a year after its first sighting\xe2\x80\x94and this turned out to be accurate within a half-degree when it was rediscovered by Franz Xaver von Zach on 31 December at Gotha, and one day later by Heinrich Olbers in Bremen.'b'In the same year, Italian astronomer Giuseppe Piazzi discovered the dwarf planet Ceres. Piazzi could only track Ceres for somewhat more than a month, following it for three degrees across the night sky. Then it disappeared temporarily behind the glare of the Sun. Several months later, when Ceres should have reappeared, Piazzi could not locate it: the mathematical tools of the time were not able to extrapolate a position from such a scant amount of data\xe2\x80\x94three degrees represent less than 1% of the total orbit.'b'Gauss also made important contributions to number theory with his 1801 book Disquisitiones Arithmeticae (Latin, Arithmetical Investigations), which, among other things, introduced the symbol \xe2\x89\xa1 for congruence and used it in a clean presentation of modular arithmetic, contained the first two proofs of the law of quadratic reciprocity, developed the theories of binary and ternary quadratic forms, stated the class number problem for them, and showed that a regular heptadecagon (17-sided polygon) can be constructed with straightedge and compass.'b"In his 1799 doctorate in absentia, A new proof of the theorem that every integral rational algebraic function of one variable can be resolved into real factors of the first or second degree, Gauss proved the fundamental theorem of algebra which states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. Mathematicians including Jean le Rond d'Alembert had produced false proofs before him, and Gauss's dissertation contains a critique of d'Alembert's work. Ironically, by today's standard, Gauss's own attempt is not acceptable, owing to the implicit use of the Jordan curve theorem. However, he subsequently produced three other proofs, the last one in 1849 being generally rigorous. His attempts clarified the concept of complex numbers considerably along the way."b'Gauss summarized his views on the pursuit of knowledge in a letter to Farkas Bolyai dated 2 September 1808 as follows:'b'Gauss supported the monarchy and opposed Napoleon, whom he saw as an outgrowth of revolution.'b'Gauss usually declined to present the intuition behind his often very elegant proofs\xe2\x80\x94he preferred them to appear "out of thin air" and erased all traces of how he discovered them.[citation needed] This is justified, if unsatisfactorily, by Gauss in his Disquisitiones Arithmeticae, where he states that all analysis (i.e., the paths one traveled to reach the solution of a problem) must be suppressed for sake of brevity.'b'Before she died, Sophie Germain was recommended by Gauss to receive her honorary degree; she never received it.[50]'b"On Gauss's recommendation, Friedrich Bessel was awarded an honorary doctor degree from G\xc3\xb6ttingen in March 1811.[47] Around that time, the two men engaged in an epistolary correspondence.[48] However, when they met in person in 1825, they quarrelled; the details are unknown.[49]"b'Though he did take in a few students, Gauss was known to dislike teaching. It is said that he attended only a single scientific conference, which was in Berlin in 1828. However, several of his students became influential mathematicians, among them Richard Dedekind and Bernhard Riemann.'b'Carl Gauss was an ardent perfectionist and a hard worker. He was never a prolific writer, refusing to publish work which he did not consider complete and above criticism. This was in keeping with his personal motto pauca sed matura ("few, but ripe"). His personal diaries indicate that he had made several important mathematical discoveries years or decades before his contemporaries published them. Scottish-American mathematician and writer Eric Temple Bell said that if Gauss had published all of his discoveries in a timely manner, he would have advanced mathematics by fifty years.[46]'b'Gauss eventually had conflicts with his sons. He did not want any of his sons to enter mathematics or science for "fear of lowering the family name", as he believed none of them would surpass his own achievements.[45] Gauss wanted Eugene to become a lawyer, but Eugene wanted to study languages. They had an argument over a party Eugene held, which Gauss refused to pay for. The son left in anger and, in about 1832, emigrated to the United States, where he was quite successful. While working for the American Fur Company in the Midwest, he learned the Sioux language. Later, he moved to Missouri and became a successful businessman. Wilhelm also moved to America in 1837 and settled in Missouri, starting as a farmer and later becoming wealthy in the shoe business in St. Louis. It took many years for Eugene\'s success to counteract his reputation among Gauss\'s friends and colleagues. See also the letter from Robert Gauss to Felix Klein on 3 September 1912.'b"Gauss had six children. With Johanna (1780\xe2\x80\x931809), his children were Joseph (1806\xe2\x80\x931873), Wilhelmina (1808\xe2\x80\x931846) and Louis (1809\xe2\x80\x931810). With Minna Waldeck he also had three children: Eugene (1811\xe2\x80\x931896), Wilhelm (1813\xe2\x80\x931879) and Therese (1816\xe2\x80\x931864). Eugene shared a good measure of Gauss's talent in languages and computation.[45] Therese kept house for Gauss until his death, after which she married."b"Gauss's personal life was overshadowed by the early death of his first wife, Johanna Osthoff, in 1809, soon followed by the death of one child, Louis. Gauss plunged into a depression from which he never fully recovered. He married again, to Johanna's best friend, Friederica Wilhelmine Waldeck, commonly known as Minna. When his second wife died in 1831 after a long illness,[44] one of his daughters, Therese, took over the household and cared for Gauss for the rest of his life. His mother lived in his house from 1817 until her death in 1839.[2]"b'Though he was not a church-goer,[42] Gauss strongly upheld religious tolerance, believing "that one is not justified in disturbing another\'s religious belief, in which they find consolation for earthly sorrows in time of trouble."[2] When his son Eugene announced that he wanted to become a Christian missionary, Gauss approved of this, saying that regardless of the problems within religious organizations, missionary work was "a highly honorable" task.[43]'b'Gauss declared he firmly believed in the afterlife, and saw spirituality as something essentially important for human beings.[39] He was quoted stating: "The world would be nonsense, the whole creation an absurdity without immortality,"[40] and for this statement he was severely criticized by the atheist Eugen D\xc3\xbchring who judged him as a narrow superstitious man.[41]'b"Dunnington further elaborates on Gauss's religious views by writing:"b'In connection to this, there is a record of a conversation between Rudolf Wagner and Gauss, in which they discussed William Whewell\'s book Of the Plurality of Worlds. In this work, Whewell had discarded the possibility of existing life in other planets, on the basis of theological arguments, but this was a position with which both Wagner and Gauss disagreed. Later Wagner explained that he did not fully believe in the Bible, though he confessed that he "envied" those who were able to easily believe.[31][35] This later led them to discuss the topic of faith, and in some other religious remarks, Gauss said that he had been more influenced by theologians like Lutheran minister Paul Gerhardt than by Moses.[36] Other religious influences included Wilhelm Braubach, Johann Peter S\xc3\xbcssmilch, and the New Testament.[37]'b"Apart from his correspondence, there are not many known details about Gauss' personal creed. Many biographers of Gauss disagree about his religious stance, with B\xc3\xbchler and others considering him a deist with very unorthodox views,[31][32][33] while Dunnington (though admitting that Gauss did not believe literally in all Christian dogmas and that it is unknown what he believed on most doctrinal and confessional questions) points out that he was, at least, a nominal Lutheran.[34]"b'Gauss was a Lutheran Protestant, a member of the St. Albans Evangelical Lutheran church in G\xc3\xb6ttingen.[28] Potential evidence that Gauss believed in God comes from his response after solving a problem that had previously defeated him: "Finally, two days ago, I succeeded\xe2\x80\x94 not on account of my hard efforts, but by the grace of the Lord."[29] One of his biographers, G. Waldo Dunnington, described Gauss\'s religious views as follows:'b"On 23 February 1855, Gauss died of a heart attack in G\xc3\xb6ttingen (then Kingdom of Hanover and now Lower Saxony);[3][18] he is interred in the Albani Cemetery there. Two people gave eulogies at his funeral: Gauss's son-in-law Heinrich Ewald, and Wolfgang Sartorius von Waltershausen, who was Gauss's close friend and biographer. Gauss's brain was preserved and was studied by Rudolf Wagner, who found its mass to be slightly above average, at 1,492\xc2\xa0grams, and the cerebral area equal to 219,588 square millimeters[26] (340.362 square inches). Highly developed convolutions were also found, which in the early 20th century were suggested as the explanation of his genius.[27]"b'In 1854, Gauss selected the topic for Bernhard Riemann\'s Habilitationsvortrag, "\xc3\x9cber die Hypothesen, welche der Geometrie zu Grunde liegen" (habilitation lecture About the hypotheses that underlie Geometry).[24] On the way home from Riemann\'s lecture, Weber reported that Gauss was full of praise and excitement.[25]'b'In 1845, he became an associated member of the Royal Institute of the Netherlands; when that became the Royal Netherlands Academy of Arts and Sciences in 1851, he joined as a foreign member.[23]'b'In 1840, Gauss published his influential Dioptrische Untersuchungen,[19] in which he gave the first systematic analysis on the formation of images under a paraxial approximation (Gaussian optics).[20] Among his results, Gauss showed that under a paraxial approximation an optical system can be characterized by its cardinal points[21] and he derived the Gaussian lens formula.[22]'b'Gauss remained mentally active into his old age, even while suffering from gout and general unhappiness.[18] For example, at the age of 62, he taught himself Russian.[18]'b'In 1831, Gauss developed a fruitful collaboration with the physics professor Wilhelm Weber, leading to new knowledge in magnetism (including finding a representation for the unit of magnetism in terms of mass, charge, and time) and the discovery of Kirchhoff\'s circuit laws in electricity.[18] It was during this time that he formulated his namesake law. They constructed the first electromechanical telegraph in 1833,[18] which connected the observatory with the institute for physics in G\xc3\xb6ttingen. Gauss ordered a magnetic observatory to be built in the garden of the observatory, and with Weber founded the "Magnetischer Verein" (magnetic club in German), which supported measurements of Earth\'s magnetic field in many regions of the world. He developed a method of measuring the horizontal intensity of the magnetic field which was in use well into the second half of the 20th century, and worked out the mathematical theory for separating the inner and outer (magnetospheric) sources of Earth\'s magnetic field.'b'On 9 October 1805,[15] Gauss married Johanna Osthoff (1780-1809), and had a son and a daughter with her.[15][16] Tragically, she died on 11 October 1809,[15][16][17] and her most recent child, Louis, died the following year.[15] He then married Minna Waldeck (1788-1831)[15][16] on 4 August 1810,[15] and had three more children.[16] Gauss was never quite the same without his first wife, so he grew to dominate his children, just like his father.[16] Tragedy struck the Gauss family again when Minna Waldeck died on 12 September 1831.[15][16]'b'In 1801, Gauss announced that he had calculated the orbit of an asteroid by the name of Ceres.[10] He also allowed some of his genius to be made public with the publication of Disquisitiones Arithmeticae,[10] and consequently gained widespread fame.[10]'b'Gauss also discovered that every positive integer is representable as a sum of at most three triangular numbers on 10 July and then jotted down in his diary the note: "\xce\x95\xce\xa5\xce\xa1\xce\x97\xce\x9a\xce\x91! num = \xce\x94 + \xce\x94\' + \xce\x94". On 1 October he published a result on the number of solutions of polynomials with coefficients in finite fields, which 150 years later led to the Weil conjectures.'b'The year 1796 was most productive for both Gauss and number theory. He discovered a construction of the heptadecagon on 30 March.[10][14] He further advanced modular arithmetic, greatly simplifying manipulations in number theory. On 8 April he became the first to prove the quadratic reciprocity law. This remarkably general law allows mathematicians to determine the solvability of any quadratic equation in modular arithmetic. The prime number theorem, conjectured on 31 May, gives a good understanding of how the prime numbers are distributed among the integers.'b"Gauss's intellectual abilities attracted the attention of the Duke of Brunswick,[6][2] who sent him to the Collegium Carolinum (now Braunschweig University of Technology),[6] which he attended from 1792 to 1795,[9] and to the University of G\xc3\xb6ttingen from 1795 to 1798.[10] While at university, Gauss independently rediscovered several important theorems.[11] His breakthrough occurred in 1796 when he showed that a regular polygon can be constructed by compass and straightedge if the number of its sides is the product of distinct Fermat primes and a power of 2.[12] This was a major discovery in an important field of mathematics; construction problems had occupied mathematicians since the days of the Ancient Greeks, and the discovery ultimately led Gauss to choose mathematics instead of philology as a career. Gauss was so pleased with this result that he requested that a regular heptadecagon be inscribed on his tombstone. The stonemason declined, stating that the difficult construction would essentially look like a circle.[13]"b'Gauss was a child prodigy. A contested story relates that, when he was eight, he figured out how to add up all the numbers from 1 to 100.[6][7][8] There are many other anecdotes about his precocity while a toddler, and he made his first ground-breaking mathematical discoveries while still a teenager. He completed his magnum opus, Disquisitiones Arithmeticae, in 1798, at the age of 21\xe2\x80\x94though it was not published until 1801. This work was fundamental in consolidating number theory as a discipline and has shaped the field to the present day.'b'Johann Carl Friedrich Gauss was born on 30 April 1777 in Brunswick (Braunschweig), in the Duchy of Brunswick-Wolfenb\xc3\xbcttel (now part of Lower Saxony, Germany), to poor, working-class parents.[3] His mother was illiterate and never recorded the date of his birth, remembering only that he had been born on a Wednesday, eight days before the Feast of the Ascension (which occurs 39 days after Easter). Gauss later solved this puzzle about his birthdate in the context of finding the date of Easter, deriving methods to compute the date in both past and future years.[4] He was christened and confirmed in a church near the school he attended as a child.[5]'b''b''b'Sometimes referred to as the Princeps mathematicorum[1] (Latin\xc2\xa0for "the foremost of mathematicians") and "the greatest mathematician since antiquity", Gauss had an exceptional influence in many fields of mathematics and science, and is ranked among history\'s most influential mathematicians.[2]'
Gaussian elimination
b'Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.'b'This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.'b'Gaussian elimination does not generalize in any way to higher order tensors (matrices are array representations of order 2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]'b'The Gaussian elimination can be performed over any field, not just the real numbers.'b'One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means any error that existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]'b'This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).'b"The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n+1) / 2 divisions, (2n3 + 3n2 \xe2\x88\x92 5n)/6 multiplications, and (2n3 + 3n2 \xe2\x88\x92 5n)/6 subtractions,[8] for a total of approximately 2n3 / 3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9] However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5)."b'All of this applies also to the reduced row echelon form, which is a particular row echelon form.'b'One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A\xe2\x88\x921. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.'b'By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:'b'To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 by 6 matrix:'b'For example, consider the following matrix'b'A variant of Gaussian elimination called Gauss\xe2\x80\x93Jordan elimination can be used for finding the inverse of a matrix, if it exists. If A is a n by n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n by n identity matrix is augmented to the right of A, forming a n by 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n by 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A\xe2\x88\x921. If the algorithm is unable to reduce the left block to I, then A is not invertible.'b'Computationally, for a n\xc3\x97n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.'b'If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B: det(A) = \xe2\x88\x8fdiag(B) / d.'b'To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:'b'The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.'b'Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss\xe2\x80\x93Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss\xe2\x80\x93Jordan elimination independently.[7]'b'The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life. The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]'b'The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.'b'Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss-Jordan elimination, to distinguish it from stopping after reaching echelon form.'b'Once y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = -1, y = 3, and x = 2. So there is a unique solution to the original system of equations.'b'Suppose the goal is to find and describe the set of solutions to the following system of linear equations:'b'A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).'b'It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).'b'For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.'b'For each row in a matrix, if the row does not consist of only zeros, then the left-most non-zero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 (see above) could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.'b"If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier."b'There are three types of elementary row operations which may be performed on the rows of a matrix:'b'Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.'b'The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called Forward Elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.'b''b''b'Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss\xe2\x80\x93Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.'b'To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.'b'In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777\xe2\x80\x931855), although it was known to Chinese mathematicians as early as 179 CE (see History section).'
Geodesy
b'Techniques for studying geodynamic phenomena on the global scale include:'b"The science of studying deformations and motions of the Earth's crust and the solid Earth as a whole is called geodynamics. Often, study of the Earth's irregular rotation is also included in its definition."b"In geodesy, temporal change can be studied by a variety of techniques. Points on the Earth's surface change their location due to a variety of mechanisms:"b'A metre was originally defined as the 10-millionth part of the length of a meridian (the target was not quite reached in actual implementation, so that is off by 200 ppm in the current definitions). This means that one kilometre is roughly equal to (1/40,000) * 360 * 60 meridional minutes of arc, which equals 0.54 nautical mile, though this is not exact because the two units are defined on different bases (the international nautical mile is defined as exactly 1,852 m, corresponding to a rounding of 1,000/0.54 m to four digits).'b'One geographical mile, defined as one minute of arc on the equator, equals 1,855.32571922 m. One nautical mile is one minute of astronomical latitude. The radius of curvature of the ellipsoid varies with latitude, being the longest at the pole and the shortest at the equator as is the nautical mile.'b'Geographical latitude and longitude are stated in the units degree, minute of arc, and second of arc. They are angles, not metric measures, and describe the direction of the local normal to the reference ellipsoid of revolution. This is approximately the same as the direction of the plumbline, i.e., local gravity, which is also the normal to the geoid surface. For this reason, astronomical position determination \xe2\x80\x93 measuring the direction of the plumbline by astronomical means \xe2\x80\x93 works fairly well provided an ellipsoidal model of the figure of the Earth is used.'b'In the future gravity, and altitude, will be measured by relativistic time dilation measured by strontium optical clocks.'b'Gravity is measured using gravimeters, of which there are two kinds. First, "absolute gravimeters" are based on measuring the acceleration of free fall (e.g., of a reflecting prism in a vacuum tube). They are used to establish the vertical geospatial control and can be used in the field. Second, "relative gravimeters" are spring-based and are more common. They are used in gravity surveys over large areas for establishing the figure of the geoid over these areas. The most accurate relative gravimeters are called "superconducting" gravimeters, which are sensitive to one-thousandth of one-billionth of Earth surface gravity. Twenty-some superconducting gravimeters are used worldwide for studying Earth\'s tides, rotation, interior, and ocean and atmospheric loading, as well as for verifying the Newtonian constant of gravitation.'b"GPS receivers have almost completely replaced terrestrial instruments for large-scale base network surveys. For planet-wide geodetic surveys, previously impossible, we can still mention satellite laser ranging (SLR) and lunar laser ranging (LLR) and very-long-baseline interferometry (VLBI) techniques. All these techniques also serve to monitor irregularities in the Earth's rotation as well as plate tectonic motions."b'Geodetic GPS receivers produce directly three-dimensional coordinates in a geocentric coordinate frame. Such a frame is, e.g., WGS84, or the frames that are regularly produced and published by the International Earth Rotation and Reference Systems Service (IERS).'b'For local detail surveys, tacheometers are commonly employed although the old-fashioned rectangular technique using angle prism and steel tape is still an inexpensive alternative. Real-time kinematic (RTK) GPS techniques are used as well. Data collected are tagged and recorded digitally for entry into a Geographic Information System (GIS) database.'b'The theodolite is used to measure horizontal and vertical angles to target points. These angles are referred to the local vertical. The tacheometer additionally determines, electronically or electro-optically, the distance to target, and is highly automated to even robotic in its operations. The method of free station position is widely used.'b'The level is used for determining height differences and height reference systems, commonly referred to mean sea level. The traditional spirit level produces these practically most useful heights above sea level directly; the more economical use of GPS instruments for height determination requires precise knowledge of the figure of the geoid, as GPS only gives heights above the GRS80 reference ellipsoid. As geoid knowledge accumulates, one may expect use of GPS heighting to spread.'b'Here we define some basic observational concepts, like angles and coordinates, defined in geodesy (and astronomy as well), mostly from the viewpoint of the local observer.'b"On the ellipsoid of revolution, geodesics may be written in terms of elliptic integrals, which are usually evaluated in terms of a series expansion\xe2\x80\x94see, for example, Vincenty's formulae. In the general case, the solution is called the geodesic for the surface considered. The differential equations for the geodesic can be solved numerically."b"In plane geometry (valid for small areas on the Earth's surface), the solutions to both problems reduce to simple trigonometry. On a sphere, however, the solution is significantly more complex, because in the inverse problem the azimuths will differ between the two end points of the connecting great circle, arc."b'In geometric geodesy, two standard problems exist\xe2\x80\x94the first (direct or forward) and the second (inverse or reverse).'b'One purpose of point positioning is the provision of known points for mapping measurements, also known as (horizontal and vertical) control. In every country, thousands of such known points exist and are normally documented by national mapping agencies. Surveyors involved in real estate and insurance will use these to tie their local measurements.'b'For surveying mappings, frequently Real Time Kinematic GPS is employed, tying in the unknown points with known terrestrial points close by in real time.'b'Nowadays all but special measurements (e.g., underground or high-precision engineering measurements) are performed with GPS. The higher-order networks are measured with static GPS, using differential measurement to determine vectors between terrestrial points. These vectors are then adjusted in traditional network fashion. A global polyhedron of permanently operating GPS stations under the auspices of the IERS is used to define a single global, geocentric reference frame which serves as the "zero order" global reference to which national measurements are attached.'b'Traditionally, a hierarchy of networks has been built to allow point positioning within a country. Highest in the hierarchy were triangulation networks. These were densified into networks of traverses (polygons), into which local mapping surveying measurements, usually with measuring tape, corner prism, and the familiar[where?] red and white poles, are tied.'b'Point positioning is the determination of the coordinates of a point on land, at sea, or in space with respect to a coordinate system. Point position is solved by computation from measurements linking the known positions of terrestrial or extraterrestrial points with the unknown terrestrial position. This may involve transformations between or among astronomical and terrestrial coordinate systems. The known points used for point positioning can be triangulation points of a higher-order network or GPS satellites.'b'In the abstract, a coordinate system as used in mathematics and geodesy is called a "coordinate system" in ISO terminology, whereas the International Earth Rotation and Reference Systems Service (IERS) uses the term "reference system". When these coordinates are realized by choosing datum points and fixing a geodetic datum, ISO says "coordinate reference system", while IERS says "reference frame". The ISO term for a datum transformation again is a "coordinate transformation".[3]'b'Changing the coordinates of a point set referring to one datum, so to make them refer to another datum, is called a datum transformation. In the case of vertical data, this consists of simply adding a constant shift to all height values. In the case of plane or spatial coordinates, datum transformation takes the form of a similarity or Helmert transformation, consisting of a rotation and scaling operation in addition to a simple translation. In the plane, a Helmert transformation has four parameters; in space, seven.'b'In case of plane or spatial coordinates, we typically need several datum points. A regional, ellipsoidal datum like ED\xc2\xa050 can be fixed by prescribing the undulation of the geoid and the deflection of the vertical in one datum point, in this case the Helmert Tower in Potsdam. However, an overdetermined ensemble of datum points can also be used.'b'In the case of height data, it suffices to choose one datum point: the reference benchmark, typically a tide gauge at the shore. Thus we have vertical data like the NAP (Normaal Amsterdams Peil), the North American Vertical Datum 1988 (NAVD\xc2\xa088), the Kronstadt datum, the Trieste datum, and so on.'b'Because geodetic point coordinates (and heights) are always obtained in a system that has been constructed itself using real observations, geodesists introduce the concept of a "geodetic datum": a physical realization of a coordinate system used for describing point locations. The realization is the result of choosing conventional coordinate values for one or more datum points.'b'None of these heights is in any way related to geodetic or ellipsoidial heights, which express the height of a point above the reference ellipsoid. Satellite positioning receivers typically provide ellipsoidal heights, unless they are fitted with special conversion software based on a model of the geoid.'b'Each has its advantages and disadvantages. Both orthometric and normal heights are heights in metres above sea level, whereas geopotential numbers are measures of potential energy (unit: m2\xc2\xa0s\xe2\x88\x922) and not metric. Orthometric and normal heights differ in the precise way in which mean sea level is conceptually continued under the continental masses. The reference surface for orthometric heights is the geoid, an equipotential surface approximating mean sea level.'b'Heights come in the following variants:'b'In geodesy, point or terrain heights are "above sea level", an irregular, physically defined surface. Therefore, a height should ideally not be referred to as a coordinate. It is more like a physical quantity, and though it can be tempting to treat height as the vertical coordinate z, in addition to the horizontal coordinates x and y, and though this actually is a good approximation of physical reality in small areas, it quickly becomes invalid for regional considerations.[specify]'b'The reverse transformation is given by:'b'It is easy enough to "translate" between polar and rectangular coordinates in the plane: let, as above, direction and distance be \xce\xb1 and s respectively, then we have'b'An example of such a projection is UTM (Universal Transverse Mercator). Within the map plane, we have rectangular coordinates x and y. In this case the north direction used for reference is the map north, not the local north. The difference between the two is called meridian convergence.'b"Rectangular coordinates in the plane can be used intuitively with respect to one's current location, in which case the x-axis will point to the local north. More formally, such coordinates can be obtained from three-dimensional coordinates using the artifice of a map projection. It is not possible to map the curved surface of the Earth onto a flat map surface without deformation. The compromise most often chosen\xe2\x80\x94called a conformal projection\xe2\x80\x94preserves angles and length ratios, so that small circles are mapped as small circles and small squares as squares."b'In surveying and mapping, important fields of application of geodesy, two general types of coordinate systems are used in the plane:'b"The coordinate transformation between these two systems is described to good approximation by (apparent) sidereal time, which takes into account variations in the Earth's axial rotation (length-of-day variations). A more accurate description also takes polar motion into account, a phenomenon closely monitored by geodesists."b'Geocentric coordinate systems used in geodesy can be divided naturally into two classes:'b'It is only because GPS satellites orbit about the geocenter, that this point becomes naturally the origin of a coordinate system defined by satellite geodetic means, as the satellite positions in space are themselves computed in such a system.'b'Prior to the era of satellite geodesy, the coordinate systems associated with a geodetic datum attempted to be geocentric, but their origins differed from the geocenter by hundreds of meters, due to regional deviations in the direction of the plumbline (vertical). These regional geodetic data, such as ED\xc2\xa050 (European Datum 1950) or NAD\xc2\xa027 (North American Datum 1927) have ellipsoids associated with them that are regional "best fits" to the geoids within their areas of validity, minimizing the deflections of the vertical over these areas.'b"The locations of points in three-dimensional space are most conveniently described by three cartesian or rectangular coordinates, X, Y and Z. Since the advent of satellite positioning, such coordinate systems are typically geocentric: the Z-axis is aligned with the Earth's (conventional or instantaneous) rotation axis."b'The geoid is "realizable", meaning it can be consistently located on the Earth by suitable simple measurements from physical objects like a tide gauge. The geoid can therefore be considered a real surface. The reference ellipsoid, however, has many possible instantiations and is not readily realizable, therefore it is an abstract surface. The third primary surface of geodetic interest\xe2\x80\x94the topographic surface of the Earth\xe2\x80\x94is a realizable surface.'b'The 1980 Geodetic Reference System (GRS\xc2\xa080) posited a 6,378,137\xc2\xa0m semi-major axis and a 1:298.257 flattening. This system was adopted at the XVII General Assembly of the International Union of Geodesy and Geophysics (IUGG). It is essentially the basis for geodetic positioning by the Global Positioning System (GPS) and is thus also in widespread use outside the geodetic community. The numerous systems that countries have used to create maps and charts are becoming obsolete as countries increasingly move to global, geocentric reference systems using the GRS\xc2\xa080 reference ellipsoid.'b'A reference ellipsoid, customarily chosen to be the same size (volume) as the geoid, is described by its semi-major axis (equatorial radius) a and flattening f. The quantity f\xc2\xa0= a \xe2\x88\x92 b/a, where b is the semi-minor axis (polar radius), is a purely geometrical one. The mechanical ellipticity of the Earth (dynamical flattening, symbol J2) can be determined to high precision by observation of satellite orbit perturbations. Its relationship with the geometrical flattening is indirect. The relationship depends on the internal density distribution, or, in simplest terms, the degree of central concentration of mass.'b'The geoid is essentially the figure of the Earth abstracted from its topographical features. It is an idealized equilibrium surface of sea water, the mean sea level surface in the absence of currents and air pressure variations, and continued under the continental masses. The geoid, unlike the reference ellipsoid, is irregular and too complicated to serve as the computational surface on which to solve geometrical problems like point positioning. The geometrical separation between the geoid and the reference ellipsoid is called the geoidal undulation. It varies globally between \xc2\xb1110\xc2\xa0m, when referred to the GRS\xc2\xa080 ellipsoid.'b"To a large extent, the shape of the Earth is the result of its rotation, which causes its equatorial bulge, and the competition of geological processes such as the collision of plates and of volcanism, resisted by the Earth's gravity field. This applies to the solid surface, the liquid surface (dynamic sea surface topography) and the Earth's atmosphere. For this reason, the study of the Earth's gravity field is called physical geodesy."b'It is primarily concerned with positioning within the temporally varying gravity field. Geodesy in the German-speaking world is divided into "higher geodesy" ("Erdmessung" or "h\xc3\xb6here Geod\xc3\xa4sie"), which is concerned with measuring the Earth on the global scale, and "practical geodesy" or "engineering geodesy" ("Ingenieurgeod\xc3\xa4sie"), which is concerned with measuring specific parts or regions of the Earth, and which includes surveying. Such geodetic operations are also applied to other astronomical bodies in the solar system. It is also the science of measuring and understanding the earth\'s geometric shape, orientation in space, and gravity field.'b'The word "geodesy" comes from the Ancient Greek word \xce\xb3\xce\xb5\xcf\x89\xce\xb4\xce\xb1\xce\xb9\xcf\x83\xce\xaf\xce\xb1 geodaisia (literally, "division of the Earth").'b''b''b"Geodesy ( /d\xca\x92i\xcb\x90\xcb\x88\xc9\x92d\xc9\xaasi/)[1], also known as geodetics, is the science of accurately measuring and understanding three of Earth's fundamental properties\xe2\x80\x94its geometric shape, its orientation in space, and its gravity field\xe2\x80\x94as well as how they change over time.[2] Geodynamical phenomena include crustal motion, tides, and polar motion, which can be studied by designing global and national control networks, applying space and terrestrial techniques, and relying on datums and coordinate systems."
Geometry
b'Leonhard Euler, in studying problems like the Seven Bridges of K\xc3\xb6nigsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry geometria situs (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots.'b'Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.'b'While the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).[53]'b"An important area of application is number theory. In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem."b'Geometry has also had a large effect on other areas of mathematics. For instance, the introduction of coordinates by Ren\xc3\xa9 Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.'b'Modern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.'b'The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.'b'Mathematics and architecture are related, since, as with other arts, architects use mathematics for several reasons. Apart from the mathematics needed when engineering buildings, architects use geometry: to define the spatial form of a building; from the Pythagoreans of the sixth century BC onwards, to create forms considered harmonious, and thus to lay out buildings and their surroundings according to mathematical, aesthetic and sometimes religious principles; to decorate buildings with mathematical objects such as tessellations; and to meet environmental goals, such as to minimise wind speeds around the bases of tall buildings.'b'Mathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.'b'Geometry has found applications in many fields, some of which are described below.'b'The study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 ("algebraic threefolds"), has been far advanced. Gr\xc3\xb6bner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane theory.'b'The field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.'b"The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.[citation needed][dubious \xe2\x80\x93 discuss]"b"Differential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is intrinsic, meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not a priori parts of some ambient flat Euclidean space."b'Euclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.'b"In the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, absolute, geometry, which is known to be true a priori by an inner faculty of mind: Euclidean geometry was synthetic a priori.[51] This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed by Riemann in his 1867 inauguration lecture \xc3\x9cber die Hypothesen, welche der Geometrie zu Grunde liegen (On the hypotheses on which geometry is based),[52] published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry."b'A different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.'b"The theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M. C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry."b'The issue of dimension still matters to geometry, in the absence of complete answers to classic questions. Dimensions 3 of space and 4 of space-time are special cases in geometric topology. Dimension 10 or 11 is a key number in string theory. Research may bring a satisfactory geometric reason for the significance of 10 and 11 dimensions.'b'Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. Dimension has gone through stages of being any natural number n, possibly infinite with the introduction of Hilbert space, and any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses definitions; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything a priori.'b'Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.'b'A topology is a mathematical structure on a set that tells how elements of the set relate spatially to each other.[37] The best-known examples of topologies come from metrics, which are ways of measuring distances between points.[49] For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.[50]'b'Manifolds are used extensively in physics, including in general relativity and string theory[48]'b'A manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space.[37] In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.[45]'b"A surface is a two-dimensional object, such as a sphere or paraboloid.[47] In differential geometry[45] and topology,[37] surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.[46]"b'In topology, a curve is defined by a function from an interval of the real numbers to another space.[37] In differential geometry, the same definition is used, but the defining function is required to be differentiable [45] Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.[46]'b'A curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.[44]'b'In differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.[42][43]'b'In Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right.[31] The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.[41]'b'Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[31] In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[40]'b'A plane is a flat, two-dimensional surface that extends infinitely far.[31] Planes are used in every area of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles;[37] it can be studied as an affine space, where collinearity and ratios can be studied but not distances;[38] it can be studied as the complex plane using techniques of complex analysis;[39] and so on.'b'Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself".[31] In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation,[34] but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[35] In differential geometry, a geodesic is a generalization of the notion of a line to curved spaces.[36]'b"Points are considered fundamental objects in Euclidean geometry. They have been defined in a variety of ways, including Euclid's definition as 'that which has no part'[31] and through the use of algebra or nested sets.[32] In many areas of geometry, such as analytic geometry, differential geometry, and topology, all objects are considered to be built up from points. However, there has been some study of geometry without reference to points.[33]"b"Euclid took an abstract approach to geometry in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792\xe2\x80\x931856), J\xc3\xa1nos Bolyai (1802\xe2\x80\x931860), Carl Friedrich Gauss (1777\xe2\x80\x931855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862\xe2\x80\x931943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry."b'The following are some of the most important concepts in geometry.[6][7]'b'Two developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, J\xc3\xa1nos Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826\xe2\x80\x931866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincar\xc3\xa9, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.'b'In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by Ren\xc3\xa9 Descartes (1596\xe2\x80\x931650) and Pierre de Fermat (1601\xe2\x80\x931665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591\xe2\x80\x931661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.'b"In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry.[26][27] Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[28] Th\xc4\x81bit ibn Qurra (known as Thebit in Latin) (836\xe2\x80\x93901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry.[4] Omar Khayy\xc3\xa1m (1048\xe2\x80\x931131) found geometric solutions to cubic equations.[29] The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c.\xc2\xa01230\xe2\x80\x93c.\xc2\xa01314), Gersonides (1288\xe2\x80\x931344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.[30]"b'Indian mathematicians also made many important contributions in geometry. The Satapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.[3] According to (Hayashi 2005, p.\xc2\xa0363), the \xc5\x9aulba S\xc5\xabtras contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples,[22] which are particular cases of Diophantine equations.[23] In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero."[24] Aryabhata\'s Aryabhatiya (499) includes the computation of areas and volumes. Brahmagupta wrote his astronomical work Br\xc4\x81hma Sphu\xe1\xb9\xada Siddh\xc4\x81nta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[25] In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron\'s formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).[25]'b"In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.[1] Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,[14] though the statement of the theorem has a long history.[15][16] Eudoxus (408\xe2\x80\x93c.\xc2\xa0355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[17] as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time,[18] introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[19] The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[20] Archimedes (c.\xc2\xa0287\xe2\x80\x93212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi.[21] He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution."b"The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC.[8][9] Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000\xe2\x80\x931800 BC) and Moscow Papyrus (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum.[10] Later clay tablets (350\xe2\x80\x9350 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries.[11] South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.[12][13]"b'Contemporary geometry has many subfields:'b''b''b'Geometry has applications to many fields, including art, architecture, physics, as well as to other branches of mathematics.'b'While geometry has evolved significantly throughout the years, there are some general concepts that are more or less fundamental to geometry. These include the concepts of points, lines, planes, surfaces, angles, and curves, as well as the more advanced notions of manifolds and topology or metric.[6]'b"Geometry arose independently in a number of early cultures as a practical way for dealing with lengths, areas, and volumes. Geometry began to see elements of formal mathematical science emerging in the West as early as the 6th century BC.[1] By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment, Euclid's Elements, set a standard for many centuries to follow.[2] Geometry arose independently in India, with texts providing rules for geometric constructions appearing as early as the 3rd century BC.[3] Islamic scientists preserved Greek ideas and expanded on them during the Middle Ages.[4] By the early 17th century, geometry had been put on a solid analytic footing by mathematicians such as Ren\xc3\xa9 Descartes and Pierre de Fermat. Since then, and into modern times, geometry has expanded into non-Euclidean geometry and manifolds, describing spaces that lie beyond the normal range of human experience.[5]"b'Geometry (from the Ancient Greek: \xce\xb3\xce\xb5\xcf\x89\xce\xbc\xce\xb5\xcf\x84\xcf\x81\xce\xaf\xce\xb1; geo- "earth", -metron "measurement") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer.'
Line (geometry)
b'The "shortness" and "straightness" of a line, interpreted as the property that the distance along the line between any two of its points is minimized (see triangle inequality), can be generalized and leads to the concept of geodesics in metric spaces.'b'A line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these, if they are coplanar and either do not intersect or are collinear.'b'In topology, a ray in a space X is a continuous embedding R+ \xe2\x86\x92 X. It is used to define the important concept of end of the space.'b'The definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.'b'In Euclidean geometry two rays with a common endpoint form an angle.'b'Thus, we would say that two different points, A and B, define a line and a decomposition of this line into the disjoint union of an open segment (A,\xe2\x80\x89B) and two rays, BC and AD (the point D is not drawn in the diagram, but is to the left of A on the line AB). These are not opposite rays since they have different initial points.'b'Given distinct points A and B, they determine a unique ray with initial point A. As two points define a unique line, this ray consists of all the points between A and B (including A and B) and all the points C on the line through A and B such that B is between A and C.[15] This is, at times, also expressed as the set of all points C such that A is not between B and C.[16] A point D, on the line determined by A and B but not in the ray with initial point A determined by B, will determine another ray with initial point A. With respect to the AB ray, the AD ray is called the opposite ray.'b'Given a line and any point A on it, we may consider A as decomposing this line into two parts. Each such part is called a ray (or half-line) and the point A is called its initial point. The point A is considered to be a member of the ray.[14] Intuitively, a ray consists of those points on a line passing through A and proceeding indefinitely, starting at A, in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.'b'In many models of projective geometry, the representation of a line rarely conforms to the notion of the "straight curve" as it is visualised in Euclidean geometry. In elliptic geometry we see a typical example of this.[13] In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.'b'In three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.'b'Perpendicular lines are lines that intersect at right angles.'b'Parallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other\xe2\x80\x94every point that is on either one of them is also on the other.'b'For a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.'b'For a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.'b'With respect to triangles we have:'b'For more general algebraic curves, lines could also be:'b'In the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.'b'In a sense,[12] all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:'b'In the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.'b'However, there are other notions of distance (such as the Manhattan distance) for which this property is not true.'b'In Euclidean geometry, the Euclidean distance d(a,b) between two points a and b may be used to express the collinearity between three points by:[10][11]'b'Equivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, k points in a plane are collinear if and only if any (k\xe2\x80\x931) pairs of points have the same pairwise slopes.'b'has a rank less than 3. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.'b'In affine coordinates, in n-dimensional space the points X=(x1, x2, ..., xn), Y=(y1, y2, ..., yn), and Z=(z1, z2, ..., zn) are collinear if the matrix'b'Three points are said to be collinear if they lie on the same line. Three points usually determine a plane, but in the case of three collinear points this does not happen.'b'The direction of the line is from a (t = 0) to b (t = 1), or in other words, in the direction of the vector b\xc2\xa0\xe2\x88\x92\xc2\xa0a. Different choices of a and b can yield the same line.'b'In more general Euclidean space, Rn (and analogously in every other affine space), the line L passing through two different points a and b (considered as vectors) is the subset'b'In three-dimensional space, a first degree equation in the variables x, y, and z defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in n-dimensional space n-1 first-degree equations in the n coordinate variables define a line under suitable conditions.'b'A ray starting at point A is described by limiting \xce\xbb. One ray is obtained if \xce\xbb \xe2\x89\xa5 0, and the opposite ray comes from \xce\xbb \xe2\x89\xa4 0.'b'where m is the slope of the line.'b'The equation of a line which passes through the pole is simply given as:'b"Similarly, a horizontal line that doesn't pass through the pole is given by the equation"b'In polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,'b'where m is the slope of the line and b is the y-intercept. When \xce\xb8 = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:'b'In polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:'b'Unlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, \xce\xb8 and p, to be specified. If p > 0, then \xce\xb8 is uniquely defined modulo 2\xcf\x80. On the other hand, if the line is through the origin (c\xc2\xa0=\xc2\xa00, p\xc2\xa0=\xc2\xa00), one drops the c/|c| term to compute sin\xce\xb8 and cos\xce\xb8, and \xce\xb8 is only defined modulo \xcf\x80.'b'The normal form (also called the Hesse normal form,[9] after the German mathematician Ludwig Otto Hesse), is based on the normal segment for a given line, which is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:'b'They may also be described as the simultaneous solutions of two linear equations'b'where:'b'In three dimensions, lines can not be described by a single linear equation, so they are frequently described by parametric equations:'b'or'b'If x0 \xe2\x89\xa0 x1, this equation may be rewritten as'b'There are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.'b'with fixed real coefficients a, b and c such that a and b are not both zero. Using this form, vertical lines correspond to the equations with b = 0.'b'where:'b'\nIn two dimensions, the equation for non-vertical lines is often given in the slope-intercept form:'b'Lines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations.'b'Any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.'b"In an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians),[7] a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point.[8] In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not."b'When geometry was first formalised by Euclid in the Elements, he defined a general line (straight or curved) to be "breadthless length" with a straight line being a line "which lies evenly with the points on itself".[5] These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms,[6] but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.'b'In a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a description or mental image of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The "definition" of line in Euclid\'s Elements falls into this category.[4] Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.'b'All definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which cannot be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition.[2] In geometry, it is frequently the case that the concept of line is taken as a primitive.[3] In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.'b''b''b'When a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.'b'In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.'b'Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself"; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the 19th century (such as non-Euclidean, projective and affine geometry).'b'The notion of line or straight line was introduced by ancient mathematicians to represent straight objects (i.e., having no curvature) with negligible width and depth. Lines are an idealization of such objects. Until the 17th century, lines were defined in this manner: "The [straight or curved] line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which [\xe2\x80\xa6] will leave from its imaginary moving some vestige in length, exempt of any width. [\xe2\x80\xa6] The straight line is that which is equally extended between its points."[1]'
Plane (geometry)
b'The plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.'b'The one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the "north pole" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.'b'Alternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)'b"In addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere on the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point). This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature."b'In the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.'b'In the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.'b'Differential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.'b'The plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.'b'At one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.'b'In addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.'b'where'b'Another vector form for the equation of a plane, known as the Hesse normal form relies on the parameter D. This form is:[5]'b'and the point r0 can be taken to be any of the given points p1,p2 or p3[6] (or any other point in the plane).'b'This plane can also be described by the "point and a normal vector" prescription above. A suitable normal vector is given by the cross product'b'These equations are parametric in d. Setting d equal to any non-zero number and substituting it into these equations will yield one solution set.'b'If D is non-zero (so for planes not through the origin) the values for a, b and c can be calculated as follows:'b"This system can be solved using Cramer's rule and basic matrix manipulations. Let"b'The plane passing through p1, p2, and p3 can be described as the set of all points (x,y,z) that satisfy the following determinant equations:'b'Let p1=(x1, y1, z1), p2=(x2, y2, z2), and p3=(x3, y3, z3) be non-collinear points.'b'where s and t range over all real numbers, v and w are given linearly independent vectors defining the plane, and r0 is the vector representing the position of an arbitrary (but fixed) point on the plane. The vectors v and w can be visualized as vectors starting at r0 and pointing in different directions along the plane. Note that v and w can be perpendicular, but cannot be parallel.'b'Alternatively, a plane may be described parametrically as the set of all points of the form'b'Thus for example a regression equation of the form y = d + ax + cz (with b = \xe2\x88\x921) establishes a best-fit plane in three-dimensional space when there are two explanatory variables.'b'is a plane having the vector n = (a, b, c) as a normal.[4] This familiar equation for a plane is called the general form of the equation of the plane.[5]'b'Conversely, it is easily shown that if a, b, c and d are constants and a, b, and c are not all zero, then the graph of the equation'b'where'b'which is the point-normal form of the equation of a plane.[3] This is just a linear equation'b'(The dot here means a dot product, not scalar multiplication.) Expanded this becomes'b'Specifically, let r0 be the position vector of some point P0 = (x0, y0, z0), and let n = (a, b, c) be a nonzero vector. The plane determined by the point P0 and the vector n consists of those points P, with position vector r, such that the vector drawn from P0 to P is perpendicular to n. Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points r such that'b'In a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its "inclination".'b'The following statements hold in three-dimensional Euclidean space but not in higher dimensions, though they have higher-dimensional analogues:'b'In a Euclidean space of any number of dimensions, a plane is uniquely determined by any of the following:'b'This section is solely concerned with planes embedded in three dimensions: specifically, in R3.'b'A plane is a ruled surface.'b'Euclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry.[1] He selected a small core of undefined terms (called common notions) and postulates (or axioms) which he then used to prove various geometrical statements. Although the plane in its modern sense is not directly given a definition anywhere in the Elements, it may be thought of as part of the common notions.[2] Euclid never used numbers to measure length, angle, or area. In this way the Euclidean plane is not quite the same as the Cartesian plane.'b''b''b'When working exclusively in two-dimensional Euclidean space, the definite article is used, so, the plane refers to the whole space. Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, or, in other words, in the plane.'b"In mathematics, a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. Planes can arise as subspaces of some higher-dimensional space, as with a room's walls extended infinitely far, or they may enjoy an independent existence in their own right, as in the setting of Euclidean geometry."
Rotation (mathematics)
b'The complex-valued matrices analogous to real orthogonal matrices are the unitary matrices. The set of all unitary matrices in a given dimension n forms a unitary group U(n) of degree n; and its subgroup representing proper rotations[clarification needed] is the special unitary group SU(n) of degree n. These complex rotations are important in the context of spinors. The elements of SU(2) are used to parametrize three-dimensional Euclidean rotations (see above), as well as respective transformations of the spin (see representation theory of SU(2)).'b'As was stated above, Euclidean rotations are applied to rigid body dynamics. Moreover, most of mathematical formalism in physics (such as the vector calculus) is rotation-invariant; see rotation for more physical aspects. Euclidean rotations and, more generally, Lorentz symmetry described above are thought to be symmetry laws of nature. In contrast, the reflectional symmetry is not a precise symmetry law of nature.'b'Rotations define important classes of symmetry: rotational symmetry is an invariance with respect to a particular rotation. The circular symmetry is an invariance with respect to all rotation about the fixed axis.'b'Whereas SO(3) rotations, in physics and astronomy, correspond to rotations of celestial sphere as a 2-sphere in the Euclidean 3-space, Lorentz transformations from SO(3;1)+ induce conformal transformations of the celestial sphere. It is a broader class of the sphere transformations known as M\xc3\xb6bius transformations.'b'If a rotation is only in the three space dimensions, i.e. in a plane that is entirely in space, then this rotation is the same as a spatial rotation in three dimensions. But a rotation in a plane spanned by a space dimension and a time dimension is a hyperbolic rotation, a transformation between two different reference frames, which is sometimes called a "Lorentz boost". These transformations demonstrate the pseudo-Euclidean nature of the Minkowski space. They are sometimes described as squeeze mappings and frequently appear on Minkowski diagrams which visualize (1 + 1)-dimensional pseudo-Euclidean geometry on planar drawings. The study of relativity is concerned with the Lorentz group generated by the space rotations and hyperbolic rotations.[2]'b'One application of this is special relativity, as it can be considered to operate in a four-dimensional space, spacetime, spanned by three space dimensions and one of time. In special relativity this space is linear and the four-dimensional rotations, called Lorentz transformations, have practical physical interpretations. The Minkowski space is not a metric space, and the term isometry is inapplicable to Lorentz transformation.'b'Affine geometry and projective geometry have not a distinct notion of rotation.'b'In spherical geometry, a direct motion of the n-sphere (an example of the elliptic geometry) is the same as a rotation of (n\xe2\x80\x89+\xe2\x80\x891)-dimensional Euclidean space about the origin (SO(n\xe2\x80\x89+\xe2\x80\x891)). For odd n, most of these motions do not have fixed points on the n-sphere and, strictly speaking, are not rotations of the sphere; such motions are sometimes referred to as Clifford translations. Rotations about a fixed point in elliptic and hyperbolic geometries are not different from Euclidean ones.'b'The doubly covering group of SO(n) is known as the Spin group, Spin(n). It can be conveniently described in terms of Clifford algebra. Unit quaternions present the group Spin(3).'b'In general (and not necessarily for Euclidean vectors) the rotation of a vector space equipped with a quadratic form can be expressed as a bivector. This formalism is used in geometric algebra and, more generally, in the Clifford algebra representation of Lie groups.'b'As was demonstrated above, there exist three multilinear algebra rotation formalisms: one of U(1), or complex numbers, for two dimensions, and yet two of versors, or quaternions, for three and four dimensions.'b'The main disadvantage of matrices is that they are more expensive to calculate and do calculations with. Also in calculations where numerical instability is a concern matrices can be more prone to it, so calculations to restore orthonormality, which are expensive to do for matrices, need to be done more often.'b'Matrices are often used for doing transformations, especially when a large number of points are being transformed, as they are a direct representation of the linear operator. Rotations represented in other ways are often converted to matrices before being used. They can be extended to represent rotations and transformations at the same time using homogeneous coordinates. Projective transformations are represented by 4\xc3\x974 matrices. They are not rotation matrices, but a transformation that represents a Euclidean rotation has a 3\xc3\x973 rotation matrix in the upper left corner.'b'More generally, coordinate rotations in any dimension are represented by orthogonal matrices. The set of all orthogonal matrices in n dimensions which describe proper rotations (determinant = +1), together with the operation of matrix multiplication, forms the special orthogonal group SO(n).'b'A single multiplication by a versor, either left or right, is itself a rotation, but in four dimensions. Any four-dimensional rotation about the origin can be represented with two quaternion multiplications: one left and one right, by two different unit quaternions.'b'where v is the rotation vector treated as a quaternion.'b'where q is the versor, q\xe2\x88\x921 is its inverse, and x is the vector treated as a quaternion with zero scalar part. The quaternion can be related to the rotation vector form of the axis angle rotation by the exponential map over the quaternions,'b'A versor (also called a rotation quaternion) consists of four real numbers, constrained so the norm of the quaternion is 1. This constraint limits the degrees of freedom of the quaternion to three, as required. Unlike matrices and complex numbers two multiplications are needed:'b'Unit quaternions, or versors, are in some ways the least intuitive representation of three-dimensional rotations. They are not the three-dimensional instance of a general approach. They are more compact than matrices and easier to work with than all other methods, so are often preferred in real-world applications.[citation needed]'b'Another possibility to represent a rotation of three-dimensional Euclidean vectors are quaternions described below.'b'Above-mentioned Euler angles and axis\xe2\x80\x93angle representations can be easily converted to a rotation matrix.'b'The set of all appropriate matrices together with the operation of matrix multiplication is the rotation group SO(3). The matrix A is a member of the three-dimensional special orthogonal group, SO(3), that is it is an orthogonal matrix with determinant 1. That it is an orthogonal matrix means that its rows are a set of orthogonal unit vectors (so they are an orthonormal basis) as are its columns, making it simple to spot and check if a matrix is a valid rotation matrix.'b'This is multiplied by a vector representing the point to give the result'b'As in two dimensions, a matrix can be used to rotate a point (x,\xe2\x80\x89y,\xe2\x80\x89z) to a point (x\xe2\x80\xb2,\xe2\x80\x89y\xe2\x80\xb2,\xe2\x80\x89z\xe2\x80\xb2). The matrix used is a 3\xc3\x973 matrix,'b'Since complex numbers form a commutative ring, vector rotations in two dimensions are commutative, unlike in higher dimensions. They have only one degree of freedom, as such rotations are entirely determined by the angle of rotation.[1]'b'and equating real and imaginary parts gives the same result as a two-dimensional matrix:'b"This can be rotated through an angle \xce\xb8 by multiplying it by ei\xce\xb8, then expanding the product using Euler's formula as follows:"b' Points on the R2 plane can be also presented as complex numbers: the point (x,\xe2\x80\x89y) in the plane is represented by the complex number'b'The coordinates of the point after rotation are x\xe2\x80\xb2,\xe2\x80\x89y\xe2\x80\xb2, and the formulae for x\xe2\x80\xb2 and y\xe2\x80\xb2 are'b'In two dimensions, to carry out a rotation using a matrix, the point (x,\xe2\x80\x89y) to be rotated counterclockwise is written as a column vector, then multiplied by a rotation matrix calculated from the angle \xce\xb8:'b'As it was already stated, a (proper) rotation is different from an arbitrary fixed-point motion in its preservation of the orientation of the vector space. Thus, the determinant of a rotation orthogonal matrix must be 1. The only other possibility for the determinant of an orthogonal matrix is \xe2\x88\x921, and this result means the transformation is a hyperplane reflection, a point reflection (for odd n), or another kind of improper rotation. Matrices of all proper rotations form the special orthogonal group.'b'A motion that preserves the origin is the same as a linear operator on vectors that preserves the same geometric structure but expressed in terms of vectors. For Euclidean vectors, this expression is their magnitude (Euclidean norm). In components, such operator is expressed with n\xe2\x80\x89\xc3\x97\xe2\x80\x89n orthogonal matrix that is multiplied to column vectors.'b'When one considers motions of the Euclidean space that preserve the origin, the distinction between points and vectors, important in pure mathematics, can be erased because there is a canonical one-to-one correspondence between points and position vectors. The same is true for geometries other than Euclidean, but whose space is an affine space with a supplementary structure; see an example below. Alternatively, the vector description of rotations can be understood as a parametrization of geometric rotations up to their composition with translations. In other words, one vector rotation presents many equivalent rotations about all points in the space.'b'A general rotation in four dimensions has only one fixed point, the centre of rotation, and no axis of rotation; see rotations in 4-dimensional Euclidean space for details. Instead the rotation has two mutually orthogonal planes of rotation, each of which is fixed in the sense that points in each plane stay within the planes. The rotation has two angles of rotation, one for each plane of rotation, through which points in the planes rotate. If these are \xcf\x891 and \xcf\x892 then all points not in the planes rotate through an angle between \xcf\x891 and \xcf\x892. Rotations in four dimensions about a fixed point have six degrees of freedom. A four-dimensional direct motion in general position is a rotation about certain point (as in all even Euclidean dimensions), but screw operations exist also.'b'A three-dimensional rotation can be specified in a number of ways. The most usual methods are:'b''b'Rotations in three-dimensional space differ from those in two dimensions in a number of important ways. Rotations in three dimensions are generally not commutative, so the order in which rotations are applied is important even about the same point. Also, unlike the two-dimensional case, a three-dimensional direct motion, in general position, is not a rotation but a screw operation. Rotations about the origin have three degrees of freedom (see rotation formalisms in three dimensions for details), the same as the number of dimensions.'b'There are no non-trivial rotations in one dimension. In two dimensions, only a single angle is needed to specify a rotation about the origin \xe2\x80\x93 the angle of rotation that specifies an element of the circle group (also known as U(1)). The rotation is acting to rotate an object counterclockwise through an angle \xce\xb8 about the origin; see below for details. Composition of rotations sums their angles modulo 1 turn, which implies that all two-dimensional rotations about the same point commute. Rotations about different points, in general, do not commute. Any two-dimensional direct motion is either a translation or a rotation; see Euclidean plane isometry for details.'b'A motion of a Euclidean space is the same as its isometry: it leaves the distance between any two points unchanged after the transformation. But a (proper) rotation also has to preserve the orientation structure. The "improper rotation" term refers to isometries that reverse (flip) the orientation. In the language of group theory the distinction is expressed as direct vs indirect isometries in the Euclidean group, where the former comprise the identity component. Any direct Euclidean motion can be represented as a composition of a rotation about the fixed point and a translation.'b'Rotations of (affine) spaces of points and of respective vector spaces are not always clearly distinguished. The former are sometimes referred to as affine rotations (although the term is misleading), whereas the latter are vector rotations. See the article below for details.'b'A representation of rotations is a particular formalism, either algebraic or geometric, used to parametrize a rotation map. This meaning is somehow inverse to the meaning in the group theory.'b'For a particular rotation:'b'The rotation group is a Lie group of rotations about a fixed point. This (common) fixed point is called the center of rotation and is usually identified with the origin. The rotation group is a point stabilizer in a broader group of (orientation-preserving) motions.'b''b''b'Mathematically, a rotation is a map. All rotations about a fixed point form a group under composition called the rotation group (of a particular space). But in mechanics and, more generally, in physics, this concept is frequently understood as a coordinate transformation (importantly, a transformation of an orthonormal basis), because for any motion of a body there is an inverse transformation which if applied to the frame of reference results in the body being at the same coordinates. For example, in two dimensions rotating a body clockwise about a point keeping the axes fixed is equivalent to rotating the axes counterclockwise about the same point while the body is kept fixed. These two types of rotation are called active and passive transformations.'b'Rotation in mathematics is a concept originating in geometry. Any rotation is a motion of a certain space that preserves at least one point. It can describe, for example, the motion of a rigid body around a fixed point. A rotation is different from other types of motions: translations, which have no fixed points, and (hyperplane) reflections, each of them having an entire (n\xe2\x80\x89\xe2\x88\x92\xe2\x80\x891)-dimensional flat of fixed points in a n-dimensional space. A clockwise rotation is a negative magnitude so a counterclockwise turn has a positive magnitude.'
Functional analysis
b'Functional analysis in its present form[update] includes the following tendencies:'b"Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn\xe2\x80\x93Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice."b'List of functional analysis topics.'b'The closed graph theorem states the following: If X is a topological space and Y is a compact Hausdorff space, then the graph of a linear map T from X to Y is closed if and only if T is continuous.[3]'b'The proof uses the Baire category theorem, and completeness of both X and Y is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if X and Y are taken to be Fr\xc3\xa9chet spaces.'b'The open mapping theorem, also known as the Banach\xe2\x80\x93Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:[2]'b'then there exists a linear extension \xcf\x88\xc2\xa0: V \xe2\x86\x92 R of \xcf\x86 to the whole space V, i.e., there exists a linear functional \xcf\x88 such that'b'Hahn\xe2\x80\x93Banach theorem:[2] If p\xc2\xa0: V \xe2\x86\x92 R is a sublinear function, and \xcf\x86\xc2\xa0: U \xe2\x86\x92 R is a linear functional on a linear subspace U \xe2\x8a\x86 V which is dominated by p on U, i.e.'b'The Hahn\xe2\x80\x93Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are "enough" continuous linear functionals defined on every normed vector space to make the study of the dual space "interesting".'b'This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.'b'where T is the multiplication operator:'b'Theorem:[1] Let A be a bounded self-adjoint operator on a Hilbert space H. Then there is a measure space (X, \xce\xa3, \xce\xbc) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H \xe2\x86\x92 L2\xce\xbc(X) such that'b'There are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let A be the operator of multiplication by t on L2[0, 1], that is'b'The theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.'b'The uniform boundedness principle or Banach\xe2\x80\x93Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn\xe2\x80\x93Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.'b'Important results of functional analysis include:'b'Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fr\xc3\xa9chet derivative article.'b'In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.'b'General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.'b'An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.'b'More generally, functional analysis includes the study of Fr\xc3\xa9chet spaces and other topological vector spaces not endowed with a norm.'b'The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.'b''b''b'In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.'b"The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra.[citation needed] The theory of nonlinear functionals was continued by students of Hadamard, in particular Fr\xc3\xa9chet and L\xc3\xa9vy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach."b'Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.'
Engineering
b'In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term.'b'Business Engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or "Management engineering" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or Business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical & electronics, power distribution & generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.'b'Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[52][67]'b"The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[64] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[65] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[61][66]"b"There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).[61][62][63]"b'Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[57]'b'The heart for example functions much like a pump,[58] the skeleton is like a linked structure with levers,[59] the brain produces electrical signals etc.[60] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.'b'Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[57]'b'Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.'b'Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[55][56]'b"Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[53][54] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems."b'The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.'b'Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[citation needed]'b'As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:'b"An example of this is the use of numerical approximations to the Navier\xe2\x80\x93Stokes equations to describe aerodynamic flow over an aircraft, or the use of Miner's rule to calculate fatigue damage. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[citation needed]"b'There is a "real and important" difference between engineering and physics as similar to any science field has to do with technology.[42][43] Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology.[44][45][46] For technology, physics is an auxiliary and in a way technology is considered as applied physics.[47] Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer\'s job. A physicist would typically require additional and relevant training.[48] Physicists and engineers engage in different lines of work.[49] But PhD physicists who specialize in sectors of technology and applied science are titled as Technology officer, R&D Engineers and System Engineers.[50]'b'In the book What Engineers Know and How They Know It,[41] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.'b'Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely "engineering scientists".[citation needed]'b'There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]'b'In Canada, many engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.[37]'b'Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:'b'Engineering companies in many established economies are facing significant challenges with regard to the number of professional engineers being trained, compared with the number retiring. This problem is very prominent in the UK where engineering has a poor image and low status.[33] There are many negative economic and political issues that this can cause, as well as ethical issues.[34] It is widely agreed that the engineering profession faces an "image crisis",[35] rather than it being fundamentally an unattractive career. Much work is needed to avoid huge problems in the UK and other western economies.'b'All overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:'b'Engineering is a key driver of innovation and human development. Sub-Saharan Africa, in particular, has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid.[citation needed] The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[31]'b'Engineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.'b'By its very nature engineering has interconnections with society, culture and human behavior. Every product or construction used by modern society is influenced by engineering. The results of engineering activity influence changes to the environment, society and economies, and its application brings with it a responsibility and public safety.'b'The engineering profession engages in a wide range of activities, from large collaboration at the societal level, and also smaller individual projects. Almost all engineering projects are obligated to some sort of financing agency: a company, a set of investors, or a government. The few types of engineering that are minimally constrained by such issues are pro bono engineering and open-design engineering.'b'In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[30]'b'There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and AEC software for civil engineering.'b'These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[29]'b'One of the most widely used design tools in the profession is computer-aided design (CAD) software like CATIA, Autodesk Inventor, DSS SolidWorks or Pro Engineer which enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.'b'As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.'b'The study of failed products is known as forensic engineering and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure.'b'Engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. However, the greater the safety factor, the less efficient the design may be.[citation needed]'b'Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.'b'Usually, multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.'b'Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.'b'According to Billy Vaughn Koen, the "engineering method is the use of heuristics to cause the best change in a poorly understood situation within the available resources." Koen argues that the definition of what makes one an engineer should not be based on what he produces, but rather how he goes about it.[28]'b"A general methodology and epistemology of engineering can be inferred from the historical case studies and comments provided by Walter Vincenti.[27] Though Vincenti's case studies are from the domain of aeronautical engineering, his conclusions can be transferred into many other branches of engineering, too."b'Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.'b'If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The crucial and unique task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.'b'In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. More than ever, engineers are now required to have a proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their career.'b'One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.'b'New specialties sometimes combine with the traditional fields and form new branches \xe2\x80\x93 for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.'b'Beyond these "Big Four", a number of other branches are recognized, though many can be thought of as sub-disciplines of the four major branches, or as cross-curricular disciplines among multiple. Historically, naval engineering and mining engineering were major branches. Other engineering fields sometimes included as major branches[citation needed] are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical,[24] geological, textile, industrial, materials,[25] and nuclear engineering.[26] These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.'b'Mechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, and mechatronics.'b'Electrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as Broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, controls, and electronics.'b'Civil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings.[21][22] Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.[23]'b'Chemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.'b'Engineering is a broad discipline which is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches:[18][19][20] chemical engineering, civil engineering, electrical engineering, and mechanical engineering.'b'In 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage.'b'Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.'b'The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[17]'b'Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[16]'b"The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[4] Chemical engineering developed in the late nineteenth century.[4] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[4] The role of the chemical engineer was the design of these chemical plants and processes.[4]"b'There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[15]'b'The United States census of 1850 listed the occupation of "engineer" for the first time with a count of 2,000.[13] There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.[14]'b'John Smeaton was the first self-proclaimed civil engineer and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbours, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Smeaton designed the third Eddystone Lighthouse (1755\xe2\x80\x9359) where he pioneered the use of \'hydraulic lime\' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. His lighthouse remained in use until 1877 and was dismantled and partially rebuilt at Plymouth Hoe where it is known as Smeaton\'s Tower. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement.'b'The inventions of Thomas Newcomen and James Watt gave rise to modern mechanical engineering. The development of specialized machines and machine tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace Britain and abroad.[4]'b'With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.'b'The first steam engine was built in 1698 by Thomas Savery.[12] The development of this device gave rise to the Industrial Revolution in the coming decades, allowing for the beginnings of mass production.'b'Ancient Chinese, Greek, Roman and Hungarian armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century B.C.,[11] the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.'b"The earliest civil engineer known by name is Imhotep.[4] As one of the officials of the Pharaoh, Djos\xc3\xa8r, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630\xe2\x80\x932611 BC.[7] Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, the first known mechanical computer,[8][9] and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.[10]"b'The pyramids in Egypt, the Acropolis and the Parthenon in Greece, the Roman aqueducts, Via Appia and the Colosseum, Teotihuac\xc3\xa1n, the Great Wall of China, the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon, and the Pharos of Alexandria were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.'b'Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[4] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.'b'The word "engine" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning "innate quality, especially mental power, hence a clever invention."[6]'b'The term engineering is derived from the word engineer, which itself dates back to 1390 when an engine\'er (literally, one who operates an engine) referred to "a constructor of military engines."[5] In this context, now obsolete, an "engine" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.'b'Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley.'b'The American Engineers\' Council for Professional Development (ECPD, the predecessor of ABET)[2] has defined "engineering" as:'b''b''b'The term engineering is derived from the Latin ingenium, meaning "cleverness" and ingeniare, meaning "to contrive, devise".[1]'b'Engineering is the creative application of science, mathematical methods, and empirical evidence to the innovation, design, construction, and maintenance of structures, machines, materials, devices, systems, processes, and organizations. The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.'
Mathematical model
b'that can be written also as:'b'The language recognized by M is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where "*" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols "1".'b'The state S1 represents that there has been an even number of 0s in the input so far, while S2 signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, M will finish in state S1, an accepting state, so the input string will be accepted.'b'M = (Q, \xce\xa3, \xce\xb4, q0, F) where'b'An example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.[4]'b'Many types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.'b'As an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.'b'The question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.'b'Assessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a "typical" set of data.'b"While it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form."b'Defining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.'b"Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics."b'A crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.'b'Any model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by a neural network or other machine learning, the optimization of parameters is called training[citation needed][why?], while the optimization of model hyperparameters is called tuning and often uses cross-validation[citation needed]. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by curve fitting[citation needed].'b"For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only."b"In general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification[citation needed]."b'An example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.'b'Sometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.'b'In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification [3] can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.'b'Usually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.'b'Mathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.'b'For example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.'b"Objectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an index of performance, as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases."b'Decision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants. The variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).'b'In business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.'b'A mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.'b'Often when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.'b'Since prehistorical times simple models such as maps and diagrams have been used.'b'Different mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.'b"It is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schr\xc3\xb6dinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schr\xc3\xb6dinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis."b"Throughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement[citation needed][example needed]. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case."b'Mathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.'b'Mathematical models are usually composed of relationships and variables. Relationships can be described by operators, such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:'b'In the physical sciences, a traditional mathematical model contains most of the following elements:'b'Mathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.'b''b''b'A mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, Earth science, meteorology) and engineering disciplines (such as computer science, artificial intelligence), as well as in the social sciences (such as economics, psychology, sociology, political science). Physicists, engineers, statisticians, operations research analysts, and economists use mathematical models most extensively[citation needed]. A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.'
Nonlinear system
b"This corresponds to a free fall problem. A very useful qualitative picture of the pendulum's dynamics may be obtained by piecing together such linearizations, as seen in the figure at right. Other techniques may be used to find (exact) phase portraits and approximate periods."b'A classic, extensively studied nonlinear problem is the dynamics of a pendulum under the influence of gravity. Using Lagrangian mechanics, it may be shown[9] that the motion of a pendulum can be described by the dimensionless nonlinear equation'b'Other methods include examining the characteristics and using the methods outlined above for ordinary differential equations.'b'Another common (though less mathematic) tactic, often seen in fluid and heat mechanics, is to use scale analysis to simplify a general, natural equation in a certain specific boundary value problem. For example, the (very) nonlinear Navier-Stokes equations can be simplified into one linear partial differential equation in the case of transient, laminar, one dimensional flow in a circular pipe; the scale analysis provides conditions under which the flow is laminar and one dimensional and also yields the simplified equation.'b'The most common basic approach to studying nonlinear partial differential equations is to change the variables (or otherwise transform the problem) so that the resulting problem is simpler (possibly even linear). Sometimes, the equation may be transformed into one or more ordinary differential equations, as seen in separation of variables, which is always useful whether or not the resulting ordinary differential equation(s) is solvable.'b'Common methods for the qualitative analysis of nonlinear ordinary differential equations include:'b'Second and higher order ordinary differential equations (more generally, systems of nonlinear equations) rarely yield closed form solutions, though implicit solutions and solutions involving nonelementary integrals are encountered.'b'and the left-hand side of the equation is not a linear function of u and its derivatives. Note that if the u2 term were replaced with u, the problem would be linear (the exponential decay problem).'b'First order ordinary differential equations are often exactly solvable by separation of variables, especially for autonomous equations. For example, the nonlinear equation'b'One of the greatest difficulties of nonlinear problems is that it is not generally possible to combine known solutions into new solutions. In linear problems, for example, a family of linearly independent solutions can be used to construct general solutions through the superposition principle. A good example of this is one-dimensional heat transport with Dirichlet boundary conditions, the solution of which can be written as a time-dependent linear combination of sinusoids of differing frequencies; this makes solutions very flexible. It is often possible to find several very specific solutions to nonlinear equations, however the lack of a superposition principle prevents the construction of new solutions.'b'A system of differential equations is said to be nonlinear if it is not a linear system. Problems involving nonlinear differential equations are extremely diverse, and methods of solution or analysis are problem dependent. Examples of nonlinear differential equations are the Navier\xe2\x80\x93Stokes equations in fluid dynamics and the Lotka\xe2\x80\x93Volterra equations in biology.'b'A nonlinear recurrence relation defines successive terms of a sequence as a nonlinear function of preceding terms. Examples of nonlinear recurrence relations are the logistic map and the relations that define the various Hofstadter sequences. Nonlinear discrete models that represent a wide class of nonlinear recurrence relationships include the NARMAX (Nonlinear Autoregressive Moving Average with eXogenous inputs) model and the related nonlinear system identification and analysis procedures.[8] These approaches can be used to study a wide class of complex nonlinear behaviors in the time, frequency, and spatio-temporal domains.'b"For a single polynomial equation, root-finding algorithms can be used to find solutions to the equation (i.e., sets of values for the variables that satisfy the equation). However, systems of algebraic equations are more complicated; their study is one motivation for the field of algebraic geometry, a difficult branch of modern mathematics. It is even difficult to decide whether a given algebraic system has complex solutions (see Hilbert's Nullstellensatz). Nevertheless, in the case of the systems with a finite number of complex solutions, these systems of polynomial equations are now well understood and efficient methods exist for solving them.[7]"b'Nonlinear algebraic equations, which are also called polynomial equations, are defined by equating polynomials to zero. For example,'b'An equation written as'b'Additivity implies homogeneity for any rational \xce\xb1, and, for continuous functions, for any real \xce\xb1. For a complex \xce\xb1, homogeneity does not follow from additivity. For example, an antilinear map is additive but not homogeneous. The conditions of additivity and homogeneity are often combined in the superposition principle'b''b''b'Some authors use the term nonlinear science for the study of nonlinear systems. This is disputed by others:'b'As nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos[5] and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.'b'Typically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one. In other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.'b'In mathematics and physical sciences, a nonlinear system is a system in which the change of the output is not proportional to the change of the input.[1] Nonlinear problems are of interest to engineers, physicists,[2][3] mathematicians, and many other scientists because most systems are inherently nonlinear in nature.[4] Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems.'
Matrix (mathematics)
b'Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]'b'For example, a function \xce\xa6(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, \xe2\x88\x80ai: \xce\xa6(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:'b'Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910\xe2\x80\x931913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:'b'The word has been used in unusual ways by at least two authors of historical importance.'b'The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.'b'Many theorems were first established for small matrices only, for example the Cayley\xe2\x80\x93Hamilton theorem was proved for 2\xc3\x972 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4\xc3\x974 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss\xe2\x80\x93Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.'b'where \xce\xa0 denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"\xe2\x80\x94later called Jacobi determinants by Sylvester\xe2\x80\x94which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker\'s Vorlesungen \xc3\xbcber die Theorie der Determinanten[113] and Weierstrass\' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.'b'The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy \xe2\x88\x92 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomial'b'An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]'b"Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley\xe2\x80\x93Hamilton theorem.[103]"b'The term "matrix" (Latin for "womb", derived from mater\xe2\x80\x94mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:'b'Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th\xe2\x80\x932nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.'b"The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H \xc2\xb7 A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices."b'Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.'b"Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]"b"A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]"b'Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]'b'The first model of quantum mechanics (Heisenberg, 1925) represented the theory\'s operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]'b'Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo\xe2\x80\x93Kobayashi\xe2\x80\x93Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]'b'Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]'b'which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]'b'Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear function'b'Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]'b'The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]'b'Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]'b'If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]'b'Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn \xe2\x86\x92 Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]'b'The Hessian matrix of a differentiable function \xc6\x92: Rn \xe2\x86\x92 R consists of the second derivatives of \xc6\x92 with respect to the several coordinate directions, that is,[81]'b'The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.'b'Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree\xe2\x80\x93Fock method.'b'Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.'b'under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.'b'Complex numbers can be represented by particular real 2-by-2 matrices via'b'There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]'b'An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant\xc2\xa01, a fact that is often used as a part of the characterization of determinants.'b'In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.'b'If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.'b'If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V\xe2\x86\x92W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector\xc2\xa0v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A\xc2\xb7v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.'b'It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.'b'Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]'b'form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or \xe2\x88\x921. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.'b'Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the condition'b'A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.'b'More generally, the set of m\xc3\x97n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n\xc2\xa0=\xc2\xa0m composition of these maps is possible, and this gives rise to the matrix ring of n\xc3\x97n matrices representing the endomorphism ring of Rn.'b'In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]'b'Linear maps Rn \xe2\x86\x92 Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V \xe2\x86\x92 W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such that'b'Matrices do not always have all their entries in the same ring\xc2\xa0\xe2\x80\x93 or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.'b'More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]'b'This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.'b'Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.'b'and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]'b'The eigendecomposition or diagonalization expresses A as a product VDV\xe2\x88\x921, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues \xce\xbb1 to \xce\xbbn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via'b'The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV\xe2\x88\x97, where U and V are unitary matrices and D is a diagonal matrix.'b'There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.'b'Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]'b"may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]"b"An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)"b'In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]'b'Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.'b'To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.'b'Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]'b'The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn\xe2\x88\x92A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(\xce\xbb)\xc2\xa0=\xc2\xa00 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley\xe2\x80\x93Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.'b'are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number \xce\xbb is an eigenvalue of an n\xc3\x97n-matrix A if and only if A\xe2\x88\x92\xce\xbbIn is not invertible, which is equivalent to'b'A number \xce\xbb and a non-zero vector v satisfying'b"Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by \xe2\x88\x921.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]"b'The determinant of a product of square matrices equals the product of their determinants:'b'The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]'b'The determinant of 2-by-2 matrices is given by'b'The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.'b'Also, the trace of a matrix is equal to that of its transpose, that is,'b'This is immediate from the definition of matrix multiplication:'b'The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:'b'The complex analogue of an orthogonal matrix is a unitary matrix.'b'An orthogonal matrix A is necessarily invertible (with inverse A\xe2\x88\x921 = AT), unitary (A\xe2\x88\x921 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or \xe2\x88\x921. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.'b'where I is the identity matrix of size n.'b'which entails'b'An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:'b'Allowing as input two different vectors instead yields the bilinear form associated to A:'b'A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.'b'produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.'b'A symmetric n\xc3\x97n-matrix A is called positive-definite if for all nonzero vectors x\xc2\xa0\xe2\x88\x88\xc2\xa0Rn the associated quadratic form given by'b'where In is the n\xc3\x97n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A\xe2\x88\x921.'b'A square matrix A is called invertible or non-singular if there exists a matrix B such that'b'By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.'b'A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = \xe2\x88\x92AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A\xe2\x88\x97 = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.'b'A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.'b'It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:'b'The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,'b'If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.'b'A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.'b'The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank\xe2\x80\x93nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]'b'The last equality follows from the above-mentioned associativity of matrix multiplication.'b'Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g\xc2\xa0: Rm \xe2\x86\x92 Rk, then the composition g \xe2\x88\x98 f is represented by BA since'b'The following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.'b'For example, the 2\xc3\x972 matrix'b'Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn \xe2\x86\x92 Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn \xe2\x86\x92 Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.'b'where A\xe2\x88\x921 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.'b'Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writing'b'is equivalent to the system of linear equations'b'Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n\xc3\x971-matrix) of n variables x1, x2, ..., xn, and b is an m\xc3\x971-column vector, then the matrix equation'b'A principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]'b'The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]'b'A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:'b'These operations are used in a number of ways, including solving linear equations and finding matrix inverses.'b'There are three types of row operations:'b'Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.'b'whereas'b'that is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:'b'Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m \xe2\x89\xa0 k. Even if both products are defined, they need not be equal, that is, generally'b'where 1 \xe2\x89\xa4 i \xe2\x89\xa4 m and 1 \xe2\x89\xa4 j \xe2\x89\xa4 p.[13] For example, the underlined entry 2340 in the product is calculated as (2 \xc3\x97 1000) + (3 \xc3\x97 100) + (4 \xc3\x97 10) = 2340:'b'Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:'b'Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A\xc2\xa0+\xc2\xa0B\xc2\xa0=\xc2\xa0B\xc2\xa0+\xc2\xa0A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A\xc2\xa0+\xc2\xa0B)T\xc2\xa0=\xc2\xa0AT\xc2\xa0+\xc2\xa0BT. Finally, (AT)T\xc2\xa0=\xc2\xa0A.'b'There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]'b'An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,\xe2\x88\x97 refers to the ith row of A, and a\xe2\x88\x97,j refers to the jth column of A. The set of all m-by-n matrices is denoted \xf0\x9d\x95\x84(m, n).'b'Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-\xc3\x97-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 \xe2\x89\xa4 i \xe2\x89\xa4 m \xe2\x88\x92 1 and 0 \xe2\x89\xa4 j \xe2\x89\xa4 n \xe2\x88\x92 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.'b'In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m \xc3\x97 n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m \xc3\x97 n as a subscript. For instance, the matrix A above is 3 \xc3\x97 4 and can be defined as A = [i \xe2\x88\x92 j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i \xe2\x88\x92 j]3\xc3\x974.'b'Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i \xe2\x88\x92 j.'b'The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):'b'Matrices are commonly written in box brackets or parentheses:'b'Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.'b'The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m\xc2\xa0\xc3\x97\xc2\xa0n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3\xc2\xa0\xc3\x97\xc2\xa02 matrix.'b'The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.'b'A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:'b''b''b'A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.'b'Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.'b"The individual items in an m \xc3\x97 n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n \xc3\x97 Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors."b'In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 \xc3\x97 3 (read "two by three"), because there are two rows and three columns:'
Vector space
b'The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[105] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.'b'generalizing the homogeneous case b = 0 above.[104] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).'b'If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x \xe2\x88\x88 W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v \xe2\x88\x88 V. An important example is the space of solutions of a system of inhomogeneous linear equations'b'Roughly, affine spaces are vector spaces whose origins are not specified.[103] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the map'b'Modules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[101] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[102] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.'b'The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.'b'Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 \xc3\x97 R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[99] K-theory studies the isomorphism classes of all vector bundles over some topological space.[100] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.'b'such that for every x in X, the fiber \xcf\x80\xe2\x88\x921(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X \xc3\x97 V \xe2\x86\x92 X makes the product X \xc3\x97 V into a "trivial" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of \xcf\x80 to \xcf\x80\xe2\x88\x921(U) is isomorphic[nb 16] to the trivial bundle U \xc3\x97 V \xe2\x86\x92 U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X \xc3\x97 V). For example, the M\xc3\xb6bius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 \xc3\x97 R, because the latter is orientable whereas the former is not.[98]'b'A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[93] More precisely, a vector bundle over X is a topological space E equipped with a continuous map'b'Riemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[94] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[95][96] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[97]'b'The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[93]'b'The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[88] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[89] They in turn are applied in digital filters[90] and as a rapid multiplication algorithm for polynomials and large integers (Sch\xc3\xb6nhage\xe2\x80\x93Strassen algorithm).[91][92]'b'Fourier series are used to solve boundary value problems in partial differential equations.[83] In 1822, Fourier first used this technique to solve the heat equation.[84] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[85] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[86] The JPEG image format is an application of the closely related discrete cosine transform.[87]'b"In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[80] A complex-number form of Fourier series is also commonly used.[79] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[81] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[82]"b'The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[79]'b'Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][77] The underlying vector space is usually the Hilbert space L2(0, 2\xcf\x80), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[78] The Fourier expansion of an L2 function f is'b"When \xce\xa9 = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by \xce\xb4, which associates to a test function f its value at the p: \xce\xb4(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax\xe2\x80\x93Milgram theorem, a consequence of the Riesz representation theorem).[76]"b'A distribution (or generalized function) is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[75] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain \xce\xa9:'b'Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[73] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[74]'b'When a field, F is explicitly stated, a common term used is F-algebra.'b'The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product \xe2\x8a\x97, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 \xe2\x8a\x97 v2 and v2 \xe2\x8a\x97 v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 \xe2\x8a\x97 v2 = \xe2\x88\x92 v2 \xe2\x8a\x97 v1 yields the exterior algebra.[72]'b'The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[71] As a vector space, it is spanned by symbols, called simple tensors'b'Examples include the vector space of n-by-n matrices, with [x, y] = xy \xe2\x88\x92 yx, the commutator of two matrices, and R3, endowed with the cross product.'b'Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):'b'Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[69]'b'General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[68] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone\xe2\x80\x93Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.'b'The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[65] As an example from physics, the time-dependent Schr\xc3\xb6dinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[66] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[67]'b'By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[62] By the Stone\xe2\x80\x93Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[63] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram\xe2\x80\x93Schmidt process, it enables one to construct a basis of orthogonal vectors.[64] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.'b'Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[60] The Hilbert space L2(\xce\xa9), with inner product given by'b'Imposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[59]'b'there exists a function f(x) belonging to the vector space Lp(\xce\xa9) such that'b"The space of integrable functions on a given domain \xce\xa9 (for example an interval) satisfying |f|p < \xe2\x88\x9e, and equipped with this norm are called Lebesgue spaces, denoted Lp(\xce\xa9).[nb 10] These spaces are complete.[58] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < \xe2\x88\x9e, satisfying the condition"b'More generally than sequences of real numbers, functions f: \xce\xa9 \xe2\x86\x92 R are endowed with a norm that replaces the above sum by the Lebesgue integral'b'is finite. The topologies on the infinite-dimensional space \xe2\x84\x93\xe2\x80\x89p are inequivalent for different p. E.g. the sequence of vectors xn = (2\xe2\x88\x92n, 2\xe2\x88\x92n, ..., 2\xe2\x88\x92n, 0, 0, ...), i.e. the first 2n components are 2\xe2\x88\x92n, the following ones are 0, converges to the zero vector for p = \xe2\x88\x9e, but does not for p = 1:'b'Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[57] A first example is the vector space \xe2\x84\x93\xe2\x80\x89p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 \xe2\x89\xa4 p \xe2\x89\xa4 \xe2\x88\x9e) given by'b'From a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V \xe2\x86\x92 W, maps between topological vector spaces are required to be continuous.[55] In particular, the (topological) dual space V\xe2\x88\x97 consists of continuous functionals V \xe2\x86\x92 R (or to C). The fundamental Hahn\xe2\x80\x93Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[56]'b'Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study\xe2\x80\x94a key piece of functional analysis\xe2\x80\x94focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[54] The image at the right shows the equivalence of the 1-norm and \xe2\x88\x9e-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.'b'A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[52] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[53] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only if'b'denotes the limit of the corresponding finite partial sums of the sequence (fi)i\xe2\x88\x88N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.'b'In such topological vector spaces one can consider series of vectors. The infinite sum'b'Convergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[50][51] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.'b'In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:'b'Coordinate space Fn can be equipped with the standard dot product:'b'where f+ denotes the positive part of f and f\xe2\x88\x92 the negative part.[47]'b'A vector space may be given a partial order \xe2\x89\xa4, under which some vectors can be compared.[46] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functions'b'From the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question\xe2\x80\x94crucial to analysis\xe2\x80\x94whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.'b'These rules ensure that the map f from the V \xc3\x97 W to V \xe2\x8a\x97 W that maps a tuple (v, w) to v \xe2\x8a\x97 w is bilinear. The universality states that given any vector space X and any bilinear map g\xc2\xa0: V \xc3\x97 W \xe2\x86\x92 X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v \xe2\x8a\x97 w) = g(v, w).[45] This is called the universal property of the tensor product, an instance of the method\xe2\x80\x94much used in advanced abstract algebra\xe2\x80\x94to indirectly define objects by specifying maps from or to this object.'b'subject to the rules'b'The tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensors'b'The tensor product V \xe2\x8a\x97F W, or simply V \xe2\x8a\x97 W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g\xc2\xa0: V \xc3\x97 W \xe2\x86\x92 X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v \xe2\x86\xa6 g(v, w) is linear in the sense above and likewise for fixed v.'b'The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.'b'the derivatives of the function f appear linearly (as opposed to f\xe2\x80\xb2\xe2\x80\xb2(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)\xe2\x80\xb2 = f\xe2\x80\xb2 + g\xe2\x80\x89\xe2\x80\xb2 and (c\xc2\xb7f)\xe2\x80\xb2 = c\xc2\xb7f\xe2\x80\xb2 for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).'b'In the corresponding map'b'An important example is the kernel of a linear map x \xe2\x86\xa6 Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equations'b'and the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.'b'The kernel ker(f) of a linear map f\xc2\xa0: V \xe2\x86\x92 W consists of vectors v that are mapped to 0 in W.[40] Both kernel and image im(f) = {f(v)\xc2\xa0: v \xe2\x88\x88 V} are subspaces of V and W, respectively.[41] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[42] Because of this, many statements such as the first isomorphism theorem (also called rank\xe2\x80\x93nullity theorem in matrix-related terms)'b'The counterpart to subspaces are quotient vector spaces.[39] Given any subspace W \xe2\x8a\x82 V, the quotient space V/W ("V modulo W") is defined as follows: as a set, it consists of v + W = {v + w\xc2\xa0: w \xe2\x88\x88 W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a \xc2\xb7 (v + W) = (a \xc2\xb7 v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space "forgets" information that is contained in the subspace W.'b'A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n \xe2\x80\x93 1.'b'A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[37][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[38]'b'In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.'b'By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in \xce\xbb, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.'b'Endomorphisms, linear maps f\xc2\xa0: V \xe2\x86\x92 V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying \xce\xbbv = f(v), where \xce\xbb is a scalar, is called an eigenvector of f with eigenvalue \xce\xbb.[nb 5][35] Equivalently, v is an element of the kernel of the difference f \xe2\x88\x92 \xce\xbb \xc2\xb7 Id (where Id is the identity map V \xe2\x86\x92 V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue \xce\xbb is equivalent to'b'The determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.'b'Moreover, after choosing bases of V and W, any linear map f\xc2\xa0: V \xe2\x86\x92 W is uniquely represented by a matrix via this assignment.[33]'b'or, using the matrix multiplication of the matrix A with the coordinate vector x:'b'Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the following'b'Once a basis of V is chosen, linear maps f\xc2\xa0: V \xe2\x86\x92 W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no "canonical" or preferred isomorphism; actually an isomorphism \xcf\x86\xc2\xa0: Fn \xe2\x86\x92 V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via \xcf\x86. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.'b'Linear maps V \xe2\x86\x92 W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V\xe2\x88\x97.[28] Via the injective natural map V \xe2\x86\x92 V\xe2\x88\x97\xe2\x88\x97, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]'b'For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.'b'An isomorphism is a linear map f\xc2\xa0: V \xe2\x86\x92 W such that there exists an inverse map g\xc2\xa0: W \xe2\x86\x92 V, which is a map such that the two possible compositions f \xe2\x88\x98 g\xc2\xa0: W \xe2\x86\x92 W and g \xe2\x88\x98 f\xc2\xa0: V \xe2\x86\x92 V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.'b'The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure\xe2\x80\x94i.e., they preserve sums and scalar multiplication:'b''b'A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(\xce\xb1) over Q depends on \xce\xb1. If \xce\xb1 satisfies some polynomial equation 'b'The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e\xe2\x88\x92x and xe\xe2\x88\x92x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.'b"Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo\xe2\x80\x93Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]"b'The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.'b'For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:'b'where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.'b'Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i \xe2\x88\x88 I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:'b'yields f(x) = a\xe2\x80\x89e\xe2\x88\x92x + bx\xe2\x80\x89e\xe2\x88\x92x, where a and b are arbitrary constants, and ex is the natural exponential function.'b'are given by triples with arbitrary a, b = a/2, and c = \xe2\x88\x925a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namely'b'Systems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions of'b'and similarly for multiplication. Such function spaces occur in many geometric situations, when \xce\xa9 is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:'b'Functions from any fixed set \xce\xa9 to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given by'b'In fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.'b'The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c \xe2\x8b\x85 (x + iy) = (c \xe2\x8b\x85 x) + i(c \xe2\x8b\x85 y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.'b'A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.'b'The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such as'b'An important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.'b"The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations."b'Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by M\xc3\xb6bius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]'b'There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse \xe2\x88\x92v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.'b'In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]'b'Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]'b'In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.'b'When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.'b'Subtraction of two vectors and division by a (non-zero) scalar can be defined as'b'Likewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.'b'These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:'b'To qualify as a vector space, the set\xc2\xa0V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.'b'In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.'b'Elements of V are commonly called vectors. Elements of\xc2\xa0F are commonly called scalars.'b'A vector space over a field F is a set\xc2\xa0V together with two operations that satisfy the eight axioms listed below.'b'In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]'b'The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.'b'and'b'A second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:'b'The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (\xe2\x88\x921)v = \xe2\x88\x92v has the opposite direction and the same length as v (blue vector pointing down in the right image).'b'The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.'b'The concept of vector space will first be explained by describing two particular examples:'b''b''b'Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.'b"Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs."b'Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.'b'Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.'b'A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.'
Linear map
b'Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.'b'A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.'b'An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).'b'A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.'b'Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.'b'Therefore, the matrix in the new basis is A\xe2\x80\xb2 = B\xe2\x88\x921AB, being B the matrix of the given basis.'b'hence'b'Substituting this in the first expression'b"Given a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v']."b'Let V and W denote vector spaces over a field, F. Let T: V \xe2\x86\x92 W be a linear map.'b'No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.'b'The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \xe2\x86\x92 V \xe2\x86\x92 W \xe2\x86\x92 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\xe2\x80\x93Singer index theorem.[8]'b'For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \xe2\x88\x92 dim(W), by rank\xe2\x80\x93nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.'b'namely the degrees of freedom minus the number of constraints.'b'For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:'b'The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.'b'These can be interpreted thus: given a linear equation f(v) = w to solve,'b'This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequence'b'The number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \xcf\x81(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \xce\xbd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.'b'ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\xe2\x80\x93nullity theorem:'b'If f\xc2\xa0: V \xe2\x86\x92 W is linear, we define the kernel and the image or range of f by'b'If V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \xc3\x97 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \xc3\x97 n invertible matrices with entries in K.'b'An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).'b'A linear transformation f: V \xe2\x86\x92 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \xe2\x86\x92 V.'b'Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.'b'Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.'b'If f\xc2\xa0: V \xe2\x86\x92 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.'b'If f1\xc2\xa0: V \xe2\x86\x92 W and f2\xc2\xa0: V \xe2\x86\x92 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).'b'The inverse of a linear map, when defined, is again a linear map.'b'The composition of linear maps is linear: if f\xc2\xa0: V \xe2\x86\x92 W and g\xc2\xa0: W \xe2\x86\x92 Z are linear, then so is their composition g \xe2\x88\x98 f\xc2\xa0: V \xe2\x86\x92 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.'b'In two-dimensional space R2 linear maps are described by 2 \xc3\x97 2 real matrices. These are some examples:'b'The matrices of a linear transformation can be represented visually:'b'where M is the matrix of f. The symbol \xe2\x88\x97 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.'b'corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),'b'Thus, the function f is entirely determined by the values of aij. If we put these values into an m \xc3\x97 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vector'b'which implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) as'b'If f\xc2\xa0: V \xe2\x86\x92 W is a linear map,'b'Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:'b'If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \xc3\x97 n matrix, then f(x) = Ax describes a linear map Rn \xe2\x86\x92 Rm (see Euclidean space).'b'Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.'b''b''b'In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.'b'A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.'b'An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\xc2\xa0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.'b'In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \xe2\x86\x92 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.'
Mathematics
b'A famous list of 23 open problems, called "Hilbert\'s problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. A solution to each of these problems carries a $1\xc2\xa0million reward, and only one (the Riemann hypothesis) is duplicated in Hilbert\'s problems.'b'The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.'b'Arguably the most prestigious award in mathematics is the Fields Medal,[58][59] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.'b'Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.'b'Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[56] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[57]'b'Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments;[55] the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference\xc2\xa0\xe2\x80\x93 with model selection and estimation; the estimated models and consequential predictions should be tested on new data.[c]'b'In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.'b'Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.'b'Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.'b'The study of space originates with geometry\xc2\xa0\xe2\x80\x93 in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincar\xc3\xa9 conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.'b'By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.'b'Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.'b'As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.'b'The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat\'s Last Theorem. The twin prime conjecture and Goldbach\'s conjecture are two unsolved problems in number theory.'b'Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model\xc2\xa0\xe2\x80\x93 the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "P = NP?" problem, one of the Millennium Prize Problems.[54] Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.'b"Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to G\xc3\xb6del's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, G\xc3\xb6del showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science,[citation needed] as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem."b'In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.[53] Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor\'s set theory and the Brouwer\xe2\x80\x93Hilbert controversy.'b'Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.'b'Axioms in traditional thought were "self-evident truths", but that conception is problematic.[51] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert\'s program to put all of mathematics on a firm axiomatic basis, but according to G\xc3\xb6del\'s incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[52]'b'Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject.[b] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th\xc2\xa0century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.[50]'b'Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen\'s meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".'b'Most of the mathematical notation in use today was not invented until the 16th century.[45] Before that, mathematics was written out in words, limiting mathematical discovery.[46] Euler (1707\xe2\x80\x931783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.[47] Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.[48] Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.[49]'b'For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid\'s proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in A Mathematician\'s Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[42] Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erd\xc5\x91s.[43][44] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.'b'Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics".[40] As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46\xc2\xa0pages.[41] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.'b"Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[39]"b'The opinions of mathematicians on this matter are varied. Many mathematicians[38] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others[who?] feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.[citation needed]'b'An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.'b'Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper.[34] However, in the 1930s G\xc3\xb6del\'s incompleteness theorems convinced many mathematicians[who?] that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."[35] Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.[36][37]'b'The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences".[12] More recently, Marcus du Sautoy has called mathematics "the Queen of Science\xc2\xa0... the main driving force behind scientific discovery".[33] In the original Latin Regina Scientiarum, as well as in German K\xc3\xb6nigin der Wissenschaften, the word corresponding to science means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to natural science follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[15]'b'Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems".[32] A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.'b'Intuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other."[29] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.'b'An early definition of mathematics in terms of logic was Benjamin Peirce\'s "the science that draws necessary conclusions" (1870).[30] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell\'s "All Mathematics is Symbolic Logic" (1903).[31]'b'Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[29] All have severe problems, none has widespread acceptance, and no reconciliation seems possible.[29]'b'Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century.[27] Starting in the 19th\xc2\xa0century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[28] Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals.[6] There is not even consensus on whether mathematics is an art or a science.[7] A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[6] Some just say, "Mathematics is what mathematicians do."[6]'b'The apparent plural form in English, like the French plural form les math\xc3\xa9matiques (and the less commonly used singular derivative la math\xc3\xa9matique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural \xcf\x84\xce\xb1 \xce\xbc\xce\xb1\xce\xb8\xce\xb7\xce\xbc\xce\xb1\xcf\x84\xce\xb9\xce\xba\xce\xac (ta math\xc4\x93matik\xc3\xa1), used by Aristotle (384\xe2\x80\x93322\xc2\xa0BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.[25] In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in English-speaking North America, math.[26]'b'In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine\'s warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.[24]'b'Similarly, one of the two main schools of thought in Pythagoreanism was known as the math\xc4\x93matikoi (\xce\xbc\xce\xb1\xce\xb8\xce\xb7\xce\xbc\xce\xb1\xcf\x84\xce\xb9\xce\xba\xce\xbf\xce\xaf)\xe2\x80\x94which at the time meant "teachers" rather than "mathematicians" in the modern sense.'b'The word mathematics comes from Ancient Greek \xce\xbc\xce\xac\xce\xb8\xce\xb7\xce\xbc\xce\xb1 (m\xc3\xa1th\xc4\x93ma), meaning "that which is learnt",[22] "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times.[23] Its adjective is \xce\xbc\xce\xb1\xce\xb8\xce\xb7\xce\xbc\xce\xb1\xcf\x84\xce\xb9\xce\xba\xcf\x8c\xcf\x82 (math\xc4\x93matik\xc3\xb3s), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, \xce\xbc\xce\xb1\xce\xb8\xce\xb7\xce\xbc\xce\xb1\xcf\x84\xce\xb9\xce\xba\xe1\xbd\xb4 \xcf\x84\xce\xad\xcf\x87\xce\xbd\xce\xb7 (math\xc4\x93matik\xe1\xb8\x97 t\xc3\xa9khn\xc4\x93), Latin: ars mathematica, meant "the mathematical art".'b'Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January\xc2\xa02006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9\xc2\xa0million, and more than 75\xc2\xa0thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[21]'b'During the Golden Age of Islam, especially during the 9th and 10th\xc2\xa0centuries, mathematics saw many important innovations building on Greek mathematics: most of them include the contributions from Persian mathematicians such as Al-Khwarismi, Omar Khayyam and Sharaf al-D\xc4\xabn al-\xe1\xb9\xac\xc5\xabs\xc4\xab.'b'Between 600 and 300\xc2\xa0BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics.[20]'b'In Babylonian mathematics, elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus.[citation needed]'b'Evidence for more complex mathematics does not appear until around 3000\xc2\xa0BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[19] The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time.'b'As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time\xc2\xa0\xe2\x80\x93 days, seasons, years.[18]'b'The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals,[17] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.'b'Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.[16]'b'Galileo Galilei (1564\xe2\x80\x931642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth."[11] Carl Friedrich Gauss (1777\xe2\x80\x931855) referred to mathematics as "the Queen of the Sciences".[12] Benjamin Peirce (1809\xe2\x80\x931880) called mathematics "the science that draws necessary conclusions".[13] David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise."[14] Albert Einstein (1879\xe2\x80\x931955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[15]'b"Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858\xe2\x80\x931932), David Hilbert (1862\xe2\x80\x931943), and others on axiomatic systems in the late 19th\xc2\xa0century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[10]"b'Mathematicians seek out patterns[8][9] and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.'b'Mathematics (from Greek \xce\xbc\xce\xac\xce\xb8\xce\xb7\xce\xbc\xce\xb1 m\xc3\xa1th\xc4\x93ma, "knowledge, study, learning") is the study of such topics as quantity,[1] structure,[2] space,[1] and change.[3][4][5] It has no generally accepted definition.[6][7]'
Linear equation
b'If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n\xc2\xa0\xe2\x80\x93\xc2\xa01)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).'b'In other words, if ai \xe2\x89\xa0 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.'b'If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 \xe2\x89\xa0 0, and rewrite the equation'b'If all the coefficients are zero, then either b \xe2\x89\xa0 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.'b'where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.'b'A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewritten'b'An everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point\xe2\x80\x93slope form or slope\xe2\x80\x93intercept form.'b'where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.'b'and'b'A linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:'b'This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).'b'This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.'b'Ergo,'b'Thus,'b'One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.'b'In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.'b'and'b'These are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:'b'and'b'Since this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.'b'becomes:'b'Further, this representation extends to systems of linear equations.'b'one can rewrite the equation in matrix form:'b'Using the order of the standard form'b'where a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.'b'Using a determinant, one gets a determinant form, easy to remember:'b'Expanding the products and regrouping the terms leads to the general form:'b'Multiplying both sides of this equation by (x2\xc2\xa0\xe2\x88\x92\xc2\xa0x1) yields a form of the line generally referred to as the symmetric form:'b'where (x1,\xc2\xa0y1) and (x2,\xc2\xa0y2) are two points on the line with x2 \xe2\x89\xa0 x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2\xc2\xa0\xe2\x88\x92\xc2\xa0y1)/(x2\xc2\xa0\xe2\x88\x92\xc2\xa0x1).'b'The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y\xc2\xa0\xe2\x88\x92\xc2\xa0y1) is proportional to the difference in the x coordinate (that is, x\xc2\xa0\xe2\x88\x92\xc2\xa0x1). The proportionality constant is m (the slope of the line).'b'where m is the slope of the line and (x1,y1) is any point on the line.'b'where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.'b'where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.'b'where A and B are not both equal to zero. The equation is usually written so that A \xe2\x89\xa5 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is \xe2\x88\x92A/B. The general form is sometimes written as:'b'In the general (or standard[1]) form the linear equation is written as:'b'Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and \xce\xb8 are variables; other letters represent constants (fixed numbers).'b'Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.'b'where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.'b'A common form of a linear equation in the two variables x and y is'b'If a = 0, then, when b = 0 every number is a solution of the equation, but if b \xe2\x89\xa0 0 there are no solutions (and the equation is said to be inconsistent.)'b'If a \xe2\x89\xa0 0, there is a unique solution'b'A linear equation in one unknown x may always be rewritten'b''b''b'This article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.'b'Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a \xe2\x89\xa0 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.'b'Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.'b'A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a \xe2\x89\xa0 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).'